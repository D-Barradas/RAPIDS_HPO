{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive hyperparameter optimization\n",
    "This notebook shows off a model selection algorithm that is now a part of Dask-ML, Hyperband.\n",
    "\n",
    "## Problem\n",
    "Any machine learning model requires tuning many \"hyper-parameters\" for good performance, and is mentioned in the Scikit-Learn docs under \"[Tuning the hyper-parameters of an estimator]\".\n",
    "\n",
    "This notebook will walk through a case of \"image denoising\". Getting good performance out of this model requires tuning these parameters:\n",
    "\n",
    "``` python\n",
    "params = {\n",
    "    'module__init',\n",
    "    'module__activation',\n",
    "    'optimizer',\n",
    "    'batch_size',\n",
    "    'optimizer__lr',\n",
    "    'optimizer__weight_decay',\n",
    "}\n",
    "```\n",
    "\n",
    "These are incredibly important, and fairly basic. They answer the questions of\n",
    "\n",
    "* How is the network initialization?\n",
    "* How is the model optimized? What are some of the hyper-parameters for `optimizer`?\n",
    "\n",
    "[Tuning the hyper-parameters of an estimator]:http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook looks at a realistic use case of hyperparameter optimization. To do this, this notebook uses\n",
    "\n",
    "* a realistic deep learning model\n",
    "* a realistic set of hyperparameters\n",
    "\n",
    "There are many hyperparameter for any model or framework. These can be specific to the model, or be related to the optimization framework used to minimize the model.\n",
    "\n",
    "This notebook will show\n",
    "\n",
    "* the model input and output (noisy and clean images respectively)\n",
    "* the parameter space we are searching over\n",
    "* a newly developed hyperparameter optimization algorithm and it's integration\n",
    "* a comparison with 3 hyperparameter selection algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Reduce number of messages/warnings displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PYTHONPATH=../src:$PYTHONPATH\n",
    "# !echo $PYTHONPATH\n",
    "!cp -u ../src/noisy_mnist.py . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distributed\n",
    "from distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "s=socket.socket()\n",
    "s.bind((\"\", 0))\n",
    "port = s.getsockname()[1]\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = LocalCluster(n_workers=-1)\n",
    "# client = Client(cluster)\n",
    "\n",
    "# client = Client(f\"localhost:{port}\")\n",
    "# client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, wait\n",
    "from dask_cuda import LocalCUDACluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-7f2ae2bc-bf79-11ee-a939-ac1f6b685fd7</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_cuda.LocalCUDACluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:47347/status\" target=\"_blank\">http://127.0.0.1:47347/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCUDACluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">74b93b83</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:47347/status\" target=\"_blank\">http://127.0.0.1:47347/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 2\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 2\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 187.57 GiB\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "    <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
       "    <td style=\"text-align: left;\"><strong>Using processes:</strong> True</td>\n",
       "</tr>\n",
       "\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-395c01a1-de07-481d-816a-37fed5ac4c90</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://127.0.0.1:39291\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 2\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:47347/status\" target=\"_blank\">http://127.0.0.1:47347/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 2\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 187.57 GiB\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:39031\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:38903/status\" target=\"_blank\">http://127.0.0.1:38903/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 93.78 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:38973\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /tmp/dask-scratch-space/worker-5ch3gn4y\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>GPU: </strong>Quadro GV100\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>GPU memory: </strong> 31.74 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 1</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:36329\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:34711/status\" target=\"_blank\">http://127.0.0.1:34711/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 93.78 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:44959\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /tmp/dask-scratch-space/worker-yquaz_5k\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>GPU: </strong>Quadro GV100\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>GPU memory: </strong> 31.75 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:39291' processes=2 threads=2, memory=187.57 GiB>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = LocalCUDACluster(dashboard_address=f\"127.0.0.1:{port}\")\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def debug_loop():\n",
    "    # subprocess.call(\"pip install git+https://github.com/stsievert/dask-ml@hyperband-scale\".split(\" \"))\n",
    "    import dask_ml\n",
    "    return dask_ml.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 396 ms, sys: 75.4 ms, total: 471 ms\n",
      "Wall time: 450 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2023.3.24'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time debug_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:36329': '2023.3.24', 'tcp://127.0.0.1:39031': '2023.3.24'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.run(debug_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 17:11:51,491 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/worker.py\", line 1255, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/utils_comm.py\", line 455, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/utils_comm.py\", line 434, in retry\n",
      "    return await coro()\n",
      "  File \"/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/core.py\", line 1393, in send_recv_from_rpc\n",
      "    comm = await self.pool.connect(self.addr)\n",
      "  File \"/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/core.py\", line 1592, in connect\n",
      "    raise RuntimeError(\"ConnectionPool is closed\")\n",
      "RuntimeError: ConnectionPool is closed\n",
      "2024-01-30 17:11:51,498 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:36329', name: 1, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/tornado/ioloop.py\", line 921, in _run\n",
      "    await val\n",
      "  File \"/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/worker.py\", line 1255, in heartbeat\n",
      "    response = await retry_operation(\n",
      "  File \"/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/utils_comm.py\", line 455, in retry_operation\n",
      "    return await retry(\n",
      "  File \"/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/utils_comm.py\", line 434, in retry\n",
      "    return await coro()\n",
      "  File \"/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/core.py\", line 1393, in send_recv_from_rpc\n",
      "    comm = await self.pool.connect(self.addr)\n",
      "  File \"/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/core.py\", line 1592, in connect\n",
      "    raise RuntimeError(\"ConnectionPool is closed\")\n",
      "RuntimeError: ConnectionPool is closed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 17:11:51,927 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-01-30 17:11:52,029 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 126 ms, sys: 71.8 ms, total: 197 ms\n",
      "Wall time: 1.96 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-7f2ae2bc-bf79-11ee-a939-ac1f6b685fd7</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_cuda.LocalCUDACluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:47347/status\" target=\"_blank\">http://127.0.0.1:47347/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCUDACluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">74b93b83</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:47347/status\" target=\"_blank\">http://127.0.0.1:47347/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 2\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 2\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 187.57 GiB\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "    <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
       "    <td style=\"text-align: left;\"><strong>Using processes:</strong> True</td>\n",
       "</tr>\n",
       "\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-395c01a1-de07-481d-816a-37fed5ac4c90</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://127.0.0.1:39291\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 2\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:47347/status\" target=\"_blank\">http://127.0.0.1:47347/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 2\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 187.57 GiB\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:39149\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:34829/status\" target=\"_blank\">http://127.0.0.1:34829/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 93.78 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:38973\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /tmp/dask-scratch-space/worker-kcac229f\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>GPU: </strong>Quadro GV100\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>GPU memory: </strong> 31.74 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 1</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:41087\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:42205/status\" target=\"_blank\">http://127.0.0.1:42205/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 93.78 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:44959\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /tmp/dask-scratch-space/worker-74spmrwk\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>GPU: </strong>Quadro GV100\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>GPU memory: </strong> 31.75 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:39291' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.56 s, sys: 330 ms, total: 1.89 s\n",
      "Wall time: 3.76 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:39149': {'status': 'OK'},\n",
       " 'tcp://127.0.0.1:41087': {'status': 'OK'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %time client.upload_file('autoencoder.py')\n",
    "%time client.upload_file('../models/autoencoder.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask ml version : 2023.3.24\n"
     ]
    }
   ],
   "source": [
    "import dask_ml\n",
    "from dask_ml.model_selection import HyperbandSearchCV\n",
    "import dask_ml\n",
    "print (f'Dask ml version : {dask_ml.__version__}' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "See below for an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:01<00:00, 6893514.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 36409886.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 1729182.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 7715888.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barradd/Documents/GitHub/RAPIDS_HPO/notebooks/noisy_mnist.py:31: FutureWarning: `seed` is a deprecated argument name for `random_noise`. It will be removed in version 0.23. Please use `rng` instead.\n",
      "  img = random_noise(x, seed=seed, **noise)\n"
     ]
    }
   ],
   "source": [
    "import noisy_mnist\n",
    "chunk_size = 70_000 // 3\n",
    "_X, _y = noisy_mnist.dataset()\n",
    "_X = _X[:chunk_size * 3]\n",
    "_y = _y[:chunk_size * 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69999, 784), dtype('float32'), 0.0, 1.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_X.shape, _X.dtype, _X.min(), _X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69999, 784), dtype('float32'), 0.0, 1.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_y.shape, _y.dtype, _y.min(), _y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dask.array<array, shape=(69999, 784), dtype=float32, chunksize=(23333, 784), chunktype=numpy.ndarray>,\n",
       " dask.array<array, shape=(69999, 784), dtype=float32, chunksize=(23333, 784), chunktype=numpy.ndarray>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.array as da\n",
    "n, d = _X.shape\n",
    "X = da.from_array(_X, chunks=(n // 3, d))\n",
    "y = da.from_array(_y, chunks=n // 3)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x200 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "rng = check_random_state(42)\n",
    "cols = 8\n",
    "w = 1.0\n",
    "fig, axs = plt.subplots(figsize=(cols*w, 2*w), ncols=cols, nrows=2)\n",
    "for col, (upper, lower) in enumerate(zip(axs[0], axs[1])):\n",
    "    if col == 0:\n",
    "        upper.text(-28, 14, 'ground\\ntruth')\n",
    "        lower.text(-28, 14, 'input')\n",
    "    i = rng.choice(len(X))\n",
    "    noisy = X[i].reshape(28, 28)\n",
    "    clean = y[i].reshape(28, 28)\n",
    "    kwargs = {'cbar': False, 'xticklabels': False, 'yticklabels': False, 'cmap': 'gray_r'}\n",
    "    sns.heatmap(noisy, ax=lower, **kwargs)\n",
    "    sns.heatmap(clean, ax=upper, **kwargs)\n",
    "plt.savefig(\"../results/input-output.svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a deep learning library (PyTorch) for this model, at least through the scikit-learn interface for PyTorch, [skorch].\n",
    "\n",
    "[skorch]:https://github.com/dnouri/skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import Autoencoder, NegLossScore\n",
    "import torch\n",
    "# from sklearn.model_selection import ParameterSampler\n",
    "# import torch\n",
    "\n",
    "def trim_params(**kwargs):\n",
    "    if kwargs['optimizer'] != 'Adam':\n",
    "        kwargs.pop('optimizer__amsgrad', None)\n",
    "    if kwargs['optimizer'] == 'Adam':\n",
    "        kwargs.pop('optimizer__lr', None)\n",
    "    if kwargs['optimizer'] != 'SGD':\n",
    "        kwargs.pop('optimizer__nesterov', None)\n",
    "        kwargs.pop('optimizer__momentum', None)\n",
    "    kwargs['optimizer'] = getattr(torch.optim, kwargs['optimizer'])\n",
    "    return kwargs\n",
    "\n",
    "class TrimParams(NegLossScore):\n",
    "    def set_params(self, **kwargs):\n",
    "        kwargs = trim_params(**kwargs)\n",
    "        return super().set_params(**kwargs)\n",
    "\n",
    "model = TrimParams(\n",
    "    module=Autoencoder,\n",
    "    criterion=torch.nn.BCELoss,\n",
    "    warm_start=True,\n",
    "    train_split=None,\n",
    "    max_epochs=1,\n",
    "    callbacks=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't show it here; I'd rather concentrate on tuning hyperparameters. But briefly, it's a simple fully connected 3 hidden layer autoencoder with a latent dimension of 49."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "The parameters I am interested in tuning are\n",
    "\n",
    "* model\n",
    "    * initialization\n",
    "    * activation function\n",
    "    * weight decay (which is similar to $\\ell_2$ regularization)\n",
    "* optimizer\n",
    "    * which optimizer to use (e.g., Adam, SGD)\n",
    "    * batch size used to approximate gradient\n",
    "    * learning rate (but not for Adam)\n",
    "    * momentum for SGD\n",
    "    \n",
    "After looking at the results, I think I was too exploratory in my tuning of step size. I should have experimented with it more to determine a reasonable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "params = {\n",
    "    'module__init': ['xavier_uniform_',\n",
    "                     'xavier_normal_',\n",
    "                     'kaiming_uniform_',\n",
    "                     'kaiming_normal_',\n",
    "                    ],\n",
    "    'module__activation': ['ReLU', 'LeakyReLU', 'ELU', 'PReLU'],\n",
    "    'optimizer': [\"SGD\"] * 5 + [\"Adam\"] * 2,\n",
    "    'batch_size': [32, 64, 128, 256, 512],\n",
    "    'optimizer__lr': np.logspace(1, -1.5, num=1000),\n",
    "    'optimizer__weight_decay': [0]*200 + np.logspace(-5, -3, num=1000).tolist(),\n",
    "    'optimizer__nesterov': [True],\n",
    "    'optimizer__momentum': np.linspace(0, 1, num=1000),\n",
    "    'train_split': [None],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am testing `optimizer` to be `SGD` or `Adam` to test \"[The Marginal Value of Adaptive Gradient Methods in Machine Learning][marginal]\". From their abstract,\n",
    "\n",
    "> We observe that the solutions found by adaptive methods generalize worse (often sig- nificantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.\n",
    "\n",
    "Their experiments in Figure 1b show that non-adaptive methods (SGD and heavy ball) perform much better than adaptive methods.\n",
    "\n",
    "They have to do some tuning for this. **Can we replicate their result?**\n",
    "\n",
    "[marginal]:https://arxiv.org/pdf/1705.08292.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for debugging; ignore this cell\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import ParameterSampler\n",
    "# import dask.array as da\n",
    "# import numpy as np\n",
    "# model = SGDClassifier()\n",
    "# params = {'alpha': np.logspace(-7, 0, num=int(1e6))}\n",
    "\n",
    "# n, d = int(10e3), 700\n",
    "# _X, _y = make_classification(n_samples=n, n_features=d,\n",
    "#                              random_state=1)\n",
    "# X = da.from_array(_X, chunks=(n // 10, d))\n",
    "# y = da.from_array(_y, chunks=n // 10)\n",
    "# X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# import msgpack\n",
    "\n",
    "def fmt(obj):\n",
    "    if isinstance(obj, list):\n",
    "        return [fmt(v) for v in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: fmt(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import msgpack\n",
    "# from sklearn.externals import joblib\n",
    "import dask.array as da\n",
    "\n",
    "def save_search(search, today, prefix, X, y):\n",
    "    pre = f\"{today}-{prefix}-\"\n",
    "\n",
    "    with open(pre + \"test.npz\", \"wb\") as f:\n",
    "        y_hat = search.predict(X)\n",
    "        y_hat = y_hat.compute()\n",
    "        np.savez(f, X=X, y=y, y_hat=y_hat)\n",
    "    # skorch models aren't pickable\n",
    "    with open(pre + \"params.json\", \"w\") as f:\n",
    "        params = {k: fmt(v) for k, v in search.get_params().items() if \"estimator\" not in k and \"param_distribution\" not in k}\n",
    "        json.dump(params, f)\n",
    "    # with open(pre + \"best-model.joblib\", \"wb\") as f:\n",
    "    #     joblib.dump(search.best_estimator_, f)\n",
    "    with open(pre + \"best-params-and-score.json\", \"w\") as f:\n",
    "        json.dump({\"params\": search.best_params_, \"score\": search.best_score_}, f)\n",
    "\n",
    "    with open(pre + \"history.json\", 'w') as f:\n",
    "        json.dump(search.history_, f)\n",
    "\n",
    "    with open(pre + \"cv_results.json\", 'w') as f:\n",
    "        json.dump(fmt(search.cv_results_), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2024-01-30\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "print(\"Today's date:\", today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 209.35 MiB </td>\n",
       "                        <td> 69.78 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (69999, 784) </td>\n",
       "                        <td> (23333, 784) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 3 chunks in 1 graph layer </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float32 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"40\" x2=\"25\" y2=\"40\" />\n",
       "  <line x1=\"0\" y1=\"80\" x2=\"25\" y2=\"80\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.96349288680034,0.0 25.96349288680034,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.981746\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >784</text>\n",
       "  <text x=\"45.963493\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.963493,60.000000)\">69999</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<array, shape=(69999, 784), dtype=float32, chunksize=(23333, 784), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dask.array<concatenate, shape=(62997, 784), dtype=float32, chunksize=(20999, 784), chunktype=numpy.ndarray>,\n",
       " dask.array<concatenate, shape=(7002, 784), dtype=float32, chunksize=(2334, 784), chunktype=numpy.ndarray>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 188.41 MiB </td>\n",
       "                        <td> 62.80 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (62997, 784) </td>\n",
       "                        <td> (20999, 784) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 3 chunks in 12 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float32 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"76\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"26\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"40\" x2=\"26\" y2=\"40\" />\n",
       "  <line x1=\"0\" y1=\"80\" x2=\"26\" y2=\"80\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"26\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"26\" y1=\"0\" x2=\"26\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 26.490396879174003,0.0 26.490396879174003,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"13.245198\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >784</text>\n",
       "  <text x=\"46.490397\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,46.490397,60.000000)\">62997</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<concatenate, shape=(62997, 784), dtype=float32, chunksize=(20999, 784), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "max_iter = 243\n",
    "history = {}\n",
    "cv_results = {}\n",
    "searches = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import HyperbandSearchCV\n",
    "\n",
    "fit_params = {}\n",
    "if isinstance(model, SGDClassifier):\n",
    "    fit_params = {'classes': da.unique(y).compute()}\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperbandSearchCV(model, params, max_iter, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62997, 784)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 418.70 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 1, loss = 0.6946833729743958\n",
      "steps = 1, loss = 0.7212424874305725\n",
      "steps = 1, loss = 1.7361098527908325\n",
      "steps = 1, loss = 1.944223165512085\n",
      "steps = 1, loss = 2.0238990783691406\n",
      "steps = 1, loss = 1.9964500665664673\n",
      "steps = 1, loss = 2.0634427070617676\n",
      "steps = 1, loss = 1.9778709411621094\n",
      "steps = 1, loss = 3.272592306137085\n",
      "steps = 1, loss = 1.0939403772354126\n",
      "steps = 1, loss = 1.4523916244506836\n",
      "steps = 1, loss = 1.6960132122039795\n",
      "steps = 1, loss = 1.9392948150634766\n",
      "steps = 1, loss = 1.3592060804367065\n",
      "steps = 1, loss = 2.2068614959716797\n",
      "steps = 1, loss = 2.3391366004943848\n",
      "steps = 1, loss = 2.309786796569824\n",
      "steps = 1, loss = 2.2433576583862305\n",
      "steps = 1, loss = 2.0454459190368652\n",
      "steps = 1, loss = 1.9503538608551025\n",
      "steps = 1, loss = 1.955540418624878\n",
      "steps = 1, loss = 2.698648691177368\n",
      "steps = 1, loss = 49.631900787353516\n",
      "steps = 1, loss = 2.791550397872925\n",
      "steps = 1, loss = 2.0951931476593018\n",
      "steps = 1, loss = 49.993465423583984\n",
      "steps = 1, loss = 1.867989182472229\n",
      "steps = 1, loss = 2.465742826461792\n",
      "steps = 27, loss = 1.9554786682128906\n",
      "steps = 81, loss = 3.0168631076812744\n",
      "steps = 81, loss = 2.659181833267212\n",
      "steps = 243, loss = 3.149925947189331\n",
      "steps = 27, loss = 3.27449893951416\n",
      "steps = 81, loss = 3.0177178382873535\n",
      "steps = 243, loss = 2.734339475631714\n",
      "steps = 27, loss = 2.8032565116882324\n",
      "steps = 27, loss = 3.4514217376708984\n",
      "steps = 81, loss = 2.675143003463745\n",
      "steps = 243, loss = 4.848584175109863\n",
      "steps = 27, loss = 2.516695737838745\n",
      "steps = 243, loss = 3.504399061203003\n",
      "steps = 81, loss = 3.8407180309295654\n",
      "steps = 81, loss = 3.123645544052124\n",
      "steps = 27, loss = 3.144871473312378\n",
      "steps = 81, loss = 4.646537780761719\n",
      "steps = 243, loss = 2.9626007080078125\n",
      "steps = 27, loss = 3.5831544399261475\n",
      "steps = 27, loss = 3.7349395751953125\n",
      "steps = 27, loss = 49.98929977416992\n",
      "steps = 27, loss = 2.904343366622925\n",
      "steps = 27, loss = 4.08957576751709\n",
      "steps = 27, loss = 3.2696235179901123\n",
      "steps = 27, loss = 3.066301107406616\n",
      "steps = 27, loss = 2.8809587955474854\n",
      "steps = 1, loss = 1.8497782945632935\n",
      "steps = 1, loss = 0.7298284769058228\n",
      "steps = 1, loss = 2.5751607418060303\n",
      "steps = 1, loss = 2.4483444690704346\n",
      "steps = 1, loss = 2.4268622398376465\n",
      "steps = 1, loss = 2.36094331741333\n",
      "steps = 1, loss = 1.9561680555343628\n",
      "steps = 1, loss = 2.0638229846954346\n",
      "steps = 1, loss = 1.5838228464126587\n",
      "steps = 1, loss = 1.8328649997711182\n",
      "steps = 1, loss = 2.0591225624084473\n",
      "steps = 1, loss = 1.9433568716049194\n",
      "steps = 1, loss = 1.6348532438278198\n",
      "steps = 1, loss = 2.4918742179870605\n",
      "steps = 1, loss = 2.1852612495422363\n",
      "steps = 27, loss = 49.97959518432617\n",
      "steps = 1, loss = 2.4593536853790283\n",
      "steps = 1, loss = 1.9990249872207642\n",
      "steps = 1, loss = 2.059290885925293\n",
      "steps = 1, loss = 4.771451950073242\n",
      "steps = 1, loss = 0.9475560784339905\n",
      "steps = 1, loss = 2.893892765045166\n",
      "steps = 1, loss = 2.1253890991210938\n",
      "steps = 1, loss = 27.348464965820312\n",
      "steps = 1, loss = 2.2108941078186035\n",
      "steps = 1, loss = 0.8778223991394043\n",
      "steps = 1, loss = 2.1777801513671875\n",
      "steps = 1, loss = 0.838050901889801\n",
      "steps = 1, loss = 1.8000773191452026\n",
      "steps = 1, loss = 3.1920971870422363\n",
      "steps = 1, loss = 1.815700888633728\n",
      "steps = 1, loss = 1.895021677017212\n",
      "steps = 1, loss = 2.0266449451446533\n",
      "steps = 1, loss = 1.2607494592666626\n",
      "steps = 1, loss = 2.5225460529327393\n",
      "steps = 9, loss = 2.4860377311706543\n",
      "steps = 9, loss = 1.8783371448516846\n",
      "steps = 81, loss = 2.0837748050689697\n",
      "steps = 9, loss = 2.7133185863494873\n",
      "steps = 9, loss = 2.9897541999816895\n",
      "steps = 9, loss = 3.3535425662994385\n",
      "steps = 9, loss = 2.51391339302063\n",
      "steps = 9, loss = 2.4977335929870605\n",
      "steps = 9, loss = 2.3969528675079346\n",
      "steps = 9, loss = 2.2307631969451904\n",
      "steps = 9, loss = 2.175215244293213\n",
      "steps = 9, loss = 2.3948283195495605\n",
      "steps = 9, loss = 3.329537868499756\n",
      "steps = 9, loss = 2.822256565093994\n",
      "steps = 9, loss = 4.0694146156311035\n",
      "steps = 9, loss = 3.5262584686279297\n",
      "steps = 9, loss = 2.3915345668792725\n",
      "steps = 9, loss = 2.42592453956604\n",
      "steps = 9, loss = 2.9351701736450195\n",
      "steps = 9, loss = 1.9568288326263428\n",
      "steps = 9, loss = 3.3475868701934814\n",
      "steps = 81, loss = 2.735326051712036\n",
      "steps = 81, loss = 2.614759683609009\n",
      "steps = 243, loss = 2.6977317333221436\n",
      "steps = 9, loss = 3.009343147277832\n",
      "steps = 9, loss = 3.345435857772827\n",
      "steps = 9, loss = 2.2356226444244385\n",
      "steps = 9, loss = 1.9612854719161987\n",
      "steps = 9, loss = 2.484097719192505\n",
      "steps = 9, loss = 2.008427619934082\n",
      "steps = 9, loss = 2.8961069583892822\n",
      "steps = 81, loss = 3.3266825675964355\n",
      "steps = 9, loss = 2.275861978530884\n",
      "steps = 9, loss = 3.480783224105835\n",
      "steps = 9, loss = 3.6916189193725586\n",
      "steps = 81, loss = 2.8283159732818604\n",
      "steps = 9, loss = 2.1134862899780273\n",
      "steps = 9, loss = 10.014500617980957\n",
      "steps = 9, loss = 2.743400812149048\n",
      "steps = 9, loss = 2.0573360919952393\n",
      "steps = 27, loss = 2.1128523349761963\n",
      "steps = 27, loss = 2.702651023864746\n",
      "steps = 81, loss = 2.3295888900756836\n",
      "steps = 27, loss = 2.1901590824127197\n",
      "steps = 27, loss = 2.711163282394409\n",
      "steps = 27, loss = 2.1675784587860107\n",
      "steps = 27, loss = 2.2185444831848145\n",
      "steps = 27, loss = 2.661496162414551\n",
      "steps = 27, loss = 2.1806721687316895\n",
      "steps = 27, loss = 2.3265392780303955\n",
      "steps = 27, loss = 2.436894178390503\n",
      "steps = 27, loss = 2.0058579444885254\n",
      "steps = 81, loss = 2.7657504081726074\n",
      "steps = 81, loss = 2.211958885192871\n",
      "steps = 1, loss = 2.2360825538635254\n",
      "steps = 243, loss = 2.6667191982269287\n",
      "steps = 1, loss = 2.626086473464966\n",
      "steps = 1, loss = 1.9813319444656372\n",
      "steps = 1, loss = 49.99180603027344\n",
      "steps = 1, loss = 1.7478437423706055\n",
      "steps = 1, loss = 0.7236571311950684\n",
      "steps = 1, loss = 2.236499071121216\n",
      "steps = 1, loss = 50.002708435058594\n",
      "steps = 1, loss = 1.8118488788604736\n",
      "steps = 1, loss = 3.8907997608184814\n",
      "steps = 1, loss = 2.2628157138824463\n",
      "steps = 1, loss = 1.9929074048995972\n",
      "steps = 1, loss = 1.7405195236206055\n",
      "steps = 1, loss = 4.2451066970825195\n",
      "steps = 1, loss = 49.992515563964844\n",
      "steps = 1, loss = 2.059277296066284\n",
      "steps = 1, loss = 2.000474452972412\n",
      "steps = 1, loss = 1.700215458869934\n",
      "steps = 1, loss = 1.7617286443710327\n",
      "steps = 1, loss = 3.3481955528259277\n",
      "steps = 1, loss = 1.1062846183776855\n",
      "steps = 1, loss = 1.8950862884521484\n",
      "steps = 1, loss = 2.123525381088257\n",
      "steps = 1, loss = 2.7890257835388184\n",
      "steps = 1, loss = 1.8255444765090942\n",
      "steps = 1, loss = 1.8439648151397705\n",
      "steps = 1, loss = 2.6905736923217773\n",
      "steps = 1, loss = 50.00276184082031\n",
      "steps = 1, loss = 1.5819200277328491\n",
      "steps = 1, loss = 0.7429335117340088\n",
      "steps = 1, loss = 1.9710605144500732\n",
      "steps = 1, loss = 1.759274959564209\n",
      "steps = 1, loss = 3.1228365898132324\n",
      "steps = 1, loss = 1.972180724143982\n",
      "steps = 1, loss = 49.9970703125\n",
      "steps = 1, loss = 1.0127860307693481\n",
      "steps = 1, loss = 2.3282415866851807\n",
      "steps = 1, loss = 1.9869052171707153\n",
      "steps = 1, loss = 1.2186030149459839\n",
      "steps = 1, loss = 1.8319636583328247\n",
      "steps = 1, loss = 2.596653699874878\n",
      "steps = 1, loss = 1.8651522397994995\n",
      "steps = 1, loss = 0.7185750603675842\n",
      "steps = 1, loss = 1.424461007118225\n",
      "steps = 1, loss = 2.6154491901397705\n",
      "steps = 1, loss = 2.888587713241577\n",
      "steps = 1, loss = 2.4360978603363037\n",
      "steps = 1, loss = 1.9482192993164062\n",
      "steps = 81, loss = 2.156916379928589\n",
      "steps = 1, loss = 2.5727882385253906\n",
      "steps = 1, loss = 2.0652916431427\n",
      "steps = 1, loss = 2.252023458480835\n",
      "steps = 1, loss = 50.01771926879883\n",
      "steps = 1, loss = 0.9560874104499817\n",
      "steps = 1, loss = 1.9605448246002197\n",
      "steps = 1, loss = 2.1818182468414307\n",
      "steps = 1, loss = 4.497021198272705\n",
      "steps = 1, loss = 1.7943971157073975\n",
      "steps = 1, loss = 1.779590368270874\n",
      "steps = 1, loss = 2.3714346885681152\n",
      "steps = 1, loss = 43.807682037353516\n",
      "steps = 1, loss = 2.3154289722442627\n",
      "steps = 1, loss = 2.462963581085205\n",
      "steps = 1, loss = 3.411799430847168\n",
      "steps = 1, loss = 15.143275260925293\n",
      "steps = 1, loss = 1.9714611768722534\n",
      "steps = 1, loss = 2.6076455116271973\n",
      "steps = 243, loss = 5.279587745666504\n",
      "steps = 1, loss = 3.1142406463623047\n",
      "steps = 1, loss = 1.9099321365356445\n",
      "steps = 1, loss = 2.4547293186187744\n",
      "steps = 1, loss = 1.991396188735962\n",
      "steps = 1, loss = 2.730315923690796\n",
      "steps = 1, loss = 1.8007487058639526\n",
      "steps = 1, loss = 2.1678292751312256\n",
      "steps = 1, loss = 2.172870397567749\n",
      "steps = 1, loss = 3.0149991512298584\n",
      "steps = 1, loss = 2.116971731185913\n",
      "steps = 1, loss = 2.055044651031494\n",
      "steps = 1, loss = 2.324657440185547\n",
      "steps = 1, loss = 2.2785818576812744\n",
      "steps = 1, loss = 2.4261302947998047\n",
      "steps = 1, loss = 2.0889439582824707\n",
      "steps = 3, loss = 49.993797302246094\n",
      "steps = 3, loss = 1.8765860795974731\n",
      "steps = 3, loss = 2.019651412963867\n",
      "steps = 3, loss = 2.5727663040161133\n",
      "steps = 3, loss = 6.677986145019531\n",
      "steps = 3, loss = 2.363363742828369\n",
      "steps = 3, loss = 2.277801752090454\n",
      "steps = 3, loss = 3.754833936691284\n",
      "steps = 3, loss = 3.252232551574707\n",
      "steps = 3, loss = 49.98957824707031\n",
      "steps = 3, loss = 50.01152420043945\n",
      "steps = 3, loss = 3.354496717453003\n",
      "steps = 3, loss = 2.6104793548583984\n",
      "steps = 3, loss = 2.268214702606201\n",
      "steps = 3, loss = 0.8998361229896545\n",
      "steps = 3, loss = 2.0046510696411133\n",
      "steps = 3, loss = 0.7712525725364685\n",
      "steps = 3, loss = 1.7427341938018799\n",
      "steps = 3, loss = 2.2661614418029785\n",
      "steps = 3, loss = 6.456460952758789\n",
      "steps = 3, loss = 5.3459858894348145\n",
      "steps = 3, loss = 49.99580001831055\n",
      "steps = 3, loss = 1.975294589996338\n",
      "steps = 3, loss = 2.339052438735962\n",
      "steps = 3, loss = 2.496412515640259\n",
      "steps = 3, loss = 1.9839504957199097\n",
      "steps = 3, loss = 2.6260735988616943\n",
      "steps = 3, loss = 2.0790224075317383\n",
      "steps = 3, loss = 2.127328634262085\n",
      "steps = 3, loss = 2.710495710372925\n",
      "steps = 3, loss = 1.7545169591903687\n",
      "steps = 3, loss = 2.7002553939819336\n",
      "steps = 3, loss = 49.989803314208984\n",
      "steps = 3, loss = 2.477513074874878\n",
      "steps = 3, loss = 7.862441539764404\n",
      "steps = 3, loss = 2.432600498199463\n",
      "steps = 3, loss = 2.0826191902160645\n",
      "steps = 3, loss = 2.0846664905548096\n",
      "steps = 3, loss = 2.0543065071105957\n",
      "steps = 3, loss = 2.4420559406280518\n",
      "steps = 3, loss = 4.5664215087890625\n",
      "steps = 3, loss = 3.2836952209472656\n",
      "steps = 3, loss = 1.9503906965255737\n",
      "steps = 3, loss = 3.3138985633850098\n",
      "steps = 3, loss = 2.078403949737549\n",
      "steps = 3, loss = 2.88492488861084\n",
      "steps = 3, loss = 2.2425663471221924\n",
      "steps = 3, loss = 2.4354593753814697\n",
      "steps = 3, loss = 3.107562780380249\n",
      "steps = 3, loss = 1.9664794206619263\n",
      "steps = 3, loss = 2.111635208129883\n",
      "steps = 3, loss = 2.4646527767181396\n",
      "steps = 3, loss = 2.621615171432495\n",
      "steps = 3, loss = 2.5352067947387695\n",
      "steps = 3, loss = 3.3682241439819336\n",
      "steps = 3, loss = 2.0846729278564453\n",
      "steps = 3, loss = 2.007955551147461\n",
      "steps = 3, loss = 2.4389586448669434\n",
      "steps = 3, loss = 2.4388723373413086\n",
      "steps = 3, loss = 2.719637155532837\n",
      "steps = 3, loss = 2.468817710876465\n",
      "steps = 3, loss = 49.9999885559082\n",
      "steps = 3, loss = 1.9300771951675415\n",
      "steps = 3, loss = 1.4975647926330566\n",
      "steps = 3, loss = 2.311025619506836\n",
      "steps = 3, loss = 2.336930751800537\n",
      "steps = 3, loss = 2.380037546157837\n",
      "steps = 3, loss = 3.3171024322509766\n",
      "steps = 3, loss = 3.1969826221466064\n",
      "steps = 3, loss = 2.4091100692749023\n",
      "steps = 3, loss = 50.0024299621582\n",
      "steps = 3, loss = 1.8232616186141968\n",
      "steps = 3, loss = 2.195385694503784\n",
      "steps = 3, loss = 2.5795958042144775\n",
      "steps = 3, loss = 2.3739585876464844\n",
      "steps = 3, loss = 1.8664931058883667\n",
      "steps = 3, loss = 2.429481267929077\n",
      "steps = 3, loss = 2.9440438747406006\n",
      "steps = 3, loss = 2.0216774940490723\n",
      "steps = 3, loss = 3.1475272178649902\n",
      "steps = 3, loss = 2.167614698410034\n",
      "steps = 9, loss = 2.0510823726654053\n",
      "steps = 9, loss = 2.075896739959717\n",
      "steps = 9, loss = 2.237671375274658\n",
      "steps = 9, loss = 2.0702133178710938\n",
      "steps = 9, loss = 2.7599685192108154\n",
      "steps = 9, loss = 2.6757798194885254\n",
      "steps = 9, loss = 2.2993416786193848\n",
      "steps = 9, loss = 1.9088267087936401\n",
      "steps = 9, loss = 2.212925672531128\n",
      "steps = 9, loss = 2.068829298019409\n",
      "steps = 9, loss = 2.5173776149749756\n",
      "steps = 9, loss = 1.7163634300231934\n",
      "steps = 9, loss = 2.176572561264038\n",
      "steps = 9, loss = 2.5000839233398438\n",
      "steps = 9, loss = 2.2232730388641357\n",
      "steps = 9, loss = 2.0115036964416504\n",
      "steps = 9, loss = 2.3460073471069336\n",
      "steps = 9, loss = 2.5091872215270996\n",
      "steps = 9, loss = 1.9115877151489258\n",
      "steps = 9, loss = 2.3167383670806885\n",
      "steps = 9, loss = 1.4744514226913452\n",
      "steps = 9, loss = 2.2718586921691895\n",
      "steps = 9, loss = 2.2754688262939453\n",
      "steps = 9, loss = 1.8127968311309814\n",
      "steps = 243, loss = 2.2286648750305176\n",
      "steps = 9, loss = 2.2893054485321045\n",
      "steps = 9, loss = 2.1051502227783203\n",
      "steps = 9, loss = 2.4690403938293457\n",
      "steps = 27, loss = 2.6549072265625\n",
      "steps = 27, loss = 2.1139907836914062\n",
      "steps = 27, loss = 2.271340847015381\n",
      "steps = 27, loss = 2.0790750980377197\n",
      "steps = 27, loss = 2.240217924118042\n",
      "steps = 27, loss = 2.4236505031585693\n",
      "steps = 27, loss = 1.90005362033844\n",
      "steps = 27, loss = 2.023689031600952\n",
      "steps = 27, loss = 1.9636955261230469\n",
      "steps = 81, loss = 2.1803252696990967\n",
      "steps = 81, loss = 2.483400821685791\n",
      "steps = 81, loss = 1.9593873023986816\n",
      "steps = 243, loss = 1.9581215381622314\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HyperbandSearchCV(estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                  max_iter=243,\n",
       "                  parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                              &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;, &#x27;ELU&#x27;,\n",
       "                                                     &#x27;PReLU&#x27;],\n",
       "                              &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                               &#x27;xavier_normal_&#x27;,\n",
       "                                               &#x27;kaiming_uniform_&#x27;,\n",
       "                                               &#x27;kaiming_normal_&#x27;],\n",
       "                              &#x27;optimizer&#x27;: [&#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;S...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                              &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                              &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, ...],\n",
       "                              &#x27;train_split&#x27;: [None]},\n",
       "                  random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;HyperbandSearchCV<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>HyperbandSearchCV(estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                  max_iter=243,\n",
       "                  parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                              &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;, &#x27;ELU&#x27;,\n",
       "                                                     &#x27;PReLU&#x27;],\n",
       "                              &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                               &#x27;xavier_normal_&#x27;,\n",
       "                                               &#x27;kaiming_uniform_&#x27;,\n",
       "                                               &#x27;kaiming_normal_&#x27;],\n",
       "                              &#x27;optimizer&#x27;: [&#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;S...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                              &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                              &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, ...],\n",
       "                              &#x27;train_split&#x27;: [None]},\n",
       "                  random_state=42)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HyperbandSearchCV(estimator=<class '__main__.TrimParams'>[uninitialized](\n",
       "  module=<class 'autoencoder.Autoencoder'>,\n",
       "),\n",
       "                  max_iter=243,\n",
       "                  parameters={'batch_size': [32, 64, 128, 256, 512],\n",
       "                              'module__activation': ['ReLU', 'LeakyReLU', 'ELU',\n",
       "                                                     'PReLU'],\n",
       "                              'module__init': ['xavier_uniform_',\n",
       "                                               'xavier_normal_',\n",
       "                                               'kaiming_uniform_',\n",
       "                                               'kaiming_normal_'],\n",
       "                              'optimizer': ['SGD', 'SGD', 'SGD', 'S...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                              'optimizer__nesterov': [True],\n",
       "                              'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, ...],\n",
       "                              'train_split': [None]},\n",
       "                  random_state=42)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_stats = timing_stats = client.profile(filename=\"hyperband.html\")\n",
    "with open(\"hyperband-timing.json\", \"w\") as f:\n",
    "    json.dump(timing_stats[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class '__main__.TrimParams'>[initialized](\n",
       "  module_=Autoencoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=784, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (2): Linear(in_features=784, out_features=196, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=196, out_features=784, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "      (3): Sigmoid()\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9581215381622314"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_split': None,\n",
       " 'optimizer__weight_decay': 0.0007760503335133571,\n",
       " 'optimizer__nesterov': True,\n",
       " 'optimizer__momentum': 0.7387387387387387,\n",
       " 'optimizer__lr': 2.209756114795902,\n",
       " 'optimizer': 'SGD',\n",
       " 'module__init': 'kaiming_uniform_',\n",
       " 'module__activation': 'LeakyReLU',\n",
       " 'batch_size': 32}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 209.35 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 38.43 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "save_search(search, today, \"hyperband\", X_test.compute(), y_test.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperband + SOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_patience = HyperbandSearchCV(model, params, max_iter, random_state=42, patience=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 418.70 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 1, loss = 2.3814570903778076\n",
      "steps = 1, loss = 1.9917110204696655\n",
      "steps = 1, loss = 2.2629213333129883\n",
      "steps = 1, loss = 1.6681660413742065\n",
      "steps = 1, loss = 2.405320167541504\n",
      "steps = 1, loss = 1.6678996086120605\n",
      "steps = 1, loss = 3.259756326675415\n",
      "steps = 1, loss = 50.00611114501953\n",
      "steps = 1, loss = 1.9744946956634521\n",
      "steps = 1, loss = 2.0408871173858643\n",
      "steps = 1, loss = 1.8700892925262451\n",
      "steps = 1, loss = 0.7225216627120972\n",
      "steps = 1, loss = 1.0191717147827148\n",
      "steps = 81, loss = 2.345111846923828\n",
      "steps = 81, loss = 2.6572659015655518\n",
      "steps = 81, loss = 4.885718822479248\n",
      "steps = 81, loss = 2.5955238342285156\n",
      "steps = 81, loss = 2.601898670196533\n",
      "steps = 81, loss = 3.9250543117523193\n",
      "steps = 82, loss = 2.714742422103882\n",
      "steps = 82, loss = 3.5274012088775635\n",
      "steps = 81, loss = 50.003150939941406\n",
      "steps = 82, loss = 3.0092742443084717\n",
      "steps = 82, loss = 2.574558734893799\n",
      "steps = 1, loss = 2.469334363937378\n",
      "steps = 1, loss = 1.4435535669326782\n",
      "steps = 81, loss = 3.0941779613494873\n",
      "steps = 1, loss = 1.9296462535858154\n",
      "steps = 1, loss = 0.6948944926261902\n",
      "steps = 1, loss = 1.312110424041748\n",
      "steps = 1, loss = 2.071232318878174\n",
      "steps = 1, loss = 2.878631353378296\n",
      "steps = 1, loss = 50.00027847290039\n",
      "steps = 1, loss = 2.075448751449585\n",
      "steps = 1, loss = 1.8597559928894043\n",
      "steps = 1, loss = 49.95998764038086\n",
      "steps = 82, loss = 3.628190040588379\n",
      "steps = 1, loss = 2.272860288619995\n",
      "steps = 1, loss = 49.99158477783203\n",
      "steps = 1, loss = 2.050224542617798\n",
      "steps = 1, loss = 1.955244541168213\n",
      "steps = 27, loss = 3.259399175643921\n",
      "steps = 27, loss = 2.8059637546539307\n",
      "steps = 27, loss = 3.548191547393799\n",
      "steps = 27, loss = 4.1513776779174805\n",
      "steps = 27, loss = 3.6225788593292236\n",
      "steps = 27, loss = 1.9565472602844238\n",
      "steps = 27, loss = 3.3757479190826416\n",
      "steps = 27, loss = 49.6789436340332\n",
      "steps = 27, loss = 2.674272298812866\n",
      "steps = 27, loss = 49.99657440185547\n",
      "steps = 27, loss = 2.891258955001831\n",
      "steps = 27, loss = 49.99158477783203\n",
      "steps = 27, loss = 3.1555936336517334\n",
      "steps = 1, loss = 2.6707444190979004\n",
      "steps = 1, loss = 1.7159487009048462\n",
      "steps = 27, loss = 3.6821348667144775\n",
      "steps = 1, loss = 2.054856777191162\n",
      "steps = 1, loss = 2.1429948806762695\n",
      "steps = 1, loss = 1.9431180953979492\n",
      "steps = 1, loss = 3.3576812744140625\n",
      "steps = 1, loss = 0.9915421605110168\n",
      "steps = 1, loss = 1.5876954793930054\n",
      "steps = 1, loss = 2.0697035789489746\n",
      "steps = 1, loss = 1.9558707475662231\n",
      "steps = 1, loss = 49.979949951171875\n",
      "steps = 1, loss = 2.3066611289978027\n",
      "steps = 1, loss = 2.1827304363250732\n",
      "steps = 1, loss = 1.8142869472503662\n",
      "steps = 1, loss = 1.863558053970337\n",
      "steps = 1, loss = 2.3693246841430664\n",
      "steps = 1, loss = 0.8231567740440369\n",
      "steps = 1, loss = 1.8966315984725952\n",
      "steps = 1, loss = 4.2258782386779785\n",
      "steps = 1, loss = 2.107987642288208\n",
      "steps = 1, loss = 2.396658420562744\n",
      "steps = 1, loss = 1.964125156402588\n",
      "steps = 1, loss = 2.5117733478546143\n",
      "steps = 1, loss = 2.5955495834350586\n",
      "steps = 1, loss = 2.0181548595428467\n",
      "steps = 1, loss = 49.90269088745117\n",
      "steps = 1, loss = 1.8584415912628174\n",
      "steps = 27, loss = 2.89794921875\n",
      "steps = 1, loss = 2.544104814529419\n",
      "steps = 1, loss = 0.7339137196540833\n",
      "steps = 1, loss = 2.4935359954833984\n",
      "steps = 1, loss = 1.2696796655654907\n",
      "steps = 1, loss = 2.0206847190856934\n",
      "steps = 1, loss = 49.417057037353516\n",
      "steps = 1, loss = 0.9859349727630615\n",
      "steps = 9, loss = 2.77457332611084\n",
      "steps = 9, loss = 2.5083537101745605\n",
      "steps = 9, loss = 2.921797752380371\n",
      "steps = 9, loss = 1.9380292892456055\n",
      "steps = 9, loss = 2.410385847091675\n",
      "steps = 9, loss = 2.9670002460479736\n",
      "steps = 9, loss = 3.3876149654388428\n",
      "steps = 9, loss = 50.00181198120117\n",
      "steps = 81, loss = 2.200026750564575\n",
      "steps = 9, loss = 3.3971776962280273\n",
      "steps = 9, loss = 2.5042831897735596\n",
      "steps = 9, loss = 3.6318986415863037\n",
      "steps = 9, loss = 2.895292282104492\n",
      "steps = 9, loss = 3.594736099243164\n",
      "steps = 9, loss = 1.898730993270874\n",
      "steps = 9, loss = 3.249774932861328\n",
      "steps = 9, loss = 2.1279561519622803\n",
      "steps = 9, loss = 3.4271841049194336\n",
      "steps = 9, loss = 50.00640869140625\n",
      "steps = 9, loss = 1.9716264009475708\n",
      "steps = 81, loss = 2.3042047023773193\n",
      "steps = 81, loss = 3.3102939128875732\n",
      "steps = 9, loss = 3.5495615005493164\n",
      "steps = 9, loss = 2.505690336227417\n",
      "steps = 9, loss = 2.144318103790283\n",
      "steps = 9, loss = 2.8100476264953613\n",
      "steps = 9, loss = 2.2016983032226562\n",
      "steps = 9, loss = 2.711458683013916\n",
      "steps = 9, loss = 2.2182986736297607\n",
      "steps = 9, loss = 2.340052843093872\n",
      "steps = 9, loss = 2.241576910018921\n",
      "steps = 9, loss = 2.4413723945617676\n",
      "steps = 9, loss = 1.9758964776992798\n",
      "steps = 9, loss = 8.770791053771973\n",
      "steps = 9, loss = 2.134320020675659\n",
      "steps = 9, loss = 2.0194313526153564\n",
      "steps = 9, loss = 2.4582982063293457\n",
      "steps = 27, loss = 2.1167843341827393\n",
      "steps = 27, loss = 2.3995521068573\n",
      "steps = 27, loss = 2.226844072341919\n",
      "steps = 27, loss = 2.1845669746398926\n",
      "steps = 27, loss = 2.6414644718170166\n",
      "steps = 27, loss = 2.0755012035369873\n",
      "steps = 81, loss = 2.7184414863586426\n",
      "steps = 81, loss = 2.829571485519409\n",
      "steps = 27, loss = 2.3166756629943848\n",
      "steps = 27, loss = 2.4936397075653076\n",
      "steps = 27, loss = 2.6729817390441895\n",
      "steps = 1, loss = 2.3031349182128906\n",
      "steps = 27, loss = 2.1906402111053467\n",
      "steps = 1, loss = 1.8351449966430664\n",
      "steps = 1, loss = 49.97838592529297\n",
      "steps = 1, loss = 2.2062172889709473\n",
      "steps = 1, loss = 1.750249981880188\n",
      "steps = 1, loss = 2.0881052017211914\n",
      "steps = 1, loss = 4.141885757446289\n",
      "steps = 27, loss = 2.223466634750366\n",
      "steps = 1, loss = 2.2711119651794434\n",
      "steps = 1, loss = 2.237309217453003\n",
      "steps = 1, loss = 1.9727119207382202\n",
      "steps = 1, loss = 50.002197265625\n",
      "steps = 1, loss = 3.4899682998657227\n",
      "steps = 1, loss = 2.204357385635376\n",
      "steps = 1, loss = 1.8178682327270508\n",
      "steps = 1, loss = 2.757283926010132\n",
      "steps = 1, loss = 2.696256399154663\n",
      "steps = 1, loss = 1.7779111862182617\n",
      "steps = 1, loss = 4.897146701812744\n",
      "steps = 1, loss = 50.01214599609375\n",
      "steps = 1, loss = 2.4810266494750977\n",
      "steps = 1, loss = 1.8385074138641357\n",
      "steps = 1, loss = 1.8224437236785889\n",
      "steps = 1, loss = 2.0086350440979004\n",
      "steps = 1, loss = 2.5816333293914795\n",
      "steps = 1, loss = 1.9388253688812256\n",
      "steps = 1, loss = 50.005977630615234\n",
      "steps = 1, loss = 1.6104464530944824\n",
      "steps = 1, loss = 10.372590065002441\n",
      "steps = 1, loss = 1.0998457670211792\n",
      "steps = 1, loss = 3.049746036529541\n",
      "steps = 1, loss = 1.1500880718231201\n",
      "steps = 1, loss = 2.5873866081237793\n",
      "steps = 1, loss = 2.5705745220184326\n",
      "steps = 1, loss = 2.2189548015594482\n",
      "steps = 1, loss = 0.719170331954956\n",
      "steps = 1, loss = 2.4345030784606934\n",
      "steps = 1, loss = 1.1239659786224365\n",
      "steps = 1, loss = 2.1659555435180664\n",
      "steps = 1, loss = 1.996869683265686\n",
      "steps = 1, loss = 2.06746506690979\n",
      "steps = 1, loss = 2.466202974319458\n",
      "steps = 1, loss = 1.7837222814559937\n",
      "steps = 1, loss = 1.9477081298828125\n",
      "steps = 1, loss = 3.071934700012207\n",
      "steps = 1, loss = 2.0393080711364746\n",
      "steps = 1, loss = 2.1219546794891357\n",
      "steps = 1, loss = 2.236818313598633\n",
      "steps = 1, loss = 1.7258622646331787\n",
      "steps = 1, loss = 3.27186918258667\n",
      "steps = 1, loss = 1.4145474433898926\n",
      "steps = 1, loss = 1.9724520444869995\n",
      "steps = 1, loss = 1.6727193593978882\n",
      "steps = 1, loss = 4.891458988189697\n",
      "steps = 1, loss = 2.452075719833374\n",
      "steps = 81, loss = 2.1697356700897217\n",
      "steps = 1, loss = 3.5977911949157715\n",
      "steps = 1, loss = 49.654266357421875\n",
      "steps = 1, loss = 2.1783664226531982\n",
      "steps = 1, loss = 1.998953104019165\n",
      "steps = 1, loss = 0.7242674231529236\n",
      "steps = 1, loss = 0.7702111601829529\n",
      "steps = 1, loss = 2.840399742126465\n",
      "steps = 1, loss = 2.3962645530700684\n",
      "steps = 1, loss = 1.9434661865234375\n",
      "steps = 1, loss = 2.326514482498169\n",
      "steps = 1, loss = 2.6854023933410645\n",
      "steps = 81, loss = 2.2138288021087646\n",
      "steps = 1, loss = 50.00050735473633\n",
      "steps = 1, loss = 1.9757732152938843\n",
      "steps = 1, loss = 2.062347650527954\n",
      "steps = 1, loss = 2.3312418460845947\n",
      "steps = 1, loss = 1.7761322259902954\n",
      "steps = 1, loss = 2.5616118907928467\n",
      "steps = 1, loss = 2.9011268615722656\n",
      "steps = 1, loss = 1.9117602109909058\n",
      "steps = 1, loss = 0.9596445560455322\n",
      "steps = 1, loss = 2.909933090209961\n",
      "steps = 1, loss = 1.9567960500717163\n",
      "steps = 1, loss = 1.9786161184310913\n",
      "steps = 1, loss = 1.7666727304458618\n",
      "steps = 1, loss = 2.8957161903381348\n",
      "steps = 1, loss = 1.7777113914489746\n",
      "steps = 1, loss = 2.525303602218628\n",
      "steps = 3, loss = 49.997344970703125\n",
      "steps = 3, loss = 7.128263473510742\n",
      "steps = 3, loss = 2.4286015033721924\n",
      "steps = 3, loss = 2.691422462463379\n",
      "steps = 3, loss = 1.9707732200622559\n",
      "steps = 3, loss = 2.5162014961242676\n",
      "steps = 3, loss = 2.4946577548980713\n",
      "steps = 3, loss = 2.6061649322509766\n",
      "steps = 3, loss = 2.6760642528533936\n",
      "steps = 3, loss = 2.4412477016448975\n",
      "steps = 3, loss = 2.140214443206787\n",
      "steps = 3, loss = 2.1121912002563477\n",
      "steps = 3, loss = 2.326748847961426\n",
      "steps = 3, loss = 50.00349807739258\n",
      "steps = 3, loss = 2.28943133354187\n",
      "steps = 3, loss = 50.00216293334961\n",
      "steps = 3, loss = 2.6478567123413086\n",
      "steps = 3, loss = 2.0072214603424072\n",
      "steps = 3, loss = 2.228785514831543\n",
      "steps = 3, loss = 5.581411361694336\n",
      "steps = 3, loss = 2.0092060565948486\n",
      "steps = 3, loss = 3.3501546382904053\n",
      "steps = 3, loss = 1.832816481590271\n",
      "steps = 3, loss = 3.076436996459961\n",
      "steps = 3, loss = 2.0635342597961426\n",
      "steps = 3, loss = 50.01383972167969\n",
      "steps = 81, loss = 2.2068026065826416\n",
      "steps = 3, loss = 3.142988681793213\n",
      "steps = 3, loss = 2.5976648330688477\n",
      "steps = 3, loss = 2.0218937397003174\n",
      "steps = 3, loss = 2.206958055496216\n",
      "steps = 3, loss = 1.931209921836853\n",
      "steps = 3, loss = 1.7300058603286743\n",
      "steps = 3, loss = 2.91823148727417\n",
      "steps = 3, loss = 1.8622519969940186\n",
      "steps = 3, loss = 2.7331864833831787\n",
      "steps = 3, loss = 0.7464034557342529\n",
      "steps = 3, loss = 1.8186097145080566\n",
      "steps = 3, loss = 2.592850685119629\n",
      "steps = 3, loss = 2.120940685272217\n",
      "steps = 3, loss = 2.0577878952026367\n",
      "steps = 3, loss = 2.4263811111450195\n",
      "steps = 3, loss = 1.9706361293792725\n",
      "steps = 3, loss = 1.9845651388168335\n",
      "steps = 3, loss = 0.9932475090026855\n",
      "steps = 3, loss = 3.0021321773529053\n",
      "steps = 3, loss = 2.4920732975006104\n",
      "steps = 3, loss = 2.492131471633911\n",
      "steps = 3, loss = 2.5142569541931152\n",
      "steps = 3, loss = 2.20029616355896\n",
      "steps = 3, loss = 3.273589611053467\n",
      "steps = 3, loss = 2.4093384742736816\n",
      "steps = 3, loss = 2.3595330715179443\n",
      "steps = 3, loss = 1.7514127492904663\n",
      "steps = 3, loss = 2.1101696491241455\n",
      "steps = 3, loss = 2.3643336296081543\n",
      "steps = 3, loss = 3.2094485759735107\n",
      "steps = 3, loss = 5.091831207275391\n",
      "steps = 3, loss = 2.091153860092163\n",
      "steps = 3, loss = 50.00749588012695\n",
      "steps = 3, loss = 50.0074577331543\n",
      "steps = 3, loss = 3.2489843368530273\n",
      "steps = 3, loss = 2.318188428878784\n",
      "steps = 3, loss = 2.4363043308258057\n",
      "steps = 3, loss = 2.2024168968200684\n",
      "steps = 3, loss = 2.6735990047454834\n",
      "steps = 3, loss = 3.432058572769165\n",
      "steps = 3, loss = 2.3389949798583984\n",
      "steps = 3, loss = 2.450594425201416\n",
      "steps = 3, loss = 1.8731918334960938\n",
      "steps = 3, loss = 7.454373359680176\n",
      "steps = 3, loss = 3.3642876148223877\n",
      "steps = 3, loss = 2.679772138595581\n",
      "steps = 3, loss = 5.466296672821045\n",
      "steps = 3, loss = 50.000762939453125\n",
      "steps = 3, loss = 2.6382734775543213\n",
      "steps = 3, loss = 2.066540241241455\n",
      "steps = 3, loss = 2.3198275566101074\n",
      "steps = 3, loss = 1.5242869853973389\n",
      "steps = 3, loss = 3.2229363918304443\n",
      "steps = 3, loss = 2.8649656772613525\n",
      "steps = 3, loss = 2.4035775661468506\n",
      "steps = 9, loss = 2.0196359157562256\n",
      "steps = 9, loss = 2.07708477973938\n",
      "steps = 9, loss = 1.7505221366882324\n",
      "steps = 9, loss = 2.5007002353668213\n",
      "steps = 9, loss = 2.4806454181671143\n",
      "steps = 9, loss = 2.071880578994751\n",
      "steps = 9, loss = 2.2062480449676514\n",
      "steps = 9, loss = 2.338782548904419\n",
      "steps = 162, loss = 2.0447380542755127\n",
      "steps = 9, loss = 2.1175789833068848\n",
      "steps = 9, loss = 2.267993450164795\n",
      "steps = 9, loss = 2.2525200843811035\n",
      "steps = 9, loss = 2.2678298950195312\n",
      "steps = 9, loss = 1.9197155237197876\n",
      "steps = 9, loss = 3.222883462905884\n",
      "steps = 9, loss = 2.4829840660095215\n",
      "steps = 9, loss = 1.899771809577942\n",
      "steps = 9, loss = 2.0655665397644043\n",
      "steps = 9, loss = 1.8307431936264038\n",
      "steps = 9, loss = 2.7364938259124756\n",
      "steps = 9, loss = 1.410565972328186\n",
      "steps = 9, loss = 1.81265127658844\n",
      "steps = 9, loss = 2.20113468170166\n",
      "steps = 9, loss = 2.450197458267212\n",
      "steps = 9, loss = 2.226022481918335\n",
      "steps = 9, loss = 2.2806899547576904\n",
      "steps = 9, loss = 2.4754226207733154\n",
      "steps = 9, loss = 2.2912189960479736\n",
      "steps = 27, loss = 2.6042635440826416\n",
      "steps = 27, loss = 2.0480594635009766\n",
      "steps = 27, loss = 2.4409573078155518\n",
      "steps = 27, loss = 2.256101131439209\n",
      "steps = 27, loss = 2.2473177909851074\n",
      "steps = 27, loss = 2.116107225418091\n",
      "steps = 27, loss = 2.069951295852661\n",
      "steps = 27, loss = 1.8862382173538208\n",
      "steps = 243, loss = 2.1734278202056885\n",
      "steps = 27, loss = 1.9546566009521484\n",
      "steps = 81, loss = 2.5102410316467285\n",
      "steps = 81, loss = 2.1892001628875732\n",
      "steps = 81, loss = 1.9617365598678589\n",
      "steps = 162, loss = 1.9599595069885254\n",
      "steps = 243, loss = 1.9570726156234741\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HyperbandSearchCV(estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                  max_iter=243,\n",
       "                  parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                              &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;, &#x27;ELU&#x27;,\n",
       "                                                     &#x27;PReLU&#x27;],\n",
       "                              &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                               &#x27;xavier_normal_&#x27;,\n",
       "                                               &#x27;kaiming_uniform_&#x27;,\n",
       "                                               &#x27;kaiming_normal_&#x27;],\n",
       "                              &#x27;optimizer&#x27;: [&#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;S...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                              &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                              &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, ...],\n",
       "                              &#x27;train_split&#x27;: [None]},\n",
       "                  patience=True, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;HyperbandSearchCV<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>HyperbandSearchCV(estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                  max_iter=243,\n",
       "                  parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                              &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;, &#x27;ELU&#x27;,\n",
       "                                                     &#x27;PReLU&#x27;],\n",
       "                              &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                               &#x27;xavier_normal_&#x27;,\n",
       "                                               &#x27;kaiming_uniform_&#x27;,\n",
       "                                               &#x27;kaiming_normal_&#x27;],\n",
       "                              &#x27;optimizer&#x27;: [&#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;S...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                              &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                              &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, ...],\n",
       "                              &#x27;train_split&#x27;: [None]},\n",
       "                  patience=True, random_state=42)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HyperbandSearchCV(estimator=<class '__main__.TrimParams'>[uninitialized](\n",
       "  module=<class 'autoencoder.Autoencoder'>,\n",
       "),\n",
       "                  max_iter=243,\n",
       "                  parameters={'batch_size': [32, 64, 128, 256, 512],\n",
       "                              'module__activation': ['ReLU', 'LeakyReLU', 'ELU',\n",
       "                                                     'PReLU'],\n",
       "                              'module__init': ['xavier_uniform_',\n",
       "                                               'xavier_normal_',\n",
       "                                               'kaiming_uniform_',\n",
       "                                               'kaiming_normal_'],\n",
       "                              'optimizer': ['SGD', 'SGD', 'SGD', 'S...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                              'optimizer__nesterov': [True],\n",
       "                              'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, ...],\n",
       "                              'train_split': [None]},\n",
       "                  patience=True, random_state=42)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_patience.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 209.35 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 38.43 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "save_search(search_patience, today, \"hyperband-w-patience\", X_test.compute(), y_test.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_stats = timing_stats = client.profile(filename=\"hyperband.html\")\n",
    "with open(\"hyperband+sop-timing.json\", \"w\") as f:\n",
    "    json.dump(timing_stats[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class '__main__.TrimParams'>[initialized](\n",
       "  module_=Autoencoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=784, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (2): Linear(in_features=784, out_features=196, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=196, out_features=784, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "      (3): Sigmoid()\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_patience.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9570726156234741"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_patience.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_split': None,\n",
       " 'optimizer__weight_decay': 0.0007760503335133571,\n",
       " 'optimizer__nesterov': True,\n",
       " 'optimizer__momentum': 0.7387387387387387,\n",
       " 'optimizer__lr': 2.209756114795902,\n",
       " 'optimizer': 'SGD',\n",
       " 'module__init': 'kaiming_uniform_',\n",
       " 'module__activation': 'LeakyReLU',\n",
       " 'batch_size': 32}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_patience.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing output of best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_test = X_test.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7002, 2, 784)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_hat = search.best_estimator_.predict(noisy_test)\n",
    "clean_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 784)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = rng.choice(len(X_test))\n",
    "print (i , type (clean_hat[i]) )\n",
    "clean_hat[i].size\n",
    "clean_hat[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00159776, 0.00159776, 0.00159776, 0.00159776, 0.00159776,\n",
       "        0.00159776, 0.00159776, 0.00159776, 0.00159776, 0.00159776,\n",
       "        0.00159776, 0.00159776, 0.00159776, 0.00159776, 0.00159776,\n",
       "        0.00159776, 0.00159776, 0.00159776, 0.00159776, 0.00159776,\n",
       "        0.00159776, 0.00159776, 0.00159776, 0.00159776, 0.00159776,\n",
       "        0.00159776, 0.00159776, 0.00159776],\n",
       "       [0.00159776, 0.00159776, 0.00159776, 0.00159776, 0.00159776,\n",
       "        0.00163152, 0.00180538, 0.00174207, 0.00184135, 0.00176202,\n",
       "        0.00167574, 0.00203793, 0.00225489, 0.00177676, 0.00164829,\n",
       "        0.00173561, 0.00183959, 0.00174157, 0.00170266, 0.00181509,\n",
       "        0.00183035, 0.00162612, 0.0016363 , 0.00160138, 0.00159776,\n",
       "        0.00159776, 0.00159776, 0.00159776],\n",
       "       [0.00159776, 0.00159776, 0.00159776, 0.00159781, 0.00159889,\n",
       "        0.00165979, 0.00178143, 0.00174289, 0.001723  , 0.00220107,\n",
       "        0.00299426, 0.00309468, 0.00363676, 0.00640339, 0.00724185,\n",
       "        0.01072401, 0.00713211, 0.00524535, 0.00719008, 0.00672413,\n",
       "        0.00582369, 0.00401896, 0.00277436, 0.00191402, 0.00159817,\n",
       "        0.00159776, 0.00159776, 0.00159776],\n",
       "       [0.00159776, 0.00159776, 0.00160341, 0.00159939, 0.00159827,\n",
       "        0.00171336, 0.00194659, 0.00221518, 0.002397  , 0.00358651,\n",
       "        0.00465973, 0.00423801, 0.00724957, 0.01201847, 0.01545883,\n",
       "        0.02417393, 0.02264782, 0.02625126, 0.03206307, 0.02137132,\n",
       "        0.01248608, 0.00917379, 0.00496469, 0.00256141, 0.00171024,\n",
       "        0.00161941, 0.00160531, 0.00159776],\n",
       "       [0.00159776, 0.00159776, 0.00162661, 0.00179314, 0.00221101,\n",
       "        0.00283427, 0.00416997, 0.00774478, 0.00748065, 0.01090515,\n",
       "        0.01350548, 0.03005515, 0.03470548, 0.07400065, 0.07263412,\n",
       "        0.12849928, 0.1430054 , 0.09472702, 0.04311084, 0.02281625,\n",
       "        0.01266869, 0.00945702, 0.00612582, 0.00459628, 0.00357764,\n",
       "        0.00171896, 0.00165387, 0.00159776],\n",
       "       [0.00159776, 0.00159776, 0.00160915, 0.00180379, 0.00208348,\n",
       "        0.00424806, 0.006299  , 0.00783249, 0.005142  , 0.01080873,\n",
       "        0.02497207, 0.04812525, 0.09361409, 0.18934877, 0.24602003,\n",
       "        0.28346708, 0.3496616 , 0.22072096, 0.09258081, 0.04398214,\n",
       "        0.02441767, 0.01346785, 0.01072513, 0.00594321, 0.00512902,\n",
       "        0.00330025, 0.00235425, 0.00166712],\n",
       "       [0.00159776, 0.00159776, 0.00160226, 0.00180887, 0.00286662,\n",
       "        0.00362439, 0.00477819, 0.00694529, 0.00706048, 0.01068445,\n",
       "        0.03439851, 0.08833183, 0.18242683, 0.3719757 , 0.53879076,\n",
       "        0.60650826, 0.48104462, 0.28224677, 0.13671216, 0.04034737,\n",
       "        0.01403947, 0.01777065, 0.015689  , 0.01020189, 0.00821633,\n",
       "        0.00773164, 0.00200869, 0.00162305],\n",
       "       [0.00159776, 0.0015974 , 0.0015915 , 0.00211649, 0.00279836,\n",
       "        0.00410189, 0.00538223, 0.0062559 , 0.00715351, 0.02489515,\n",
       "        0.06320561, 0.15057291, 0.30454266, 0.49566367, 0.7095585 ,\n",
       "        0.73452395, 0.5802702 , 0.43783844, 0.21193676, 0.03591587,\n",
       "        0.01420841, 0.02033245, 0.02121546, 0.00943487, 0.01121368,\n",
       "        0.01078508, 0.00215882, 0.00161492],\n",
       "       [0.00159776, 0.00159229, 0.00237116, 0.00245707, 0.0021209 ,\n",
       "        0.00369477, 0.00481969, 0.00555528, 0.00789274, 0.03357423,\n",
       "        0.08991042, 0.24653187, 0.53066605, 0.69419664, 0.7754839 ,\n",
       "        0.67069536, 0.4822023 , 0.28109893, 0.1385407 , 0.03643382,\n",
       "        0.0196389 , 0.02057854, 0.02068239, 0.01453463, 0.01317927,\n",
       "        0.01000522, 0.00342115, 0.00161835],\n",
       "       [0.00160334, 0.00159269, 0.0025621 , 0.00271108, 0.00210644,\n",
       "        0.0038382 , 0.00602678, 0.00550848, 0.00990034, 0.0458698 ,\n",
       "        0.19758579, 0.43848798, 0.7045941 , 0.76227117, 0.67639935,\n",
       "        0.53614223, 0.3435967 , 0.20170331, 0.13123676, 0.03841982,\n",
       "        0.01314856, 0.01536128, 0.02011298, 0.01473582, 0.01110492,\n",
       "        0.00651062, 0.00285655, 0.00161708],\n",
       "       [0.00160361, 0.0015807 , 0.00236975, 0.00305399, 0.00269059,\n",
       "        0.00475951, 0.00792952, 0.00788108, 0.01228895, 0.045911  ,\n",
       "        0.2317318 , 0.6050785 , 0.73909837, 0.5779255 , 0.5564949 ,\n",
       "        0.5035904 , 0.34290054, 0.19244787, 0.10098303, 0.03564612,\n",
       "        0.01188336, 0.01261191, 0.01354002, 0.00769158, 0.00291964,\n",
       "        0.00245497, 0.00182913, 0.00161745],\n",
       "       [0.00159819, 0.00161214, 0.00236112, 0.00318479, 0.00409582,\n",
       "        0.0052305 , 0.00564796, 0.00494475, 0.01109352, 0.04376471,\n",
       "        0.26595807, 0.6493802 , 0.66065437, 0.51566833, 0.45722803,\n",
       "        0.5540145 , 0.41862723, 0.19968218, 0.12136996, 0.04150335,\n",
       "        0.00951035, 0.01201198, 0.0083903 , 0.00398239, 0.00220295,\n",
       "        0.00195029, 0.00178399, 0.00161733],\n",
       "       [0.00159776, 0.0016014 , 0.00193735, 0.00246881, 0.00390554,\n",
       "        0.00443593, 0.00317552, 0.00380299, 0.01227284, 0.05404135,\n",
       "        0.23943442, 0.6106721 , 0.5902149 , 0.6378683 , 0.6560208 ,\n",
       "        0.70701987, 0.46227345, 0.32853964, 0.17904983, 0.03808285,\n",
       "        0.01472644, 0.01033522, 0.00773659, 0.00381043, 0.00174208,\n",
       "        0.00194432, 0.00161816, 0.0015991 ],\n",
       "       [0.00159776, 0.00159712, 0.00166506, 0.00206168, 0.0031124 ,\n",
       "        0.0024785 , 0.0026038 , 0.00559274, 0.0163326 , 0.06807289,\n",
       "        0.22445133, 0.40823352, 0.45124713, 0.61750543, 0.75155723,\n",
       "        0.68613476, 0.5598898 , 0.34317175, 0.10066221, 0.02728235,\n",
       "        0.02083532, 0.01518943, 0.00699753, 0.00288176, 0.00166343,\n",
       "        0.00165477, 0.00162739, 0.00160112],\n",
       "       [0.00159776, 0.00159759, 0.00157029, 0.00220189, 0.00320122,\n",
       "        0.00287158, 0.00326449, 0.00641833, 0.0189562 , 0.0541338 ,\n",
       "        0.19006194, 0.24968134, 0.36778203, 0.6468404 , 0.7700585 ,\n",
       "        0.72956866, 0.5873972 , 0.24127398, 0.07069936, 0.01650173,\n",
       "        0.01906216, 0.01893796, 0.008083  , 0.00335741, 0.00230269,\n",
       "        0.00195018, 0.00168948, 0.00159789],\n",
       "       [0.00159776, 0.00159744, 0.00159266, 0.00237784, 0.00262823,\n",
       "        0.00274836, 0.003786  , 0.00549453, 0.01272597, 0.0449116 ,\n",
       "        0.12477774, 0.20453283, 0.41963965, 0.67709076, 0.75894594,\n",
       "        0.7205083 , 0.4581096 , 0.15739417, 0.05162483, 0.02911578,\n",
       "        0.0210759 , 0.02101335, 0.00805142, 0.00318908, 0.0026621 ,\n",
       "        0.0026234 , 0.00193699, 0.00159686],\n",
       "       [0.00159776, 0.0015986 , 0.0015864 , 0.00263243, 0.00351431,\n",
       "        0.00396688, 0.00364883, 0.00563154, 0.01707197, 0.06387098,\n",
       "        0.12281384, 0.2142091 , 0.44525516, 0.63805807, 0.66708106,\n",
       "        0.6138447 , 0.34941044, 0.10466559, 0.06131675, 0.04049989,\n",
       "        0.02268396, 0.02178537, 0.005651  , 0.00336036, 0.00387061,\n",
       "        0.00378449, 0.00219683, 0.00159848],\n",
       "       [0.00159776, 0.00159774, 0.00158593, 0.00240004, 0.00563206,\n",
       "        0.00570283, 0.00413747, 0.00527713, 0.03030085, 0.11561988,\n",
       "        0.18573329, 0.38079247, 0.536392  , 0.6009563 , 0.610725  ,\n",
       "        0.5430732 , 0.32668373, 0.11157131, 0.069992  , 0.05812652,\n",
       "        0.0264969 , 0.01375288, 0.00463339, 0.00410064, 0.00746449,\n",
       "        0.00578219, 0.00195756, 0.00159833],\n",
       "       [0.00159781, 0.00160337, 0.00170257, 0.00294042, 0.0054577 ,\n",
       "        0.0071296 , 0.00715474, 0.00852102, 0.05922956, 0.15601894,\n",
       "        0.24250127, 0.3629422 , 0.5455503 , 0.56969565, 0.7226385 ,\n",
       "        0.6908485 , 0.4213459 , 0.16681108, 0.05256748, 0.03359358,\n",
       "        0.01684646, 0.00570713, 0.00402603, 0.00577318, 0.01052859,\n",
       "        0.00603011, 0.00211095, 0.00159939],\n",
       "       [0.001598  , 0.0016128 , 0.00202678, 0.00393363, 0.00503754,\n",
       "        0.00705718, 0.00957904, 0.01552087, 0.0728352 , 0.18100159,\n",
       "        0.33976093, 0.45450884, 0.5359725 , 0.71737754, 0.86810607,\n",
       "        0.8009351 , 0.4734527 , 0.17867537, 0.03726957, 0.01593213,\n",
       "        0.00552689, 0.00373617, 0.00333705, 0.00654442, 0.0068319 ,\n",
       "        0.00394853, 0.00189378, 0.00160001],\n",
       "       [0.00159776, 0.00159599, 0.00200216, 0.00337789, 0.00372122,\n",
       "        0.00562225, 0.00730406, 0.01178199, 0.06132642, 0.14386016,\n",
       "        0.37068707, 0.50178534, 0.6447917 , 0.84948933, 0.9172472 ,\n",
       "        0.82476467, 0.4585268 , 0.09346472, 0.01561512, 0.00970482,\n",
       "        0.00450862, 0.00308619, 0.00367923, 0.00521998, 0.00434909,\n",
       "        0.00280584, 0.00165985, 0.00160275],\n",
       "       [0.00159778, 0.00159593, 0.00156852, 0.0030956 , 0.00445708,\n",
       "        0.00602567, 0.00720849, 0.0099416 , 0.03569556, 0.08265785,\n",
       "        0.22961214, 0.4678376 , 0.6496677 , 0.82995236, 0.8649674 ,\n",
       "        0.71822095, 0.19671585, 0.03498708, 0.01174164, 0.00589334,\n",
       "        0.00351777, 0.00343919, 0.00304336, 0.00275934, 0.0031583 ,\n",
       "        0.00206406, 0.0016224 , 0.00159776],\n",
       "       [0.00159778, 0.00159776, 0.00160041, 0.00260206, 0.00525543,\n",
       "        0.00590027, 0.00911267, 0.01072084, 0.0137523 , 0.04985142,\n",
       "        0.17805861, 0.43583244, 0.63051647, 0.69159734, 0.71149445,\n",
       "        0.4807989 , 0.10787921, 0.02258253, 0.0072411 , 0.0037976 ,\n",
       "        0.00480006, 0.00354382, 0.00280391, 0.00252336, 0.00223617,\n",
       "        0.00181723, 0.00159781, 0.00159776],\n",
       "       [0.00159776, 0.00159776, 0.00159921, 0.00218889, 0.00389322,\n",
       "        0.005087  , 0.01071688, 0.01519364, 0.02166556, 0.03760337,\n",
       "        0.09272625, 0.26403597, 0.4772629 , 0.47971812, 0.42326882,\n",
       "        0.20071694, 0.04537665, 0.01100187, 0.00340523, 0.00313252,\n",
       "        0.00389251, 0.00336475, 0.00313351, 0.00258457, 0.00205208,\n",
       "        0.00167801, 0.0015964 , 0.00159776],\n",
       "       [0.00159776, 0.00159776, 0.00159209, 0.00187619, 0.00259219,\n",
       "        0.0032943 , 0.00717373, 0.00831798, 0.016333  , 0.02701702,\n",
       "        0.03506507, 0.07007308, 0.13686767, 0.1415537 , 0.13726671,\n",
       "        0.07203306, 0.02143827, 0.00897754, 0.00395154, 0.00233376,\n",
       "        0.00231588, 0.00240722, 0.00251372, 0.00211552, 0.00160053,\n",
       "        0.00159572, 0.00159786, 0.00159776],\n",
       "       [0.00159776, 0.00159776, 0.00159665, 0.0015969 , 0.00181035,\n",
       "        0.00280996, 0.00332377, 0.0036043 , 0.00755922, 0.01544557,\n",
       "        0.0187047 , 0.03048388, 0.03892406, 0.03317718, 0.02575976,\n",
       "        0.0185629 , 0.00873653, 0.00407223, 0.00286559, 0.00181308,\n",
       "        0.00145488, 0.00153189, 0.00170549, 0.00170901, 0.00174542,\n",
       "        0.00160131, 0.00159776, 0.00159776],\n",
       "       [0.00159776, 0.00159776, 0.00159776, 0.00159775, 0.00166319,\n",
       "        0.00226657, 0.00261395, 0.00301879, 0.00587215, 0.00875698,\n",
       "        0.01247919, 0.01674941, 0.01716908, 0.01597242, 0.01135599,\n",
       "        0.00681768, 0.0044715 , 0.00352784, 0.00225447, 0.00193498,\n",
       "        0.00155853, 0.00168166, 0.00161766, 0.00160931, 0.00159747,\n",
       "        0.00159776, 0.00159776, 0.00159776],\n",
       "       [0.00159776, 0.00159776, 0.00159776, 0.00159776, 0.00159781,\n",
       "        0.001598  , 0.00160214, 0.00161143, 0.00175622, 0.00192377,\n",
       "        0.00223868, 0.00379477, 0.00342748, 0.00318721, 0.00304168,\n",
       "        0.00305754, 0.00264505, 0.0020552 , 0.0015927 , 0.00158181,\n",
       "        0.00159556, 0.00159728, 0.00159776, 0.00159776, 0.00159776,\n",
       "        0.00159776, 0.00159776, 0.00159776]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_dimension = clean_hat[i][1]\n",
    "print (last_dimension.shape)\n",
    "last_dimension.reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 69.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 24 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import check_random_state\n",
    "import seaborn as sns\n",
    "\n",
    "cols = 8\n",
    "w = 1.0\n",
    "fig, axs = plt.subplots(figsize=(cols*w, 3*w), ncols=cols, nrows=3)\n",
    "\n",
    "rng = check_random_state(42)\n",
    "for col, (upper, middle, lower) in enumerate(zip(axs[0], axs[1], axs[2])):\n",
    "    if col == 0:\n",
    "        upper.text(-28, 14, 'ground\\ntruth')\n",
    "        middle.text(-28, 14, 'input')\n",
    "        lower.text(-28, 14, 'output')\n",
    "    i = rng.choice(len(X_test))\n",
    "    noisy = X_test[i].reshape(28, 28)\n",
    "    clean = y_test[i].reshape(28, 28)\n",
    "    clean_hat_i = clean_hat[i][1].reshape(28,28)\n",
    "    kwargs = {'cbar': False, 'xticklabels': False, 'yticklabels': False, 'cmap': 'gray_r'}\n",
    "    sns.heatmap(noisy, ax=middle, **kwargs)\n",
    "    sns.heatmap(clean, ax=upper, **kwargs)\n",
    "    sns.heatmap(clean_hat_i, ax=lower, **kwargs)\n",
    "# plt.savefig(\"imgs/best-out.svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting parameters for Hyperband\n",
    "Need to know two things:\n",
    "\n",
    "1. how many \"epochs\" or \"passes through data\" to train model\n",
    "2. how many configs to evaluate\n",
    "    * this is some measure of how complex the search space is\n",
    "    \n",
    "This determines\n",
    "\n",
    "* The `max_iter` argument for `HyperbandCV`\n",
    "* the chunks size for the array to pass in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(243, 32, 4743)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_calls = search.metadata_['partial_fit_calls']\n",
    "num_calls = max_iter\n",
    "\n",
    "n_workers = 32 or len(client.cluster.workers)\n",
    "num_models = max(n_workers, total_calls // num_calls)\n",
    "num_calls, num_models, search.metadata_['partial_fit_calls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import IncrementalSearchCV\n",
    "\n",
    "passive_search = IncrementalSearchCV(\n",
    "    model,\n",
    "    params,\n",
    "    decay_rate=0,\n",
    "    patience=False,\n",
    "    n_initial_parameters=num_models,\n",
    "    max_iter=num_calls,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/dask_ml/model_selection/_incremental.py:1047: FutureWarning: decay_rate is deprecated in InverseDecaySearchCV. Use InverseDecaySearchCV to use decay_rate=0\n",
      "  warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 418.70 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 1, loss = 2.0529050827026367\n",
      "steps = 1, loss = 2.016209363937378\n",
      "steps = 1, loss = 1.7782996892929077\n",
      "steps = 1, loss = 0.7060245871543884\n",
      "steps = 1, loss = 2.1401007175445557\n",
      "steps = 1, loss = 1.039259433746338\n",
      "steps = 1, loss = 1.5065793991088867\n",
      "steps = 1, loss = 2.793207883834839\n",
      "steps = 1, loss = 49.985382080078125\n",
      "steps = 1, loss = 1.9677457809448242\n",
      "steps = 1, loss = 1.7911102771759033\n",
      "steps = 1, loss = 49.951480865478516\n",
      "steps = 1, loss = 4.272833824157715\n",
      "steps = 1, loss = 2.6291937828063965\n",
      "steps = 1, loss = 2.4976720809936523\n",
      "steps = 1, loss = 2.0125246047973633\n",
      "steps = 1, loss = 0.714982271194458\n",
      "steps = 1, loss = 1.979778528213501\n",
      "steps = 1, loss = 1.9447377920150757\n",
      "steps = 1, loss = 1.9673126935958862\n",
      "steps = 1, loss = 2.038898468017578\n",
      "steps = 1, loss = 2.4749724864959717\n",
      "steps = 1, loss = 2.685521125793457\n",
      "steps = 1, loss = 1.1485666036605835\n",
      "steps = 1, loss = 1.9092943668365479\n",
      "steps = 1, loss = 2.9808261394500732\n",
      "steps = 1, loss = 2.307446241378784\n",
      "steps = 1, loss = 1.8994688987731934\n",
      "steps = 1, loss = 2.607888698577881\n",
      "steps = 1, loss = 2.4630143642425537\n",
      "steps = 1, loss = 2.047713041305542\n",
      "steps = 1, loss = 0.741396963596344\n",
      "steps = 2, loss = 1.8439034223556519\n",
      "steps = 2, loss = 49.96749496459961\n",
      "steps = 2, loss = 2.15445876121521\n",
      "steps = 2, loss = 5.526248455047607\n",
      "steps = 2, loss = 1.9598718881607056\n",
      "steps = 2, loss = 2.1506943702697754\n",
      "steps = 2, loss = 3.3354783058166504\n",
      "steps = 2, loss = 49.985382080078125\n",
      "steps = 2, loss = 2.6906378269195557\n",
      "steps = 2, loss = 1.421391248703003\n",
      "steps = 2, loss = 1.6308648586273193\n",
      "steps = 2, loss = 2.2878856658935547\n",
      "steps = 2, loss = 2.7020795345306396\n",
      "steps = 2, loss = 2.2241201400756836\n",
      "steps = 2, loss = 1.9406191110610962\n",
      "steps = 2, loss = 2.2119202613830566\n",
      "steps = 2, loss = 2.1108803749084473\n",
      "steps = 2, loss = 2.7732293605804443\n",
      "steps = 2, loss = 0.7837679386138916\n",
      "steps = 2, loss = 2.707589626312256\n",
      "steps = 2, loss = 2.089663505554199\n",
      "steps = 2, loss = 2.0865330696105957\n",
      "steps = 2, loss = 2.4093544483184814\n",
      "steps = 2, loss = 2.7059106826782227\n",
      "steps = 2, loss = 2.590911865234375\n",
      "steps = 2, loss = 1.5732039213180542\n",
      "steps = 2, loss = 2.0793204307556152\n",
      "steps = 2, loss = 3.1793293952941895\n",
      "steps = 2, loss = 2.5358781814575195\n",
      "steps = 2, loss = 2.962259531021118\n",
      "steps = 2, loss = 2.3242437839508057\n",
      "steps = 2, loss = 1.1305040121078491\n",
      "steps = 3, loss = 2.597278118133545\n",
      "steps = 3, loss = 5.253407955169678\n",
      "steps = 3, loss = 2.6350021362304688\n",
      "steps = 3, loss = 2.3419370651245117\n",
      "steps = 3, loss = 2.043032169342041\n",
      "steps = 3, loss = 2.869290351867676\n",
      "steps = 3, loss = 1.722176432609558\n",
      "steps = 3, loss = 49.985382080078125\n",
      "steps = 3, loss = 2.213190793991089\n",
      "steps = 3, loss = 2.11399507522583\n",
      "steps = 3, loss = 2.7007696628570557\n",
      "steps = 3, loss = 1.9421257972717285\n",
      "steps = 3, loss = 2.6630613803863525\n",
      "steps = 3, loss = 2.490514039993286\n",
      "steps = 3, loss = 2.1503448486328125\n",
      "steps = 3, loss = 1.996060848236084\n",
      "steps = 3, loss = 2.8296971321105957\n",
      "steps = 3, loss = 2.429234743118286\n",
      "steps = 3, loss = 2.4022274017333984\n",
      "steps = 3, loss = 49.769813537597656\n",
      "steps = 3, loss = 1.065323829650879\n",
      "steps = 3, loss = 2.8068366050720215\n",
      "steps = 3, loss = 1.5166456699371338\n",
      "steps = 3, loss = 1.7146302461624146\n",
      "steps = 3, loss = 2.4032154083251953\n",
      "steps = 3, loss = 3.9062252044677734\n",
      "steps = 3, loss = 1.7807618379592896\n",
      "steps = 3, loss = 2.7445003986358643\n",
      "steps = 3, loss = 2.045555353164673\n",
      "steps = 3, loss = 3.184183359146118\n",
      "steps = 3, loss = 2.26914119720459\n",
      "steps = 3, loss = 2.747941255569458\n",
      "steps = 4, loss = 2.9569385051727295\n",
      "steps = 4, loss = 3.0820229053497314\n",
      "steps = 4, loss = 4.19793701171875\n",
      "steps = 4, loss = 2.474759101867676\n",
      "steps = 4, loss = 2.358034610748291\n",
      "steps = 4, loss = 1.3757776021957397\n",
      "steps = 4, loss = 2.024322509765625\n",
      "steps = 4, loss = 2.6169185638427734\n",
      "steps = 4, loss = 2.2244157791137695\n",
      "steps = 4, loss = 49.985382080078125\n",
      "steps = 4, loss = 2.8130476474761963\n",
      "steps = 4, loss = 2.819601535797119\n",
      "steps = 4, loss = 2.3573079109191895\n",
      "steps = 4, loss = 1.84060800075531\n",
      "steps = 4, loss = 2.899993419647217\n",
      "steps = 4, loss = 2.508741855621338\n",
      "steps = 4, loss = 2.0094473361968994\n",
      "steps = 4, loss = 49.447933197021484\n",
      "steps = 4, loss = 2.700148582458496\n",
      "steps = 4, loss = 1.6816412210464478\n",
      "steps = 4, loss = 2.7999043464660645\n",
      "steps = 4, loss = 2.1012659072875977\n",
      "steps = 4, loss = 5.477890491485596\n",
      "steps = 4, loss = 1.877340316772461\n",
      "steps = 4, loss = 2.8619072437286377\n",
      "steps = 4, loss = 2.833281993865967\n",
      "steps = 4, loss = 2.725348949432373\n",
      "steps = 4, loss = 2.6419224739074707\n",
      "steps = 4, loss = 2.5670580863952637\n",
      "steps = 4, loss = 2.1477649211883545\n",
      "steps = 4, loss = 1.8178247213363647\n",
      "steps = 4, loss = 2.555094003677368\n",
      "steps = 5, loss = 2.7206764221191406\n",
      "steps = 5, loss = 2.051110029220581\n",
      "steps = 5, loss = 2.6590402126312256\n",
      "steps = 5, loss = 3.017570972442627\n",
      "steps = 5, loss = 2.7165603637695312\n",
      "steps = 5, loss = 3.138129949569702\n",
      "steps = 5, loss = 2.2283411026000977\n",
      "steps = 5, loss = 1.5442852973937988\n",
      "steps = 5, loss = 2.8847436904907227\n",
      "steps = 5, loss = 2.5916013717651367\n",
      "steps = 5, loss = 2.5531835556030273\n",
      "steps = 5, loss = 1.86957848072052\n",
      "steps = 5, loss = 3.107787847518921\n",
      "steps = 5, loss = 2.4644217491149902\n",
      "steps = 5, loss = 2.538081645965576\n",
      "steps = 5, loss = 2.5652964115142822\n",
      "steps = 5, loss = 2.1568524837493896\n",
      "steps = 5, loss = 49.985382080078125\n",
      "steps = 5, loss = 2.4541501998901367\n",
      "steps = 5, loss = 49.94317626953125\n",
      "steps = 5, loss = 1.924318552017212\n",
      "steps = 5, loss = 2.7217416763305664\n",
      "steps = 5, loss = 1.7649791240692139\n",
      "steps = 5, loss = 2.018523931503296\n",
      "steps = 5, loss = 2.103372097015381\n",
      "steps = 5, loss = 2.9809212684631348\n",
      "steps = 5, loss = 3.1873607635498047\n",
      "steps = 5, loss = 1.9063560962677002\n",
      "steps = 5, loss = 6.303105354309082\n",
      "steps = 5, loss = 3.05046010017395\n",
      "steps = 5, loss = 2.717050552368164\n",
      "steps = 5, loss = 2.703908681869507\n",
      "steps = 6, loss = 3.0513112545013428\n",
      "steps = 6, loss = 49.95500564575195\n",
      "steps = 6, loss = 1.9584287405014038\n",
      "steps = 6, loss = 2.5279762744903564\n",
      "steps = 6, loss = 2.091933012008667\n",
      "steps = 6, loss = 1.9430917501449585\n",
      "steps = 6, loss = 2.8108932971954346\n",
      "steps = 6, loss = 2.6359121799468994\n",
      "steps = 6, loss = 2.5387861728668213\n",
      "steps = 6, loss = 2.071587562561035\n",
      "steps = 6, loss = 2.5872349739074707\n",
      "steps = 6, loss = 2.2973220348358154\n",
      "steps = 6, loss = 2.6011717319488525\n",
      "steps = 6, loss = 2.2426559925079346\n",
      "steps = 6, loss = 2.0612401962280273\n",
      "steps = 6, loss = 2.3474080562591553\n",
      "steps = 6, loss = 3.418271780014038\n",
      "steps = 6, loss = 1.641309380531311\n",
      "steps = 6, loss = 5.57136869430542\n",
      "steps = 6, loss = 2.749326229095459\n",
      "steps = 6, loss = 2.894216299057007\n",
      "steps = 6, loss = 49.985382080078125\n",
      "steps = 6, loss = 2.808222770690918\n",
      "steps = 6, loss = 2.7983384132385254\n",
      "steps = 6, loss = 3.099562644958496\n",
      "steps = 6, loss = 1.8165464401245117\n",
      "steps = 6, loss = 2.939646005630493\n",
      "steps = 6, loss = 2.5998146533966064\n",
      "steps = 6, loss = 1.9044442176818848\n",
      "steps = 6, loss = 3.639691114425659\n",
      "steps = 6, loss = 2.5663487911224365\n",
      "steps = 6, loss = 3.09289813041687\n",
      "steps = 7, loss = 2.6513562202453613\n",
      "steps = 7, loss = 3.6673433780670166\n",
      "steps = 7, loss = 2.89023494720459\n",
      "steps = 7, loss = 1.710795521736145\n",
      "steps = 7, loss = 1.8605800867080688\n",
      "steps = 7, loss = 3.17086124420166\n",
      "steps = 7, loss = 2.1295368671417236\n",
      "steps = 7, loss = 3.1399548053741455\n",
      "steps = 7, loss = 1.9952160120010376\n",
      "steps = 7, loss = 2.172095775604248\n",
      "steps = 7, loss = 3.388713836669922\n",
      "steps = 7, loss = 2.6212642192840576\n",
      "steps = 7, loss = 3.5289406776428223\n",
      "steps = 7, loss = 2.6211893558502197\n",
      "steps = 7, loss = 2.8971123695373535\n",
      "steps = 7, loss = 1.9372400045394897\n",
      "steps = 7, loss = 2.7425782680511475\n",
      "steps = 7, loss = 2.5227699279785156\n",
      "steps = 7, loss = 2.581343650817871\n",
      "steps = 7, loss = 6.047523021697998\n",
      "steps = 7, loss = 2.907040596008301\n",
      "steps = 7, loss = 2.8791022300720215\n",
      "steps = 7, loss = 49.985382080078125\n",
      "steps = 7, loss = 2.6231701374053955\n",
      "steps = 7, loss = 2.1101975440979004\n",
      "steps = 7, loss = 2.3701047897338867\n",
      "steps = 7, loss = 2.5796353816986084\n",
      "steps = 7, loss = 49.92472457885742\n",
      "steps = 7, loss = 1.9799681901931763\n",
      "steps = 7, loss = 2.764787197113037\n",
      "steps = 7, loss = 2.470196485519409\n",
      "steps = 7, loss = 2.061830759048462\n",
      "steps = 8, loss = 3.310945987701416\n",
      "steps = 8, loss = 2.9891510009765625\n",
      "steps = 8, loss = 2.859175443649292\n",
      "steps = 8, loss = 2.05400013923645\n",
      "steps = 8, loss = 1.9135217666625977\n",
      "steps = 8, loss = 2.1812102794647217\n",
      "steps = 8, loss = 2.131646156311035\n",
      "steps = 8, loss = 3.1193625926971436\n",
      "steps = 8, loss = 49.985382080078125\n",
      "steps = 8, loss = 49.950626373291016\n",
      "steps = 8, loss = 1.7774235010147095\n",
      "steps = 8, loss = 2.456411600112915\n",
      "steps = 8, loss = 2.6633188724517822\n",
      "steps = 8, loss = 3.4709746837615967\n",
      "steps = 8, loss = 2.858938455581665\n",
      "steps = 8, loss = 1.983112096786499\n",
      "steps = 8, loss = 2.850182056427002\n",
      "steps = 8, loss = 3.080871343612671\n",
      "steps = 8, loss = 2.032064199447632\n",
      "steps = 8, loss = 6.042267799377441\n",
      "steps = 8, loss = 3.2773964405059814\n",
      "steps = 8, loss = 2.7157986164093018\n",
      "steps = 8, loss = 2.797452449798584\n",
      "steps = 8, loss = 3.3393142223358154\n",
      "steps = 8, loss = 2.6403114795684814\n",
      "steps = 8, loss = 2.9898996353149414\n",
      "steps = 8, loss = 2.014082193374634\n",
      "steps = 8, loss = 2.795830249786377\n",
      "steps = 8, loss = 2.648104429244995\n",
      "steps = 8, loss = 2.9164414405822754\n",
      "steps = 8, loss = 2.8077738285064697\n",
      "steps = 8, loss = 2.0855350494384766\n",
      "steps = 9, loss = 2.081615686416626\n",
      "steps = 9, loss = 3.4717323780059814\n",
      "steps = 9, loss = 2.635096788406372\n",
      "steps = 9, loss = 49.985382080078125\n",
      "steps = 9, loss = 49.94499588012695\n",
      "steps = 9, loss = 3.1367740631103516\n",
      "steps = 9, loss = 2.470402479171753\n",
      "steps = 9, loss = 3.5537867546081543\n",
      "steps = 9, loss = 2.7260711193084717\n",
      "steps = 9, loss = 3.085447311401367\n",
      "steps = 9, loss = 2.5063133239746094\n",
      "steps = 9, loss = 2.8916690349578857\n",
      "steps = 9, loss = 3.0199811458587646\n",
      "steps = 9, loss = 2.760760545730591\n",
      "steps = 9, loss = 1.8167389631271362\n",
      "steps = 9, loss = 3.133239984512329\n",
      "steps = 9, loss = 2.3473942279815674\n",
      "steps = 9, loss = 2.056968927383423\n",
      "steps = 9, loss = 1.941215991973877\n",
      "steps = 9, loss = 6.963540554046631\n",
      "steps = 9, loss = 2.0074782371520996\n",
      "steps = 9, loss = 3.421706438064575\n",
      "steps = 9, loss = 2.886622428894043\n",
      "steps = 9, loss = 3.061100959777832\n",
      "steps = 9, loss = 2.5347867012023926\n",
      "steps = 9, loss = 2.632354974746704\n",
      "steps = 9, loss = 3.4068443775177\n",
      "steps = 9, loss = 2.6193554401397705\n",
      "steps = 9, loss = 2.0627026557922363\n",
      "steps = 9, loss = 2.114814043045044\n",
      "steps = 9, loss = 2.2309160232543945\n",
      "steps = 9, loss = 2.76956844329834\n",
      "steps = 10, loss = 2.09887433052063\n",
      "steps = 10, loss = 2.686452627182007\n",
      "steps = 10, loss = 3.3528764247894287\n",
      "steps = 10, loss = 2.910283088684082\n",
      "steps = 10, loss = 49.93679428100586\n",
      "steps = 10, loss = 2.6838104724884033\n",
      "steps = 10, loss = 2.7991421222686768\n",
      "steps = 10, loss = 49.985382080078125\n",
      "steps = 10, loss = 3.386063814163208\n",
      "steps = 10, loss = 2.817533493041992\n",
      "steps = 10, loss = 1.9522043466567993\n",
      "steps = 10, loss = 3.0376944541931152\n",
      "steps = 10, loss = 3.6069881916046143\n",
      "steps = 10, loss = 2.128394365310669\n",
      "steps = 10, loss = 3.1415798664093018\n",
      "steps = 10, loss = 2.042161226272583\n",
      "steps = 10, loss = 2.8122751712799072\n",
      "steps = 10, loss = 2.8407483100891113\n",
      "steps = 10, loss = 3.160072088241577\n",
      "steps = 10, loss = 3.009920597076416\n",
      "steps = 10, loss = 6.414122104644775\n",
      "steps = 10, loss = 2.777493715286255\n",
      "steps = 10, loss = 3.5161454677581787\n",
      "steps = 10, loss = 2.2887237071990967\n",
      "steps = 10, loss = 2.0891144275665283\n",
      "steps = 10, loss = 1.8572036027908325\n",
      "steps = 10, loss = 1.9746770858764648\n",
      "steps = 10, loss = 3.0881569385528564\n",
      "steps = 10, loss = 2.567178249359131\n",
      "steps = 10, loss = 3.1392810344696045\n",
      "steps = 10, loss = 2.1166272163391113\n",
      "steps = 10, loss = 2.7678232192993164\n",
      "steps = 11, loss = 6.974545001983643\n",
      "steps = 11, loss = 2.884282112121582\n",
      "steps = 11, loss = 3.620490789413452\n",
      "steps = 11, loss = 2.8632543087005615\n",
      "steps = 11, loss = 2.658824920654297\n",
      "steps = 11, loss = 2.524639368057251\n",
      "steps = 11, loss = 3.57259202003479\n",
      "steps = 11, loss = 1.8814793825149536\n",
      "steps = 11, loss = 2.0910282135009766\n",
      "steps = 11, loss = 2.6137855052948\n",
      "steps = 11, loss = 2.063504219055176\n",
      "steps = 11, loss = 3.109771966934204\n",
      "steps = 11, loss = 3.1159913539886475\n",
      "steps = 11, loss = 3.0861260890960693\n",
      "steps = 11, loss = 2.6031346321105957\n",
      "steps = 11, loss = 3.0501012802124023\n",
      "steps = 11, loss = 2.6125733852386475\n",
      "steps = 11, loss = 2.340857982635498\n",
      "steps = 11, loss = 2.20176362991333\n",
      "steps = 11, loss = 2.1555497646331787\n",
      "steps = 11, loss = 2.8817622661590576\n",
      "steps = 11, loss = 3.6222190856933594\n",
      "steps = 11, loss = 3.098357677459717\n",
      "steps = 11, loss = 3.1084036827087402\n",
      "steps = 11, loss = 2.859174966812134\n",
      "steps = 11, loss = 49.985382080078125\n",
      "steps = 11, loss = 2.1184940338134766\n",
      "steps = 11, loss = 49.345272064208984\n",
      "steps = 11, loss = 2.7643983364105225\n",
      "steps = 11, loss = 2.107440233230591\n",
      "steps = 11, loss = 1.9928619861602783\n",
      "steps = 11, loss = 2.68643856048584\n",
      "steps = 12, loss = 49.985382080078125\n",
      "steps = 12, loss = 2.8804149627685547\n",
      "steps = 12, loss = 2.1529743671417236\n",
      "steps = 12, loss = 6.216836929321289\n",
      "steps = 12, loss = 2.5651814937591553\n",
      "steps = 12, loss = 1.8965911865234375\n",
      "steps = 12, loss = 3.073219060897827\n",
      "steps = 12, loss = 2.92356276512146\n",
      "steps = 12, loss = 2.943903684616089\n",
      "steps = 12, loss = 3.1394753456115723\n",
      "steps = 12, loss = 2.684821844100952\n",
      "steps = 12, loss = 2.9955034255981445\n",
      "steps = 12, loss = 2.937476396560669\n",
      "steps = 12, loss = 3.1278491020202637\n",
      "steps = 12, loss = 2.7369179725646973\n",
      "steps = 12, loss = 3.562526226043701\n",
      "steps = 12, loss = 2.002654552459717\n",
      "steps = 12, loss = 2.3701839447021484\n",
      "steps = 12, loss = 2.632995367050171\n",
      "steps = 12, loss = 2.08011794090271\n",
      "steps = 12, loss = 2.674067497253418\n",
      "steps = 12, loss = 3.026665210723877\n",
      "steps = 12, loss = 3.359518051147461\n",
      "steps = 12, loss = 3.6157143115997314\n",
      "steps = 12, loss = 2.17842960357666\n",
      "steps = 12, loss = 2.7015340328216553\n",
      "steps = 12, loss = 3.0774290561676025\n",
      "steps = 12, loss = 2.1142261028289795\n",
      "steps = 12, loss = 3.421513319015503\n",
      "steps = 12, loss = 2.7643017768859863\n",
      "steps = 12, loss = 2.072519540786743\n",
      "steps = 12, loss = 49.896671295166016\n",
      "steps = 13, loss = 3.0304410457611084\n",
      "steps = 13, loss = 2.757880210876465\n",
      "steps = 13, loss = 3.0453267097473145\n",
      "steps = 13, loss = 2.0734622478485107\n",
      "steps = 13, loss = 2.0185859203338623\n",
      "steps = 13, loss = 2.608732223510742\n",
      "steps = 13, loss = 3.6214096546173096\n",
      "steps = 13, loss = 2.084963083267212\n",
      "steps = 13, loss = 2.104083776473999\n",
      "steps = 13, loss = 3.15472412109375\n",
      "steps = 13, loss = 1.9142282009124756\n",
      "steps = 13, loss = 2.2586145401000977\n",
      "steps = 13, loss = 2.133557081222534\n",
      "steps = 13, loss = 2.2130706310272217\n",
      "steps = 13, loss = 3.1892709732055664\n",
      "steps = 13, loss = 49.985382080078125\n",
      "steps = 13, loss = 3.3984079360961914\n",
      "steps = 13, loss = 3.601590871810913\n",
      "steps = 13, loss = 2.750079870223999\n",
      "steps = 13, loss = 2.358942985534668\n",
      "steps = 13, loss = 2.6151351928710938\n",
      "steps = 13, loss = 2.4182727336883545\n",
      "steps = 13, loss = 3.0533344745635986\n",
      "steps = 13, loss = 2.668395757675171\n",
      "steps = 13, loss = 6.040439128875732\n",
      "steps = 13, loss = 2.8052563667297363\n",
      "steps = 13, loss = 2.969010353088379\n",
      "steps = 13, loss = 49.926753997802734\n",
      "steps = 13, loss = 2.8662703037261963\n",
      "steps = 13, loss = 3.601151704788208\n",
      "steps = 13, loss = 2.8806629180908203\n",
      "steps = 13, loss = 2.695497989654541\n",
      "steps = 14, loss = 3.0309438705444336\n",
      "steps = 14, loss = 2.060534954071045\n",
      "steps = 14, loss = 2.7205746173858643\n",
      "steps = 14, loss = 2.6114072799682617\n",
      "steps = 14, loss = 2.158527135848999\n",
      "steps = 14, loss = 2.1784374713897705\n",
      "steps = 14, loss = 10.31024169921875\n",
      "steps = 14, loss = 3.226452112197876\n",
      "steps = 14, loss = 1.945367693901062\n",
      "steps = 14, loss = 2.1459383964538574\n",
      "steps = 14, loss = 3.229846239089966\n",
      "steps = 14, loss = 2.6772525310516357\n",
      "steps = 14, loss = 6.524369239807129\n",
      "steps = 14, loss = 2.7829396724700928\n",
      "steps = 14, loss = 2.8994719982147217\n",
      "steps = 14, loss = 3.258761405944824\n",
      "steps = 14, loss = 2.052341938018799\n",
      "steps = 14, loss = 2.71938419342041\n",
      "steps = 14, loss = 2.6839940547943115\n",
      "steps = 14, loss = 3.638500452041626\n",
      "steps = 14, loss = 2.6318552494049072\n",
      "steps = 14, loss = 2.4897255897521973\n",
      "steps = 14, loss = 2.1028194427490234\n",
      "steps = 14, loss = 3.3278720378875732\n",
      "steps = 14, loss = 3.734367609024048\n",
      "steps = 14, loss = 2.2735228538513184\n",
      "steps = 14, loss = 3.1221835613250732\n",
      "steps = 14, loss = 49.985382080078125\n",
      "steps = 14, loss = 3.6015074253082275\n",
      "steps = 14, loss = 2.642282247543335\n",
      "steps = 14, loss = 3.126617193222046\n",
      "steps = 14, loss = 2.9149317741394043\n",
      "steps = 15, loss = 1.9599908590316772\n",
      "steps = 15, loss = 49.985382080078125\n",
      "steps = 15, loss = 2.5366265773773193\n",
      "steps = 15, loss = 2.9480700492858887\n",
      "steps = 15, loss = 3.810594320297241\n",
      "steps = 15, loss = 2.067230224609375\n",
      "steps = 15, loss = 3.087272882461548\n",
      "steps = 15, loss = 3.244384288787842\n",
      "steps = 15, loss = 2.741940498352051\n",
      "steps = 15, loss = 3.7916083335876465\n",
      "steps = 15, loss = 2.1258928775787354\n",
      "steps = 15, loss = 2.6899073123931885\n",
      "steps = 15, loss = 2.170287609100342\n",
      "steps = 15, loss = 2.0455667972564697\n",
      "steps = 15, loss = 2.589902639389038\n",
      "steps = 15, loss = 2.7026798725128174\n",
      "steps = 15, loss = 3.2719709873199463\n",
      "steps = 15, loss = 2.8734805583953857\n",
      "steps = 15, loss = 2.6552281379699707\n",
      "steps = 15, loss = 2.2038962841033936\n",
      "steps = 15, loss = 3.423236846923828\n",
      "steps = 15, loss = 2.2337937355041504\n",
      "steps = 15, loss = 2.846726417541504\n",
      "steps = 15, loss = 3.135485887527466\n",
      "steps = 15, loss = 2.305866241455078\n",
      "steps = 15, loss = 3.0628390312194824\n",
      "steps = 15, loss = 7.528947830200195\n",
      "steps = 15, loss = 3.4798777103424072\n",
      "steps = 15, loss = 2.5674808025360107\n",
      "steps = 15, loss = 3.013334035873413\n",
      "steps = 15, loss = 2.6962451934814453\n",
      "steps = 15, loss = 2.901671886444092\n",
      "steps = 16, loss = 2.894865036010742\n",
      "steps = 16, loss = 2.693121910095215\n",
      "steps = 16, loss = 2.669822931289673\n",
      "steps = 16, loss = 3.671375036239624\n",
      "steps = 16, loss = 1.9811265468597412\n",
      "steps = 16, loss = 3.556729793548584\n",
      "steps = 16, loss = 3.291005849838257\n",
      "steps = 16, loss = 2.177915334701538\n",
      "steps = 16, loss = 3.2657783031463623\n",
      "steps = 16, loss = 2.2033047676086426\n",
      "steps = 16, loss = 3.1077539920806885\n",
      "steps = 16, loss = 2.7796571254730225\n",
      "steps = 16, loss = 3.1224727630615234\n",
      "steps = 16, loss = 49.985382080078125\n",
      "steps = 16, loss = 2.7260546684265137\n",
      "steps = 16, loss = 2.8269433975219727\n",
      "steps = 16, loss = 2.5819530487060547\n",
      "steps = 16, loss = 2.6594290733337402\n",
      "steps = 16, loss = 2.7693376541137695\n",
      "steps = 16, loss = 2.113011598587036\n",
      "steps = 16, loss = 2.514843702316284\n",
      "steps = 16, loss = 2.242537260055542\n",
      "steps = 16, loss = 2.0910072326660156\n",
      "steps = 16, loss = 2.646782159805298\n",
      "steps = 16, loss = 3.1408333778381348\n",
      "steps = 16, loss = 3.695830821990967\n",
      "steps = 16, loss = 3.625244140625\n",
      "steps = 16, loss = 3.167372703552246\n",
      "steps = 16, loss = 2.352079391479492\n",
      "steps = 16, loss = 3.4555327892303467\n",
      "steps = 16, loss = 6.679059982299805\n",
      "steps = 16, loss = 2.1033694744110107\n",
      "steps = 17, loss = 3.120957851409912\n",
      "steps = 17, loss = 49.985382080078125\n",
      "steps = 17, loss = 2.544679641723633\n",
      "steps = 17, loss = 2.5999178886413574\n",
      "steps = 17, loss = 3.341804027557373\n",
      "steps = 17, loss = 3.3332717418670654\n",
      "steps = 17, loss = 3.16342830657959\n",
      "steps = 17, loss = 2.7167468070983887\n",
      "steps = 17, loss = 2.44490385055542\n",
      "steps = 17, loss = 2.079702138900757\n",
      "steps = 17, loss = 2.1072299480438232\n",
      "steps = 17, loss = 2.713050365447998\n",
      "steps = 17, loss = 2.8306796550750732\n",
      "steps = 17, loss = 3.14080810546875\n",
      "steps = 17, loss = 2.6147220134735107\n",
      "steps = 17, loss = 3.0320897102355957\n",
      "steps = 17, loss = 2.654994487762451\n",
      "steps = 17, loss = 7.305074691772461\n",
      "steps = 17, loss = 2.253592014312744\n",
      "steps = 17, loss = 2.689931869506836\n",
      "steps = 17, loss = 2.786226272583008\n",
      "steps = 17, loss = 3.567491054534912\n",
      "steps = 17, loss = 2.3646345138549805\n",
      "steps = 17, loss = 1.9822542667388916\n",
      "steps = 17, loss = 3.7150444984436035\n",
      "steps = 17, loss = 3.111794948577881\n",
      "steps = 17, loss = 2.8719706535339355\n",
      "steps = 17, loss = 2.2147951126098633\n",
      "steps = 17, loss = 2.542691946029663\n",
      "steps = 17, loss = 2.258267879486084\n",
      "steps = 17, loss = 2.091794967651367\n",
      "steps = 17, loss = 3.3438539505004883\n",
      "steps = 18, loss = 3.0984411239624023\n",
      "steps = 18, loss = 3.160766839981079\n",
      "steps = 18, loss = 2.6985509395599365\n",
      "steps = 18, loss = 2.782343864440918\n",
      "steps = 18, loss = 2.6992626190185547\n",
      "steps = 18, loss = 3.9892125129699707\n",
      "steps = 18, loss = 7.098485469818115\n",
      "steps = 18, loss = 2.731797933578491\n",
      "steps = 18, loss = 2.8665833473205566\n",
      "steps = 18, loss = 2.3182075023651123\n",
      "steps = 18, loss = 3.1897737979888916\n",
      "steps = 18, loss = 3.259613275527954\n",
      "steps = 18, loss = 2.8690333366394043\n",
      "steps = 18, loss = 3.3117613792419434\n",
      "steps = 18, loss = 2.1331281661987305\n",
      "steps = 18, loss = 1.999601125717163\n",
      "steps = 18, loss = 2.82067608833313\n",
      "steps = 18, loss = 2.61022686958313\n",
      "steps = 18, loss = 2.116274356842041\n",
      "steps = 18, loss = 2.4109036922454834\n",
      "steps = 18, loss = 2.6582634449005127\n",
      "steps = 18, loss = 3.884521007537842\n",
      "steps = 18, loss = 3.5001020431518555\n",
      "steps = 18, loss = 3.0457448959350586\n",
      "steps = 18, loss = 3.2419283390045166\n",
      "steps = 18, loss = 2.816380739212036\n",
      "steps = 18, loss = 2.8687312602996826\n",
      "steps = 18, loss = 2.165904998779297\n",
      "steps = 18, loss = 2.238635301589966\n",
      "steps = 18, loss = 2.2488648891448975\n",
      "steps = 18, loss = 49.985382080078125\n",
      "steps = 18, loss = 3.278346538543701\n",
      "steps = 19, loss = 2.0184199810028076\n",
      "steps = 19, loss = 3.2622056007385254\n",
      "steps = 19, loss = 3.379852533340454\n",
      "steps = 19, loss = 2.7636334896087646\n",
      "steps = 19, loss = 3.8058300018310547\n",
      "steps = 19, loss = 2.8033037185668945\n",
      "steps = 19, loss = 2.0843424797058105\n",
      "steps = 19, loss = 49.985382080078125\n",
      "steps = 19, loss = 2.588282823562622\n",
      "steps = 19, loss = 2.7771475315093994\n",
      "steps = 19, loss = 2.8487191200256348\n",
      "steps = 19, loss = 6.842865943908691\n",
      "steps = 19, loss = 2.717327356338501\n",
      "steps = 19, loss = 2.698904037475586\n",
      "steps = 19, loss = 2.104132890701294\n",
      "steps = 19, loss = 49.93497848510742\n",
      "steps = 19, loss = 2.1449060440063477\n",
      "steps = 19, loss = 2.8589582443237305\n",
      "steps = 19, loss = 2.140961170196533\n",
      "steps = 19, loss = 2.731600761413574\n",
      "steps = 19, loss = 2.4544687271118164\n",
      "steps = 19, loss = 2.8393449783325195\n",
      "steps = 19, loss = 3.6992225646972656\n",
      "steps = 19, loss = 2.8896756172180176\n",
      "steps = 19, loss = 3.5723602771759033\n",
      "steps = 19, loss = 3.1971826553344727\n",
      "steps = 19, loss = 3.2223522663116455\n",
      "steps = 19, loss = 2.2812273502349854\n",
      "steps = 19, loss = 3.3389334678649902\n",
      "steps = 19, loss = 2.375441074371338\n",
      "steps = 19, loss = 2.952627182006836\n",
      "steps = 19, loss = 3.6001932621002197\n",
      "steps = 20, loss = 3.431058406829834\n",
      "steps = 20, loss = 3.113865852355957\n",
      "steps = 20, loss = 3.203279972076416\n",
      "steps = 20, loss = 3.2004740238189697\n",
      "steps = 20, loss = 2.4637227058410645\n",
      "steps = 20, loss = 49.93507766723633\n",
      "steps = 20, loss = 2.1752519607543945\n",
      "steps = 20, loss = 3.0995616912841797\n",
      "steps = 20, loss = 2.930957794189453\n",
      "steps = 20, loss = 3.58085036277771\n",
      "steps = 20, loss = 2.7085390090942383\n",
      "steps = 20, loss = 2.0171542167663574\n",
      "steps = 20, loss = 2.0832135677337646\n",
      "steps = 20, loss = 2.4552524089813232\n",
      "steps = 20, loss = 2.406583309173584\n",
      "steps = 20, loss = 2.861159563064575\n",
      "steps = 20, loss = 7.606690883636475\n",
      "steps = 20, loss = 3.391127824783325\n",
      "steps = 20, loss = 2.8500900268554688\n",
      "steps = 20, loss = 2.7237420082092285\n",
      "steps = 20, loss = 2.2917768955230713\n",
      "steps = 20, loss = 3.9718780517578125\n",
      "steps = 20, loss = 48.55454635620117\n",
      "steps = 20, loss = 2.5965981483459473\n",
      "steps = 20, loss = 3.2284202575683594\n",
      "steps = 20, loss = 2.7114367485046387\n",
      "steps = 20, loss = 2.142597198486328\n",
      "steps = 20, loss = 2.658220052719116\n",
      "steps = 20, loss = 3.4793882369995117\n",
      "steps = 20, loss = 2.6342203617095947\n",
      "steps = 20, loss = 49.985382080078125\n",
      "steps = 20, loss = 2.868701457977295\n",
      "steps = 21, loss = 1.9063295125961304\n",
      "steps = 21, loss = 3.144001007080078\n",
      "steps = 21, loss = 2.4791572093963623\n",
      "steps = 21, loss = 2.1364407539367676\n",
      "steps = 21, loss = 3.411001682281494\n",
      "steps = 21, loss = 49.94676208496094\n",
      "steps = 21, loss = 2.59964656829834\n",
      "steps = 21, loss = 2.8837811946868896\n",
      "steps = 21, loss = 2.875159978866577\n",
      "steps = 21, loss = 2.325025796890259\n",
      "steps = 21, loss = 2.506199598312378\n",
      "steps = 21, loss = 2.882187843322754\n",
      "steps = 21, loss = 2.706191301345825\n",
      "steps = 21, loss = 4.061958312988281\n",
      "steps = 21, loss = 2.0325522422790527\n",
      "steps = 21, loss = 2.72273850440979\n",
      "steps = 21, loss = 3.369462728500366\n",
      "steps = 21, loss = 3.2368881702423096\n",
      "steps = 21, loss = 2.792583703994751\n",
      "steps = 21, loss = 2.1686291694641113\n",
      "steps = 21, loss = 2.693014144897461\n",
      "steps = 21, loss = 3.160761833190918\n",
      "steps = 21, loss = 3.3668220043182373\n",
      "steps = 21, loss = 3.44387149810791\n",
      "steps = 21, loss = 49.985382080078125\n",
      "steps = 21, loss = 3.42091965675354\n",
      "steps = 21, loss = 7.120547294616699\n",
      "steps = 21, loss = 2.866795778274536\n",
      "steps = 21, loss = 1.9577927589416504\n",
      "steps = 21, loss = 2.7642874717712402\n",
      "steps = 21, loss = 2.7973344326019287\n",
      "steps = 21, loss = 2.8111701011657715\n",
      "steps = 22, loss = 49.985382080078125\n",
      "steps = 22, loss = 3.07609486579895\n",
      "steps = 22, loss = 3.261425018310547\n",
      "steps = 22, loss = 3.5003819465637207\n",
      "steps = 22, loss = 49.93636703491211\n",
      "steps = 22, loss = 2.865962266921997\n",
      "steps = 22, loss = 3.1154866218566895\n",
      "steps = 22, loss = 3.2587356567382812\n",
      "steps = 22, loss = 2.3763980865478516\n",
      "steps = 22, loss = 2.561690092086792\n",
      "steps = 22, loss = 2.7923924922943115\n",
      "steps = 22, loss = 2.7088050842285156\n",
      "steps = 22, loss = 2.7050364017486572\n",
      "steps = 22, loss = 2.799065351486206\n",
      "steps = 22, loss = 3.3385422229766846\n",
      "steps = 22, loss = 2.1396286487579346\n",
      "steps = 22, loss = 2.0438320636749268\n",
      "steps = 22, loss = 2.0014846324920654\n",
      "steps = 22, loss = 2.1889312267303467\n",
      "steps = 22, loss = 4.110341548919678\n",
      "steps = 22, loss = 3.167029857635498\n",
      "steps = 22, loss = 2.8825676441192627\n",
      "steps = 22, loss = 2.7063236236572266\n",
      "steps = 22, loss = 2.3511223793029785\n",
      "steps = 22, loss = 2.5364391803741455\n",
      "steps = 22, loss = 3.6147587299346924\n",
      "steps = 22, loss = 7.577415943145752\n",
      "steps = 22, loss = 2.8923847675323486\n",
      "steps = 22, loss = 2.5375776290893555\n",
      "steps = 22, loss = 2.7364556789398193\n",
      "steps = 22, loss = 2.838608980178833\n",
      "steps = 22, loss = 3.645207643508911\n",
      "steps = 23, loss = 2.5757155418395996\n",
      "steps = 23, loss = 3.284024238586426\n",
      "steps = 23, loss = 2.834017276763916\n",
      "steps = 23, loss = 2.2133920192718506\n",
      "steps = 23, loss = 2.8847148418426514\n",
      "steps = 23, loss = 3.301483154296875\n",
      "steps = 23, loss = 2.272155523300171\n",
      "steps = 23, loss = 2.5932655334472656\n",
      "steps = 23, loss = 2.9167592525482178\n",
      "steps = 23, loss = 7.28117036819458\n",
      "steps = 23, loss = 2.731140613555908\n",
      "steps = 23, loss = 2.6542932987213135\n",
      "steps = 23, loss = 2.8717424869537354\n",
      "steps = 23, loss = 3.5039422512054443\n",
      "steps = 23, loss = 3.4223926067352295\n",
      "steps = 23, loss = 49.944671630859375\n",
      "steps = 23, loss = 3.3604321479797363\n",
      "steps = 23, loss = 3.0513997077941895\n",
      "steps = 23, loss = 2.059262752532959\n",
      "steps = 23, loss = 2.097959280014038\n",
      "steps = 23, loss = 3.3841376304626465\n",
      "steps = 23, loss = 2.770273208618164\n",
      "steps = 23, loss = 3.5666959285736084\n",
      "steps = 23, loss = 2.872744560241699\n",
      "steps = 23, loss = 2.1035914421081543\n",
      "steps = 23, loss = 2.7628684043884277\n",
      "steps = 23, loss = 2.6616125106811523\n",
      "steps = 23, loss = 4.044303894042969\n",
      "steps = 23, loss = 2.9006128311157227\n",
      "steps = 23, loss = 3.8002688884735107\n",
      "steps = 23, loss = 49.985382080078125\n",
      "steps = 23, loss = 2.3800697326660156\n",
      "steps = 24, loss = 2.7247602939605713\n",
      "steps = 24, loss = 2.475034475326538\n",
      "steps = 24, loss = 7.863889694213867\n",
      "steps = 24, loss = 2.6708035469055176\n",
      "steps = 24, loss = 2.3881139755249023\n",
      "steps = 24, loss = 2.086324453353882\n",
      "steps = 24, loss = 3.366365671157837\n",
      "steps = 24, loss = 49.985382080078125\n",
      "steps = 24, loss = 3.2435710430145264\n",
      "steps = 24, loss = 3.3810818195343018\n",
      "steps = 24, loss = 2.708564519882202\n",
      "steps = 24, loss = 2.841142177581787\n",
      "steps = 24, loss = 2.580111026763916\n",
      "steps = 24, loss = 3.1936073303222656\n",
      "steps = 24, loss = 2.833651304244995\n",
      "steps = 24, loss = 2.6225733757019043\n",
      "steps = 24, loss = 2.0427820682525635\n",
      "steps = 24, loss = 2.1371333599090576\n",
      "steps = 24, loss = 2.667370557785034\n",
      "steps = 24, loss = 2.9122133255004883\n",
      "steps = 24, loss = 3.3117125034332275\n",
      "steps = 24, loss = 3.282266139984131\n",
      "steps = 24, loss = 2.215888738632202\n",
      "steps = 24, loss = 2.055507183074951\n",
      "steps = 24, loss = 4.211857795715332\n",
      "steps = 24, loss = 2.5294151306152344\n",
      "steps = 24, loss = 49.94063949584961\n",
      "steps = 24, loss = 3.489905595779419\n",
      "steps = 24, loss = 2.857736587524414\n",
      "steps = 24, loss = 2.6887545585632324\n",
      "steps = 24, loss = 2.865424871444702\n",
      "steps = 24, loss = 3.4971132278442383\n",
      "steps = 25, loss = 2.620366334915161\n",
      "steps = 25, loss = 2.603017807006836\n",
      "steps = 25, loss = 2.0601956844329834\n",
      "steps = 25, loss = 2.4077088832855225\n",
      "steps = 25, loss = 2.866821050643921\n",
      "steps = 25, loss = 2.2312023639678955\n",
      "steps = 25, loss = 3.269000291824341\n",
      "steps = 25, loss = 3.223972797393799\n",
      "steps = 25, loss = 2.8690335750579834\n",
      "steps = 25, loss = 2.923736333847046\n",
      "steps = 25, loss = 3.418119430541992\n",
      "steps = 25, loss = 2.650963068008423\n",
      "steps = 25, loss = 49.95848083496094\n",
      "steps = 25, loss = 2.811171770095825\n",
      "steps = 25, loss = 2.0708377361297607\n",
      "steps = 25, loss = 2.6733038425445557\n",
      "steps = 25, loss = 2.7262561321258545\n",
      "steps = 25, loss = 2.6979756355285645\n",
      "steps = 25, loss = 3.2963132858276367\n",
      "steps = 25, loss = 7.608949661254883\n",
      "steps = 25, loss = 2.659987449645996\n",
      "steps = 25, loss = 3.342458963394165\n",
      "steps = 25, loss = 2.1863791942596436\n",
      "steps = 25, loss = 2.721018075942993\n",
      "steps = 25, loss = 2.980358362197876\n",
      "steps = 25, loss = 2.7182462215423584\n",
      "steps = 25, loss = 4.498733043670654\n",
      "steps = 25, loss = 3.1986947059631348\n",
      "steps = 25, loss = 3.183152914047241\n",
      "steps = 25, loss = 3.5184402465820312\n",
      "steps = 25, loss = 2.086482524871826\n",
      "steps = 25, loss = 49.985382080078125\n",
      "steps = 26, loss = 2.2658331394195557\n",
      "steps = 26, loss = 2.751370429992676\n",
      "steps = 26, loss = 2.790945291519165\n",
      "steps = 26, loss = 2.756133556365967\n",
      "steps = 26, loss = 4.076229572296143\n",
      "steps = 26, loss = 2.1164658069610596\n",
      "steps = 26, loss = 2.949422597885132\n",
      "steps = 26, loss = 3.3444323539733887\n",
      "steps = 26, loss = 2.0809366703033447\n",
      "steps = 26, loss = 3.3804819583892822\n",
      "steps = 26, loss = 7.373338222503662\n",
      "steps = 26, loss = 2.444227933883667\n",
      "steps = 26, loss = 49.957763671875\n",
      "steps = 26, loss = 3.1310248374938965\n",
      "steps = 26, loss = 2.6693737506866455\n",
      "steps = 26, loss = 3.459052085876465\n",
      "steps = 26, loss = 3.3279614448547363\n",
      "steps = 26, loss = 3.621896743774414\n",
      "steps = 26, loss = 2.8852388858795166\n",
      "steps = 26, loss = 2.6505184173583984\n",
      "steps = 26, loss = 3.446171522140503\n",
      "steps = 26, loss = 2.7333669662475586\n",
      "steps = 26, loss = 3.564462184906006\n",
      "steps = 26, loss = 2.7388901710510254\n",
      "steps = 26, loss = 3.0311291217803955\n",
      "steps = 26, loss = 2.822925090789795\n",
      "steps = 26, loss = 2.0294947624206543\n",
      "steps = 26, loss = 2.8321800231933594\n",
      "steps = 26, loss = 2.1054842472076416\n",
      "steps = 26, loss = 2.9525201320648193\n",
      "steps = 26, loss = 49.985382080078125\n",
      "steps = 26, loss = 2.922647714614868\n",
      "steps = 27, loss = 3.01192045211792\n",
      "steps = 27, loss = 2.700000762939453\n",
      "steps = 27, loss = 2.864955186843872\n",
      "steps = 27, loss = 2.137399673461914\n",
      "steps = 27, loss = 48.6558837890625\n",
      "steps = 27, loss = 3.280311107635498\n",
      "steps = 27, loss = 3.5412395000457764\n",
      "steps = 27, loss = 2.4645674228668213\n",
      "steps = 27, loss = 2.953639030456543\n",
      "steps = 27, loss = 3.2806665897369385\n",
      "steps = 27, loss = 2.9548659324645996\n",
      "steps = 27, loss = 2.809063673019409\n",
      "steps = 27, loss = 3.0952234268188477\n",
      "steps = 27, loss = 1.9999898672103882\n",
      "steps = 27, loss = 2.930905818939209\n",
      "steps = 27, loss = 49.985382080078125\n",
      "steps = 27, loss = 2.6517629623413086\n",
      "steps = 27, loss = 2.2893619537353516\n",
      "steps = 27, loss = 2.7121376991271973\n",
      "steps = 27, loss = 2.8508663177490234\n",
      "steps = 27, loss = 3.335859537124634\n",
      "steps = 27, loss = 2.5775835514068604\n",
      "steps = 27, loss = 4.253603935241699\n",
      "steps = 27, loss = 2.284149169921875\n",
      "steps = 27, loss = 3.3259940147399902\n",
      "steps = 27, loss = 2.0875372886657715\n",
      "steps = 27, loss = 2.6672775745391846\n",
      "steps = 27, loss = 3.3278608322143555\n",
      "steps = 27, loss = 49.94881057739258\n",
      "steps = 27, loss = 3.3747239112854004\n",
      "steps = 27, loss = 3.486509084701538\n",
      "steps = 27, loss = 8.095527648925781\n",
      "steps = 28, loss = 2.6428539752960205\n",
      "steps = 28, loss = 2.7426598072052\n",
      "steps = 28, loss = 2.935607671737671\n",
      "steps = 28, loss = 2.981851100921631\n",
      "steps = 28, loss = 2.0999698638916016\n",
      "steps = 28, loss = 4.2729105949401855\n",
      "steps = 28, loss = 2.4863383769989014\n",
      "steps = 28, loss = 2.5263800621032715\n",
      "steps = 28, loss = 2.690915107727051\n",
      "steps = 28, loss = 3.346313714981079\n",
      "steps = 28, loss = 3.3384838104248047\n",
      "steps = 28, loss = 3.1177244186401367\n",
      "steps = 28, loss = 2.30458664894104\n",
      "steps = 28, loss = 2.142178773880005\n",
      "steps = 28, loss = 3.2820615768432617\n",
      "steps = 28, loss = 2.984506130218506\n",
      "steps = 28, loss = 7.932504177093506\n",
      "steps = 28, loss = 3.4715418815612793\n",
      "steps = 28, loss = 2.9428799152374268\n",
      "steps = 28, loss = 2.044659376144409\n",
      "steps = 28, loss = 2.963733673095703\n",
      "steps = 28, loss = 49.985382080078125\n",
      "steps = 28, loss = 2.706578493118286\n",
      "steps = 28, loss = 49.8968505859375\n",
      "steps = 28, loss = 3.870098829269409\n",
      "steps = 28, loss = 2.8640124797821045\n",
      "steps = 28, loss = 2.096057891845703\n",
      "steps = 28, loss = 3.160572052001953\n",
      "steps = 28, loss = 3.4856514930725098\n",
      "steps = 28, loss = 49.93795394897461\n",
      "steps = 28, loss = 2.8713767528533936\n",
      "steps = 28, loss = 2.710710048675537\n",
      "steps = 29, loss = 3.3611557483673096\n",
      "steps = 29, loss = 2.7838587760925293\n",
      "steps = 29, loss = 4.0722527503967285\n",
      "steps = 29, loss = 3.3988823890686035\n",
      "steps = 29, loss = 49.4886360168457\n",
      "steps = 29, loss = 3.4502084255218506\n",
      "steps = 29, loss = 3.363145589828491\n",
      "steps = 29, loss = 2.93799090385437\n",
      "steps = 29, loss = 3.508575439453125\n",
      "steps = 29, loss = 1.9928576946258545\n",
      "steps = 29, loss = 2.8818182945251465\n",
      "steps = 29, loss = 2.6615588665008545\n",
      "steps = 29, loss = 2.687385082244873\n",
      "steps = 29, loss = 49.985382080078125\n",
      "steps = 29, loss = 8.307894706726074\n",
      "steps = 29, loss = 2.724585771560669\n",
      "steps = 29, loss = 2.0545220375061035\n",
      "steps = 29, loss = 3.0560245513916016\n",
      "steps = 29, loss = 2.7482452392578125\n",
      "steps = 29, loss = 2.5113017559051514\n",
      "steps = 29, loss = 2.102707862854004\n",
      "steps = 29, loss = 3.623746871948242\n",
      "steps = 29, loss = 2.9804086685180664\n",
      "steps = 29, loss = 2.6726033687591553\n",
      "steps = 29, loss = 3.5182502269744873\n",
      "steps = 29, loss = 2.3285467624664307\n",
      "steps = 29, loss = 2.8647050857543945\n",
      "steps = 29, loss = 2.109504222869873\n",
      "steps = 29, loss = 49.779052734375\n",
      "steps = 29, loss = 2.73075795173645\n",
      "steps = 29, loss = 3.4291281700134277\n",
      "steps = 29, loss = 3.0171210765838623\n",
      "steps = 30, loss = 49.985382080078125\n",
      "steps = 30, loss = 3.3151886463165283\n",
      "steps = 30, loss = 2.0979673862457275\n",
      "steps = 30, loss = 2.723738431930542\n",
      "steps = 30, loss = 1.9679220914840698\n",
      "steps = 30, loss = 2.974468469619751\n",
      "steps = 30, loss = 3.0282833576202393\n",
      "steps = 30, loss = 3.237377405166626\n",
      "steps = 30, loss = 49.959163665771484\n",
      "steps = 30, loss = 2.697000741958618\n",
      "steps = 30, loss = 3.352055072784424\n",
      "steps = 30, loss = 2.7249796390533447\n",
      "steps = 30, loss = 2.537524938583374\n",
      "steps = 30, loss = 2.1041908264160156\n",
      "steps = 30, loss = 3.021974563598633\n",
      "steps = 30, loss = 2.7233943939208984\n",
      "steps = 30, loss = 18.909669876098633\n",
      "steps = 30, loss = 2.9077744483947754\n",
      "steps = 30, loss = 3.396134614944458\n",
      "steps = 30, loss = 2.86326265335083\n",
      "steps = 30, loss = 2.5165154933929443\n",
      "steps = 30, loss = 2.3312058448791504\n",
      "steps = 30, loss = 3.447516679763794\n",
      "steps = 30, loss = 2.6907029151916504\n",
      "steps = 30, loss = 3.343950033187866\n",
      "steps = 30, loss = 3.409979820251465\n",
      "steps = 30, loss = 2.9695522785186768\n",
      "steps = 30, loss = 3.406825065612793\n",
      "steps = 30, loss = 49.95093536376953\n",
      "steps = 30, loss = 2.088629961013794\n",
      "steps = 30, loss = 2.572162628173828\n",
      "steps = 30, loss = 9.035765647888184\n",
      "steps = 31, loss = 2.9337122440338135\n",
      "steps = 31, loss = 2.5116612911224365\n",
      "steps = 31, loss = 2.107938051223755\n",
      "steps = 31, loss = 3.048490285873413\n",
      "steps = 31, loss = 3.3771674633026123\n",
      "steps = 31, loss = 49.93180465698242\n",
      "steps = 31, loss = 3.2569661140441895\n",
      "steps = 31, loss = 49.96792984008789\n",
      "steps = 31, loss = 2.0734446048736572\n",
      "steps = 31, loss = 3.191295862197876\n",
      "steps = 31, loss = 3.3682701587677\n",
      "steps = 31, loss = 2.7736124992370605\n",
      "steps = 31, loss = 2.660374402999878\n",
      "steps = 31, loss = 8.911503791809082\n",
      "steps = 31, loss = 49.985382080078125\n",
      "steps = 31, loss = 2.7276453971862793\n",
      "steps = 31, loss = 3.038743734359741\n",
      "steps = 31, loss = 3.342118501663208\n",
      "steps = 31, loss = 2.724735975265503\n",
      "steps = 31, loss = 14.58751392364502\n",
      "steps = 31, loss = 2.8647267818450928\n",
      "steps = 31, loss = 2.547882556915283\n",
      "steps = 31, loss = 3.3581104278564453\n",
      "steps = 31, loss = 3.414921522140503\n",
      "steps = 31, loss = 3.513731002807617\n",
      "steps = 31, loss = 2.6009349822998047\n",
      "steps = 31, loss = 2.7411139011383057\n",
      "steps = 31, loss = 2.088447332382202\n",
      "steps = 31, loss = 2.9750213623046875\n",
      "steps = 31, loss = 2.3465564250946045\n",
      "steps = 31, loss = 2.0715878009796143\n",
      "steps = 31, loss = 2.5328831672668457\n",
      "steps = 32, loss = 8.77923583984375\n",
      "steps = 32, loss = 2.6760365962982178\n",
      "steps = 32, loss = 9.947395324707031\n",
      "steps = 32, loss = 2.3326025009155273\n",
      "steps = 32, loss = 2.592379093170166\n",
      "steps = 32, loss = 3.0011444091796875\n",
      "steps = 32, loss = 3.13527250289917\n",
      "steps = 32, loss = 3.568284273147583\n",
      "steps = 32, loss = 3.617539644241333\n",
      "steps = 32, loss = 2.863036870956421\n",
      "steps = 32, loss = 2.123023271560669\n",
      "steps = 32, loss = 49.94007110595703\n",
      "steps = 32, loss = 3.374804973602295\n",
      "steps = 32, loss = 2.7705204486846924\n",
      "steps = 32, loss = 2.1413373947143555\n",
      "steps = 32, loss = 49.94556427001953\n",
      "steps = 32, loss = 2.7133913040161133\n",
      "steps = 32, loss = 2.882652759552002\n",
      "steps = 32, loss = 2.376723051071167\n",
      "steps = 32, loss = 2.5806117057800293\n",
      "steps = 32, loss = 3.666231870651245\n",
      "steps = 32, loss = 3.513463020324707\n",
      "steps = 32, loss = 2.561720132827759\n",
      "steps = 32, loss = 1.9859883785247803\n",
      "steps = 32, loss = 2.8391129970550537\n",
      "steps = 32, loss = 2.9893581867218018\n",
      "steps = 32, loss = 3.0979268550872803\n",
      "steps = 32, loss = 2.9031333923339844\n",
      "steps = 32, loss = 49.985382080078125\n",
      "steps = 32, loss = 3.192101240158081\n",
      "steps = 32, loss = 3.4299449920654297\n",
      "steps = 32, loss = 3.3818540573120117\n",
      "steps = 33, loss = 2.0282857418060303\n",
      "steps = 33, loss = 3.3442304134368896\n",
      "steps = 33, loss = 3.5718743801116943\n",
      "steps = 33, loss = 7.809164047241211\n",
      "steps = 33, loss = 3.132375478744507\n",
      "steps = 33, loss = 3.0034596920013428\n",
      "steps = 33, loss = 1.9462592601776123\n",
      "steps = 33, loss = 49.89870834350586\n",
      "steps = 33, loss = 3.295520782470703\n",
      "steps = 33, loss = 2.7686500549316406\n",
      "steps = 33, loss = 2.7452666759490967\n",
      "steps = 33, loss = 2.1382486820220947\n",
      "steps = 33, loss = 3.4181272983551025\n",
      "steps = 33, loss = 3.3068459033966064\n",
      "steps = 33, loss = 2.1020586490631104\n",
      "steps = 33, loss = 3.3856797218322754\n",
      "steps = 33, loss = 2.586305618286133\n",
      "steps = 33, loss = 2.8332579135894775\n",
      "steps = 33, loss = 2.804725170135498\n",
      "steps = 33, loss = 2.401723861694336\n",
      "steps = 33, loss = 2.789912462234497\n",
      "steps = 33, loss = 2.8630173206329346\n",
      "steps = 33, loss = 3.2232418060302734\n",
      "steps = 33, loss = 3.502598762512207\n",
      "steps = 33, loss = 49.932823181152344\n",
      "steps = 33, loss = 2.8803141117095947\n",
      "steps = 33, loss = 49.985382080078125\n",
      "steps = 33, loss = 9.575247764587402\n",
      "steps = 33, loss = 2.6216611862182617\n",
      "steps = 33, loss = 2.7307233810424805\n",
      "steps = 33, loss = 2.8071603775024414\n",
      "steps = 33, loss = 3.446033000946045\n",
      "steps = 34, loss = 2.590780735015869\n",
      "steps = 34, loss = 2.1279540061950684\n",
      "steps = 34, loss = 2.722476005554199\n",
      "steps = 34, loss = 2.7291817665100098\n",
      "steps = 34, loss = 3.242222309112549\n",
      "steps = 34, loss = 3.434976816177368\n",
      "steps = 34, loss = 3.272813081741333\n",
      "steps = 34, loss = 3.3748111724853516\n",
      "steps = 34, loss = 2.633028030395508\n",
      "steps = 34, loss = 2.9911067485809326\n",
      "steps = 34, loss = 3.4286108016967773\n",
      "steps = 34, loss = 3.3988289833068848\n",
      "steps = 34, loss = 2.899569034576416\n",
      "steps = 34, loss = 49.985382080078125\n",
      "steps = 34, loss = 2.5869550704956055\n",
      "steps = 34, loss = 2.1337575912475586\n",
      "steps = 34, loss = 2.4047064781188965\n",
      "steps = 34, loss = 2.637528419494629\n",
      "steps = 34, loss = 2.0893099308013916\n",
      "steps = 34, loss = 49.96520233154297\n",
      "steps = 34, loss = 6.7639665603637695\n",
      "steps = 34, loss = 8.899688720703125\n",
      "steps = 34, loss = 2.738579034805298\n",
      "steps = 34, loss = 2.0522360801696777\n",
      "steps = 34, loss = 2.9441351890563965\n",
      "steps = 34, loss = 2.8026130199432373\n",
      "steps = 34, loss = 2.8633134365081787\n",
      "steps = 34, loss = 49.91306686401367\n",
      "steps = 34, loss = 3.4454891681671143\n",
      "steps = 34, loss = 3.13850736618042\n",
      "steps = 34, loss = 3.521695852279663\n",
      "steps = 34, loss = 3.4903008937835693\n",
      "steps = 35, loss = 2.147892951965332\n",
      "steps = 35, loss = 2.617130756378174\n",
      "steps = 35, loss = 2.1535210609436035\n",
      "steps = 35, loss = 6.692688941955566\n",
      "steps = 35, loss = 49.9288215637207\n",
      "steps = 35, loss = 2.919980525970459\n",
      "steps = 35, loss = 3.4174129962921143\n",
      "steps = 35, loss = 2.8033924102783203\n",
      "steps = 35, loss = 3.336198091506958\n",
      "steps = 35, loss = 3.4058921337127686\n",
      "steps = 35, loss = 2.812445640563965\n",
      "steps = 35, loss = 2.8279693126678467\n",
      "steps = 35, loss = 3.1946587562561035\n",
      "steps = 35, loss = 2.4326589107513428\n",
      "steps = 35, loss = 3.0029454231262207\n",
      "steps = 35, loss = 2.870276927947998\n",
      "steps = 35, loss = 3.1859679222106934\n",
      "steps = 35, loss = 3.175650119781494\n",
      "steps = 35, loss = 49.942962646484375\n",
      "steps = 35, loss = 3.3790414333343506\n",
      "steps = 35, loss = 3.387887477874756\n",
      "steps = 35, loss = 49.985382080078125\n",
      "steps = 35, loss = 2.6812896728515625\n",
      "steps = 35, loss = 2.827021360397339\n",
      "steps = 35, loss = 2.031139850616455\n",
      "steps = 35, loss = 8.95146369934082\n",
      "steps = 35, loss = 2.1419456005096436\n",
      "steps = 35, loss = 2.712390899658203\n",
      "steps = 35, loss = 3.378099203109741\n",
      "steps = 35, loss = 2.8626060485839844\n",
      "steps = 35, loss = 3.220069169998169\n",
      "steps = 35, loss = 2.7935543060302734\n",
      "steps = 36, loss = 2.578601360321045\n",
      "steps = 36, loss = 3.0149521827697754\n",
      "steps = 36, loss = 2.6293163299560547\n",
      "steps = 36, loss = 49.95791244506836\n",
      "steps = 36, loss = 2.1013617515563965\n",
      "steps = 36, loss = 3.2192835807800293\n",
      "steps = 36, loss = 1.9740583896636963\n",
      "steps = 36, loss = 2.8608176708221436\n",
      "steps = 36, loss = 2.6497037410736084\n",
      "steps = 36, loss = 3.3404104709625244\n",
      "steps = 36, loss = 3.045248508453369\n",
      "steps = 36, loss = 2.162754535675049\n",
      "steps = 36, loss = 49.94388198852539\n",
      "steps = 36, loss = 49.985382080078125\n",
      "steps = 36, loss = 3.515165090560913\n",
      "steps = 36, loss = 3.3740851879119873\n",
      "steps = 36, loss = 3.789069414138794\n",
      "steps = 36, loss = 2.4562346935272217\n",
      "steps = 36, loss = 3.478734254837036\n",
      "steps = 36, loss = 9.834766387939453\n",
      "steps = 36, loss = 3.3956477642059326\n",
      "steps = 36, loss = 3.317528486251831\n",
      "steps = 36, loss = 2.6398849487304688\n",
      "steps = 36, loss = 2.730651617050171\n",
      "steps = 36, loss = 37.20448303222656\n",
      "steps = 36, loss = 3.482788324356079\n",
      "steps = 36, loss = 2.9623658657073975\n",
      "steps = 36, loss = 3.0004565715789795\n",
      "steps = 36, loss = 2.8796732425689697\n",
      "steps = 36, loss = 3.4812686443328857\n",
      "steps = 36, loss = 2.7455646991729736\n",
      "steps = 36, loss = 20.93596649169922\n",
      "steps = 37, loss = 2.10412859916687\n",
      "steps = 37, loss = 5.419936656951904\n",
      "steps = 37, loss = 3.5743026733398438\n",
      "steps = 37, loss = 2.7504031658172607\n",
      "steps = 37, loss = 2.7691307067871094\n",
      "steps = 37, loss = 2.0970027446746826\n",
      "steps = 37, loss = 3.4273879528045654\n",
      "steps = 37, loss = 2.475119113922119\n",
      "steps = 37, loss = 9.441439628601074\n",
      "steps = 37, loss = 2.6583738327026367\n",
      "steps = 37, loss = 2.173137664794922\n",
      "steps = 37, loss = 3.027170419692993\n",
      "steps = 37, loss = 49.900569915771484\n",
      "steps = 37, loss = 2.6931161880493164\n",
      "steps = 37, loss = 3.0909359455108643\n",
      "steps = 37, loss = 2.5823166370391846\n",
      "steps = 37, loss = 3.921933889389038\n",
      "steps = 37, loss = 3.2516725063323975\n",
      "steps = 37, loss = 49.985382080078125\n",
      "steps = 37, loss = 49.93724822998047\n",
      "steps = 37, loss = 3.0192503929138184\n",
      "steps = 37, loss = 2.7328202724456787\n",
      "steps = 37, loss = 3.5069339275360107\n",
      "steps = 37, loss = 3.4487669467926025\n",
      "steps = 37, loss = 2.8807079792022705\n",
      "steps = 37, loss = 2.764070749282837\n",
      "steps = 37, loss = 3.3390674591064453\n",
      "steps = 37, loss = 3.4000327587127686\n",
      "steps = 37, loss = 3.4762237071990967\n",
      "steps = 37, loss = 2.2755680084228516\n",
      "steps = 37, loss = 2.8832809925079346\n",
      "steps = 37, loss = 3.6415820121765137\n",
      "steps = 38, loss = 2.692735195159912\n",
      "steps = 38, loss = 3.7113544940948486\n",
      "steps = 38, loss = 3.250943660736084\n",
      "steps = 38, loss = 2.369800329208374\n",
      "steps = 38, loss = 3.3972578048706055\n",
      "steps = 38, loss = 3.6567137241363525\n",
      "steps = 38, loss = 2.8957769870758057\n",
      "steps = 38, loss = 49.985382080078125\n",
      "steps = 38, loss = 2.6722042560577393\n",
      "steps = 38, loss = 2.645829916000366\n",
      "steps = 38, loss = 47.026119232177734\n",
      "steps = 38, loss = 2.139751434326172\n",
      "steps = 38, loss = 3.4613800048828125\n",
      "steps = 38, loss = 3.2777185440063477\n",
      "steps = 38, loss = 9.393548965454102\n",
      "steps = 38, loss = 2.489619731903076\n",
      "steps = 38, loss = 3.4967942237854004\n",
      "steps = 38, loss = 2.8847122192382812\n",
      "steps = 38, loss = 2.0616345405578613\n",
      "steps = 38, loss = 2.863154888153076\n",
      "steps = 38, loss = 3.829665422439575\n",
      "steps = 38, loss = 2.357356071472168\n",
      "steps = 38, loss = 49.95043182373047\n",
      "steps = 38, loss = 3.2080206871032715\n",
      "steps = 38, loss = 2.6102187633514404\n",
      "steps = 38, loss = 2.804182767868042\n",
      "steps = 38, loss = 3.0141775608062744\n",
      "steps = 38, loss = 3.492361068725586\n",
      "steps = 38, loss = 2.6039533615112305\n",
      "steps = 38, loss = 2.7103750705718994\n",
      "steps = 38, loss = 3.617964029312134\n",
      "steps = 38, loss = 2.177985191345215\n",
      "steps = 39, loss = 49.945247650146484\n",
      "steps = 39, loss = 3.2287654876708984\n",
      "steps = 39, loss = 6.103710651397705\n",
      "steps = 39, loss = 3.7272205352783203\n",
      "steps = 39, loss = 2.4932918548583984\n",
      "steps = 39, loss = 3.0201833248138428\n",
      "steps = 39, loss = 3.386268138885498\n",
      "steps = 39, loss = 3.4503729343414307\n",
      "steps = 39, loss = 3.3097753524780273\n",
      "steps = 39, loss = 2.6770193576812744\n",
      "steps = 39, loss = 3.35109806060791\n",
      "steps = 39, loss = 2.559438943862915\n",
      "steps = 39, loss = 2.1758627891540527\n",
      "steps = 39, loss = 2.121131420135498\n",
      "steps = 39, loss = 2.0911571979522705\n",
      "steps = 39, loss = 2.6994659900665283\n",
      "steps = 39, loss = 2.6942403316497803\n",
      "steps = 39, loss = 3.625577926635742\n",
      "steps = 39, loss = 49.985382080078125\n",
      "steps = 39, loss = 2.151564121246338\n",
      "steps = 39, loss = 3.116835594177246\n",
      "steps = 39, loss = 3.282003879547119\n",
      "steps = 39, loss = 2.7176167964935303\n",
      "steps = 39, loss = 2.7187230587005615\n",
      "steps = 39, loss = 2.6192336082458496\n",
      "steps = 39, loss = 2.891758918762207\n",
      "steps = 39, loss = 3.0061099529266357\n",
      "steps = 39, loss = 8.984987258911133\n",
      "steps = 39, loss = 3.9028260707855225\n",
      "steps = 39, loss = 2.8600029945373535\n",
      "steps = 39, loss = 3.2792422771453857\n",
      "steps = 39, loss = 3.3964903354644775\n",
      "steps = 40, loss = 2.6902177333831787\n",
      "steps = 40, loss = 49.985382080078125\n",
      "steps = 40, loss = 2.7284133434295654\n",
      "steps = 40, loss = 2.0843305587768555\n",
      "steps = 40, loss = 2.7450125217437744\n",
      "steps = 40, loss = 2.8620760440826416\n",
      "steps = 40, loss = 2.795659303665161\n",
      "steps = 40, loss = 3.471646308898926\n",
      "steps = 40, loss = 3.8453292846679688\n",
      "steps = 40, loss = 3.378420114517212\n",
      "steps = 40, loss = 3.410104513168335\n",
      "steps = 40, loss = 2.1805989742279053\n",
      "steps = 40, loss = 10.049159049987793\n",
      "steps = 40, loss = 3.3668506145477295\n",
      "steps = 40, loss = 2.679072618484497\n",
      "steps = 40, loss = 49.946712493896484\n",
      "steps = 40, loss = 3.819399833679199\n",
      "steps = 40, loss = 2.8504977226257324\n",
      "steps = 40, loss = 3.3643856048583984\n",
      "steps = 40, loss = 2.993248701095581\n",
      "steps = 40, loss = 2.1483919620513916\n",
      "steps = 40, loss = 2.089423418045044\n",
      "steps = 40, loss = 2.9053189754486084\n",
      "steps = 40, loss = 2.538888692855835\n",
      "steps = 40, loss = 3.3076865673065186\n",
      "steps = 40, loss = 3.336411714553833\n",
      "steps = 40, loss = 2.720499277114868\n",
      "steps = 40, loss = 3.386080026626587\n",
      "steps = 40, loss = 3.4617233276367188\n",
      "steps = 40, loss = 3.0071403980255127\n",
      "steps = 40, loss = 2.5064258575439453\n",
      "steps = 40, loss = 2.597933292388916\n",
      "steps = 41, loss = 2.809215545654297\n",
      "steps = 41, loss = 3.3258984088897705\n",
      "steps = 41, loss = 2.270798444747925\n",
      "steps = 41, loss = 2.7155468463897705\n",
      "steps = 41, loss = 2.71114182472229\n",
      "steps = 41, loss = 2.143012523651123\n",
      "steps = 41, loss = 3.3957059383392334\n",
      "steps = 41, loss = 2.926805257797241\n",
      "steps = 41, loss = 2.534057378768921\n",
      "steps = 41, loss = 3.355593204498291\n",
      "steps = 41, loss = 3.0172462463378906\n",
      "steps = 41, loss = 3.62113618850708\n",
      "steps = 41, loss = 3.260124444961548\n",
      "steps = 41, loss = 3.573237419128418\n",
      "steps = 41, loss = 3.4509103298187256\n",
      "steps = 41, loss = 3.314736843109131\n",
      "steps = 41, loss = 49.985382080078125\n",
      "steps = 41, loss = 2.941419839859009\n",
      "steps = 41, loss = 3.4490067958831787\n",
      "steps = 41, loss = 2.815239191055298\n",
      "steps = 41, loss = 2.533865213394165\n",
      "steps = 41, loss = 2.861232280731201\n",
      "steps = 41, loss = 2.2295186519622803\n",
      "steps = 41, loss = 2.103304386138916\n",
      "steps = 41, loss = 9.938237190246582\n",
      "steps = 41, loss = 2.771512508392334\n",
      "steps = 41, loss = 2.197500705718994\n",
      "steps = 41, loss = 3.935839891433716\n",
      "steps = 41, loss = 3.772783041000366\n",
      "steps = 41, loss = 49.939430236816406\n",
      "steps = 41, loss = 2.6994481086730957\n",
      "steps = 41, loss = 3.110707998275757\n",
      "steps = 42, loss = 3.0268235206604004\n",
      "steps = 42, loss = 4.05814266204834\n",
      "steps = 42, loss = 3.80521297454834\n",
      "steps = 42, loss = 2.959678888320923\n",
      "steps = 42, loss = 9.847623825073242\n",
      "steps = 42, loss = 3.5904810428619385\n",
      "steps = 42, loss = 3.3793246746063232\n",
      "steps = 42, loss = 2.0996289253234863\n",
      "steps = 42, loss = 2.4747278690338135\n",
      "steps = 42, loss = 2.7458295822143555\n",
      "steps = 42, loss = 3.3850390911102295\n",
      "steps = 42, loss = 4.02173376083374\n",
      "steps = 42, loss = 2.155287981033325\n",
      "steps = 42, loss = 2.1243765354156494\n",
      "steps = 42, loss = 3.400167942047119\n",
      "steps = 42, loss = 3.3910298347473145\n",
      "steps = 42, loss = 2.730268955230713\n",
      "steps = 42, loss = 2.736638307571411\n",
      "steps = 42, loss = 2.7410476207733154\n",
      "steps = 42, loss = 2.823540210723877\n",
      "steps = 42, loss = 2.5558674335479736\n",
      "steps = 42, loss = 2.6642773151397705\n",
      "steps = 42, loss = 3.5502490997314453\n",
      "steps = 42, loss = 2.660832166671753\n",
      "steps = 42, loss = 2.2135257720947266\n",
      "steps = 42, loss = 49.902591705322266\n",
      "steps = 42, loss = 49.985382080078125\n",
      "steps = 42, loss = 3.47737193107605\n",
      "steps = 42, loss = 3.36171817779541\n",
      "steps = 42, loss = 3.0531673431396484\n",
      "steps = 42, loss = 2.740312099456787\n",
      "steps = 42, loss = 2.877631902694702\n",
      "steps = 43, loss = 3.3968801498413086\n",
      "steps = 43, loss = 2.867421865463257\n",
      "steps = 43, loss = 2.2204391956329346\n",
      "steps = 43, loss = 2.9612274169921875\n",
      "steps = 43, loss = 3.434770107269287\n",
      "steps = 43, loss = 3.5450825691223145\n",
      "steps = 43, loss = 49.91972732543945\n",
      "steps = 43, loss = 2.093242883682251\n",
      "steps = 43, loss = 2.7503037452697754\n",
      "steps = 43, loss = 3.0215823650360107\n",
      "steps = 43, loss = 2.709754467010498\n",
      "steps = 43, loss = 3.41861891746521\n",
      "steps = 43, loss = 2.1412887573242188\n",
      "steps = 43, loss = 2.283079147338867\n",
      "steps = 43, loss = 4.141820907592773\n",
      "steps = 43, loss = 4.887361526489258\n",
      "steps = 43, loss = 2.588204860687256\n",
      "steps = 43, loss = 3.269564390182495\n",
      "steps = 43, loss = 2.724728584289551\n",
      "steps = 43, loss = 3.4207417964935303\n",
      "steps = 43, loss = 2.5030384063720703\n",
      "steps = 43, loss = 2.9657161235809326\n",
      "steps = 43, loss = 2.8320603370666504\n",
      "steps = 43, loss = 3.264061689376831\n",
      "steps = 43, loss = 10.76254653930664\n",
      "steps = 43, loss = 2.4734418392181396\n",
      "steps = 43, loss = 2.698685884475708\n",
      "steps = 43, loss = 3.489809989929199\n",
      "steps = 43, loss = 2.5696775913238525\n",
      "steps = 43, loss = 49.985382080078125\n",
      "steps = 43, loss = 3.5808398723602295\n",
      "steps = 43, loss = 2.8616409301757812\n",
      "steps = 44, loss = 2.992088556289673\n",
      "steps = 44, loss = 3.6498396396636963\n",
      "steps = 44, loss = 2.582181692123413\n",
      "steps = 44, loss = 3.523324728012085\n",
      "steps = 44, loss = 3.424928665161133\n",
      "steps = 44, loss = 3.399746894836426\n",
      "steps = 44, loss = 4.910687446594238\n",
      "steps = 44, loss = 2.7303876876831055\n",
      "steps = 44, loss = 2.4513516426086426\n",
      "steps = 44, loss = 3.451724052429199\n",
      "steps = 44, loss = 2.7711689472198486\n",
      "steps = 44, loss = 2.2490978240966797\n",
      "steps = 44, loss = 2.159449338912964\n",
      "steps = 44, loss = 3.6971371173858643\n",
      "steps = 44, loss = 3.028635025024414\n",
      "steps = 44, loss = 2.8781468868255615\n",
      "steps = 44, loss = 3.462373733520508\n",
      "steps = 44, loss = 2.589211940765381\n",
      "steps = 44, loss = 3.2437565326690674\n",
      "steps = 44, loss = 2.8828508853912354\n",
      "steps = 44, loss = 9.82254409790039\n",
      "steps = 44, loss = 49.939910888671875\n",
      "steps = 44, loss = 2.09981107711792\n",
      "steps = 44, loss = 2.7467041015625\n",
      "steps = 44, loss = 2.2343881130218506\n",
      "steps = 44, loss = 3.029531955718994\n",
      "steps = 44, loss = 3.433825731277466\n",
      "steps = 44, loss = 4.261963844299316\n",
      "steps = 44, loss = 3.0126242637634277\n",
      "steps = 44, loss = 49.985382080078125\n",
      "steps = 44, loss = 2.769228219985962\n",
      "steps = 44, loss = 2.789872646331787\n",
      "steps = 45, loss = 2.0949079990386963\n",
      "steps = 45, loss = 4.319621562957764\n",
      "steps = 45, loss = 3.385165214538574\n",
      "steps = 45, loss = 3.0142548084259033\n",
      "steps = 45, loss = 2.649585247039795\n",
      "steps = 45, loss = 3.378133773803711\n",
      "steps = 45, loss = 2.9717180728912354\n",
      "steps = 45, loss = 2.9058265686035156\n",
      "steps = 45, loss = 3.543241500854492\n",
      "steps = 45, loss = 2.090256929397583\n",
      "steps = 45, loss = 3.4554944038391113\n",
      "steps = 45, loss = 3.3497323989868164\n",
      "steps = 45, loss = 10.214010238647461\n",
      "steps = 45, loss = 4.620553970336914\n",
      "steps = 45, loss = 2.7291975021362305\n",
      "steps = 45, loss = 2.8354225158691406\n",
      "steps = 45, loss = 3.364650249481201\n",
      "steps = 45, loss = 2.885343074798584\n",
      "steps = 45, loss = 2.9848570823669434\n",
      "steps = 45, loss = 49.985382080078125\n",
      "steps = 45, loss = 3.6488442420959473\n",
      "steps = 45, loss = 2.0860040187835693\n",
      "steps = 45, loss = 2.7186312675476074\n",
      "steps = 45, loss = 3.3952043056488037\n",
      "steps = 45, loss = 2.2299296855926514\n",
      "steps = 45, loss = 2.8598055839538574\n",
      "steps = 45, loss = 49.969635009765625\n",
      "steps = 45, loss = 2.5886571407318115\n",
      "steps = 45, loss = 2.6343066692352295\n",
      "steps = 45, loss = 3.4575822353363037\n",
      "steps = 45, loss = 2.770434856414795\n",
      "steps = 45, loss = 2.5534045696258545\n",
      "steps = 46, loss = 3.1814486980438232\n",
      "steps = 46, loss = 3.5162034034729004\n",
      "steps = 46, loss = 4.969760894775391\n",
      "steps = 46, loss = 3.39676833152771\n",
      "steps = 46, loss = 3.0870306491851807\n",
      "steps = 46, loss = 2.796555995941162\n",
      "steps = 46, loss = 2.596435070037842\n",
      "steps = 46, loss = 3.5766868591308594\n",
      "steps = 46, loss = 3.0297422409057617\n",
      "steps = 46, loss = 2.797476291656494\n",
      "steps = 46, loss = 2.5430636405944824\n",
      "steps = 46, loss = 2.030148983001709\n",
      "steps = 46, loss = 3.379807949066162\n",
      "steps = 46, loss = 3.516771078109741\n",
      "steps = 46, loss = 2.732581853866577\n",
      "steps = 46, loss = 2.616743564605713\n",
      "steps = 46, loss = 3.508899450302124\n",
      "steps = 46, loss = 3.0183520317077637\n",
      "steps = 46, loss = 2.7981040477752686\n",
      "steps = 46, loss = 3.3363616466522217\n",
      "steps = 46, loss = 2.102391004562378\n",
      "steps = 46, loss = 3.4976232051849365\n",
      "steps = 46, loss = 2.6754469871520996\n",
      "steps = 46, loss = 2.8795053958892822\n",
      "steps = 46, loss = 49.93711853027344\n",
      "steps = 46, loss = 2.749699831008911\n",
      "steps = 46, loss = 2.2502777576446533\n",
      "steps = 46, loss = 3.4523465633392334\n",
      "steps = 46, loss = 49.985382080078125\n",
      "steps = 46, loss = 4.4524641036987305\n",
      "steps = 46, loss = 9.778952598571777\n",
      "steps = 46, loss = 2.1122000217437744\n",
      "steps = 47, loss = 2.185450315475464\n",
      "steps = 47, loss = 3.3533027172088623\n",
      "steps = 47, loss = 2.861151933670044\n",
      "steps = 47, loss = 2.141526699066162\n",
      "steps = 47, loss = 3.535015344619751\n",
      "steps = 47, loss = 3.365084648132324\n",
      "steps = 47, loss = 2.694709539413452\n",
      "steps = 47, loss = 3.0354673862457275\n",
      "steps = 47, loss = 2.069359064102173\n",
      "steps = 47, loss = 3.3038082122802734\n",
      "steps = 47, loss = 2.6304147243499756\n",
      "steps = 47, loss = 3.6238937377929688\n",
      "steps = 47, loss = 2.7474803924560547\n",
      "steps = 47, loss = 2.710247278213501\n",
      "steps = 47, loss = 3.392359733581543\n",
      "steps = 47, loss = 3.945295572280884\n",
      "steps = 47, loss = 2.2584054470062256\n",
      "steps = 47, loss = 49.94099044799805\n",
      "steps = 47, loss = 3.017948865890503\n",
      "steps = 47, loss = 2.516310214996338\n",
      "steps = 47, loss = 10.113798141479492\n",
      "steps = 47, loss = 4.529855251312256\n",
      "steps = 47, loss = 3.0240461826324463\n",
      "steps = 47, loss = 49.985382080078125\n",
      "steps = 47, loss = 2.910609483718872\n",
      "steps = 47, loss = 3.647747039794922\n",
      "steps = 47, loss = 2.810068130493164\n",
      "steps = 47, loss = 3.2539327144622803\n",
      "steps = 47, loss = 2.724090099334717\n",
      "steps = 47, loss = 3.5121848583221436\n",
      "steps = 47, loss = 5.489717483520508\n",
      "steps = 47, loss = 2.6054530143737793\n",
      "steps = 48, loss = 2.257477283477783\n",
      "steps = 48, loss = 2.71903395652771\n",
      "steps = 48, loss = 2.5400123596191406\n",
      "steps = 48, loss = 2.6641900539398193\n",
      "steps = 48, loss = 3.232907295227051\n",
      "steps = 48, loss = 3.3804283142089844\n",
      "steps = 48, loss = 3.014737606048584\n",
      "steps = 48, loss = 6.261621475219727\n",
      "steps = 48, loss = 2.1563303470611572\n",
      "steps = 48, loss = 3.5402109622955322\n",
      "steps = 48, loss = 2.646336317062378\n",
      "steps = 48, loss = 3.025310754776001\n",
      "steps = 48, loss = 10.020296096801758\n",
      "steps = 48, loss = 49.94292449951172\n",
      "steps = 48, loss = 3.352011203765869\n",
      "steps = 48, loss = 2.813624382019043\n",
      "steps = 48, loss = 3.1875643730163574\n",
      "steps = 48, loss = 3.3905351161956787\n",
      "steps = 48, loss = 4.6079182624816895\n",
      "steps = 48, loss = 2.0969743728637695\n",
      "steps = 48, loss = 3.2818567752838135\n",
      "steps = 48, loss = 49.985382080078125\n",
      "steps = 48, loss = 2.7223377227783203\n",
      "steps = 48, loss = 2.7174060344696045\n",
      "steps = 48, loss = 2.09149169921875\n",
      "steps = 48, loss = 2.858241081237793\n",
      "steps = 48, loss = 3.4189488887786865\n",
      "steps = 48, loss = 3.24579119682312\n",
      "steps = 48, loss = 2.6306848526000977\n",
      "steps = 48, loss = 3.593637466430664\n",
      "steps = 48, loss = 2.6327266693115234\n",
      "steps = 48, loss = 2.5370216369628906\n",
      "steps = 49, loss = 10.097685813903809\n",
      "steps = 49, loss = 4.739506244659424\n",
      "steps = 49, loss = 3.319441318511963\n",
      "steps = 49, loss = 3.6608994007110596\n",
      "steps = 49, loss = 2.2784931659698486\n",
      "steps = 49, loss = 3.3909897804260254\n",
      "steps = 49, loss = 49.93777084350586\n",
      "steps = 49, loss = 3.058218240737915\n",
      "steps = 49, loss = 3.4466564655303955\n",
      "steps = 49, loss = 2.660005807876587\n",
      "steps = 49, loss = 3.172283887863159\n",
      "steps = 49, loss = 2.87778902053833\n",
      "steps = 49, loss = 49.985382080078125\n",
      "steps = 49, loss = 2.867089033126831\n",
      "steps = 49, loss = 2.7468905448913574\n",
      "steps = 49, loss = 2.1009721755981445\n",
      "steps = 49, loss = 2.8393595218658447\n",
      "steps = 49, loss = 3.3742058277130127\n",
      "steps = 49, loss = 3.5947623252868652\n",
      "steps = 49, loss = 2.732478618621826\n",
      "steps = 49, loss = 3.033186674118042\n",
      "steps = 49, loss = 3.0296623706817627\n",
      "steps = 49, loss = 5.521394729614258\n",
      "steps = 49, loss = 3.4624481201171875\n",
      "steps = 49, loss = 3.075178384780884\n",
      "steps = 49, loss = 3.300909996032715\n",
      "steps = 49, loss = 2.0906524658203125\n",
      "steps = 49, loss = 2.1200766563415527\n",
      "steps = 49, loss = 3.1943538188934326\n",
      "steps = 49, loss = 2.6604154109954834\n",
      "steps = 49, loss = 2.712963104248047\n",
      "steps = 49, loss = 3.0997233390808105\n",
      "steps = 50, loss = 49.985382080078125\n",
      "steps = 50, loss = 2.276108980178833\n",
      "steps = 50, loss = 3.6006433963775635\n",
      "steps = 50, loss = 3.439016103744507\n",
      "steps = 50, loss = 3.015381336212158\n",
      "steps = 50, loss = 3.0517146587371826\n",
      "steps = 50, loss = 2.0902185440063477\n",
      "steps = 50, loss = 2.9286577701568604\n",
      "steps = 50, loss = 3.117137908935547\n",
      "steps = 50, loss = 2.646812915802002\n",
      "steps = 50, loss = 3.2640974521636963\n",
      "steps = 50, loss = 3.3780360221862793\n",
      "steps = 50, loss = 2.7196288108825684\n",
      "steps = 50, loss = 49.96435546875\n",
      "steps = 50, loss = 2.8415324687957764\n",
      "steps = 50, loss = 3.474266767501831\n",
      "steps = 50, loss = 3.3614344596862793\n",
      "steps = 50, loss = 2.6415305137634277\n",
      "steps = 50, loss = 43.240657806396484\n",
      "steps = 50, loss = 4.792023181915283\n",
      "steps = 50, loss = 3.362032413482666\n",
      "steps = 50, loss = 4.7980241775512695\n",
      "steps = 50, loss = 2.8600265979766846\n",
      "steps = 50, loss = 3.4300429821014404\n",
      "steps = 50, loss = 2.7318015098571777\n",
      "steps = 50, loss = 3.3314974308013916\n",
      "steps = 50, loss = 2.7195186614990234\n",
      "steps = 50, loss = 2.5396060943603516\n",
      "steps = 50, loss = 2.660569667816162\n",
      "steps = 50, loss = 10.93405532836914\n",
      "steps = 50, loss = 2.162297487258911\n",
      "steps = 50, loss = 3.0301687717437744\n",
      "steps = 51, loss = 2.0335235595703125\n",
      "steps = 51, loss = 3.2933883666992188\n",
      "steps = 51, loss = 3.0225179195404053\n",
      "steps = 51, loss = 3.3840057849884033\n",
      "steps = 51, loss = 3.587949752807617\n",
      "steps = 51, loss = 2.866574764251709\n",
      "steps = 51, loss = 2.7113547325134277\n",
      "steps = 51, loss = 2.14408016204834\n",
      "steps = 51, loss = 2.8594746589660645\n",
      "steps = 51, loss = 3.0656356811523438\n",
      "steps = 51, loss = 5.2503156661987305\n",
      "steps = 51, loss = 3.0074472427368164\n",
      "steps = 51, loss = 1.965354084968567\n",
      "steps = 51, loss = 3.4586358070373535\n",
      "steps = 51, loss = 3.3271145820617676\n",
      "steps = 51, loss = 2.2934587001800537\n",
      "steps = 51, loss = 2.6846237182617188\n",
      "steps = 51, loss = 49.985382080078125\n",
      "steps = 51, loss = 3.6446638107299805\n",
      "steps = 51, loss = 4.892952919006348\n",
      "steps = 51, loss = 2.802149534225464\n",
      "steps = 51, loss = 3.4644715785980225\n",
      "steps = 51, loss = 2.8642051219940186\n",
      "steps = 51, loss = 3.383229970932007\n",
      "steps = 51, loss = 2.7475790977478027\n",
      "steps = 51, loss = 48.64124298095703\n",
      "steps = 51, loss = 3.1483395099639893\n",
      "steps = 51, loss = 2.6101486682891846\n",
      "steps = 51, loss = 2.977933645248413\n",
      "steps = 51, loss = 3.2915420532226562\n",
      "steps = 51, loss = 10.21950912475586\n",
      "steps = 51, loss = 2.6986923217773438\n",
      "steps = 52, loss = 3.372709035873413\n",
      "steps = 52, loss = 2.514225482940674\n",
      "steps = 52, loss = 3.5035288333892822\n",
      "steps = 52, loss = 6.264426231384277\n",
      "steps = 52, loss = 3.688178300857544\n",
      "steps = 52, loss = 2.091665506362915\n",
      "steps = 52, loss = 4.972545146942139\n",
      "steps = 52, loss = 3.423278331756592\n",
      "steps = 52, loss = 2.270322561264038\n",
      "steps = 52, loss = 2.723527431488037\n",
      "steps = 52, loss = 3.6534199714660645\n",
      "steps = 52, loss = 2.525364875793457\n",
      "steps = 52, loss = 49.985382080078125\n",
      "steps = 52, loss = 2.4443914890289307\n",
      "steps = 52, loss = 2.1452860832214355\n",
      "steps = 52, loss = 2.7177884578704834\n",
      "steps = 52, loss = 3.2975804805755615\n",
      "steps = 52, loss = 3.075195550918579\n",
      "steps = 52, loss = 2.86875581741333\n",
      "steps = 52, loss = 6.444321155548096\n",
      "steps = 52, loss = 2.971112012863159\n",
      "steps = 52, loss = 2.6879966259002686\n",
      "steps = 52, loss = 3.0142393112182617\n",
      "steps = 52, loss = 3.26516056060791\n",
      "steps = 52, loss = 3.3412535190582275\n",
      "steps = 52, loss = 2.8582231998443604\n",
      "steps = 52, loss = 2.7178735733032227\n",
      "steps = 52, loss = 3.395880699157715\n",
      "steps = 52, loss = 3.1828224658966064\n",
      "steps = 52, loss = 2.7366271018981934\n",
      "steps = 52, loss = 10.675810813903809\n",
      "steps = 52, loss = 2.2941884994506836\n",
      "steps = 53, loss = 10.598589897155762\n",
      "steps = 53, loss = 2.732999086380005\n",
      "steps = 53, loss = 5.101190567016602\n",
      "steps = 53, loss = 2.7159464359283447\n",
      "steps = 53, loss = 49.985382080078125\n",
      "steps = 53, loss = 2.6747589111328125\n",
      "steps = 53, loss = 3.1074602603912354\n",
      "steps = 53, loss = 2.5644171237945557\n",
      "steps = 53, loss = 2.8945741653442383\n",
      "steps = 53, loss = 3.568845272064209\n",
      "steps = 53, loss = 2.5766565799713135\n",
      "steps = 53, loss = 2.758432626724243\n",
      "steps = 53, loss = 6.22010612487793\n",
      "steps = 53, loss = 2.751868486404419\n",
      "steps = 53, loss = 3.5897164344787598\n",
      "steps = 53, loss = 3.440922498703003\n",
      "steps = 53, loss = 2.099891185760498\n",
      "steps = 53, loss = 3.421060800552368\n",
      "steps = 53, loss = 2.266692876815796\n",
      "steps = 53, loss = 2.8762195110321045\n",
      "steps = 53, loss = 3.3949942588806152\n",
      "steps = 53, loss = 3.1615641117095947\n",
      "steps = 53, loss = 2.9711763858795166\n",
      "steps = 53, loss = 3.3692374229431152\n",
      "steps = 53, loss = 2.315911054611206\n",
      "steps = 53, loss = 2.7967848777770996\n",
      "steps = 53, loss = 2.2560715675354004\n",
      "steps = 53, loss = 3.7074015140533447\n",
      "steps = 53, loss = 3.028233528137207\n",
      "steps = 53, loss = 2.9560205936431885\n",
      "steps = 53, loss = 3.546705484390259\n",
      "steps = 53, loss = 3.3823587894439697\n",
      "steps = 54, loss = 6.352252006530762\n",
      "steps = 54, loss = 3.3780486583709717\n",
      "steps = 54, loss = 2.6466519832611084\n",
      "steps = 54, loss = 2.7290518283843994\n",
      "steps = 54, loss = 2.8748538494110107\n",
      "steps = 54, loss = 5.157354831695557\n",
      "steps = 54, loss = 2.550015449523926\n",
      "steps = 54, loss = 3.4537365436553955\n",
      "steps = 54, loss = 10.661639213562012\n",
      "steps = 54, loss = 3.4619674682617188\n",
      "steps = 54, loss = 3.0226948261260986\n",
      "steps = 54, loss = 2.199007034301758\n",
      "steps = 54, loss = 3.7336442470550537\n",
      "steps = 54, loss = 2.1144752502441406\n",
      "steps = 54, loss = 3.4445242881774902\n",
      "steps = 54, loss = 3.6711783409118652\n",
      "steps = 54, loss = 3.1039562225341797\n",
      "steps = 54, loss = 3.3697736263275146\n",
      "steps = 54, loss = 2.1423399448394775\n",
      "steps = 54, loss = 3.545835018157959\n",
      "steps = 54, loss = 2.96122145652771\n",
      "steps = 54, loss = 2.9080591201782227\n",
      "steps = 54, loss = 2.6323623657226562\n",
      "steps = 54, loss = 2.859368324279785\n",
      "steps = 54, loss = 2.325216293334961\n",
      "steps = 54, loss = 49.985382080078125\n",
      "steps = 54, loss = 2.7114288806915283\n",
      "steps = 54, loss = 3.5205636024475098\n",
      "steps = 54, loss = 2.848207712173462\n",
      "steps = 54, loss = 3.4002413749694824\n",
      "steps = 54, loss = 2.698831081390381\n",
      "steps = 54, loss = 2.3801095485687256\n",
      "steps = 55, loss = 2.857247829437256\n",
      "steps = 55, loss = 3.0133895874023438\n",
      "steps = 55, loss = 3.457857847213745\n",
      "steps = 55, loss = 2.6625185012817383\n",
      "steps = 55, loss = 2.902085781097412\n",
      "steps = 55, loss = 6.506761074066162\n",
      "steps = 55, loss = 2.3243610858917236\n",
      "steps = 55, loss = 3.5426559448242188\n",
      "steps = 55, loss = 2.7262685298919678\n",
      "steps = 55, loss = 3.5318186283111572\n",
      "steps = 55, loss = 2.1097397804260254\n",
      "steps = 55, loss = 2.911205530166626\n",
      "steps = 55, loss = 2.7309939861297607\n",
      "steps = 55, loss = 3.112199306488037\n",
      "steps = 55, loss = 49.94002151489258\n",
      "steps = 55, loss = 2.5771148204803467\n",
      "steps = 55, loss = 3.3311192989349365\n",
      "steps = 55, loss = 3.347461223602295\n",
      "steps = 55, loss = 3.740053415298462\n",
      "steps = 55, loss = 2.715073585510254\n",
      "steps = 55, loss = 49.985382080078125\n",
      "steps = 55, loss = 2.2347426414489746\n",
      "steps = 55, loss = 11.50497817993164\n",
      "steps = 55, loss = 3.4244959354400635\n",
      "steps = 55, loss = 2.7173101902008057\n",
      "steps = 55, loss = 3.366417169570923\n",
      "steps = 55, loss = 2.115997791290283\n",
      "steps = 55, loss = 5.2281270027160645\n",
      "steps = 55, loss = 2.6419119834899902\n",
      "steps = 55, loss = 3.0363481044769287\n",
      "steps = 55, loss = 3.6147210597991943\n",
      "steps = 55, loss = 2.0913450717926025\n",
      "steps = 56, loss = 5.354307651519775\n",
      "steps = 56, loss = 3.3754892349243164\n",
      "steps = 56, loss = 2.3454830646514893\n",
      "steps = 56, loss = 2.758800745010376\n",
      "steps = 56, loss = 2.124708652496338\n",
      "steps = 56, loss = 6.059261798858643\n",
      "steps = 56, loss = 49.941497802734375\n",
      "steps = 56, loss = 49.985382080078125\n",
      "steps = 56, loss = 2.6172420978546143\n",
      "steps = 56, loss = 2.733227252960205\n",
      "steps = 56, loss = 3.7934296131134033\n",
      "steps = 56, loss = 3.1432607173919678\n",
      "steps = 56, loss = 2.8100852966308594\n",
      "steps = 56, loss = 2.335094690322876\n",
      "steps = 56, loss = 3.3827707767486572\n",
      "steps = 56, loss = 3.601236581802368\n",
      "steps = 56, loss = 2.642206907272339\n",
      "steps = 56, loss = 10.718924522399902\n",
      "steps = 56, loss = 2.919809103012085\n",
      "steps = 56, loss = 2.0243353843688965\n",
      "steps = 56, loss = 2.936325788497925\n",
      "steps = 56, loss = 3.581062078475952\n",
      "steps = 56, loss = 2.8761050701141357\n",
      "steps = 56, loss = 3.4691884517669678\n",
      "steps = 56, loss = 2.7529165744781494\n",
      "steps = 56, loss = 2.099769353866577\n",
      "steps = 56, loss = 3.4523441791534424\n",
      "steps = 56, loss = 3.532755136489868\n",
      "steps = 56, loss = 3.3635146617889404\n",
      "steps = 56, loss = 3.0266644954681396\n",
      "steps = 56, loss = 2.744048595428467\n",
      "steps = 56, loss = 2.875613212585449\n",
      "steps = 57, loss = 2.9559035301208496\n",
      "steps = 57, loss = 2.66253924369812\n",
      "steps = 57, loss = 49.985382080078125\n",
      "steps = 57, loss = 3.3712427616119385\n",
      "steps = 57, loss = 7.162752628326416\n",
      "steps = 57, loss = 3.6451456546783447\n",
      "steps = 57, loss = 3.5169456005096436\n",
      "steps = 57, loss = 3.3987512588500977\n",
      "steps = 57, loss = 5.398378372192383\n",
      "steps = 57, loss = 2.77132511138916\n",
      "steps = 57, loss = 2.62085223197937\n",
      "steps = 57, loss = 3.5145347118377686\n",
      "steps = 57, loss = 10.972444534301758\n",
      "steps = 57, loss = 3.3484678268432617\n",
      "steps = 57, loss = 2.6994500160217285\n",
      "steps = 57, loss = 3.8186535835266113\n",
      "steps = 57, loss = 2.2188544273376465\n",
      "steps = 57, loss = 2.949469804763794\n",
      "steps = 57, loss = 3.1382503509521484\n",
      "steps = 57, loss = 3.3771140575408936\n",
      "steps = 57, loss = 2.6827003955841064\n",
      "steps = 57, loss = 3.021085500717163\n",
      "steps = 57, loss = 3.1806676387786865\n",
      "steps = 57, loss = 2.1424918174743652\n",
      "steps = 57, loss = 2.557325601577759\n",
      "steps = 57, loss = 2.7843663692474365\n",
      "steps = 57, loss = 3.387057304382324\n",
      "steps = 57, loss = 49.94626998901367\n",
      "steps = 57, loss = 2.3545167446136475\n",
      "steps = 57, loss = 2.858832597732544\n",
      "steps = 57, loss = 2.7119030952453613\n",
      "steps = 57, loss = 2.0292646884918213\n",
      "steps = 58, loss = 2.5893442630767822\n",
      "steps = 58, loss = 6.8619818687438965\n",
      "steps = 58, loss = 2.84709095954895\n",
      "steps = 58, loss = 2.7860097885131836\n",
      "steps = 58, loss = 2.6828956604003906\n",
      "steps = 58, loss = 2.9273204803466797\n",
      "steps = 58, loss = 49.88622283935547\n",
      "steps = 58, loss = 3.844135046005249\n",
      "steps = 58, loss = 2.711127996444702\n",
      "steps = 58, loss = 3.7010703086853027\n",
      "steps = 58, loss = 2.3647046089172363\n",
      "steps = 58, loss = 2.1459875106811523\n",
      "steps = 58, loss = 3.2784860134124756\n",
      "steps = 58, loss = 2.2232210636138916\n",
      "steps = 58, loss = 3.3683977127075195\n",
      "steps = 58, loss = 3.534196615219116\n",
      "steps = 58, loss = 2.046328544616699\n",
      "steps = 58, loss = 5.478538990020752\n",
      "steps = 58, loss = 3.149296283721924\n",
      "steps = 58, loss = 3.0201170444488525\n",
      "steps = 58, loss = 2.858095645904541\n",
      "steps = 58, loss = 3.3210151195526123\n",
      "steps = 58, loss = 2.963587760925293\n",
      "steps = 58, loss = 3.6060986518859863\n",
      "steps = 58, loss = 11.33976936340332\n",
      "steps = 58, loss = 2.9559645652770996\n",
      "steps = 58, loss = 49.985382080078125\n",
      "steps = 58, loss = 3.660956859588623\n",
      "steps = 58, loss = 3.3246662616729736\n",
      "steps = 58, loss = 2.5123236179351807\n",
      "steps = 58, loss = 2.725627899169922\n",
      "steps = 58, loss = 2.906195640563965\n",
      "steps = 59, loss = 2.8551576137542725\n",
      "steps = 59, loss = 3.033142328262329\n",
      "steps = 59, loss = 6.788549423217773\n",
      "steps = 59, loss = 3.0307531356811523\n",
      "steps = 59, loss = 5.544373512268066\n",
      "steps = 59, loss = 3.3907546997070312\n",
      "steps = 59, loss = 2.3630189895629883\n",
      "steps = 59, loss = 12.227304458618164\n",
      "steps = 59, loss = 3.359593391418457\n",
      "steps = 59, loss = 2.091736078262329\n",
      "steps = 59, loss = 3.356459617614746\n",
      "steps = 59, loss = 3.8514046669006348\n",
      "steps = 59, loss = 3.0526340007781982\n",
      "steps = 59, loss = 3.2731127738952637\n",
      "steps = 59, loss = 3.3488850593566895\n",
      "steps = 59, loss = 3.5778372287750244\n",
      "steps = 59, loss = 2.7869417667388916\n",
      "steps = 59, loss = 3.580312490463257\n",
      "steps = 59, loss = 3.157379627227783\n",
      "steps = 59, loss = 2.669196844100952\n",
      "steps = 59, loss = 3.010730504989624\n",
      "steps = 59, loss = 2.2295548915863037\n",
      "steps = 59, loss = 2.716799736022949\n",
      "steps = 59, loss = 2.6666171550750732\n",
      "steps = 59, loss = 49.929683685302734\n",
      "steps = 59, loss = 2.899528980255127\n",
      "steps = 59, loss = 49.985382080078125\n",
      "steps = 59, loss = 2.965665102005005\n",
      "steps = 59, loss = 2.6308505535125732\n",
      "steps = 59, loss = 3.504079580307007\n",
      "steps = 59, loss = 2.089186429977417\n",
      "steps = 59, loss = 2.7263996601104736\n",
      "steps = 60, loss = 3.9046428203582764\n",
      "steps = 60, loss = 7.959029674530029\n",
      "steps = 60, loss = 3.586663007736206\n",
      "steps = 60, loss = 2.670076847076416\n",
      "steps = 60, loss = 3.414595127105713\n",
      "steps = 60, loss = 3.3509140014648438\n",
      "steps = 60, loss = 2.0718517303466797\n",
      "steps = 60, loss = 2.770038366317749\n",
      "steps = 60, loss = 49.985382080078125\n",
      "steps = 60, loss = 3.456925868988037\n",
      "steps = 60, loss = 2.990758180618286\n",
      "steps = 60, loss = 3.500528335571289\n",
      "steps = 60, loss = 3.1879148483276367\n",
      "steps = 60, loss = 11.462407112121582\n",
      "steps = 60, loss = 3.0233843326568604\n",
      "steps = 60, loss = 2.8145546913146973\n",
      "steps = 60, loss = 2.9346671104431152\n",
      "steps = 60, loss = 2.0878384113311768\n",
      "steps = 60, loss = 3.583730936050415\n",
      "steps = 60, loss = 6.786870956420898\n",
      "steps = 60, loss = 2.383511543273926\n",
      "steps = 60, loss = 2.8735733032226562\n",
      "steps = 60, loss = 2.752777576446533\n",
      "steps = 60, loss = 2.099335193634033\n",
      "steps = 60, loss = 2.733367443084717\n",
      "steps = 60, loss = 5.673201560974121\n",
      "steps = 60, loss = 2.6233232021331787\n",
      "steps = 60, loss = 2.824420928955078\n",
      "steps = 60, loss = 3.364987850189209\n",
      "steps = 60, loss = 2.841238260269165\n",
      "steps = 60, loss = 3.0466814041137695\n",
      "steps = 60, loss = 3.4491019248962402\n",
      "steps = 61, loss = 3.9290215969085693\n",
      "steps = 61, loss = 5.696025848388672\n",
      "steps = 61, loss = 2.710986614227295\n",
      "steps = 61, loss = 3.004456043243408\n",
      "steps = 61, loss = 2.142249584197998\n",
      "steps = 61, loss = 3.360724925994873\n",
      "steps = 61, loss = 2.6646804809570312\n",
      "steps = 61, loss = 2.687429666519165\n",
      "steps = 61, loss = 2.8580899238586426\n",
      "steps = 61, loss = 3.181657075881958\n",
      "steps = 61, loss = 2.7123055458068848\n",
      "steps = 61, loss = 2.506465435028076\n",
      "steps = 61, loss = 3.3609111309051514\n",
      "steps = 61, loss = 12.328024864196777\n",
      "steps = 61, loss = 3.3585288524627686\n",
      "steps = 61, loss = 3.0791172981262207\n",
      "steps = 61, loss = 3.414796829223633\n",
      "steps = 61, loss = 3.592989206314087\n",
      "steps = 61, loss = 3.3752894401550293\n",
      "steps = 61, loss = 3.5026185512542725\n",
      "steps = 61, loss = 2.934624671936035\n",
      "steps = 61, loss = 2.784463405609131\n",
      "steps = 61, loss = 2.022038459777832\n",
      "steps = 61, loss = 2.2167294025421143\n",
      "steps = 61, loss = 2.6698687076568604\n",
      "steps = 61, loss = 3.5446088314056396\n",
      "steps = 61, loss = 7.63471794128418\n",
      "steps = 61, loss = 3.0179240703582764\n",
      "steps = 61, loss = 2.826814889907837\n",
      "steps = 61, loss = 49.985382080078125\n",
      "steps = 61, loss = 3.386552572250366\n",
      "steps = 61, loss = 2.39219069480896\n",
      "steps = 62, loss = 3.349134922027588\n",
      "steps = 62, loss = 2.1821768283843994\n",
      "steps = 62, loss = 49.985382080078125\n",
      "steps = 62, loss = 2.7176356315612793\n",
      "steps = 62, loss = 2.0198521614074707\n",
      "steps = 62, loss = 3.3184361457824707\n",
      "steps = 62, loss = 2.7238545417785645\n",
      "steps = 62, loss = 2.390326738357544\n",
      "steps = 62, loss = 2.8171792030334473\n",
      "steps = 62, loss = 6.939410209655762\n",
      "steps = 62, loss = 2.556100606918335\n",
      "steps = 62, loss = 3.340345859527588\n",
      "steps = 62, loss = 5.759068965911865\n",
      "steps = 62, loss = 11.793508529663086\n",
      "steps = 62, loss = 3.190660238265991\n",
      "steps = 62, loss = 3.405348300933838\n",
      "steps = 62, loss = 3.334970712661743\n",
      "steps = 62, loss = 3.0084176063537598\n",
      "steps = 62, loss = 2.593874216079712\n",
      "steps = 62, loss = 3.5980424880981445\n",
      "steps = 62, loss = 2.828174352645874\n",
      "steps = 62, loss = 2.733834743499756\n",
      "steps = 62, loss = 3.012932538986206\n",
      "steps = 62, loss = 3.00703763961792\n",
      "steps = 62, loss = 2.630059242248535\n",
      "steps = 62, loss = 3.9372222423553467\n",
      "steps = 62, loss = 2.0913290977478027\n",
      "steps = 62, loss = 2.855177402496338\n",
      "steps = 62, loss = 3.4504408836364746\n",
      "steps = 62, loss = 2.982640027999878\n",
      "steps = 62, loss = 2.6683435440063477\n",
      "steps = 62, loss = 3.493149518966675\n",
      "steps = 63, loss = 2.6608667373657227\n",
      "steps = 63, loss = 2.734074831008911\n",
      "steps = 63, loss = 2.855464220046997\n",
      "steps = 63, loss = 2.7785396575927734\n",
      "steps = 63, loss = 3.5458362102508545\n",
      "steps = 63, loss = 3.990196704864502\n",
      "steps = 63, loss = 2.753873825073242\n",
      "steps = 63, loss = 3.0313241481781006\n",
      "steps = 63, loss = 3.5868990421295166\n",
      "steps = 63, loss = 2.050208330154419\n",
      "steps = 63, loss = 3.3578908443450928\n",
      "steps = 63, loss = 49.985382080078125\n",
      "steps = 63, loss = 11.735062599182129\n",
      "steps = 63, loss = 3.465888738632202\n",
      "steps = 63, loss = 2.4104652404785156\n",
      "steps = 63, loss = 3.397949457168579\n",
      "steps = 63, loss = 5.891345977783203\n",
      "steps = 63, loss = 3.220303773880005\n",
      "steps = 63, loss = 2.581598997116089\n",
      "steps = 63, loss = 2.0384740829467773\n",
      "steps = 63, loss = 2.7883996963500977\n",
      "steps = 63, loss = 2.8114120960235596\n",
      "steps = 63, loss = 3.4529478549957275\n",
      "steps = 63, loss = 3.347606897354126\n",
      "steps = 63, loss = 3.0036654472351074\n",
      "steps = 63, loss = 3.020784378051758\n",
      "steps = 63, loss = 6.621089458465576\n",
      "steps = 63, loss = 2.8735265731811523\n",
      "steps = 63, loss = 2.6498758792877197\n",
      "steps = 63, loss = 3.1276025772094727\n",
      "steps = 63, loss = 2.099403142929077\n",
      "steps = 63, loss = 3.479477643966675\n",
      "steps = 64, loss = 3.782710552215576\n",
      "steps = 64, loss = 35.16718673706055\n",
      "steps = 64, loss = 3.303825616836548\n",
      "steps = 64, loss = 11.568553924560547\n",
      "steps = 64, loss = 3.3952043056488037\n",
      "steps = 64, loss = 3.9966001510620117\n",
      "steps = 64, loss = 3.0331246852874756\n",
      "steps = 64, loss = 3.007561206817627\n",
      "steps = 64, loss = 3.3561131954193115\n",
      "steps = 64, loss = 2.854515790939331\n",
      "steps = 64, loss = 2.5434610843658447\n",
      "steps = 64, loss = 7.4513959884643555\n",
      "steps = 64, loss = 3.300726890563965\n",
      "steps = 64, loss = 49.985382080078125\n",
      "steps = 64, loss = 3.3457791805267334\n",
      "steps = 64, loss = 3.0717642307281494\n",
      "steps = 64, loss = 3.505042552947998\n",
      "steps = 64, loss = 2.8092236518859863\n",
      "steps = 64, loss = 2.7368478775024414\n",
      "steps = 64, loss = 2.7202374935150146\n",
      "steps = 64, loss = 2.857046127319336\n",
      "steps = 64, loss = 2.4073283672332764\n",
      "steps = 64, loss = 5.893522262573242\n",
      "steps = 64, loss = 3.024902582168579\n",
      "steps = 64, loss = 2.090372085571289\n",
      "steps = 64, loss = 3.6257619857788086\n",
      "steps = 64, loss = 2.736964702606201\n",
      "steps = 64, loss = 3.213287353515625\n",
      "steps = 64, loss = 3.659822702407837\n",
      "steps = 64, loss = 2.5875062942504883\n",
      "steps = 64, loss = 2.2747738361358643\n",
      "steps = 64, loss = 3.5095338821411133\n",
      "steps = 65, loss = 3.5739667415618896\n",
      "steps = 65, loss = 3.551877975463867\n",
      "steps = 65, loss = 2.1002938747406006\n",
      "steps = 65, loss = 3.3539178371429443\n",
      "steps = 65, loss = 3.2426204681396484\n",
      "steps = 65, loss = 2.949657917022705\n",
      "steps = 65, loss = 2.1612534523010254\n",
      "steps = 65, loss = 3.4786200523376465\n",
      "steps = 65, loss = 3.426832675933838\n",
      "steps = 65, loss = 49.985382080078125\n",
      "steps = 65, loss = 3.4323792457580566\n",
      "steps = 65, loss = 3.4047162532806396\n",
      "steps = 65, loss = 2.938171148300171\n",
      "steps = 65, loss = 3.019684314727783\n",
      "steps = 65, loss = 2.9841647148132324\n",
      "steps = 65, loss = 2.7553510665893555\n",
      "steps = 65, loss = 3.06392502784729\n",
      "steps = 65, loss = 2.7484488487243652\n",
      "steps = 65, loss = 4.050247669219971\n",
      "steps = 65, loss = 3.5732548236846924\n",
      "steps = 65, loss = 3.058012008666992\n",
      "steps = 65, loss = 2.8827614784240723\n",
      "steps = 65, loss = 3.3890864849090576\n",
      "steps = 65, loss = 6.400299549102783\n",
      "steps = 65, loss = 2.4273436069488525\n",
      "steps = 65, loss = 6.031014919281006\n",
      "steps = 65, loss = 2.8748998641967773\n",
      "steps = 65, loss = 2.0084309577941895\n",
      "steps = 65, loss = 2.8256919384002686\n",
      "steps = 65, loss = 2.6312694549560547\n",
      "steps = 65, loss = 2.735295534133911\n",
      "steps = 65, loss = 11.116661071777344\n",
      "steps = 66, loss = 49.985382080078125\n",
      "steps = 66, loss = 2.8053977489471436\n",
      "steps = 66, loss = 2.1427252292633057\n",
      "steps = 66, loss = 2.857326030731201\n",
      "steps = 66, loss = 3.1027560234069824\n",
      "steps = 66, loss = 2.713120460510254\n",
      "steps = 66, loss = 3.071847677230835\n",
      "steps = 66, loss = 3.2348530292510986\n",
      "steps = 66, loss = 2.0450186729431152\n",
      "steps = 66, loss = 3.454676866531372\n",
      "steps = 66, loss = 2.6992406845092773\n",
      "steps = 66, loss = 4.072873592376709\n",
      "steps = 66, loss = 3.3606655597686768\n",
      "steps = 66, loss = 2.713437080383301\n",
      "steps = 66, loss = 3.349100112915039\n",
      "steps = 66, loss = 3.6212270259857178\n",
      "steps = 66, loss = 2.9384024143218994\n",
      "steps = 66, loss = 2.8953118324279785\n",
      "steps = 66, loss = 2.8671531677246094\n",
      "steps = 66, loss = 2.5910067558288574\n",
      "steps = 66, loss = 2.942645788192749\n",
      "steps = 66, loss = 3.2862117290496826\n",
      "steps = 66, loss = 3.4692916870117188\n",
      "steps = 66, loss = 7.38675594329834\n",
      "steps = 66, loss = 3.421513319015503\n",
      "steps = 66, loss = 12.062471389770508\n",
      "steps = 66, loss = 6.012973785400391\n",
      "steps = 66, loss = 3.014242649078369\n",
      "steps = 66, loss = 2.1294193267822266\n",
      "steps = 66, loss = 2.435727596282959\n",
      "steps = 66, loss = 3.759617805480957\n",
      "steps = 66, loss = 3.4034767150878906\n",
      "steps = 67, loss = 2.5752768516540527\n",
      "steps = 67, loss = 2.099346160888672\n",
      "steps = 67, loss = 3.501007556915283\n",
      "steps = 67, loss = 2.75485897064209\n",
      "steps = 67, loss = 3.331423759460449\n",
      "steps = 67, loss = 2.6584105491638184\n",
      "steps = 67, loss = 2.913963556289673\n",
      "steps = 67, loss = 3.266484498977661\n",
      "steps = 67, loss = 4.110227584838867\n",
      "steps = 67, loss = 49.985382080078125\n",
      "steps = 67, loss = 3.4354746341705322\n",
      "steps = 67, loss = 2.8664748668670654\n",
      "steps = 67, loss = 2.1754567623138428\n",
      "steps = 67, loss = 3.6409645080566406\n",
      "steps = 67, loss = 3.3499491214752197\n",
      "steps = 67, loss = 3.4448015689849854\n",
      "steps = 67, loss = 3.0882010459899902\n",
      "steps = 67, loss = 6.159771919250488\n",
      "steps = 67, loss = 2.9776902198791504\n",
      "steps = 67, loss = 2.798898458480835\n",
      "steps = 67, loss = 2.873581647872925\n",
      "steps = 67, loss = 3.0192437171936035\n",
      "steps = 67, loss = 2.1107280254364014\n",
      "steps = 67, loss = 2.7337424755096436\n",
      "steps = 67, loss = 3.462493419647217\n",
      "steps = 67, loss = 3.046359062194824\n",
      "steps = 67, loss = 2.7932236194610596\n",
      "steps = 67, loss = 3.5358803272247314\n",
      "steps = 67, loss = 6.5884552001953125\n",
      "steps = 67, loss = 2.4479684829711914\n",
      "steps = 67, loss = 12.606432914733887\n",
      "steps = 67, loss = 3.532471179962158\n",
      "steps = 68, loss = 3.6591529846191406\n",
      "steps = 68, loss = 6.1255202293396\n",
      "steps = 68, loss = 3.355858325958252\n",
      "steps = 68, loss = 3.2534430027008057\n",
      "steps = 68, loss = 3.856400966644287\n",
      "steps = 68, loss = 2.9247422218322754\n",
      "steps = 68, loss = 3.3442635536193848\n",
      "steps = 68, loss = 2.712881565093994\n",
      "steps = 68, loss = 7.36475133895874\n",
      "steps = 68, loss = 2.9541168212890625\n",
      "steps = 68, loss = 2.7120797634124756\n",
      "steps = 68, loss = 2.0975887775421143\n",
      "steps = 68, loss = 2.937673330307007\n",
      "steps = 68, loss = 4.130401611328125\n",
      "steps = 68, loss = 2.8569986820220947\n",
      "steps = 68, loss = 2.6552579402923584\n",
      "steps = 68, loss = 3.4348363876342773\n",
      "steps = 68, loss = 49.985382080078125\n",
      "steps = 68, loss = 3.4145100116729736\n",
      "steps = 68, loss = 2.734783887863159\n",
      "steps = 68, loss = 2.5804860591888428\n",
      "steps = 68, loss = 3.3453991413116455\n",
      "steps = 68, loss = 3.257187843322754\n",
      "steps = 68, loss = 3.013103485107422\n",
      "steps = 68, loss = 12.720762252807617\n",
      "steps = 68, loss = 2.7620058059692383\n",
      "steps = 68, loss = 3.0577213764190674\n",
      "steps = 68, loss = 2.1428744792938232\n",
      "steps = 68, loss = 2.454399824142456\n",
      "steps = 68, loss = 2.218005895614624\n",
      "steps = 68, loss = 3.6513171195983887\n",
      "steps = 68, loss = 3.101097822189331\n",
      "steps = 69, loss = 2.068594217300415\n",
      "steps = 69, loss = 3.8768746852874756\n",
      "steps = 69, loss = 2.853450059890747\n",
      "steps = 69, loss = 3.102640390396118\n",
      "steps = 69, loss = 2.091539144515991\n",
      "steps = 69, loss = 3.5578465461730957\n",
      "steps = 69, loss = 2.725682497024536\n",
      "steps = 69, loss = 3.3619205951690674\n",
      "steps = 69, loss = 3.541572332382202\n",
      "steps = 69, loss = 3.0276899337768555\n",
      "steps = 69, loss = 2.077531337738037\n",
      "steps = 69, loss = 3.3923327922821045\n",
      "steps = 69, loss = 3.332228183746338\n",
      "steps = 69, loss = 2.640839099884033\n",
      "steps = 69, loss = 2.5887138843536377\n",
      "steps = 69, loss = 4.137638092041016\n",
      "steps = 69, loss = 2.718374729156494\n",
      "steps = 69, loss = 6.189635753631592\n",
      "steps = 69, loss = 2.4509685039520264\n",
      "steps = 69, loss = 3.2664105892181396\n",
      "steps = 69, loss = 3.1209030151367188\n",
      "steps = 69, loss = 2.552816390991211\n",
      "steps = 69, loss = 2.5192999839782715\n",
      "steps = 69, loss = 3.0514721870422363\n",
      "steps = 69, loss = 7.644142150878906\n",
      "steps = 69, loss = 11.966148376464844\n",
      "steps = 69, loss = 3.0035154819488525\n",
      "steps = 69, loss = 2.6846096515655518\n",
      "steps = 69, loss = 2.924830913543701\n",
      "steps = 69, loss = 49.985382080078125\n",
      "steps = 69, loss = 3.418562412261963\n",
      "steps = 69, loss = 3.2052018642425537\n",
      "steps = 70, loss = 3.126690626144409\n",
      "steps = 70, loss = 3.415607452392578\n",
      "steps = 70, loss = 49.985382080078125\n",
      "steps = 70, loss = 2.8228049278259277\n",
      "steps = 70, loss = 2.923384428024292\n",
      "steps = 70, loss = 2.5814967155456543\n",
      "steps = 70, loss = 3.5805320739746094\n",
      "steps = 70, loss = 3.345853090286255\n",
      "steps = 70, loss = 2.184363603591919\n",
      "steps = 70, loss = 2.952378988265991\n",
      "steps = 70, loss = 3.341035842895508\n",
      "steps = 70, loss = 2.145432949066162\n",
      "steps = 70, loss = 3.8009722232818604\n",
      "steps = 70, loss = 2.8727431297302246\n",
      "steps = 70, loss = 2.469958782196045\n",
      "steps = 70, loss = 2.100111722946167\n",
      "steps = 70, loss = 2.6418378353118896\n",
      "steps = 70, loss = 6.288181781768799\n",
      "steps = 70, loss = 2.7350308895111084\n",
      "steps = 70, loss = 3.0918617248535156\n",
      "steps = 70, loss = 2.8458094596862793\n",
      "steps = 70, loss = 3.295600175857544\n",
      "steps = 70, loss = 2.7577695846557617\n",
      "steps = 70, loss = 3.015270948410034\n",
      "steps = 70, loss = 4.1913275718688965\n",
      "steps = 70, loss = 3.4413986206054688\n",
      "steps = 70, loss = 3.5872275829315186\n",
      "steps = 70, loss = 3.507937431335449\n",
      "steps = 70, loss = 3.565032720565796\n",
      "steps = 70, loss = 2.913306713104248\n",
      "steps = 70, loss = 12.507692337036133\n",
      "steps = 70, loss = 6.701043605804443\n",
      "steps = 71, loss = 3.1407196521759033\n",
      "steps = 71, loss = 7.2057881355285645\n",
      "steps = 71, loss = 2.6957521438598633\n",
      "steps = 71, loss = 3.5822887420654297\n",
      "steps = 71, loss = 2.021768569946289\n",
      "steps = 71, loss = 2.751053810119629\n",
      "steps = 71, loss = 3.336353302001953\n",
      "steps = 71, loss = 2.2472317218780518\n",
      "steps = 71, loss = 2.1425371170043945\n",
      "steps = 71, loss = 2.9647631645202637\n",
      "steps = 71, loss = 2.66198992729187\n",
      "steps = 71, loss = 2.4775235652923584\n",
      "steps = 71, loss = 13.197460174560547\n",
      "steps = 71, loss = 4.212064266204834\n",
      "steps = 71, loss = 3.2872958183288574\n",
      "steps = 71, loss = 2.8563663959503174\n",
      "steps = 71, loss = 2.7705934047698975\n",
      "steps = 71, loss = 5.749534606933594\n",
      "steps = 71, loss = 3.010120153427124\n",
      "steps = 71, loss = 2.7062439918518066\n",
      "steps = 71, loss = 3.3928205966949463\n",
      "steps = 71, loss = 3.329590082168579\n",
      "steps = 71, loss = 2.713620901107788\n",
      "steps = 71, loss = 4.069471836090088\n",
      "steps = 71, loss = 3.1587257385253906\n",
      "steps = 71, loss = 2.901183605194092\n",
      "steps = 71, loss = 3.3558948040008545\n",
      "steps = 71, loss = 3.4575765132904053\n",
      "steps = 71, loss = 3.3790900707244873\n",
      "steps = 71, loss = 2.65716814994812\n",
      "steps = 71, loss = 3.5660908222198486\n",
      "steps = 71, loss = 49.985382080078125\n",
      "steps = 72, loss = 12.357914924621582\n",
      "steps = 72, loss = 2.965579032897949\n",
      "steps = 72, loss = 2.7190163135528564\n",
      "steps = 72, loss = 3.321547508239746\n",
      "steps = 72, loss = 2.6052358150482178\n",
      "steps = 72, loss = 2.474287748336792\n",
      "steps = 72, loss = 2.5556559562683105\n",
      "steps = 72, loss = 4.222049236297607\n",
      "steps = 72, loss = 3.4993808269500732\n",
      "steps = 72, loss = 2.117144823074341\n",
      "steps = 72, loss = 2.918135643005371\n",
      "steps = 72, loss = 3.378453254699707\n",
      "steps = 72, loss = 49.985382080078125\n",
      "steps = 72, loss = 49.90163040161133\n",
      "steps = 72, loss = 3.29787015914917\n",
      "steps = 72, loss = 5.902956962585449\n",
      "steps = 72, loss = 3.82798433303833\n",
      "steps = 72, loss = 7.2479095458984375\n",
      "steps = 72, loss = 3.3865432739257812\n",
      "steps = 72, loss = 2.761214256286621\n",
      "steps = 72, loss = 2.730520486831665\n",
      "steps = 72, loss = 2.79290509223938\n",
      "steps = 72, loss = 2.0914933681488037\n",
      "steps = 72, loss = 3.000998020172119\n",
      "steps = 72, loss = 2.9629430770874023\n",
      "steps = 72, loss = 2.853231430053711\n",
      "steps = 72, loss = 3.228952169418335\n",
      "steps = 72, loss = 2.06325364112854\n",
      "steps = 72, loss = 3.3251123428344727\n",
      "steps = 72, loss = 3.142523765563965\n",
      "steps = 72, loss = 3.988835096359253\n",
      "steps = 72, loss = 3.315774440765381\n",
      "steps = 73, loss = 3.167250633239746\n",
      "steps = 73, loss = 2.8721323013305664\n",
      "steps = 73, loss = 2.7576804161071777\n",
      "steps = 73, loss = 2.493143320083618\n",
      "steps = 73, loss = 3.333972215652466\n",
      "steps = 73, loss = 49.965301513671875\n",
      "steps = 73, loss = 3.333386182785034\n",
      "steps = 73, loss = 2.9943387508392334\n",
      "steps = 73, loss = 2.1000897884368896\n",
      "steps = 73, loss = 3.393955707550049\n",
      "steps = 73, loss = 3.592636823654175\n",
      "steps = 73, loss = 3.4350533485412598\n",
      "steps = 73, loss = 3.4580094814300537\n",
      "steps = 73, loss = 3.0699715614318848\n",
      "steps = 73, loss = 2.5981907844543457\n",
      "steps = 73, loss = 2.8858814239501953\n",
      "steps = 73, loss = 6.886898994445801\n",
      "steps = 73, loss = 3.8625752925872803\n",
      "steps = 73, loss = 3.493171453475952\n",
      "steps = 73, loss = 3.327575922012329\n",
      "steps = 73, loss = 2.5901153087615967\n",
      "steps = 73, loss = 2.7347283363342285\n",
      "steps = 73, loss = 2.7529053688049316\n",
      "steps = 73, loss = 49.985382080078125\n",
      "steps = 73, loss = 6.0890679359436035\n",
      "steps = 73, loss = 2.087057113647461\n",
      "steps = 73, loss = 2.1262876987457275\n",
      "steps = 73, loss = 3.0125813484191895\n",
      "steps = 73, loss = 3.4217023849487305\n",
      "steps = 73, loss = 4.2767558097839355\n",
      "steps = 73, loss = 12.958548545837402\n",
      "steps = 73, loss = 2.73555850982666\n",
      "steps = 74, loss = 2.6695291996002197\n",
      "steps = 74, loss = 3.5018599033355713\n",
      "steps = 74, loss = 2.9678900241851807\n",
      "steps = 74, loss = 4.280996322631836\n",
      "steps = 74, loss = 2.6585028171539307\n",
      "steps = 74, loss = 2.721411943435669\n",
      "steps = 74, loss = 3.675076961517334\n",
      "steps = 74, loss = 49.985382080078125\n",
      "steps = 74, loss = 2.0913944244384766\n",
      "steps = 74, loss = 2.0905354022979736\n",
      "steps = 74, loss = 3.1686453819274902\n",
      "steps = 74, loss = 2.2645578384399414\n",
      "steps = 74, loss = 2.739942789077759\n",
      "steps = 74, loss = 3.551673650741577\n",
      "steps = 74, loss = 2.5013256072998047\n",
      "steps = 74, loss = 13.239134788513184\n",
      "steps = 74, loss = 2.489009380340576\n",
      "steps = 74, loss = 6.974444389343262\n",
      "steps = 74, loss = 2.8551855087280273\n",
      "steps = 74, loss = 5.757769584655762\n",
      "steps = 74, loss = 3.0001842975616455\n",
      "steps = 74, loss = 49.996700286865234\n",
      "steps = 74, loss = 3.363635540008545\n",
      "steps = 74, loss = 2.9929280281066895\n",
      "steps = 74, loss = 4.173691272735596\n",
      "steps = 74, loss = 3.3216304779052734\n",
      "steps = 74, loss = 3.4026482105255127\n",
      "steps = 74, loss = 3.3332762718200684\n",
      "steps = 74, loss = 2.9952807426452637\n",
      "steps = 74, loss = 3.3206071853637695\n",
      "steps = 74, loss = 2.813824415206909\n",
      "steps = 74, loss = 3.369358777999878\n",
      "steps = 75, loss = 3.0055761337280273\n",
      "steps = 75, loss = 2.144285202026367\n",
      "steps = 75, loss = 3.1919586658477783\n",
      "steps = 75, loss = 49.985382080078125\n",
      "steps = 75, loss = 3.3255505561828613\n",
      "steps = 75, loss = 3.3183000087738037\n",
      "steps = 75, loss = 2.504965305328369\n",
      "steps = 75, loss = 2.5830814838409424\n",
      "steps = 75, loss = 2.6805145740509033\n",
      "steps = 75, loss = 2.916689395904541\n",
      "steps = 75, loss = 2.8551836013793945\n",
      "steps = 75, loss = 12.582451820373535\n",
      "steps = 75, loss = 2.0555262565612793\n",
      "steps = 75, loss = 2.712186098098755\n",
      "steps = 75, loss = 3.4293577671051025\n",
      "steps = 75, loss = 2.6804308891296387\n",
      "steps = 75, loss = 7.36146354675293\n",
      "steps = 75, loss = 2.2291436195373535\n",
      "steps = 75, loss = 2.8301806449890137\n",
      "steps = 75, loss = 7.467381000518799\n",
      "steps = 75, loss = 2.876532554626465\n",
      "steps = 75, loss = 3.56648850440979\n",
      "steps = 75, loss = 2.7146201133728027\n",
      "steps = 75, loss = 3.592324733734131\n",
      "steps = 75, loss = 3.3270671367645264\n",
      "steps = 75, loss = 4.319121360778809\n",
      "steps = 75, loss = 3.500835418701172\n",
      "steps = 75, loss = 3.0180323123931885\n",
      "steps = 75, loss = 3.6304287910461426\n",
      "steps = 75, loss = 3.606919765472412\n",
      "steps = 75, loss = 5.9292097091674805\n",
      "steps = 75, loss = 49.942630767822266\n",
      "steps = 76, loss = 2.90580677986145\n",
      "steps = 76, loss = 49.985382080078125\n",
      "steps = 76, loss = 3.594778060913086\n",
      "steps = 76, loss = 6.531579971313477\n",
      "steps = 76, loss = 3.58621883392334\n",
      "steps = 76, loss = 3.5944771766662598\n",
      "steps = 76, loss = 3.209756851196289\n",
      "steps = 76, loss = 4.362658500671387\n",
      "steps = 76, loss = 2.103205919265747\n",
      "steps = 76, loss = 2.8708510398864746\n",
      "steps = 76, loss = 2.664849042892456\n",
      "steps = 76, loss = 3.327427625656128\n",
      "steps = 76, loss = 3.3616015911102295\n",
      "steps = 76, loss = 3.4109718799591064\n",
      "steps = 76, loss = 2.5174052715301514\n",
      "steps = 76, loss = 2.1011037826538086\n",
      "steps = 76, loss = 2.8579866886138916\n",
      "steps = 76, loss = 2.5427017211914062\n",
      "steps = 76, loss = 3.541527032852173\n",
      "steps = 76, loss = 13.600937843322754\n",
      "steps = 76, loss = 3.0391035079956055\n",
      "steps = 76, loss = 2.0990607738494873\n",
      "steps = 76, loss = 3.0108835697174072\n",
      "steps = 76, loss = 3.0727312564849854\n",
      "steps = 76, loss = 2.5290350914001465\n",
      "steps = 76, loss = 3.0871055126190186\n",
      "steps = 76, loss = 2.735081911087036\n",
      "steps = 76, loss = 2.7569353580474854\n",
      "steps = 76, loss = 3.444200277328491\n",
      "steps = 76, loss = 3.4275898933410645\n",
      "steps = 76, loss = 6.088408946990967\n",
      "steps = 76, loss = 49.892250061035156\n",
      "steps = 77, loss = 2.171764373779297\n",
      "steps = 77, loss = 13.43549919128418\n",
      "steps = 77, loss = 2.7138099670410156\n",
      "steps = 77, loss = 3.0153305530548096\n",
      "steps = 77, loss = 49.985382080078125\n",
      "steps = 77, loss = 2.5237221717834473\n",
      "steps = 77, loss = 3.322490930557251\n",
      "steps = 77, loss = 2.6294808387756348\n",
      "steps = 77, loss = 3.201542854309082\n",
      "steps = 77, loss = 2.657032012939453\n",
      "steps = 77, loss = 2.9441261291503906\n",
      "steps = 77, loss = 49.79985427856445\n",
      "steps = 77, loss = 3.663487195968628\n",
      "steps = 77, loss = 3.005580425262451\n",
      "steps = 77, loss = 3.7798643112182617\n",
      "steps = 77, loss = 3.3525421619415283\n",
      "steps = 77, loss = 2.1425063610076904\n",
      "steps = 77, loss = 3.0502545833587646\n",
      "steps = 77, loss = 3.2622265815734863\n",
      "steps = 77, loss = 3.394721746444702\n",
      "steps = 77, loss = 5.659005641937256\n",
      "steps = 77, loss = 2.7153453826904297\n",
      "steps = 77, loss = 2.753689765930176\n",
      "steps = 77, loss = 7.390998363494873\n",
      "steps = 77, loss = 3.2229976654052734\n",
      "steps = 77, loss = 2.8554952144622803\n",
      "steps = 77, loss = 3.5419840812683105\n",
      "steps = 77, loss = 3.6190738677978516\n",
      "steps = 77, loss = 2.3356058597564697\n",
      "steps = 77, loss = 2.614946126937866\n",
      "steps = 77, loss = 4.379858493804932\n",
      "steps = 77, loss = 49.879634857177734\n",
      "steps = 78, loss = 2.6952602863311768\n",
      "steps = 78, loss = 3.051060199737549\n",
      "steps = 78, loss = 13.441794395446777\n",
      "steps = 78, loss = 2.560853958129883\n",
      "steps = 78, loss = 2.728255271911621\n",
      "steps = 78, loss = 2.996506452560425\n",
      "steps = 78, loss = 4.3894877433776855\n",
      "steps = 78, loss = 2.852228879928589\n",
      "steps = 78, loss = 3.455198049545288\n",
      "steps = 78, loss = 3.224121570587158\n",
      "steps = 78, loss = 7.169726371765137\n",
      "steps = 78, loss = 3.7870707511901855\n",
      "steps = 78, loss = 5.870375156402588\n",
      "steps = 78, loss = 3.589446544647217\n",
      "steps = 78, loss = 2.4221765995025635\n",
      "steps = 78, loss = 2.7066614627838135\n",
      "steps = 78, loss = 2.719911813735962\n",
      "steps = 78, loss = 2.519690752029419\n",
      "steps = 78, loss = 3.511042594909668\n",
      "steps = 78, loss = 49.985382080078125\n",
      "steps = 78, loss = 3.2475533485412598\n",
      "steps = 78, loss = 2.6110122203826904\n",
      "steps = 78, loss = 3.380133867263794\n",
      "steps = 78, loss = 2.7726147174835205\n",
      "steps = 78, loss = 49.99240493774414\n",
      "steps = 78, loss = 3.3111441135406494\n",
      "steps = 78, loss = 3.5583324432373047\n",
      "steps = 78, loss = 2.961470603942871\n",
      "steps = 78, loss = 2.083306074142456\n",
      "steps = 78, loss = 3.411815643310547\n",
      "steps = 78, loss = 3.364675283432007\n",
      "steps = 78, loss = 2.0917365550994873\n",
      "steps = 79, loss = 49.985382080078125\n",
      "steps = 79, loss = 3.5038952827453613\n",
      "steps = 79, loss = 3.061950922012329\n",
      "steps = 79, loss = 2.99503231048584\n",
      "steps = 79, loss = 3.3743255138397217\n",
      "steps = 79, loss = 2.854431390762329\n",
      "steps = 79, loss = 2.52358341217041\n",
      "steps = 79, loss = 3.3087174892425537\n",
      "steps = 79, loss = 3.4433887004852295\n",
      "steps = 79, loss = 5.820058822631836\n",
      "steps = 79, loss = 13.023037910461426\n",
      "steps = 79, loss = 2.699150323867798\n",
      "steps = 79, loss = 2.091576337814331\n",
      "steps = 79, loss = 2.2302944660186768\n",
      "steps = 79, loss = 7.258543968200684\n",
      "steps = 79, loss = 3.316592216491699\n",
      "steps = 79, loss = 3.3049511909484863\n",
      "steps = 79, loss = 4.41454553604126\n",
      "steps = 79, loss = 49.94859313964844\n",
      "steps = 79, loss = 2.7217512130737305\n",
      "steps = 79, loss = 2.6411094665527344\n",
      "steps = 79, loss = 2.5888359546661377\n",
      "steps = 79, loss = 2.662327289581299\n",
      "steps = 79, loss = 3.657337188720703\n",
      "steps = 79, loss = 3.0575551986694336\n",
      "steps = 79, loss = 2.6101365089416504\n",
      "steps = 79, loss = 3.2348763942718506\n",
      "steps = 79, loss = 3.3202483654022217\n",
      "steps = 79, loss = 2.1220405101776123\n",
      "steps = 79, loss = 2.7427761554718018\n",
      "steps = 79, loss = 3.4053807258605957\n",
      "steps = 79, loss = 2.5927488803863525\n",
      "steps = 80, loss = 7.050446033477783\n",
      "steps = 80, loss = 2.1189918518066406\n",
      "steps = 80, loss = 3.259753704071045\n",
      "steps = 80, loss = 5.724515438079834\n",
      "steps = 80, loss = 2.2852792739868164\n",
      "steps = 80, loss = 3.0151286125183105\n",
      "steps = 80, loss = 4.45379638671875\n",
      "steps = 80, loss = 3.0877928733825684\n",
      "steps = 80, loss = 2.7127177715301514\n",
      "steps = 80, loss = 3.47122859954834\n",
      "steps = 80, loss = 2.56628155708313\n",
      "steps = 80, loss = 2.8525102138519287\n",
      "steps = 80, loss = 2.8543689250946045\n",
      "steps = 80, loss = 2.9992358684539795\n",
      "steps = 80, loss = 3.6019577980041504\n",
      "steps = 80, loss = 2.3240602016448975\n",
      "steps = 80, loss = 3.026819944381714\n",
      "steps = 80, loss = 3.5133559703826904\n",
      "steps = 80, loss = 3.000929832458496\n",
      "steps = 80, loss = 3.3813037872314453\n",
      "steps = 80, loss = 2.143962860107422\n",
      "steps = 80, loss = 49.811248779296875\n",
      "steps = 80, loss = 3.313363552093506\n",
      "steps = 80, loss = 2.540292501449585\n",
      "steps = 80, loss = 2.715322971343994\n",
      "steps = 80, loss = 3.281924247741699\n",
      "steps = 80, loss = 2.7133097648620605\n",
      "steps = 80, loss = 13.688136100769043\n",
      "steps = 80, loss = 3.4017157554626465\n",
      "steps = 80, loss = 49.985382080078125\n",
      "steps = 80, loss = 3.390340805053711\n",
      "steps = 80, loss = 3.1096880435943604\n",
      "steps = 81, loss = 2.2519729137420654\n",
      "steps = 81, loss = 3.438675880432129\n",
      "steps = 81, loss = 2.977787494659424\n",
      "steps = 81, loss = 49.985382080078125\n",
      "steps = 81, loss = 3.393192768096924\n",
      "steps = 81, loss = 5.904640197753906\n",
      "steps = 81, loss = 49.95650100708008\n",
      "steps = 81, loss = 3.417189359664917\n",
      "steps = 81, loss = 2.6128244400024414\n",
      "steps = 81, loss = 2.552600622177124\n",
      "steps = 81, loss = 3.1091558933258057\n",
      "steps = 81, loss = 3.33268666267395\n",
      "steps = 81, loss = 3.006227970123291\n",
      "steps = 81, loss = 3.3151440620422363\n",
      "steps = 81, loss = 3.4850738048553467\n",
      "steps = 81, loss = 2.583282470703125\n",
      "steps = 81, loss = 3.6345081329345703\n",
      "steps = 81, loss = 2.73561692237854\n",
      "steps = 81, loss = 4.4993815422058105\n",
      "steps = 81, loss = 3.277479887008667\n",
      "steps = 81, loss = 2.2618749141693115\n",
      "steps = 81, loss = 2.885495901107788\n",
      "steps = 81, loss = 2.8689515590667725\n",
      "steps = 81, loss = 2.7003471851348877\n",
      "steps = 81, loss = 3.451969623565674\n",
      "steps = 81, loss = 2.757020950317383\n",
      "steps = 81, loss = 3.6747243404388428\n",
      "steps = 81, loss = 6.724216461181641\n",
      "steps = 81, loss = 2.0990824699401855\n",
      "steps = 81, loss = 2.265986680984497\n",
      "steps = 81, loss = 2.901553153991699\n",
      "steps = 81, loss = 13.380871772766113\n",
      "steps = 82, loss = 2.2252020835876465\n",
      "steps = 82, loss = 7.218858242034912\n",
      "steps = 82, loss = 14.220531463623047\n",
      "steps = 82, loss = 3.0013675689697266\n",
      "steps = 82, loss = 49.95370101928711\n",
      "steps = 82, loss = 3.0404069423675537\n",
      "steps = 82, loss = 3.2912800312042236\n",
      "steps = 82, loss = 5.641086578369141\n",
      "steps = 82, loss = 3.4676930904388428\n",
      "steps = 82, loss = 49.985382080078125\n",
      "steps = 82, loss = 3.505553960800171\n",
      "steps = 82, loss = 2.51558256149292\n",
      "steps = 82, loss = 2.967381238937378\n",
      "steps = 82, loss = 2.7196667194366455\n",
      "steps = 82, loss = 3.4092655181884766\n",
      "steps = 82, loss = 3.7157275676727295\n",
      "steps = 82, loss = 2.1419425010681152\n",
      "steps = 82, loss = 3.121108293533325\n",
      "steps = 82, loss = 2.5585973262786865\n",
      "steps = 82, loss = 4.516162395477295\n",
      "steps = 82, loss = 2.7146339416503906\n",
      "steps = 82, loss = 2.1545581817626953\n",
      "steps = 82, loss = 3.020799398422241\n",
      "steps = 82, loss = 2.9330356121063232\n",
      "steps = 82, loss = 3.3108725547790527\n",
      "steps = 82, loss = 3.686788558959961\n",
      "steps = 82, loss = 3.5581393241882324\n",
      "steps = 82, loss = 2.221045970916748\n",
      "steps = 82, loss = 2.8548972606658936\n",
      "steps = 82, loss = 3.4924745559692383\n",
      "steps = 82, loss = 3.308215856552124\n",
      "steps = 82, loss = 2.8929336071014404\n",
      "steps = 83, loss = 3.3068649768829346\n",
      "steps = 83, loss = 5.8575873374938965\n",
      "steps = 83, loss = 2.7359116077423096\n",
      "steps = 83, loss = 2.130626916885376\n",
      "steps = 83, loss = 3.4131641387939453\n",
      "steps = 83, loss = 2.634509325027466\n",
      "steps = 83, loss = 49.956581115722656\n",
      "steps = 83, loss = 2.8056721687316895\n",
      "steps = 83, loss = 2.9208178520202637\n",
      "steps = 83, loss = 2.5687198638916016\n",
      "steps = 83, loss = 3.140611171722412\n",
      "steps = 83, loss = 3.4276843070983887\n",
      "steps = 83, loss = 2.427639961242676\n",
      "steps = 83, loss = 6.856749057769775\n",
      "steps = 83, loss = 49.985382080078125\n",
      "steps = 83, loss = 4.560097694396973\n",
      "steps = 83, loss = 2.3323006629943848\n",
      "steps = 83, loss = 3.44473934173584\n",
      "steps = 83, loss = 2.100294351577759\n",
      "steps = 83, loss = 3.312256097793579\n",
      "steps = 83, loss = 3.4891014099121094\n",
      "steps = 83, loss = 2.1358089447021484\n",
      "steps = 83, loss = 3.0056328773498535\n",
      "steps = 83, loss = 3.440999746322632\n",
      "steps = 83, loss = 2.7585885524749756\n",
      "steps = 83, loss = 2.7093472480773926\n",
      "steps = 83, loss = 3.408822536468506\n",
      "steps = 83, loss = 3.5727975368499756\n",
      "steps = 83, loss = 2.8697991371154785\n",
      "steps = 83, loss = 2.6079235076904297\n",
      "steps = 83, loss = 3.727407693862915\n",
      "steps = 83, loss = 13.909839630126953\n",
      "steps = 84, loss = 3.437925100326538\n",
      "steps = 84, loss = 2.090886116027832\n",
      "steps = 84, loss = 49.985382080078125\n",
      "steps = 84, loss = 2.852713108062744\n",
      "steps = 84, loss = 1.9666917324066162\n",
      "steps = 84, loss = 3.4433658123016357\n",
      "steps = 84, loss = 3.3919882774353027\n",
      "steps = 84, loss = 5.759476661682129\n",
      "steps = 84, loss = 2.2590091228485107\n",
      "steps = 84, loss = 2.993011713027954\n",
      "steps = 84, loss = 3.30690598487854\n",
      "steps = 84, loss = 7.770836353302002\n",
      "steps = 84, loss = 2.5623135566711426\n",
      "steps = 84, loss = 2.7392892837524414\n",
      "steps = 84, loss = 14.251075744628906\n",
      "steps = 84, loss = 2.0736098289489746\n",
      "steps = 84, loss = 3.424281597137451\n",
      "steps = 84, loss = 2.6142613887786865\n",
      "steps = 84, loss = 2.5702810287475586\n",
      "steps = 84, loss = 3.1623051166534424\n",
      "steps = 84, loss = 3.1370694637298584\n",
      "steps = 84, loss = 3.3947460651397705\n",
      "steps = 84, loss = 2.567471742630005\n",
      "steps = 84, loss = 4.55910062789917\n",
      "steps = 84, loss = 3.2848122119903564\n",
      "steps = 84, loss = 3.4965450763702393\n",
      "steps = 84, loss = 2.563843011856079\n",
      "steps = 84, loss = 3.5004007816314697\n",
      "steps = 84, loss = 2.7222087383270264\n",
      "steps = 84, loss = 3.2996809482574463\n",
      "steps = 84, loss = 49.31502914428711\n",
      "steps = 84, loss = 2.7569241523742676\n",
      "steps = 85, loss = 3.4481046199798584\n",
      "steps = 85, loss = 3.4511091709136963\n",
      "steps = 85, loss = 2.2254321575164795\n",
      "steps = 85, loss = 3.2540664672851562\n",
      "steps = 85, loss = 2.7255709171295166\n",
      "steps = 85, loss = 2.7233798503875732\n",
      "steps = 85, loss = 14.269532203674316\n",
      "steps = 85, loss = 2.679027795791626\n",
      "steps = 85, loss = 2.5966081619262695\n",
      "steps = 85, loss = 3.316969633102417\n",
      "steps = 85, loss = 3.347104072570801\n",
      "steps = 85, loss = 2.9276390075683594\n",
      "steps = 85, loss = 3.297755002975464\n",
      "steps = 85, loss = 4.583199501037598\n",
      "steps = 85, loss = 2.565589666366577\n",
      "steps = 85, loss = 2.991166353225708\n",
      "steps = 85, loss = 5.805338382720947\n",
      "steps = 85, loss = 3.7828660011291504\n",
      "steps = 85, loss = 3.42319655418396\n",
      "steps = 85, loss = 3.345534563064575\n",
      "steps = 85, loss = 2.6693978309631348\n",
      "steps = 85, loss = 49.74233627319336\n",
      "steps = 85, loss = 2.824260950088501\n",
      "steps = 85, loss = 2.0918686389923096\n",
      "steps = 85, loss = 1.9564828872680664\n",
      "steps = 85, loss = 3.297049045562744\n",
      "steps = 85, loss = 2.5763468742370605\n",
      "steps = 85, loss = 49.985382080078125\n",
      "steps = 85, loss = 2.8542046546936035\n",
      "steps = 85, loss = 6.607365131378174\n",
      "steps = 85, loss = 2.662860870361328\n",
      "steps = 85, loss = 3.1482369899749756\n",
      "steps = 86, loss = 3.3954432010650635\n",
      "steps = 86, loss = 3.46102237701416\n",
      "steps = 86, loss = 3.342412233352661\n",
      "steps = 86, loss = 2.717628002166748\n",
      "steps = 86, loss = 4.619056224822998\n",
      "steps = 86, loss = 3.1745352745056152\n",
      "steps = 86, loss = 3.3011083602905273\n",
      "steps = 86, loss = 3.455366849899292\n",
      "steps = 86, loss = 3.5547902584075928\n",
      "steps = 86, loss = 7.167290210723877\n",
      "steps = 86, loss = 2.7115724086761475\n",
      "steps = 86, loss = 5.785709857940674\n",
      "steps = 86, loss = 2.247997999191284\n",
      "steps = 86, loss = 49.921974182128906\n",
      "steps = 86, loss = 2.581716775894165\n",
      "steps = 86, loss = 2.143771171569824\n",
      "steps = 86, loss = 2.0078744888305664\n",
      "steps = 86, loss = 2.853992462158203\n",
      "steps = 86, loss = 2.7163543701171875\n",
      "steps = 86, loss = 3.3458755016326904\n",
      "steps = 86, loss = 3.575792074203491\n",
      "steps = 86, loss = 3.716628313064575\n",
      "steps = 86, loss = 2.897608995437622\n",
      "steps = 86, loss = 2.996774435043335\n",
      "steps = 86, loss = 3.301610231399536\n",
      "steps = 86, loss = 49.985382080078125\n",
      "steps = 86, loss = 2.7156271934509277\n",
      "steps = 86, loss = 14.866142272949219\n",
      "steps = 86, loss = 2.969020128250122\n",
      "steps = 86, loss = 2.590925931930542\n",
      "steps = 86, loss = 3.4422214031219482\n",
      "steps = 86, loss = 2.6285715103149414\n",
      "steps = 87, loss = 4.66839075088501\n",
      "steps = 87, loss = 2.9486358165740967\n",
      "steps = 87, loss = 3.0019586086273193\n",
      "steps = 87, loss = 3.342095375061035\n",
      "steps = 87, loss = 2.79803466796875\n",
      "steps = 87, loss = 2.674910306930542\n",
      "steps = 87, loss = 13.932744026184082\n",
      "steps = 87, loss = 3.537052869796753\n",
      "steps = 87, loss = 2.8327291011810303\n",
      "steps = 87, loss = 3.473834276199341\n",
      "steps = 87, loss = 3.539139986038208\n",
      "steps = 87, loss = 2.60117506980896\n",
      "steps = 87, loss = 2.1943747997283936\n",
      "steps = 87, loss = 49.953758239746094\n",
      "steps = 87, loss = 3.195892333984375\n",
      "steps = 87, loss = 3.493772268295288\n",
      "steps = 87, loss = 3.3597426414489746\n",
      "steps = 87, loss = 6.40345573425293\n",
      "steps = 87, loss = 2.7365469932556152\n",
      "steps = 87, loss = 3.457545757293701\n",
      "steps = 87, loss = 49.985382080078125\n",
      "steps = 87, loss = 2.0992372035980225\n",
      "steps = 87, loss = 2.593569278717041\n",
      "steps = 87, loss = 6.025134563446045\n",
      "steps = 87, loss = 3.303683280944824\n",
      "steps = 87, loss = 2.8687336444854736\n",
      "steps = 87, loss = 2.0931921005249023\n",
      "steps = 87, loss = 2.917494535446167\n",
      "steps = 87, loss = 3.4305288791656494\n",
      "steps = 87, loss = 3.4555389881134033\n",
      "steps = 87, loss = 2.7586898803710938\n",
      "steps = 87, loss = 3.307915449142456\n",
      "steps = 88, loss = 2.9971981048583984\n",
      "steps = 88, loss = 2.1163642406463623\n",
      "steps = 88, loss = 49.985382080078125\n",
      "steps = 88, loss = 2.64098858833313\n",
      "steps = 88, loss = 3.3742294311523438\n",
      "steps = 88, loss = 3.337707042694092\n",
      "steps = 88, loss = 2.715613842010498\n",
      "steps = 88, loss = 2.8290023803710938\n",
      "steps = 88, loss = 49.96171188354492\n",
      "steps = 88, loss = 3.299560308456421\n",
      "steps = 88, loss = 3.7419121265411377\n",
      "steps = 88, loss = 2.1418538093566895\n",
      "steps = 88, loss = 3.0078890323638916\n",
      "steps = 88, loss = 14.51840591430664\n",
      "steps = 88, loss = 2.9753475189208984\n",
      "steps = 88, loss = 3.207645893096924\n",
      "steps = 88, loss = 3.554961919784546\n",
      "steps = 88, loss = 2.7112526893615723\n",
      "steps = 88, loss = 3.536254405975342\n",
      "steps = 88, loss = 2.0165481567382812\n",
      "steps = 88, loss = 2.962453842163086\n",
      "steps = 88, loss = 3.278909683227539\n",
      "steps = 88, loss = 2.854459047317505\n",
      "steps = 88, loss = 3.6496760845184326\n",
      "steps = 88, loss = 4.680691719055176\n",
      "steps = 88, loss = 5.670384883880615\n",
      "steps = 88, loss = 3.485426902770996\n",
      "steps = 88, loss = 2.599102735519409\n",
      "steps = 88, loss = 2.7222301959991455\n",
      "steps = 88, loss = 3.550009250640869\n",
      "steps = 88, loss = 3.482861280441284\n",
      "steps = 88, loss = 6.964378833770752\n",
      "steps = 89, loss = 31.773155212402344\n",
      "steps = 89, loss = 49.985382080078125\n",
      "steps = 89, loss = 3.4113945960998535\n",
      "steps = 89, loss = 5.847145080566406\n",
      "steps = 89, loss = 2.988654136657715\n",
      "steps = 89, loss = 3.4285402297973633\n",
      "steps = 89, loss = 49.84062957763672\n",
      "steps = 89, loss = 2.896770477294922\n",
      "steps = 89, loss = 2.7214510440826416\n",
      "steps = 89, loss = 2.0921177864074707\n",
      "steps = 89, loss = 2.7108047008514404\n",
      "steps = 89, loss = 7.4300971031188965\n",
      "steps = 89, loss = 3.24662446975708\n",
      "steps = 89, loss = 2.570841073989868\n",
      "steps = 89, loss = 2.5281260013580322\n",
      "steps = 89, loss = 3.3751416206359863\n",
      "steps = 89, loss = 3.5008835792541504\n",
      "steps = 89, loss = 3.4150145053863525\n",
      "steps = 89, loss = 3.4116809368133545\n",
      "steps = 89, loss = 3.441765546798706\n",
      "steps = 89, loss = 2.7347044944763184\n",
      "steps = 89, loss = 3.1282732486724854\n",
      "steps = 89, loss = 3.2893471717834473\n",
      "steps = 89, loss = 3.5330612659454346\n",
      "steps = 89, loss = 2.851271152496338\n",
      "steps = 89, loss = 14.754640579223633\n",
      "steps = 89, loss = 4.693596363067627\n",
      "steps = 89, loss = 3.2082178592681885\n",
      "steps = 89, loss = 2.620105743408203\n",
      "steps = 89, loss = 2.5944595336914062\n",
      "steps = 89, loss = 2.0561962127685547\n",
      "steps = 89, loss = 3.4447741508483887\n",
      "steps = 90, loss = 2.15738582611084\n",
      "steps = 90, loss = 2.1008386611938477\n",
      "steps = 90, loss = 3.2387807369232178\n",
      "steps = 90, loss = 2.8932154178619385\n",
      "steps = 90, loss = 2.0432910919189453\n",
      "steps = 90, loss = 49.985382080078125\n",
      "steps = 90, loss = 6.5372700691223145\n",
      "steps = 90, loss = 3.383319139480591\n",
      "steps = 90, loss = 3.53322434425354\n",
      "steps = 90, loss = 14.727252960205078\n",
      "steps = 90, loss = 2.999239206314087\n",
      "steps = 90, loss = 2.611799716949463\n",
      "steps = 90, loss = 3.4379923343658447\n",
      "steps = 90, loss = 2.73787784576416\n",
      "steps = 90, loss = 3.74369740486145\n",
      "steps = 90, loss = 3.4006454944610596\n",
      "steps = 90, loss = 3.0301270484924316\n",
      "steps = 90, loss = 3.577159881591797\n",
      "steps = 90, loss = 3.5674381256103516\n",
      "steps = 90, loss = 2.766010046005249\n",
      "steps = 90, loss = 3.53646183013916\n",
      "steps = 90, loss = 3.560112237930298\n",
      "steps = 90, loss = 2.568837881088257\n",
      "steps = 90, loss = 3.1496336460113525\n",
      "steps = 90, loss = 5.764626502990723\n",
      "steps = 90, loss = 3.2979938983917236\n",
      "steps = 90, loss = 2.761648178100586\n",
      "steps = 90, loss = 2.86946702003479\n",
      "steps = 90, loss = 49.91614532470703\n",
      "steps = 90, loss = 3.4239673614501953\n",
      "steps = 90, loss = 4.750205993652344\n",
      "steps = 90, loss = 2.6814072132110596\n",
      "steps = 91, loss = 2.8152976036071777\n",
      "steps = 91, loss = 2.995030164718628\n",
      "steps = 91, loss = 1.992942214012146\n",
      "steps = 91, loss = 2.8681132793426514\n",
      "steps = 91, loss = 3.6127712726593018\n",
      "steps = 91, loss = 3.4156277179718018\n",
      "steps = 91, loss = 3.2514219284057617\n",
      "steps = 91, loss = 2.618009567260742\n",
      "steps = 91, loss = 2.02347993850708\n",
      "steps = 91, loss = 3.3382537364959717\n",
      "steps = 91, loss = 2.8543877601623535\n",
      "steps = 91, loss = 3.441920518875122\n",
      "steps = 91, loss = 3.49745512008667\n",
      "steps = 91, loss = 2.7979838848114014\n",
      "steps = 91, loss = 2.716156005859375\n",
      "steps = 91, loss = 3.525502920150757\n",
      "steps = 91, loss = 3.386932134628296\n",
      "steps = 91, loss = 3.407819986343384\n",
      "steps = 91, loss = 5.731590270996094\n",
      "steps = 91, loss = 4.7609639167785645\n",
      "steps = 91, loss = 2.6929550170898438\n",
      "steps = 91, loss = 7.2846808433532715\n",
      "steps = 91, loss = 2.7918200492858887\n",
      "steps = 91, loss = 49.95138168334961\n",
      "steps = 91, loss = 3.5718724727630615\n",
      "steps = 91, loss = 3.3626372814178467\n",
      "steps = 91, loss = 2.1416208744049072\n",
      "steps = 91, loss = 3.294355869293213\n",
      "steps = 91, loss = 14.233983993530273\n",
      "steps = 91, loss = 2.6055405139923096\n",
      "steps = 91, loss = 2.713301181793213\n",
      "steps = 91, loss = 49.985382080078125\n",
      "steps = 92, loss = 3.5879316329956055\n",
      "steps = 92, loss = 2.6075422763824463\n",
      "steps = 92, loss = 2.721886157989502\n",
      "steps = 92, loss = 2.775440216064453\n",
      "steps = 92, loss = 2.7580487728118896\n",
      "steps = 92, loss = 15.483190536499023\n",
      "steps = 92, loss = 4.774081707000732\n",
      "steps = 92, loss = 49.9446907043457\n",
      "steps = 92, loss = 3.412461519241333\n",
      "steps = 92, loss = 2.6134860515594482\n",
      "steps = 92, loss = 3.7685184478759766\n",
      "steps = 92, loss = 3.2820088863372803\n",
      "steps = 92, loss = 3.330355644226074\n",
      "steps = 92, loss = 2.040275812149048\n",
      "steps = 92, loss = 3.7961771488189697\n",
      "steps = 92, loss = 3.4163496494293213\n",
      "steps = 92, loss = 3.54233717918396\n",
      "steps = 92, loss = 2.851231575012207\n",
      "steps = 92, loss = 5.846862316131592\n",
      "steps = 92, loss = 2.737750291824341\n",
      "steps = 92, loss = 2.9974818229675293\n",
      "steps = 92, loss = 2.7717838287353516\n",
      "steps = 92, loss = 3.3302814960479736\n",
      "steps = 92, loss = 3.2520861625671387\n",
      "steps = 92, loss = 2.0921003818511963\n",
      "steps = 92, loss = 2.7498373985290527\n",
      "steps = 92, loss = 7.052170276641846\n",
      "steps = 92, loss = 3.395364284515381\n",
      "steps = 92, loss = 2.986783981323242\n",
      "steps = 92, loss = 2.1162586212158203\n",
      "steps = 92, loss = 3.2845141887664795\n",
      "steps = 92, loss = 49.985382080078125\n",
      "steps = 93, loss = 2.5613057613372803\n",
      "steps = 93, loss = 3.4414734840393066\n",
      "steps = 93, loss = 2.8600680828094482\n",
      "steps = 93, loss = 2.738355875015259\n",
      "steps = 93, loss = 2.869447708129883\n",
      "steps = 93, loss = 2.9388110637664795\n",
      "steps = 93, loss = 2.7282135486602783\n",
      "steps = 93, loss = 2.1009135246276855\n",
      "steps = 93, loss = 2.7621145248413086\n",
      "steps = 93, loss = 2.1501715183258057\n",
      "steps = 93, loss = 3.6077628135681152\n",
      "steps = 93, loss = 3.3901984691619873\n",
      "steps = 93, loss = 4.828949451446533\n",
      "steps = 93, loss = 49.985382080078125\n",
      "steps = 93, loss = 3.1073415279388428\n",
      "steps = 93, loss = 3.5760059356689453\n",
      "steps = 93, loss = 2.630694627761841\n",
      "steps = 93, loss = 3.4431862831115723\n",
      "steps = 93, loss = 2.9973480701446533\n",
      "steps = 93, loss = 14.790969848632812\n",
      "steps = 93, loss = 2.2193474769592285\n",
      "steps = 93, loss = 2.5821168422698975\n",
      "steps = 93, loss = 3.4677979946136475\n",
      "steps = 93, loss = 5.881129741668701\n",
      "steps = 93, loss = 6.77522611618042\n",
      "steps = 93, loss = 3.470928192138672\n",
      "steps = 93, loss = 3.2934446334838867\n",
      "steps = 93, loss = 3.4250214099884033\n",
      "steps = 93, loss = 3.6276416778564453\n",
      "steps = 93, loss = 3.5357277393341064\n",
      "steps = 93, loss = 49.89482498168945\n",
      "steps = 93, loss = 3.282961368560791\n",
      "steps = 94, loss = 2.0911734104156494\n",
      "steps = 94, loss = 3.0434858798980713\n",
      "steps = 94, loss = 3.4435386657714844\n",
      "steps = 94, loss = 5.765651702880859\n",
      "steps = 94, loss = 3.2815499305725098\n",
      "steps = 94, loss = 2.986384153366089\n",
      "steps = 94, loss = 3.018394708633423\n",
      "steps = 94, loss = 15.293791770935059\n",
      "steps = 94, loss = 2.3012051582336426\n",
      "steps = 94, loss = 2.6639420986175537\n",
      "steps = 94, loss = 4.82853364944458\n",
      "steps = 94, loss = 3.757035970687866\n",
      "steps = 94, loss = 3.3680922985076904\n",
      "steps = 94, loss = 49.985382080078125\n",
      "steps = 94, loss = 2.689862012863159\n",
      "steps = 94, loss = 3.4396955966949463\n",
      "steps = 94, loss = 3.2826945781707764\n",
      "steps = 94, loss = 3.3649003505706787\n",
      "steps = 94, loss = 7.253413677215576\n",
      "steps = 94, loss = 2.853276491165161\n",
      "steps = 94, loss = 2.1295032501220703\n",
      "steps = 94, loss = 3.54760479927063\n",
      "steps = 94, loss = 2.7471225261688232\n",
      "steps = 94, loss = 3.623436212539673\n",
      "steps = 94, loss = 2.6254701614379883\n",
      "steps = 94, loss = 2.6635422706604004\n",
      "steps = 94, loss = 2.724107027053833\n",
      "steps = 94, loss = 2.5889294147491455\n",
      "steps = 94, loss = 3.5729830265045166\n",
      "steps = 94, loss = 49.96820831298828\n",
      "steps = 94, loss = 3.4831299781799316\n",
      "steps = 94, loss = 3.5462050437927246\n",
      "steps = 95, loss = 2.4688503742218018\n",
      "steps = 95, loss = 2.6049487590789795\n",
      "steps = 95, loss = 3.299975633621216\n",
      "steps = 95, loss = 49.985382080078125\n",
      "steps = 95, loss = 2.7436530590057373\n",
      "steps = 95, loss = 2.6399729251861572\n",
      "steps = 95, loss = 2.1434359550476074\n",
      "steps = 95, loss = 7.409225940704346\n",
      "steps = 95, loss = 2.9574077129364014\n",
      "steps = 95, loss = 2.8536617755889893\n",
      "steps = 95, loss = 16.16414451599121\n",
      "steps = 95, loss = 2.6551856994628906\n",
      "steps = 95, loss = 2.2573788166046143\n",
      "steps = 95, loss = 2.7169089317321777\n",
      "steps = 95, loss = 3.3081984519958496\n",
      "steps = 95, loss = 3.480400800704956\n",
      "steps = 95, loss = 2.9913957118988037\n",
      "steps = 95, loss = 3.590193510055542\n",
      "steps = 95, loss = 3.4681572914123535\n",
      "steps = 95, loss = 2.7163498401641846\n",
      "steps = 95, loss = 49.947635650634766\n",
      "steps = 95, loss = 2.8192498683929443\n",
      "steps = 95, loss = 5.818123817443848\n",
      "steps = 95, loss = 4.858133316040039\n",
      "steps = 95, loss = 3.472620725631714\n",
      "steps = 95, loss = 3.6467950344085693\n",
      "steps = 95, loss = 3.7277579307556152\n",
      "steps = 95, loss = 3.5806615352630615\n",
      "steps = 95, loss = 3.28708553314209\n",
      "steps = 95, loss = 2.033688545227051\n",
      "steps = 95, loss = 3.60014009475708\n",
      "steps = 95, loss = 3.46185302734375\n",
      "steps = 96, loss = 2.868581533432007\n",
      "steps = 96, loss = 3.0052073001861572\n",
      "steps = 96, loss = 3.3108088970184326\n",
      "steps = 96, loss = 2.6710546016693115\n",
      "steps = 96, loss = 2.7271652221679688\n",
      "steps = 96, loss = 2.0813677310943604\n",
      "steps = 96, loss = 2.9963388442993164\n",
      "steps = 96, loss = 3.571226119995117\n",
      "steps = 96, loss = 2.5896337032318115\n",
      "steps = 96, loss = 6.007369041442871\n",
      "steps = 96, loss = 3.3301379680633545\n",
      "steps = 96, loss = 2.7614238262176514\n",
      "steps = 96, loss = 14.838132858276367\n",
      "steps = 96, loss = 3.6117987632751465\n",
      "steps = 96, loss = 3.3320698738098145\n",
      "steps = 96, loss = 3.1086196899414062\n",
      "steps = 96, loss = 2.7374134063720703\n",
      "steps = 96, loss = 49.949771881103516\n",
      "steps = 96, loss = 2.869136095046997\n",
      "steps = 96, loss = 14.871133804321289\n",
      "steps = 96, loss = 3.2896554470062256\n",
      "steps = 96, loss = 3.5781028270721436\n",
      "steps = 96, loss = 3.485576629638672\n",
      "steps = 96, loss = 3.6216046810150146\n",
      "steps = 96, loss = 3.5273654460906982\n",
      "steps = 96, loss = 2.6506989002227783\n",
      "steps = 96, loss = 49.985382080078125\n",
      "steps = 96, loss = 4.9085917472839355\n",
      "steps = 96, loss = 6.422237396240234\n",
      "steps = 96, loss = 3.6649227142333984\n",
      "steps = 96, loss = 3.5711233615875244\n",
      "steps = 96, loss = 2.0997626781463623\n",
      "steps = 97, loss = 2.027736186981201\n",
      "steps = 97, loss = 3.327038288116455\n",
      "steps = 97, loss = 2.739978551864624\n",
      "steps = 97, loss = 16.003923416137695\n",
      "steps = 97, loss = 4.905261516571045\n",
      "steps = 97, loss = 2.7237584590911865\n",
      "steps = 97, loss = 7.70709753036499\n",
      "steps = 97, loss = 3.3639371395111084\n",
      "steps = 97, loss = 3.2781198024749756\n",
      "steps = 97, loss = 2.6078455448150635\n",
      "steps = 97, loss = 2.5731260776519775\n",
      "steps = 97, loss = 3.3993618488311768\n",
      "steps = 97, loss = 2.945115804672241\n",
      "steps = 97, loss = 3.4186997413635254\n",
      "steps = 97, loss = 3.6177453994750977\n",
      "steps = 97, loss = 2.9849367141723633\n",
      "steps = 97, loss = 49.985382080078125\n",
      "steps = 97, loss = 3.5104074478149414\n",
      "steps = 97, loss = 5.641397953033447\n",
      "steps = 97, loss = 3.4418020248413086\n",
      "steps = 97, loss = 2.1729395389556885\n",
      "steps = 97, loss = 3.7074952125549316\n",
      "steps = 97, loss = 3.303793430328369\n",
      "steps = 97, loss = 2.74920916557312\n",
      "steps = 97, loss = 2.0913898944854736\n",
      "steps = 97, loss = 2.708264112472534\n",
      "steps = 97, loss = 2.644352912902832\n",
      "steps = 97, loss = 3.485460042953491\n",
      "steps = 97, loss = 3.585226058959961\n",
      "steps = 97, loss = 2.8525338172912598\n",
      "steps = 97, loss = 49.940757751464844\n",
      "steps = 97, loss = 2.6374077796936035\n",
      "steps = 98, loss = 2.296140670776367\n",
      "steps = 98, loss = 2.6958775520324707\n",
      "steps = 98, loss = 3.3579964637756348\n",
      "steps = 98, loss = 14.633731842041016\n",
      "steps = 98, loss = 3.652010917663574\n",
      "steps = 98, loss = 49.95099639892578\n",
      "steps = 98, loss = 2.865147829055786\n",
      "steps = 98, loss = 3.5114545822143555\n",
      "steps = 98, loss = 2.764993190765381\n",
      "steps = 98, loss = 3.3172030448913574\n",
      "steps = 98, loss = 3.7579543590545654\n",
      "steps = 98, loss = 3.5108981132507324\n",
      "steps = 98, loss = 2.881347894668579\n",
      "steps = 98, loss = 2.6611382961273193\n",
      "steps = 98, loss = 2.739252805709839\n",
      "steps = 98, loss = 2.995244026184082\n",
      "steps = 98, loss = 3.4098362922668457\n",
      "steps = 98, loss = 49.985382080078125\n",
      "steps = 98, loss = 3.578287124633789\n",
      "steps = 98, loss = 3.1323585510253906\n",
      "steps = 98, loss = 3.2868642807006836\n",
      "steps = 98, loss = 3.4133920669555664\n",
      "steps = 98, loss = 2.870757579803467\n",
      "steps = 98, loss = 5.870323657989502\n",
      "steps = 98, loss = 2.635606288909912\n",
      "steps = 98, loss = 6.486660003662109\n",
      "steps = 98, loss = 4.95955228805542\n",
      "steps = 98, loss = 2.836803913116455\n",
      "steps = 98, loss = 3.513335704803467\n",
      "steps = 98, loss = 2.1556265354156494\n",
      "steps = 98, loss = 3.497619867324829\n",
      "steps = 98, loss = 2.1016664505004883\n",
      "steps = 99, loss = 2.103985071182251\n",
      "steps = 99, loss = 2.7170395851135254\n",
      "steps = 99, loss = 3.444211959838867\n",
      "steps = 99, loss = 49.985382080078125\n",
      "steps = 99, loss = 2.737521171569824\n",
      "steps = 99, loss = 3.3710954189300537\n",
      "steps = 99, loss = 3.3928091526031494\n",
      "steps = 99, loss = 4.9651288986206055\n",
      "steps = 99, loss = 3.2681143283843994\n",
      "steps = 99, loss = 2.8203585147857666\n",
      "steps = 99, loss = 2.7130706310272217\n",
      "steps = 99, loss = 3.6363446712493896\n",
      "steps = 99, loss = 2.423093557357788\n",
      "steps = 99, loss = 3.0405666828155518\n",
      "steps = 99, loss = 3.282628059387207\n",
      "steps = 99, loss = 2.1418285369873047\n",
      "steps = 99, loss = 49.96055603027344\n",
      "steps = 99, loss = 2.666891098022461\n",
      "steps = 99, loss = 3.4275081157684326\n",
      "steps = 99, loss = 2.729299783706665\n",
      "steps = 99, loss = 3.461214065551758\n",
      "steps = 99, loss = 49.94301986694336\n",
      "steps = 99, loss = 3.526474714279175\n",
      "steps = 99, loss = 3.6439881324768066\n",
      "steps = 99, loss = 2.6681957244873047\n",
      "steps = 99, loss = 7.2346014976501465\n",
      "steps = 99, loss = 2.990989923477173\n",
      "steps = 99, loss = 2.5612003803253174\n",
      "steps = 99, loss = 3.6470038890838623\n",
      "steps = 99, loss = 5.908533573150635\n",
      "steps = 99, loss = 2.8540892601013184\n",
      "steps = 99, loss = 16.385971069335938\n",
      "steps = 100, loss = 49.985382080078125\n",
      "steps = 100, loss = 15.403800964355469\n",
      "steps = 100, loss = 5.965456962585449\n",
      "steps = 100, loss = 3.371997833251953\n",
      "steps = 100, loss = 3.27311635017395\n",
      "steps = 100, loss = 2.661980390548706\n",
      "steps = 100, loss = 3.3355467319488525\n",
      "steps = 100, loss = 3.5273776054382324\n",
      "steps = 100, loss = 2.9828896522521973\n",
      "steps = 100, loss = 2.092465877532959\n",
      "steps = 100, loss = 49.95465850830078\n",
      "steps = 100, loss = 3.450183868408203\n",
      "steps = 100, loss = 2.2341442108154297\n",
      "steps = 100, loss = 2.606626272201538\n",
      "steps = 100, loss = 2.6438236236572266\n",
      "steps = 100, loss = 7.556753158569336\n",
      "steps = 100, loss = 2.0729994773864746\n",
      "steps = 100, loss = 3.412965774536133\n",
      "steps = 100, loss = 2.6834075450897217\n",
      "steps = 100, loss = 2.851163864135742\n",
      "steps = 100, loss = 2.6203513145446777\n",
      "steps = 100, loss = 3.662532329559326\n",
      "steps = 100, loss = 3.4402518272399902\n",
      "steps = 100, loss = 4.9774603843688965\n",
      "steps = 100, loss = 2.7388741970062256\n",
      "steps = 100, loss = 3.0160510540008545\n",
      "steps = 100, loss = 3.1355834007263184\n",
      "steps = 100, loss = 2.72285532951355\n",
      "steps = 100, loss = 8.160463333129883\n",
      "steps = 100, loss = 3.374929666519165\n",
      "steps = 100, loss = 3.5482819080352783\n",
      "steps = 100, loss = 3.487241268157959\n",
      "steps = 101, loss = 3.4036519527435303\n",
      "steps = 101, loss = 2.764174699783325\n",
      "steps = 101, loss = 2.1012072563171387\n",
      "steps = 101, loss = 3.5481927394866943\n",
      "steps = 101, loss = 2.8463642597198486\n",
      "steps = 101, loss = 3.439467191696167\n",
      "steps = 101, loss = 3.6976356506347656\n",
      "steps = 101, loss = 2.6788690090179443\n",
      "steps = 101, loss = 3.3754513263702393\n",
      "steps = 101, loss = 3.9941153526306152\n",
      "steps = 101, loss = 3.2822601795196533\n",
      "steps = 101, loss = 2.9933013916015625\n",
      "steps = 101, loss = 2.1740763187408447\n",
      "steps = 101, loss = 2.821169376373291\n",
      "steps = 101, loss = 49.95810317993164\n",
      "steps = 101, loss = 5.03294038772583\n",
      "steps = 101, loss = 2.571467638015747\n",
      "steps = 101, loss = 2.9606635570526123\n",
      "steps = 101, loss = 6.267977714538574\n",
      "steps = 101, loss = 2.8292222023010254\n",
      "steps = 101, loss = 2.1607184410095215\n",
      "steps = 101, loss = 3.3301925659179688\n",
      "steps = 101, loss = 2.607353925704956\n",
      "steps = 101, loss = 2.7389445304870605\n",
      "steps = 101, loss = 3.6004045009613037\n",
      "steps = 101, loss = 49.985382080078125\n",
      "steps = 101, loss = 3.555640935897827\n",
      "steps = 101, loss = 15.492622375488281\n",
      "steps = 101, loss = 2.8696906566619873\n",
      "steps = 101, loss = 5.893976211547852\n",
      "steps = 101, loss = 3.5525758266448975\n",
      "steps = 101, loss = 3.534904956817627\n",
      "steps = 102, loss = 2.8683488368988037\n",
      "steps = 102, loss = 16.580997467041016\n",
      "steps = 102, loss = 2.8538427352905273\n",
      "steps = 102, loss = 3.6970958709716797\n",
      "steps = 102, loss = 2.69003963470459\n",
      "steps = 102, loss = 3.5684585571289062\n",
      "steps = 102, loss = 49.99711608886719\n",
      "steps = 102, loss = 2.5580930709838867\n",
      "steps = 102, loss = 3.301677942276001\n",
      "steps = 102, loss = 3.4503400325775146\n",
      "steps = 102, loss = 2.6844451427459717\n",
      "steps = 102, loss = 2.7169830799102783\n",
      "steps = 102, loss = 2.066072702407837\n",
      "steps = 102, loss = 7.172224521636963\n",
      "steps = 102, loss = 3.690403699874878\n",
      "steps = 102, loss = 2.9892630577087402\n",
      "steps = 102, loss = 5.9885334968566895\n",
      "steps = 102, loss = 3.48895001411438\n",
      "steps = 102, loss = 3.417189121246338\n",
      "steps = 102, loss = 3.5603673458099365\n",
      "steps = 102, loss = 2.7271409034729004\n",
      "steps = 102, loss = 2.719146251678467\n",
      "steps = 102, loss = 2.918813943862915\n",
      "steps = 102, loss = 3.381216049194336\n",
      "steps = 102, loss = 3.0697762966156006\n",
      "steps = 102, loss = 3.5399458408355713\n",
      "steps = 102, loss = 2.1417951583862305\n",
      "steps = 102, loss = 3.372025489807129\n",
      "steps = 102, loss = 5.036810874938965\n",
      "steps = 102, loss = 2.197791814804077\n",
      "steps = 102, loss = 3.278329849243164\n",
      "steps = 102, loss = 49.985382080078125\n",
      "steps = 103, loss = 2.8367135524749756\n",
      "steps = 103, loss = 2.988288402557373\n",
      "steps = 103, loss = 3.433465003967285\n",
      "steps = 103, loss = 3.276444435119629\n",
      "steps = 103, loss = 3.410020351409912\n",
      "steps = 103, loss = 3.5830652713775635\n",
      "steps = 103, loss = 2.81803560256958\n",
      "steps = 103, loss = 3.4779226779937744\n",
      "steps = 103, loss = 2.7161214351654053\n",
      "steps = 103, loss = 3.2233707904815674\n",
      "steps = 103, loss = 16.38591194152832\n",
      "steps = 103, loss = 3.7055134773254395\n",
      "steps = 103, loss = 2.054049491882324\n",
      "steps = 103, loss = 2.853226900100708\n",
      "steps = 103, loss = 3.3700644969940186\n",
      "steps = 103, loss = 2.690765619277954\n",
      "steps = 103, loss = 3.4194209575653076\n",
      "steps = 103, loss = 2.2322757244110107\n",
      "steps = 103, loss = 49.985382080078125\n",
      "steps = 103, loss = 3.6416714191436768\n",
      "steps = 103, loss = 3.0361576080322266\n",
      "steps = 103, loss = 6.162921905517578\n",
      "steps = 103, loss = 5.058065414428711\n",
      "steps = 103, loss = 7.117353439331055\n",
      "steps = 103, loss = 49.9949836730957\n",
      "steps = 103, loss = 2.7360167503356934\n",
      "steps = 103, loss = 2.626923084259033\n",
      "steps = 103, loss = 3.1302804946899414\n",
      "steps = 103, loss = 2.1444666385650635\n",
      "steps = 103, loss = 3.4400129318237305\n",
      "steps = 103, loss = 3.0816800594329834\n",
      "steps = 103, loss = 3.0690011978149414\n",
      "steps = 104, loss = 7.041460990905762\n",
      "steps = 104, loss = 3.598038911819458\n",
      "steps = 104, loss = 3.509190559387207\n",
      "steps = 104, loss = 2.906491279602051\n",
      "steps = 104, loss = 2.8600692749023438\n",
      "steps = 104, loss = 2.992464780807495\n",
      "steps = 104, loss = 2.428941011428833\n",
      "steps = 104, loss = 2.69736385345459\n",
      "steps = 104, loss = 2.699448823928833\n",
      "steps = 104, loss = 2.7633516788482666\n",
      "steps = 104, loss = 3.746575117111206\n",
      "steps = 104, loss = 3.2787489891052246\n",
      "steps = 104, loss = 3.1448490619659424\n",
      "steps = 104, loss = 49.993526458740234\n",
      "steps = 104, loss = 3.7031068801879883\n",
      "steps = 104, loss = 3.604062557220459\n",
      "steps = 104, loss = 2.659778118133545\n",
      "steps = 104, loss = 2.9303739070892334\n",
      "steps = 104, loss = 2.270979166030884\n",
      "steps = 104, loss = 2.106564521789551\n",
      "steps = 104, loss = 3.4405157566070557\n",
      "steps = 104, loss = 5.889281749725342\n",
      "steps = 104, loss = 5.106473922729492\n",
      "steps = 104, loss = 2.8677608966827393\n",
      "steps = 104, loss = 3.454383373260498\n",
      "steps = 104, loss = 49.985382080078125\n",
      "steps = 104, loss = 3.3495733737945557\n",
      "steps = 104, loss = 3.177854061126709\n",
      "steps = 104, loss = 2.1009349822998047\n",
      "steps = 104, loss = 3.350991725921631\n",
      "steps = 104, loss = 15.842949867248535\n",
      "steps = 104, loss = 2.7372961044311523\n",
      "steps = 105, loss = 2.59574031829834\n",
      "steps = 105, loss = 3.4765517711639404\n",
      "steps = 105, loss = 7.437595367431641\n",
      "steps = 105, loss = 2.6011757850646973\n",
      "steps = 105, loss = 16.5546875\n",
      "steps = 105, loss = 3.5008785724639893\n",
      "steps = 105, loss = 2.69187068939209\n",
      "steps = 105, loss = 3.38974928855896\n",
      "steps = 105, loss = 2.678053617477417\n",
      "steps = 105, loss = 2.13177227973938\n",
      "steps = 105, loss = 2.706509828567505\n",
      "steps = 105, loss = 5.975091934204102\n",
      "steps = 105, loss = 2.7428698539733887\n",
      "steps = 105, loss = 2.4933695793151855\n",
      "steps = 105, loss = 3.4502675533294678\n",
      "steps = 105, loss = 2.7233340740203857\n",
      "steps = 105, loss = 3.7408621311187744\n",
      "steps = 105, loss = 2.0518240928649902\n",
      "steps = 105, loss = 2.7984988689422607\n",
      "steps = 105, loss = 2.0917136669158936\n",
      "steps = 105, loss = 3.2729077339172363\n",
      "steps = 105, loss = 49.985382080078125\n",
      "steps = 105, loss = 3.394351005554199\n",
      "steps = 105, loss = 3.2671494483947754\n",
      "steps = 105, loss = 2.8508925437927246\n",
      "steps = 105, loss = 3.418109893798828\n",
      "steps = 105, loss = 5.096045017242432\n",
      "steps = 105, loss = 3.597438097000122\n",
      "steps = 105, loss = 2.7314910888671875\n",
      "steps = 105, loss = 2.9808664321899414\n",
      "steps = 105, loss = 49.91945266723633\n",
      "steps = 105, loss = 3.509092092514038\n",
      "steps = 106, loss = 2.649245262145996\n",
      "steps = 106, loss = 2.1023366451263428\n",
      "steps = 106, loss = 2.766531467437744\n",
      "steps = 106, loss = 5.150242328643799\n",
      "steps = 106, loss = 2.1385819911956787\n",
      "steps = 106, loss = 2.9909322261810303\n",
      "steps = 106, loss = 49.94682312011719\n",
      "steps = 106, loss = 3.4820327758789062\n",
      "steps = 106, loss = 3.43741774559021\n",
      "steps = 106, loss = 3.48370099067688\n",
      "steps = 106, loss = 3.138472318649292\n",
      "steps = 106, loss = 3.600339412689209\n",
      "steps = 106, loss = 2.5576834678649902\n",
      "steps = 106, loss = 2.8702552318573\n",
      "steps = 106, loss = 49.985382080078125\n",
      "steps = 106, loss = 6.532845973968506\n",
      "steps = 106, loss = 3.4409897327423096\n",
      "steps = 106, loss = 3.2761802673339844\n",
      "steps = 106, loss = 15.718339920043945\n",
      "steps = 106, loss = 2.8673529624938965\n",
      "steps = 106, loss = 3.1102981567382812\n",
      "steps = 106, loss = 3.6222012042999268\n",
      "steps = 106, loss = 3.45090389251709\n",
      "steps = 106, loss = 6.201814651489258\n",
      "steps = 106, loss = 2.023770332336426\n",
      "steps = 106, loss = 2.7598321437835693\n",
      "steps = 106, loss = 2.7080204486846924\n",
      "steps = 106, loss = 3.7755653858184814\n",
      "steps = 106, loss = 3.770348310470581\n",
      "steps = 106, loss = 2.607623815536499\n",
      "steps = 106, loss = 2.7394845485687256\n",
      "steps = 106, loss = 3.5160937309265137\n",
      "steps = 107, loss = 3.4046132564544678\n",
      "steps = 107, loss = 2.091613531112671\n",
      "steps = 107, loss = 5.141434669494629\n",
      "steps = 107, loss = 2.7456984519958496\n",
      "steps = 107, loss = 2.643129825592041\n",
      "steps = 107, loss = 7.243436813354492\n",
      "steps = 107, loss = 3.2512705326080322\n",
      "steps = 107, loss = 2.9802374839782715\n",
      "steps = 107, loss = 2.7105205059051514\n",
      "steps = 107, loss = 3.4799935817718506\n",
      "steps = 107, loss = 3.333625316619873\n",
      "steps = 107, loss = 3.4526891708374023\n",
      "steps = 107, loss = 49.94676208496094\n",
      "steps = 107, loss = 49.281219482421875\n",
      "steps = 107, loss = 3.34242844581604\n",
      "steps = 107, loss = 3.4794840812683105\n",
      "steps = 107, loss = 5.818337917327881\n",
      "steps = 107, loss = 2.852557897567749\n",
      "steps = 107, loss = 2.7249908447265625\n",
      "steps = 107, loss = 3.2652368545532227\n",
      "steps = 107, loss = 16.019638061523438\n",
      "steps = 107, loss = 3.462252140045166\n",
      "steps = 107, loss = 3.62357497215271\n",
      "steps = 107, loss = 2.973717212677002\n",
      "steps = 107, loss = 2.749290704727173\n",
      "steps = 107, loss = 2.7018167972564697\n",
      "steps = 107, loss = 2.9489753246307373\n",
      "steps = 107, loss = 2.7826764583587646\n",
      "steps = 107, loss = 2.545574903488159\n",
      "steps = 107, loss = 2.2992565631866455\n",
      "steps = 107, loss = 3.7720298767089844\n",
      "steps = 107, loss = 49.985382080078125\n",
      "steps = 108, loss = 2.7179365158081055\n",
      "steps = 108, loss = 3.5965018272399902\n",
      "steps = 108, loss = 3.0979256629943848\n",
      "steps = 108, loss = 2.710693120956421\n",
      "steps = 108, loss = 2.9839444160461426\n",
      "steps = 108, loss = 5.165436267852783\n",
      "steps = 108, loss = 2.7152938842773438\n",
      "steps = 108, loss = 2.333674192428589\n",
      "steps = 108, loss = 3.6042776107788086\n",
      "steps = 108, loss = 2.9504616260528564\n",
      "steps = 108, loss = 3.7809414863586426\n",
      "steps = 108, loss = 3.3921873569488525\n",
      "steps = 108, loss = 3.430121421813965\n",
      "steps = 108, loss = 16.449871063232422\n",
      "steps = 108, loss = 5.929575443267822\n",
      "steps = 108, loss = 3.786830186843872\n",
      "steps = 108, loss = 3.6483147144317627\n",
      "steps = 108, loss = 2.787975788116455\n",
      "steps = 108, loss = 3.44557785987854\n",
      "steps = 108, loss = 2.6280767917633057\n",
      "steps = 108, loss = 49.985382080078125\n",
      "steps = 108, loss = 49.971675872802734\n",
      "steps = 108, loss = 7.522241592407227\n",
      "steps = 108, loss = 2.578592300415039\n",
      "steps = 108, loss = 2.1432583332061768\n",
      "steps = 108, loss = 2.991626262664795\n",
      "steps = 108, loss = 2.9849541187286377\n",
      "steps = 108, loss = 3.5063860416412354\n",
      "steps = 108, loss = 3.417839765548706\n",
      "steps = 108, loss = 3.072343587875366\n",
      "steps = 108, loss = 3.2695374488830566\n",
      "steps = 108, loss = 2.853290319442749\n",
      "steps = 109, loss = 2.984766721725464\n",
      "steps = 109, loss = 3.509305238723755\n",
      "steps = 109, loss = 7.528868675231934\n",
      "steps = 109, loss = 3.4349818229675293\n",
      "steps = 109, loss = 3.375412702560425\n",
      "steps = 109, loss = 3.7977054119110107\n",
      "steps = 109, loss = 2.3910224437713623\n",
      "steps = 109, loss = 2.988658905029297\n",
      "steps = 109, loss = 16.736684799194336\n",
      "steps = 109, loss = 2.737433433532715\n",
      "steps = 109, loss = 2.716829538345337\n",
      "steps = 109, loss = 2.707688093185425\n",
      "steps = 109, loss = 2.664778709411621\n",
      "steps = 109, loss = 3.2684247493743896\n",
      "steps = 109, loss = 2.2965121269226074\n",
      "steps = 109, loss = 3.6646339893341064\n",
      "steps = 109, loss = 2.837555408477783\n",
      "steps = 109, loss = 3.63238263130188\n",
      "steps = 109, loss = 3.524604082107544\n",
      "steps = 109, loss = 5.18621826171875\n",
      "steps = 109, loss = 3.4270777702331543\n",
      "steps = 109, loss = 2.5641021728515625\n",
      "steps = 109, loss = 2.778972625732422\n",
      "steps = 109, loss = 49.985382080078125\n",
      "steps = 109, loss = 6.056900501251221\n",
      "steps = 109, loss = 2.8529720306396484\n",
      "steps = 109, loss = 2.7223258018493652\n",
      "steps = 109, loss = 2.1441872119903564\n",
      "steps = 109, loss = 3.5728490352630615\n",
      "steps = 109, loss = 49.96548080444336\n",
      "steps = 109, loss = 3.4971837997436523\n",
      "steps = 109, loss = 2.926424980163574\n",
      "steps = 110, loss = 3.377570390701294\n",
      "steps = 110, loss = 2.1266579627990723\n",
      "steps = 110, loss = 3.4044814109802246\n",
      "steps = 110, loss = 3.318408966064453\n",
      "steps = 110, loss = 2.9772684574127197\n",
      "steps = 110, loss = 3.664386510848999\n",
      "steps = 110, loss = 14.795361518859863\n",
      "steps = 110, loss = 3.603463649749756\n",
      "steps = 110, loss = 2.641920804977417\n",
      "steps = 110, loss = 5.9966936111450195\n",
      "steps = 110, loss = 2.925976276397705\n",
      "steps = 110, loss = 3.009000539779663\n",
      "steps = 110, loss = 49.95589065551758\n",
      "steps = 110, loss = 2.8499300479888916\n",
      "steps = 110, loss = 2.093233823776245\n",
      "steps = 110, loss = 2.7203733921051025\n",
      "steps = 110, loss = 2.7222399711608887\n",
      "steps = 110, loss = 3.0690720081329346\n",
      "steps = 110, loss = 3.8154401779174805\n",
      "steps = 110, loss = 3.358426332473755\n",
      "steps = 110, loss = 3.259321451187134\n",
      "steps = 110, loss = 5.19426155090332\n",
      "steps = 110, loss = 16.605295181274414\n",
      "steps = 110, loss = 2.717242479324341\n",
      "steps = 110, loss = 2.6597719192504883\n",
      "steps = 110, loss = 49.985382080078125\n",
      "steps = 110, loss = 3.525397539138794\n",
      "steps = 110, loss = 2.2857577800750732\n",
      "steps = 110, loss = 3.684926748275757\n",
      "steps = 110, loss = 3.441486358642578\n",
      "steps = 110, loss = 3.0835683345794678\n",
      "steps = 110, loss = 2.7385330200195312\n",
      "steps = 111, loss = 3.6885411739349365\n",
      "steps = 111, loss = 3.5984089374542236\n",
      "steps = 111, loss = 2.8012306690216064\n",
      "steps = 111, loss = 3.398664951324463\n",
      "steps = 111, loss = 3.054225444793701\n",
      "steps = 111, loss = 2.8681211471557617\n",
      "steps = 111, loss = 49.95408630371094\n",
      "steps = 111, loss = 3.4074246883392334\n",
      "steps = 111, loss = 2.9372177124023438\n",
      "steps = 111, loss = 3.3458776473999023\n",
      "steps = 111, loss = 2.7649688720703125\n",
      "steps = 111, loss = 2.6043293476104736\n",
      "steps = 111, loss = 3.8513262271881104\n",
      "steps = 111, loss = 2.1014938354492188\n",
      "steps = 111, loss = 6.067944049835205\n",
      "steps = 111, loss = 2.1288185119628906\n",
      "steps = 111, loss = 2.8018360137939453\n",
      "steps = 111, loss = 3.5880961418151855\n",
      "steps = 111, loss = 3.4599294662475586\n",
      "steps = 111, loss = 16.289020538330078\n",
      "steps = 111, loss = 2.7335593700408936\n",
      "steps = 111, loss = 2.9967002868652344\n",
      "steps = 111, loss = 2.1348676681518555\n",
      "steps = 111, loss = 3.558405876159668\n",
      "steps = 111, loss = 3.314175844192505\n",
      "steps = 111, loss = 3.268277645111084\n",
      "steps = 111, loss = 5.248378753662109\n",
      "steps = 111, loss = 2.7389116287231445\n",
      "steps = 111, loss = 10.661163330078125\n",
      "steps = 111, loss = 2.9874939918518066\n",
      "steps = 111, loss = 2.658027410507202\n",
      "steps = 111, loss = 49.985382080078125\n",
      "steps = 112, loss = 2.7445220947265625\n",
      "steps = 112, loss = 6.003541469573975\n",
      "steps = 112, loss = 3.25832462310791\n",
      "steps = 112, loss = 5.2389044761657715\n",
      "steps = 112, loss = 2.9773972034454346\n",
      "steps = 112, loss = 3.847898006439209\n",
      "steps = 112, loss = 6.790746688842773\n",
      "steps = 112, loss = 2.6777780055999756\n",
      "steps = 112, loss = 3.267031192779541\n",
      "steps = 112, loss = 3.676222562789917\n",
      "steps = 112, loss = 3.690539836883545\n",
      "steps = 112, loss = 2.7275712490081787\n",
      "steps = 112, loss = 49.96886444091797\n",
      "steps = 112, loss = 2.1953513622283936\n",
      "steps = 112, loss = 2.93143892288208\n",
      "steps = 112, loss = 2.091826915740967\n",
      "steps = 112, loss = 2.7494606971740723\n",
      "steps = 112, loss = 3.556711196899414\n",
      "steps = 112, loss = 3.2613375186920166\n",
      "steps = 112, loss = 16.539579391479492\n",
      "steps = 112, loss = 2.8524832725524902\n",
      "steps = 112, loss = 49.985382080078125\n",
      "steps = 112, loss = 2.664062976837158\n",
      "steps = 112, loss = 2.749174118041992\n",
      "steps = 112, loss = 2.092146635055542\n",
      "steps = 112, loss = 3.446397304534912\n",
      "steps = 112, loss = 3.0349411964416504\n",
      "steps = 112, loss = 3.554621696472168\n",
      "steps = 112, loss = 2.724701166152954\n",
      "steps = 112, loss = 3.0117263793945312\n",
      "steps = 112, loss = 3.4909896850585938\n",
      "steps = 112, loss = 3.3571510314941406\n",
      "steps = 113, loss = 3.151134967803955\n",
      "steps = 113, loss = 2.3442704677581787\n",
      "steps = 113, loss = 2.818929433822632\n",
      "steps = 113, loss = 2.6595113277435303\n",
      "steps = 113, loss = 3.3852298259735107\n",
      "steps = 113, loss = 2.1428658962249756\n",
      "steps = 113, loss = 2.8533380031585693\n",
      "steps = 113, loss = 2.7165400981903076\n",
      "steps = 113, loss = 3.419390916824341\n",
      "steps = 113, loss = 2.694418430328369\n",
      "steps = 113, loss = 2.9821889400482178\n",
      "steps = 113, loss = 3.858703851699829\n",
      "steps = 113, loss = 3.5844476222991943\n",
      "steps = 113, loss = 49.984073638916016\n",
      "steps = 113, loss = 5.416192054748535\n",
      "steps = 113, loss = 3.1930477619171143\n",
      "steps = 113, loss = 3.1004302501678467\n",
      "steps = 113, loss = 2.672485589981079\n",
      "steps = 113, loss = 3.7153384685516357\n",
      "steps = 113, loss = 2.078474760055542\n",
      "steps = 113, loss = 2.7408499717712402\n",
      "steps = 113, loss = 2.7183053493499756\n",
      "steps = 113, loss = 3.7918732166290283\n",
      "steps = 113, loss = 3.526914119720459\n",
      "steps = 113, loss = 3.4348676204681396\n",
      "steps = 113, loss = 5.262906551361084\n",
      "steps = 113, loss = 6.081047058105469\n",
      "steps = 113, loss = 49.985382080078125\n",
      "steps = 113, loss = 3.497856378555298\n",
      "steps = 113, loss = 3.2627968788146973\n",
      "steps = 113, loss = 16.425537109375\n",
      "steps = 113, loss = 2.6301863193511963\n",
      "steps = 114, loss = 3.7313921451568604\n",
      "steps = 114, loss = 2.9870147705078125\n",
      "steps = 114, loss = 2.684267282485962\n",
      "steps = 114, loss = 5.310133457183838\n",
      "steps = 114, loss = 3.459084987640381\n",
      "steps = 114, loss = 6.256394863128662\n",
      "steps = 114, loss = 3.3867404460906982\n",
      "steps = 114, loss = 2.82171368598938\n",
      "steps = 114, loss = 16.394807815551758\n",
      "steps = 114, loss = 3.6425745487213135\n",
      "steps = 114, loss = 3.505934476852417\n",
      "steps = 114, loss = 3.89880633354187\n",
      "steps = 114, loss = 2.100529909133911\n",
      "steps = 114, loss = 3.0599722862243652\n",
      "steps = 114, loss = 2.5938026905059814\n",
      "steps = 114, loss = 3.618184804916382\n",
      "steps = 114, loss = 3.4418649673461914\n",
      "steps = 114, loss = 3.2656359672546387\n",
      "steps = 114, loss = 2.1422622203826904\n",
      "steps = 114, loss = 2.7506439685821533\n",
      "steps = 114, loss = 49.985382080078125\n",
      "steps = 114, loss = 49.98237609863281\n",
      "steps = 114, loss = 2.868042469024658\n",
      "steps = 114, loss = 2.653315544128418\n",
      "steps = 114, loss = 3.608215093612671\n",
      "steps = 114, loss = 3.5091187953948975\n",
      "steps = 114, loss = 3.0513432025909424\n",
      "steps = 114, loss = 2.11765193939209\n",
      "steps = 114, loss = 2.764371633529663\n",
      "steps = 114, loss = 4.778243064880371\n",
      "steps = 114, loss = 2.945932626724243\n",
      "steps = 114, loss = 2.7382547855377197\n",
      "steps = 115, loss = 3.7043662071228027\n",
      "steps = 115, loss = 17.101224899291992\n",
      "steps = 115, loss = 3.1037330627441406\n",
      "steps = 115, loss = 3.4045534133911133\n",
      "steps = 115, loss = 3.414262533187866\n",
      "steps = 115, loss = 3.482748508453369\n",
      "steps = 115, loss = 49.985382080078125\n",
      "steps = 115, loss = 3.5107386112213135\n",
      "steps = 115, loss = 3.4525890350341797\n",
      "steps = 115, loss = 2.535674810409546\n",
      "steps = 115, loss = 2.8521358966827393\n",
      "steps = 115, loss = 3.8944315910339355\n",
      "steps = 115, loss = 2.6058905124664307\n",
      "steps = 115, loss = 5.864624977111816\n",
      "steps = 115, loss = 2.808201313018799\n",
      "steps = 115, loss = 2.9762682914733887\n",
      "steps = 115, loss = 3.254991054534912\n",
      "steps = 115, loss = 3.470020294189453\n",
      "steps = 115, loss = 2.691222906112671\n",
      "steps = 115, loss = 3.6050608158111572\n",
      "steps = 115, loss = 3.7319204807281494\n",
      "steps = 115, loss = 2.7446556091308594\n",
      "steps = 115, loss = 5.2967071533203125\n",
      "steps = 115, loss = 2.6640090942382812\n",
      "steps = 115, loss = 2.1142823696136475\n",
      "steps = 115, loss = 2.724522352218628\n",
      "steps = 115, loss = 49.98101806640625\n",
      "steps = 115, loss = 3.9357078075408936\n",
      "steps = 115, loss = 2.748279094696045\n",
      "steps = 115, loss = 2.091806411743164\n",
      "steps = 115, loss = 2.091975212097168\n",
      "steps = 115, loss = 2.743460178375244\n",
      "steps = 116, loss = 3.2092015743255615\n",
      "steps = 116, loss = 2.8529319763183594\n",
      "steps = 116, loss = 2.9702134132385254\n",
      "steps = 116, loss = 2.1428725719451904\n",
      "steps = 116, loss = 49.985382080078125\n",
      "steps = 116, loss = 3.9050369262695312\n",
      "steps = 116, loss = 2.6318585872650146\n",
      "steps = 116, loss = 3.5447356700897217\n",
      "steps = 116, loss = 2.756329298019409\n",
      "steps = 116, loss = 2.8011505603790283\n",
      "steps = 116, loss = 3.4948570728302\n",
      "steps = 116, loss = 2.7804503440856934\n",
      "steps = 116, loss = 2.980889320373535\n",
      "steps = 116, loss = 2.156097650527954\n",
      "steps = 116, loss = 17.545949935913086\n",
      "steps = 116, loss = 3.332223415374756\n",
      "steps = 116, loss = 3.2594356536865234\n",
      "steps = 116, loss = 5.99841833114624\n",
      "steps = 116, loss = 2.8533496856689453\n",
      "steps = 116, loss = 3.4192817211151123\n",
      "steps = 116, loss = 3.632084846496582\n",
      "steps = 116, loss = 2.718463182449341\n",
      "steps = 116, loss = 3.333157539367676\n",
      "steps = 116, loss = 3.3732070922851562\n",
      "steps = 116, loss = 4.1892290115356445\n",
      "steps = 116, loss = 3.6608357429504395\n",
      "steps = 116, loss = 2.720977306365967\n",
      "steps = 116, loss = 49.96912384033203\n",
      "steps = 116, loss = 2.6739566326141357\n",
      "steps = 116, loss = 5.320072174072266\n",
      "steps = 116, loss = 3.7560195922851562\n",
      "steps = 116, loss = 2.0251505374908447\n",
      "steps = 117, loss = 2.7383415699005127\n",
      "steps = 117, loss = 4.195550918579102\n",
      "steps = 117, loss = 17.016664505004883\n",
      "steps = 117, loss = 3.5850558280944824\n",
      "steps = 117, loss = 2.6935722827911377\n",
      "steps = 117, loss = 3.9434168338775635\n",
      "steps = 117, loss = 2.321074962615967\n",
      "steps = 117, loss = 2.1435468196868896\n",
      "steps = 117, loss = 49.96658706665039\n",
      "steps = 117, loss = 3.502018928527832\n",
      "steps = 117, loss = 3.7275121212005615\n",
      "steps = 117, loss = 2.100783109664917\n",
      "steps = 117, loss = 2.580005645751953\n",
      "steps = 117, loss = 3.5127620697021484\n",
      "steps = 117, loss = 2.868171453475952\n",
      "steps = 117, loss = 2.985675573348999\n",
      "steps = 117, loss = 3.4904139041900635\n",
      "steps = 117, loss = 2.6191675662994385\n",
      "steps = 117, loss = 6.099091053009033\n",
      "steps = 117, loss = 3.262451648712158\n",
      "steps = 117, loss = 2.8481991291046143\n",
      "steps = 117, loss = 3.03330397605896\n",
      "steps = 117, loss = 3.6554951667785645\n",
      "steps = 117, loss = 2.809666156768799\n",
      "steps = 117, loss = 2.7660350799560547\n",
      "steps = 117, loss = 3.323855400085449\n",
      "steps = 117, loss = 2.9900169372558594\n",
      "steps = 117, loss = 5.364113807678223\n",
      "steps = 117, loss = 3.7713515758514404\n",
      "steps = 117, loss = 49.985382080078125\n",
      "steps = 117, loss = 3.565960168838501\n",
      "steps = 117, loss = 2.765245199203491\n",
      "steps = 118, loss = 2.8520939350128174\n",
      "steps = 118, loss = 2.252624750137329\n",
      "steps = 118, loss = 2.6163556575775146\n",
      "steps = 118, loss = 2.6072094440460205\n",
      "steps = 118, loss = 2.5606565475463867\n",
      "steps = 118, loss = 3.133927345275879\n",
      "steps = 118, loss = 3.339155435562134\n",
      "steps = 118, loss = 3.1492257118225098\n",
      "steps = 118, loss = 2.0876636505126953\n",
      "steps = 118, loss = 3.0438010692596436\n",
      "steps = 118, loss = 2.7247307300567627\n",
      "steps = 118, loss = 49.96407699584961\n",
      "steps = 118, loss = 6.231454372406006\n",
      "steps = 118, loss = 3.7716383934020996\n",
      "steps = 118, loss = 2.717794418334961\n",
      "steps = 118, loss = 3.9383292198181152\n",
      "steps = 118, loss = 3.5250158309936523\n",
      "steps = 118, loss = 2.975041151046753\n",
      "steps = 118, loss = 3.245352268218994\n",
      "steps = 118, loss = 3.4601831436157227\n",
      "steps = 118, loss = 3.4067282676696777\n",
      "steps = 118, loss = 3.251800537109375\n",
      "steps = 118, loss = 2.745823383331299\n",
      "steps = 118, loss = 3.413707971572876\n",
      "steps = 118, loss = 17.552087783813477\n",
      "steps = 118, loss = 5.350477695465088\n",
      "steps = 118, loss = 3.3648793697357178\n",
      "steps = 118, loss = 49.985382080078125\n",
      "steps = 118, loss = 2.0921192169189453\n",
      "steps = 118, loss = 3.652174472808838\n",
      "steps = 118, loss = 2.7583751678466797\n",
      "steps = 118, loss = 4.415230751037598\n",
      "steps = 119, loss = 2.7381205558776855\n",
      "steps = 119, loss = 3.2496962547302246\n",
      "steps = 119, loss = 6.001413822174072\n",
      "steps = 119, loss = 3.9499404430389404\n",
      "steps = 119, loss = 3.323751449584961\n",
      "steps = 119, loss = 2.8533811569213867\n",
      "steps = 119, loss = 2.142796039581299\n",
      "steps = 119, loss = 3.584904432296753\n",
      "steps = 119, loss = 2.6657235622406006\n",
      "steps = 119, loss = 4.908206462860107\n",
      "steps = 119, loss = 3.5929453372955322\n",
      "steps = 119, loss = 49.94900894165039\n",
      "steps = 119, loss = 2.721147060394287\n",
      "steps = 119, loss = 2.7221033573150635\n",
      "steps = 119, loss = 3.6798582077026367\n",
      "steps = 119, loss = 3.795562505722046\n",
      "steps = 119, loss = 2.771284341812134\n",
      "steps = 119, loss = 3.507035970687866\n",
      "steps = 119, loss = 5.373626708984375\n",
      "steps = 119, loss = 3.256286382675171\n",
      "steps = 119, loss = 3.6949498653411865\n",
      "steps = 119, loss = 49.985382080078125\n",
      "steps = 119, loss = 3.0567588806152344\n",
      "steps = 119, loss = 2.1907734870910645\n",
      "steps = 119, loss = 16.841033935546875\n",
      "steps = 119, loss = 2.725038766860962\n",
      "steps = 119, loss = 2.986666202545166\n",
      "steps = 119, loss = 2.9795963764190674\n",
      "steps = 119, loss = 2.6854166984558105\n",
      "steps = 119, loss = 3.456925392150879\n",
      "steps = 119, loss = 2.718674659729004\n",
      "steps = 119, loss = 2.050703763961792\n",
      "steps = 120, loss = 3.408221960067749\n",
      "steps = 120, loss = 3.7038135528564453\n",
      "steps = 120, loss = 2.7109601497650146\n",
      "steps = 120, loss = 2.984384775161743\n",
      "steps = 120, loss = 2.7806451320648193\n",
      "steps = 120, loss = 2.1395153999328613\n",
      "steps = 120, loss = 2.620659351348877\n",
      "steps = 120, loss = 3.4650943279266357\n",
      "steps = 120, loss = 5.4130635261535645\n",
      "steps = 120, loss = 2.9419777393341064\n",
      "steps = 120, loss = 3.441012382507324\n",
      "steps = 120, loss = 2.651252031326294\n",
      "steps = 120, loss = 2.8103411197662354\n",
      "steps = 120, loss = 3.116999864578247\n",
      "steps = 120, loss = 2.7385663986206055\n",
      "steps = 120, loss = 3.9857678413391113\n",
      "steps = 120, loss = 4.663954734802246\n",
      "steps = 120, loss = 3.473797082901001\n",
      "steps = 120, loss = 2.1010146141052246\n",
      "steps = 120, loss = 3.2593905925750732\n",
      "steps = 120, loss = 2.7280540466308594\n",
      "steps = 120, loss = 17.067188262939453\n",
      "steps = 120, loss = 2.7657034397125244\n",
      "steps = 120, loss = 2.3656325340270996\n",
      "steps = 120, loss = 3.610215425491333\n",
      "steps = 120, loss = 6.135990619659424\n",
      "steps = 120, loss = 49.895023345947266\n",
      "steps = 120, loss = 2.8682923316955566\n",
      "steps = 120, loss = 3.5792808532714844\n",
      "steps = 120, loss = 3.5974271297454834\n",
      "steps = 120, loss = 49.985382080078125\n",
      "steps = 120, loss = 3.8109779357910156\n",
      "steps = 121, loss = 3.7009694576263428\n",
      "steps = 121, loss = 3.366036891937256\n",
      "steps = 121, loss = 2.7249701023101807\n",
      "steps = 121, loss = 3.8110103607177734\n",
      "steps = 121, loss = 3.9798905849456787\n",
      "steps = 121, loss = 2.797712564468384\n",
      "steps = 121, loss = 3.0793261528015137\n",
      "steps = 121, loss = 3.435107707977295\n",
      "steps = 121, loss = 2.3396997451782227\n",
      "steps = 121, loss = 2.6812007427215576\n",
      "steps = 121, loss = 2.7730519771575928\n",
      "steps = 121, loss = 2.8521194458007812\n",
      "steps = 121, loss = 3.691857099533081\n",
      "steps = 121, loss = 3.3382155895233154\n",
      "steps = 121, loss = 11.229455947875977\n",
      "steps = 121, loss = 2.9783685207366943\n",
      "steps = 121, loss = 3.014601707458496\n",
      "steps = 121, loss = 49.89827346801758\n",
      "steps = 121, loss = 2.703486680984497\n",
      "steps = 121, loss = 49.985382080078125\n",
      "steps = 121, loss = 2.973860740661621\n",
      "steps = 121, loss = 2.0922141075134277\n",
      "steps = 121, loss = 3.2488746643066406\n",
      "steps = 121, loss = 49.93252182006836\n",
      "steps = 121, loss = 6.294541835784912\n",
      "steps = 121, loss = 2.746198892593384\n",
      "steps = 121, loss = 2.6804778575897217\n",
      "steps = 121, loss = 3.653193235397339\n",
      "steps = 121, loss = 5.398468494415283\n",
      "steps = 121, loss = 16.975048065185547\n",
      "steps = 121, loss = 3.499358892440796\n",
      "steps = 121, loss = 3.4082305431365967\n",
      "steps = 122, loss = 2.3171157836914062\n",
      "steps = 122, loss = 2.978346347808838\n",
      "steps = 122, loss = 3.835036039352417\n",
      "steps = 122, loss = 3.5778708457946777\n",
      "steps = 122, loss = 49.985382080078125\n",
      "steps = 122, loss = 2.785435438156128\n",
      "steps = 122, loss = 2.853436231613159\n",
      "steps = 122, loss = 3.4424126148223877\n",
      "steps = 122, loss = 5.423651695251465\n",
      "steps = 122, loss = 49.95686721801758\n",
      "steps = 122, loss = 2.6715660095214844\n",
      "steps = 122, loss = 2.7225589752197266\n",
      "steps = 122, loss = 9.920343399047852\n",
      "steps = 122, loss = 2.780113935470581\n",
      "steps = 122, loss = 2.1427152156829834\n",
      "steps = 122, loss = 16.96611785888672\n",
      "steps = 122, loss = 2.790889263153076\n",
      "steps = 122, loss = 5.990609169006348\n",
      "steps = 122, loss = 1.9813278913497925\n",
      "steps = 122, loss = 3.624056100845337\n",
      "steps = 122, loss = 3.9919486045837402\n",
      "steps = 122, loss = 2.659022331237793\n",
      "steps = 122, loss = 3.5019121170043945\n",
      "steps = 122, loss = 3.2533485889434814\n",
      "steps = 122, loss = 3.46028470993042\n",
      "steps = 122, loss = 3.7284913063049316\n",
      "steps = 122, loss = 3.084102153778076\n",
      "steps = 122, loss = 3.5118536949157715\n",
      "steps = 122, loss = 2.998295783996582\n",
      "steps = 122, loss = 2.7188658714294434\n",
      "steps = 122, loss = 3.363400459289551\n",
      "steps = 122, loss = 3.2031643390655518\n",
      "steps = 123, loss = 3.4496865272521973\n",
      "steps = 123, loss = 17.187488555908203\n",
      "steps = 123, loss = 5.4593963623046875\n",
      "steps = 123, loss = 2.868417978286743\n",
      "steps = 123, loss = 3.2565650939941406\n",
      "steps = 123, loss = 3.5952138900756836\n",
      "steps = 123, loss = 2.5678212642669678\n",
      "steps = 123, loss = 2.0929205417633057\n",
      "steps = 123, loss = 2.1308064460754395\n",
      "steps = 123, loss = 4.025406837463379\n",
      "steps = 123, loss = 49.964813232421875\n",
      "steps = 123, loss = 3.7544500827789307\n",
      "steps = 123, loss = 3.3260185718536377\n",
      "steps = 123, loss = 2.983140707015991\n",
      "steps = 123, loss = 3.0615062713623047\n",
      "steps = 123, loss = 5.538965225219727\n",
      "steps = 123, loss = 2.9432928562164307\n",
      "steps = 123, loss = 2.7948508262634277\n",
      "steps = 123, loss = 2.7387888431549072\n",
      "steps = 123, loss = 2.76611590385437\n",
      "steps = 123, loss = 49.985382080078125\n",
      "steps = 123, loss = 3.8498923778533936\n",
      "steps = 123, loss = 3.3988451957702637\n",
      "steps = 123, loss = 3.63156795501709\n",
      "steps = 123, loss = 2.807208776473999\n",
      "steps = 123, loss = 3.7533721923828125\n",
      "steps = 123, loss = 2.569761037826538\n",
      "steps = 123, loss = 2.8639767169952393\n",
      "steps = 123, loss = 2.8320014476776123\n",
      "steps = 123, loss = 3.4117226600646973\n",
      "steps = 123, loss = 2.101217269897461\n",
      "steps = 123, loss = 6.167498588562012\n",
      "steps = 124, loss = 49.950927734375\n",
      "steps = 124, loss = 2.141077995300293\n",
      "steps = 124, loss = 5.4620280265808105\n",
      "steps = 124, loss = 2.7981228828430176\n",
      "steps = 124, loss = 2.0562045574188232\n",
      "steps = 124, loss = 2.826021194458008\n",
      "steps = 124, loss = 2.8932316303253174\n",
      "steps = 124, loss = 2.9790194034576416\n",
      "steps = 124, loss = 2.7150118350982666\n",
      "steps = 124, loss = 3.8643455505371094\n",
      "steps = 124, loss = 3.5111682415008545\n",
      "steps = 124, loss = 2.731804847717285\n",
      "steps = 124, loss = 2.8540093898773193\n",
      "steps = 124, loss = 5.444672584533691\n",
      "steps = 124, loss = 3.7658212184906006\n",
      "steps = 124, loss = 3.5014965534210205\n",
      "steps = 124, loss = 3.4963934421539307\n",
      "steps = 124, loss = 17.419893264770508\n",
      "steps = 124, loss = 2.7185027599334717\n",
      "steps = 124, loss = 3.304410934448242\n",
      "steps = 124, loss = 2.5513691902160645\n",
      "steps = 124, loss = 3.0047643184661865\n",
      "steps = 124, loss = 3.7130279541015625\n",
      "steps = 124, loss = 3.2529053688049316\n",
      "steps = 124, loss = 3.4548425674438477\n",
      "steps = 124, loss = 23.783061981201172\n",
      "steps = 124, loss = 6.284450054168701\n",
      "steps = 124, loss = 2.059804916381836\n",
      "steps = 124, loss = 4.023066520690918\n",
      "steps = 124, loss = 3.0164754390716553\n",
      "steps = 124, loss = 49.985382080078125\n",
      "steps = 124, loss = 3.5768771171569824\n",
      "steps = 125, loss = 17.762496948242188\n",
      "steps = 125, loss = 3.3745980262756348\n",
      "steps = 125, loss = 2.7279481887817383\n",
      "steps = 125, loss = 3.244162082672119\n",
      "steps = 125, loss = 2.85037899017334\n",
      "steps = 125, loss = 2.197112798690796\n",
      "steps = 125, loss = 3.862656593322754\n",
      "steps = 125, loss = 3.24002742767334\n",
      "steps = 125, loss = 2.740961790084839\n",
      "steps = 125, loss = 2.6183342933654785\n",
      "steps = 125, loss = 3.486523389816284\n",
      "steps = 125, loss = 2.7983124256134033\n",
      "steps = 125, loss = 2.58622670173645\n",
      "steps = 125, loss = 2.09323787689209\n",
      "steps = 125, loss = 3.7658798694610596\n",
      "steps = 125, loss = 4.0304670333862305\n",
      "steps = 125, loss = 6.0270843505859375\n",
      "steps = 125, loss = 3.4395751953125\n",
      "steps = 125, loss = 2.9714035987854004\n",
      "steps = 125, loss = 2.094378709793091\n",
      "steps = 125, loss = 5.453690528869629\n",
      "steps = 125, loss = 2.8824756145477295\n",
      "steps = 125, loss = 3.4002652168273926\n",
      "steps = 125, loss = 49.985382080078125\n",
      "steps = 125, loss = 3.541712760925293\n",
      "steps = 125, loss = 3.6326963901519775\n",
      "steps = 125, loss = 49.91842269897461\n",
      "steps = 125, loss = 2.7918193340301514\n",
      "steps = 125, loss = 3.458406686782837\n",
      "steps = 125, loss = 2.7351558208465576\n",
      "steps = 125, loss = 4.826420307159424\n",
      "steps = 125, loss = 2.7233047485351562\n",
      "steps = 126, loss = 4.061007022857666\n",
      "steps = 126, loss = 3.512373685836792\n",
      "steps = 126, loss = 3.536409616470337\n",
      "steps = 126, loss = 3.2046263217926025\n",
      "steps = 126, loss = 17.65340805053711\n",
      "steps = 126, loss = 3.2548646926879883\n",
      "steps = 126, loss = 2.8692991733551025\n",
      "steps = 126, loss = 2.1027369499206543\n",
      "steps = 126, loss = 6.231484413146973\n",
      "steps = 126, loss = 3.4765751361846924\n",
      "steps = 126, loss = 3.2534589767456055\n",
      "steps = 126, loss = 3.080181121826172\n",
      "steps = 126, loss = 2.991826295852661\n",
      "steps = 126, loss = 3.4275946617126465\n",
      "steps = 126, loss = 3.8858911991119385\n",
      "steps = 126, loss = 2.7399699687957764\n",
      "steps = 126, loss = 2.6663730144500732\n",
      "steps = 126, loss = 2.5241692066192627\n",
      "steps = 126, loss = 2.9811995029449463\n",
      "steps = 126, loss = 49.985382080078125\n",
      "steps = 126, loss = 5.499520301818848\n",
      "steps = 126, loss = 3.476762294769287\n",
      "steps = 126, loss = 2.8073792457580566\n",
      "steps = 126, loss = 3.0358550548553467\n",
      "steps = 126, loss = 2.165992021560669\n",
      "steps = 126, loss = 2.2419753074645996\n",
      "steps = 126, loss = 3.6092171669006348\n",
      "steps = 126, loss = 2.7678539752960205\n",
      "steps = 126, loss = 49.937564849853516\n",
      "steps = 126, loss = 4.848524570465088\n",
      "steps = 126, loss = 3.798675537109375\n",
      "steps = 126, loss = 2.867652654647827\n",
      "steps = 127, loss = 49.985382080078125\n",
      "steps = 127, loss = 2.971425771713257\n",
      "steps = 127, loss = 5.584582805633545\n",
      "steps = 127, loss = 3.490062713623047\n",
      "steps = 127, loss = 3.7520573139190674\n",
      "steps = 127, loss = 5.483745098114014\n",
      "steps = 127, loss = 3.243511915206909\n",
      "steps = 127, loss = 2.725705862045288\n",
      "steps = 127, loss = 2.753103256225586\n",
      "steps = 127, loss = 3.88730525970459\n",
      "steps = 127, loss = 2.679703712463379\n",
      "steps = 127, loss = 3.005626916885376\n",
      "steps = 127, loss = 2.5356154441833496\n",
      "steps = 127, loss = 2.786149501800537\n",
      "steps = 127, loss = 3.7971882820129395\n",
      "steps = 127, loss = 2.0854859352111816\n",
      "steps = 127, loss = 2.1205849647521973\n",
      "steps = 127, loss = 3.451314687728882\n",
      "steps = 127, loss = 2.8746650218963623\n",
      "steps = 127, loss = 3.56475830078125\n",
      "steps = 127, loss = 2.8528642654418945\n",
      "steps = 127, loss = 2.0921807289123535\n",
      "steps = 127, loss = 17.79594612121582\n",
      "steps = 127, loss = 3.3545682430267334\n",
      "steps = 127, loss = 2.8004345893859863\n",
      "steps = 127, loss = 3.3910038471221924\n",
      "steps = 127, loss = 3.213461399078369\n",
      "steps = 127, loss = 4.055948257446289\n",
      "steps = 127, loss = 6.317323207855225\n",
      "steps = 127, loss = 2.684281587600708\n",
      "steps = 127, loss = 3.1832151412963867\n",
      "steps = 127, loss = 49.947792053222656\n",
      "steps = 128, loss = 5.458728790283203\n",
      "steps = 128, loss = 2.7412257194519043\n",
      "steps = 128, loss = 5.529095649719238\n",
      "steps = 128, loss = 49.96119689941406\n",
      "steps = 128, loss = 3.517136335372925\n",
      "steps = 128, loss = 4.085470676422119\n",
      "steps = 128, loss = 3.337286949157715\n",
      "steps = 128, loss = 18.196521759033203\n",
      "steps = 128, loss = 3.021578550338745\n",
      "steps = 128, loss = 2.981099843978882\n",
      "steps = 128, loss = 2.5787341594696045\n",
      "steps = 128, loss = 3.162993907928467\n",
      "steps = 128, loss = 2.381570339202881\n",
      "steps = 128, loss = 3.6123886108398438\n",
      "steps = 128, loss = 2.1527345180511475\n",
      "steps = 128, loss = 49.985382080078125\n",
      "steps = 128, loss = 3.56953763961792\n",
      "steps = 128, loss = 3.832021713256836\n",
      "steps = 128, loss = 3.4292759895324707\n",
      "steps = 128, loss = 2.910993814468384\n",
      "steps = 128, loss = 2.1030611991882324\n",
      "steps = 128, loss = 2.8711726665496826\n",
      "steps = 128, loss = 3.5221667289733887\n",
      "steps = 128, loss = 3.252673625946045\n",
      "steps = 128, loss = 6.01777458190918\n",
      "steps = 128, loss = 3.381767988204956\n",
      "steps = 128, loss = 2.9006452560424805\n",
      "steps = 128, loss = 2.76934814453125\n",
      "steps = 128, loss = 2.8160197734832764\n",
      "steps = 128, loss = 3.910875082015991\n",
      "steps = 128, loss = 2.6320109367370605\n",
      "steps = 128, loss = 2.890169143676758\n",
      "steps = 129, loss = 18.078405380249023\n",
      "steps = 129, loss = 3.34277081489563\n",
      "steps = 129, loss = 3.3390300273895264\n",
      "steps = 129, loss = 4.086097240447998\n",
      "steps = 129, loss = 5.537527561187744\n",
      "steps = 129, loss = 2.7453441619873047\n",
      "steps = 129, loss = 2.8200221061706543\n",
      "steps = 129, loss = 3.3943586349487305\n",
      "steps = 129, loss = 2.7197248935699463\n",
      "steps = 129, loss = 3.2488973140716553\n",
      "steps = 129, loss = 3.7140722274780273\n",
      "steps = 129, loss = 2.1412642002105713\n",
      "steps = 129, loss = 2.787142038345337\n",
      "steps = 129, loss = 2.854275941848755\n",
      "steps = 129, loss = 2.977238655090332\n",
      "steps = 129, loss = 2.6705098152160645\n",
      "steps = 129, loss = 3.4527194499969482\n",
      "steps = 129, loss = 8.139851570129395\n",
      "steps = 129, loss = 3.9273476600646973\n",
      "steps = 129, loss = 2.812868118286133\n",
      "steps = 129, loss = 2.0309152603149414\n",
      "steps = 129, loss = 3.4235188961029053\n",
      "steps = 129, loss = 2.7220985889434814\n",
      "steps = 129, loss = 3.478156328201294\n",
      "steps = 129, loss = 49.975650787353516\n",
      "steps = 129, loss = 2.366833209991455\n",
      "steps = 129, loss = 6.1805291175842285\n",
      "steps = 129, loss = 2.7195446491241455\n",
      "steps = 129, loss = 3.5381276607513428\n",
      "steps = 129, loss = 3.8456664085388184\n",
      "steps = 129, loss = 2.7383313179016113\n",
      "steps = 129, loss = 49.985382080078125\n",
      "steps = 130, loss = 2.6784543991088867\n",
      "steps = 130, loss = 49.98286819458008\n",
      "steps = 130, loss = 2.724325180053711\n",
      "steps = 130, loss = 3.015810012817383\n",
      "steps = 130, loss = 3.452207565307617\n",
      "steps = 130, loss = 2.7195541858673096\n",
      "steps = 130, loss = 2.097400665283203\n",
      "steps = 130, loss = 3.345353603363037\n",
      "steps = 130, loss = 3.3435142040252686\n",
      "steps = 130, loss = 3.427603244781494\n",
      "steps = 130, loss = 2.8514440059661865\n",
      "steps = 130, loss = 17.546850204467773\n",
      "steps = 130, loss = 2.8139219284057617\n",
      "steps = 130, loss = 3.8468050956726074\n",
      "steps = 130, loss = 2.8001351356506348\n",
      "steps = 130, loss = 3.485696792602539\n",
      "steps = 130, loss = 2.9699246883392334\n",
      "steps = 130, loss = 2.09324312210083\n",
      "steps = 130, loss = 4.090241432189941\n",
      "steps = 130, loss = 2.3065929412841797\n",
      "steps = 130, loss = 6.300127029418945\n",
      "steps = 130, loss = 2.641507625579834\n",
      "steps = 130, loss = 3.2402031421661377\n",
      "steps = 130, loss = 3.9265637397766113\n",
      "steps = 130, loss = 5.515205383300781\n",
      "steps = 130, loss = 49.985382080078125\n",
      "steps = 130, loss = 3.1093966960906982\n",
      "steps = 130, loss = 2.7447004318237305\n",
      "steps = 130, loss = 3.417661666870117\n",
      "steps = 130, loss = 4.8901143074035645\n",
      "steps = 130, loss = 2.722639322280884\n",
      "steps = 130, loss = 3.4121925830841064\n",
      "steps = 131, loss = 3.4805214405059814\n",
      "steps = 131, loss = 2.031129837036133\n",
      "steps = 131, loss = 2.8264365196228027\n",
      "steps = 131, loss = 17.234495162963867\n",
      "steps = 131, loss = 3.4419589042663574\n",
      "steps = 131, loss = 2.6752119064331055\n",
      "steps = 131, loss = 2.7329511642456055\n",
      "steps = 131, loss = 3.9498836994171143\n",
      "steps = 131, loss = 3.6909549236297607\n",
      "steps = 131, loss = 49.985382080078125\n",
      "steps = 131, loss = 3.873920440673828\n",
      "steps = 131, loss = 2.7766895294189453\n",
      "steps = 131, loss = 3.5488290786743164\n",
      "steps = 131, loss = 2.6721293926239014\n",
      "steps = 131, loss = 3.611318349838257\n",
      "steps = 131, loss = 2.8533599376678467\n",
      "steps = 131, loss = 2.973938465118408\n",
      "steps = 131, loss = 4.507784843444824\n",
      "steps = 131, loss = 49.96323013305664\n",
      "steps = 131, loss = 3.218641996383667\n",
      "steps = 131, loss = 2.7195310592651367\n",
      "steps = 131, loss = 3.2453653812408447\n",
      "steps = 131, loss = 2.738170862197876\n",
      "steps = 131, loss = 3.383697032928467\n",
      "steps = 131, loss = 2.1910152435302734\n",
      "steps = 131, loss = 3.445807933807373\n",
      "steps = 131, loss = 2.974539279937744\n",
      "steps = 131, loss = 2.7230448722839355\n",
      "steps = 131, loss = 6.146509647369385\n",
      "steps = 131, loss = 5.5553765296936035\n",
      "steps = 131, loss = 2.1430130004882812\n",
      "steps = 131, loss = 4.105433940887451\n",
      "steps = 132, loss = 3.3620243072509766\n",
      "steps = 132, loss = 3.248542308807373\n",
      "steps = 132, loss = 3.554617404937744\n",
      "steps = 132, loss = 2.9829959869384766\n",
      "steps = 132, loss = 2.104254961013794\n",
      "steps = 132, loss = 2.766157865524292\n",
      "steps = 132, loss = 4.116927623748779\n",
      "steps = 132, loss = 3.0202012062072754\n",
      "steps = 132, loss = 2.109927177429199\n",
      "steps = 132, loss = 2.7390027046203613\n",
      "steps = 132, loss = 6.351257801055908\n",
      "steps = 132, loss = 2.5735177993774414\n",
      "steps = 132, loss = 49.985382080078125\n",
      "steps = 132, loss = 3.9647376537323\n",
      "steps = 132, loss = 5.5720624923706055\n",
      "steps = 132, loss = 3.4808268547058105\n",
      "steps = 132, loss = 2.9793670177459717\n",
      "steps = 132, loss = 2.835602045059204\n",
      "steps = 132, loss = 3.4314746856689453\n",
      "steps = 132, loss = 49.945098876953125\n",
      "steps = 132, loss = 18.396026611328125\n",
      "steps = 132, loss = 3.898967742919922\n",
      "steps = 132, loss = 3.2605483531951904\n",
      "steps = 132, loss = 3.070183038711548\n",
      "steps = 132, loss = 2.868417263031006\n",
      "steps = 132, loss = 3.4765501022338867\n",
      "steps = 132, loss = 3.6342103481292725\n",
      "steps = 132, loss = 3.3204104900360107\n",
      "steps = 132, loss = 2.76784610748291\n",
      "steps = 132, loss = 2.101959466934204\n",
      "steps = 132, loss = 2.6329221725463867\n",
      "steps = 132, loss = 4.130239486694336\n",
      "steps = 133, loss = 2.8011367321014404\n",
      "steps = 133, loss = 3.5401675701141357\n",
      "steps = 133, loss = 3.044464349746704\n",
      "steps = 133, loss = 2.9691431522369385\n",
      "steps = 133, loss = 2.852468967437744\n",
      "steps = 133, loss = 2.518780469894409\n",
      "steps = 133, loss = 2.8275203704833984\n",
      "steps = 133, loss = 3.537868022918701\n",
      "steps = 133, loss = 3.342261552810669\n",
      "steps = 133, loss = 3.400908946990967\n",
      "steps = 133, loss = 17.88714027404785\n",
      "steps = 133, loss = 49.904232025146484\n",
      "steps = 133, loss = 3.317330837249756\n",
      "steps = 133, loss = 2.7251787185668945\n",
      "steps = 133, loss = 6.194526195526123\n",
      "steps = 133, loss = 5.544384479522705\n",
      "steps = 133, loss = 4.12323522567749\n",
      "steps = 133, loss = 2.750948429107666\n",
      "steps = 133, loss = 3.895228385925293\n",
      "steps = 133, loss = 3.677546262741089\n",
      "steps = 133, loss = 49.985382080078125\n",
      "steps = 133, loss = 3.4193472862243652\n",
      "steps = 133, loss = 2.743821620941162\n",
      "steps = 133, loss = 50.06167221069336\n",
      "steps = 133, loss = 3.238083600997925\n",
      "steps = 133, loss = 3.345952033996582\n",
      "steps = 133, loss = 2.1949520111083984\n",
      "steps = 133, loss = 4.6988935470581055\n",
      "steps = 133, loss = 3.9646506309509277\n",
      "steps = 133, loss = 2.673370122909546\n",
      "steps = 133, loss = 2.628121852874756\n",
      "steps = 133, loss = 2.0923476219177246\n",
      "steps = 134, loss = 2.8078486919403076\n",
      "steps = 134, loss = 2.956798553466797\n",
      "steps = 134, loss = 2.8395888805389404\n",
      "steps = 134, loss = 5.594408988952637\n",
      "steps = 134, loss = 3.242786169052124\n",
      "steps = 134, loss = 3.9880518913269043\n",
      "steps = 134, loss = 2.973466157913208\n",
      "steps = 134, loss = 2.72804594039917\n",
      "steps = 134, loss = 49.985382080078125\n",
      "steps = 134, loss = 3.309231758117676\n",
      "steps = 134, loss = 49.959590911865234\n",
      "steps = 134, loss = 1.987494945526123\n",
      "steps = 134, loss = 3.4289050102233887\n",
      "steps = 134, loss = 4.139036655426025\n",
      "steps = 134, loss = 3.92317795753479\n",
      "steps = 134, loss = 6.320647239685059\n",
      "steps = 134, loss = 2.853689670562744\n",
      "steps = 134, loss = 3.5004377365112305\n",
      "steps = 134, loss = 3.4016780853271484\n",
      "steps = 134, loss = 2.2835605144500732\n",
      "steps = 134, loss = 5.479215621948242\n",
      "steps = 134, loss = 3.276815176010132\n",
      "steps = 134, loss = 2.720073699951172\n",
      "steps = 134, loss = 2.142421245574951\n",
      "steps = 134, loss = 3.59021258354187\n",
      "steps = 134, loss = 17.5410213470459\n",
      "steps = 134, loss = 3.4307665824890137\n",
      "steps = 134, loss = 2.5496678352355957\n",
      "steps = 134, loss = 3.037304639816284\n",
      "steps = 134, loss = 2.8315224647521973\n",
      "steps = 134, loss = 3.6632888317108154\n",
      "steps = 134, loss = 2.7243261337280273\n",
      "steps = 135, loss = 4.002295017242432\n",
      "steps = 135, loss = 4.162018299102783\n",
      "steps = 135, loss = 4.329390048980713\n",
      "steps = 135, loss = 3.335350751876831\n",
      "steps = 135, loss = 3.7799060344696045\n",
      "steps = 135, loss = 2.848508834838867\n",
      "steps = 135, loss = 2.882073402404785\n",
      "steps = 135, loss = 2.6982195377349854\n",
      "steps = 135, loss = 2.868901252746582\n",
      "steps = 135, loss = 2.1847312450408936\n",
      "steps = 135, loss = 3.341341495513916\n",
      "steps = 135, loss = 2.6739602088928223\n",
      "steps = 135, loss = 2.835601806640625\n",
      "steps = 135, loss = 3.2458560466766357\n",
      "steps = 135, loss = 2.739258289337158\n",
      "steps = 135, loss = 49.985382080078125\n",
      "steps = 135, loss = 5.596045017242432\n",
      "steps = 135, loss = 3.3736929893493652\n",
      "steps = 135, loss = 2.3178672790527344\n",
      "steps = 135, loss = 49.940792083740234\n",
      "steps = 135, loss = 3.472914218902588\n",
      "steps = 135, loss = 17.439250946044922\n",
      "steps = 135, loss = 6.2466511726379395\n",
      "steps = 135, loss = 2.978260040283203\n",
      "steps = 135, loss = 3.948312282562256\n",
      "steps = 135, loss = 3.4114487171173096\n",
      "steps = 135, loss = 2.7681827545166016\n",
      "steps = 135, loss = 2.8529067039489746\n",
      "steps = 135, loss = 2.553755760192871\n",
      "steps = 135, loss = 2.102126121520996\n",
      "steps = 135, loss = 3.588778018951416\n",
      "steps = 135, loss = 3.5456020832061768\n",
      "steps = 136, loss = 3.230578660964966\n",
      "steps = 136, loss = 2.8527162075042725\n",
      "steps = 136, loss = 3.5150060653686523\n",
      "steps = 136, loss = 2.162757635116577\n",
      "steps = 136, loss = 2.9680886268615723\n",
      "steps = 136, loss = 49.91944122314453\n",
      "steps = 136, loss = 49.985382080078125\n",
      "steps = 136, loss = 2.5419039726257324\n",
      "steps = 136, loss = 6.408042907714844\n",
      "steps = 136, loss = 2.7491822242736816\n",
      "steps = 136, loss = 3.9446606636047363\n",
      "steps = 136, loss = 2.0924017429351807\n",
      "steps = 136, loss = 4.002084732055664\n",
      "steps = 136, loss = 17.74785804748535\n",
      "steps = 136, loss = 5.568663120269775\n",
      "steps = 136, loss = 3.2773795127868652\n",
      "steps = 136, loss = 2.5497922897338867\n",
      "steps = 136, loss = 2.8093552589416504\n",
      "steps = 136, loss = 2.8404319286346436\n",
      "steps = 136, loss = 3.0515239238739014\n",
      "steps = 136, loss = 3.4984657764434814\n",
      "steps = 136, loss = 2.0880823135375977\n",
      "steps = 136, loss = 2.725554943084717\n",
      "steps = 136, loss = 5.424291610717773\n",
      "steps = 136, loss = 3.5289523601531982\n",
      "steps = 136, loss = 2.7121987342834473\n",
      "steps = 136, loss = 3.3804497718811035\n",
      "steps = 136, loss = 2.5857386589050293\n",
      "steps = 136, loss = 3.2263379096984863\n",
      "steps = 136, loss = 3.4066460132598877\n",
      "steps = 136, loss = 3.2354788780212402\n",
      "steps = 136, loss = 4.1544599533081055\n",
      "steps = 137, loss = 4.024275779724121\n",
      "steps = 137, loss = 3.504159450531006\n",
      "steps = 137, loss = 2.727466106414795\n",
      "steps = 137, loss = 5.63195276260376\n",
      "steps = 137, loss = 2.682136058807373\n",
      "steps = 137, loss = 3.972834825515747\n",
      "steps = 137, loss = 2.8520939350128174\n",
      "steps = 137, loss = 2.223210573196411\n",
      "steps = 137, loss = 5.449435234069824\n",
      "steps = 137, loss = 2.9723498821258545\n",
      "steps = 137, loss = 49.985382080078125\n",
      "steps = 137, loss = 2.800294876098633\n",
      "steps = 137, loss = 2.1423795223236084\n",
      "steps = 137, loss = 49.957942962646484\n",
      "steps = 137, loss = 3.485805034637451\n",
      "steps = 137, loss = 6.200641632080078\n",
      "steps = 137, loss = 4.171313285827637\n",
      "steps = 137, loss = 2.8538196086883545\n",
      "steps = 137, loss = 2.0928003787994385\n",
      "steps = 137, loss = 18.017087936401367\n",
      "steps = 137, loss = 2.711432695388794\n",
      "steps = 137, loss = 2.980992555618286\n",
      "steps = 137, loss = 3.4446096420288086\n",
      "steps = 137, loss = 3.355813503265381\n",
      "steps = 137, loss = 2.715496301651001\n",
      "steps = 137, loss = 2.572908401489258\n",
      "steps = 137, loss = 3.410494327545166\n",
      "steps = 137, loss = 3.722872495651245\n",
      "steps = 137, loss = 2.7203593254089355\n",
      "steps = 137, loss = 3.4879748821258545\n",
      "steps = 137, loss = 3.240265130996704\n",
      "steps = 137, loss = 3.40459942817688\n",
      "steps = 138, loss = 2.8608155250549316\n",
      "steps = 138, loss = 22.00644874572754\n",
      "steps = 138, loss = 3.2434399127960205\n",
      "steps = 138, loss = 2.7395105361938477\n",
      "steps = 138, loss = 2.674401044845581\n",
      "steps = 138, loss = 3.5309596061706543\n",
      "steps = 138, loss = 2.7760016918182373\n",
      "steps = 138, loss = 4.037933826446533\n",
      "steps = 138, loss = 5.602450370788574\n",
      "steps = 138, loss = 49.9647331237793\n",
      "steps = 138, loss = 6.3623223304748535\n",
      "steps = 138, loss = 3.126610517501831\n",
      "steps = 138, loss = 3.462178945541382\n",
      "steps = 138, loss = 3.5642266273498535\n",
      "steps = 138, loss = 2.218296527862549\n",
      "steps = 138, loss = 3.617032766342163\n",
      "steps = 138, loss = 2.660675525665283\n",
      "steps = 138, loss = 3.021824598312378\n",
      "steps = 138, loss = 3.4890472888946533\n",
      "steps = 138, loss = 3.487269639968872\n",
      "steps = 138, loss = 2.869140148162842\n",
      "steps = 138, loss = 2.9771652221679688\n",
      "steps = 138, loss = 3.400941848754883\n",
      "steps = 138, loss = 4.192703723907471\n",
      "steps = 138, loss = 2.1022939682006836\n",
      "steps = 138, loss = 49.985382080078125\n",
      "steps = 138, loss = 3.997671365737915\n",
      "steps = 138, loss = 2.76853084564209\n",
      "steps = 138, loss = 18.59069061279297\n",
      "steps = 138, loss = 2.1590893268585205\n",
      "steps = 138, loss = 2.669769525527954\n",
      "steps = 138, loss = 4.550675392150879\n",
      "steps = 139, loss = 2.6772162914276123\n",
      "steps = 139, loss = 3.040619373321533\n",
      "steps = 139, loss = 2.2527048587799072\n",
      "steps = 139, loss = 3.672424793243408\n",
      "steps = 139, loss = 5.662939071655273\n",
      "steps = 139, loss = 17.472700119018555\n",
      "steps = 139, loss = 4.009941101074219\n",
      "steps = 139, loss = 2.067747116088867\n",
      "steps = 139, loss = 7.674887657165527\n",
      "steps = 139, loss = 2.8634119033813477\n",
      "steps = 139, loss = 2.903392791748047\n",
      "steps = 139, loss = 2.9730989933013916\n",
      "steps = 139, loss = 2.628761053085327\n",
      "steps = 139, loss = 3.388355255126953\n",
      "steps = 139, loss = 2.994929552078247\n",
      "steps = 139, loss = 3.553598165512085\n",
      "steps = 139, loss = 6.374264717102051\n",
      "steps = 139, loss = 4.016538143157959\n",
      "steps = 139, loss = 2.1407759189605713\n",
      "steps = 139, loss = 3.324146032333374\n",
      "steps = 139, loss = 3.663926124572754\n",
      "steps = 139, loss = 2.7348709106445312\n",
      "steps = 139, loss = 2.7201597690582275\n",
      "steps = 139, loss = 2.702543258666992\n",
      "steps = 139, loss = 4.052314758300781\n",
      "steps = 139, loss = 2.85440731048584\n",
      "steps = 139, loss = 3.2400314807891846\n",
      "steps = 139, loss = 3.4734599590301514\n",
      "steps = 139, loss = 3.3899831771850586\n",
      "steps = 139, loss = 4.196294784545898\n",
      "steps = 139, loss = 49.985382080078125\n",
      "steps = 139, loss = 49.96659469604492\n",
      "steps = 140, loss = 2.773127317428589\n",
      "steps = 140, loss = 2.2919561862945557\n",
      "steps = 140, loss = 3.380387544631958\n",
      "steps = 140, loss = 2.782618761062622\n",
      "steps = 140, loss = 49.91823959350586\n",
      "steps = 140, loss = 5.203057765960693\n",
      "steps = 140, loss = 2.723828077316284\n",
      "steps = 140, loss = 3.501357316970825\n",
      "steps = 140, loss = 2.8570666313171387\n",
      "steps = 140, loss = 3.2313616275787354\n",
      "steps = 140, loss = 2.59102201461792\n",
      "steps = 140, loss = 3.351330041885376\n",
      "steps = 140, loss = 2.7461400032043457\n",
      "steps = 140, loss = 2.09338116645813\n",
      "steps = 140, loss = 3.2230727672576904\n",
      "steps = 140, loss = 2.74457049369812\n",
      "steps = 140, loss = 3.46882700920105\n",
      "steps = 140, loss = 49.985382080078125\n",
      "steps = 140, loss = 48.44717025756836\n",
      "steps = 140, loss = 18.189905166625977\n",
      "steps = 140, loss = 3.0451724529266357\n",
      "steps = 140, loss = 5.476093292236328\n",
      "steps = 140, loss = 2.990478992462158\n",
      "steps = 140, loss = 4.049753189086914\n",
      "steps = 140, loss = 2.9657740592956543\n",
      "steps = 140, loss = 3.3677237033843994\n",
      "steps = 140, loss = 2.8511228561401367\n",
      "steps = 140, loss = 6.462576389312744\n",
      "steps = 140, loss = 3.523538112640381\n",
      "steps = 140, loss = 4.009606838226318\n",
      "steps = 140, loss = 4.1944122314453125\n",
      "steps = 140, loss = 3.385101318359375\n",
      "steps = 141, loss = 2.88022780418396\n",
      "steps = 141, loss = 3.5878546237945557\n",
      "steps = 141, loss = 2.8700850009918213\n",
      "steps = 141, loss = 6.297492504119873\n",
      "steps = 141, loss = 2.740842819213867\n",
      "steps = 141, loss = 3.2408294677734375\n",
      "steps = 141, loss = 2.9253275394439697\n",
      "steps = 141, loss = 4.070798873901367\n",
      "steps = 141, loss = 5.604022026062012\n",
      "steps = 141, loss = 2.871492624282837\n",
      "steps = 141, loss = 49.985382080078125\n",
      "steps = 141, loss = 3.036729097366333\n",
      "steps = 141, loss = 4.0439043045043945\n",
      "steps = 141, loss = 2.843700885772705\n",
      "steps = 141, loss = 2.673262357711792\n",
      "steps = 141, loss = 2.770197868347168\n",
      "steps = 141, loss = 18.175535202026367\n",
      "steps = 141, loss = 1.975020408630371\n",
      "steps = 141, loss = 2.9754180908203125\n",
      "steps = 141, loss = 49.9683952331543\n",
      "steps = 141, loss = 3.0641841888427734\n",
      "steps = 141, loss = 2.1037583351135254\n",
      "steps = 141, loss = 2.648081064224243\n",
      "steps = 141, loss = 3.3636715412139893\n",
      "steps = 141, loss = 4.869447708129883\n",
      "steps = 141, loss = 3.4924328327178955\n",
      "steps = 141, loss = 4.220992088317871\n",
      "steps = 141, loss = 2.073134422302246\n",
      "steps = 141, loss = 3.5504021644592285\n",
      "steps = 141, loss = 3.6701457500457764\n",
      "steps = 141, loss = 3.636920928955078\n",
      "steps = 141, loss = 3.373460054397583\n",
      "steps = 142, loss = 3.2310099601745605\n",
      "steps = 142, loss = 3.489293336868286\n",
      "steps = 142, loss = 4.214350700378418\n",
      "steps = 142, loss = 2.7546026706695557\n",
      "steps = 142, loss = 6.372880458831787\n",
      "steps = 142, loss = 2.9658727645874023\n",
      "steps = 142, loss = 2.936033248901367\n",
      "steps = 142, loss = 2.7263073921203613\n",
      "steps = 142, loss = 3.4171690940856934\n",
      "steps = 142, loss = 2.8536698818206787\n",
      "steps = 142, loss = 49.985382080078125\n",
      "steps = 142, loss = 2.864287853240967\n",
      "steps = 142, loss = 2.6567940711975098\n",
      "steps = 142, loss = 3.5256810188293457\n",
      "steps = 142, loss = 4.041622161865234\n",
      "steps = 142, loss = 2.999295949935913\n",
      "steps = 142, loss = 3.5014119148254395\n",
      "steps = 142, loss = 49.93194580078125\n",
      "steps = 142, loss = 2.0049233436584473\n",
      "steps = 142, loss = 4.07219123840332\n",
      "steps = 142, loss = 2.6276540756225586\n",
      "steps = 142, loss = 3.4672558307647705\n",
      "steps = 142, loss = 3.3103039264678955\n",
      "steps = 142, loss = 5.235091686248779\n",
      "steps = 142, loss = 5.542236804962158\n",
      "steps = 142, loss = 2.9070072174072266\n",
      "steps = 142, loss = 3.3032631874084473\n",
      "steps = 142, loss = 2.0419628620147705\n",
      "steps = 142, loss = 2.092301607131958\n",
      "steps = 142, loss = 2.861659288406372\n",
      "steps = 142, loss = 3.448974370956421\n",
      "steps = 142, loss = 18.245248794555664\n",
      "steps = 143, loss = 5.07775354385376\n",
      "steps = 143, loss = 18.84347152709961\n",
      "steps = 143, loss = 3.507789134979248\n",
      "steps = 143, loss = 4.2411298751831055\n",
      "steps = 143, loss = 3.5557289123535156\n",
      "steps = 143, loss = 2.989903450012207\n",
      "steps = 143, loss = 2.7040677070617676\n",
      "steps = 143, loss = 4.093708515167236\n",
      "steps = 143, loss = 3.5800468921661377\n",
      "steps = 143, loss = 2.8789308071136475\n",
      "steps = 143, loss = 3.0417850017547607\n",
      "steps = 143, loss = 2.5662498474121094\n",
      "steps = 143, loss = 2.9088492393493652\n",
      "steps = 143, loss = 3.542780637741089\n",
      "steps = 143, loss = 2.7705025672912598\n",
      "steps = 143, loss = 2.103924036026001\n",
      "steps = 143, loss = 5.601873397827148\n",
      "steps = 143, loss = 3.240314245223999\n",
      "steps = 143, loss = 2.8013758659362793\n",
      "steps = 143, loss = 2.975421667098999\n",
      "steps = 143, loss = 2.191411018371582\n",
      "steps = 143, loss = 3.715315103530884\n",
      "steps = 143, loss = 2.8719747066497803\n",
      "steps = 143, loss = 2.7420928478240967\n",
      "steps = 143, loss = 49.973793029785156\n",
      "steps = 143, loss = 2.7713570594787598\n",
      "steps = 143, loss = 49.985382080078125\n",
      "steps = 143, loss = 3.5159342288970947\n",
      "steps = 143, loss = 6.418915748596191\n",
      "steps = 143, loss = 4.075994491577148\n",
      "steps = 143, loss = 3.4751510620117188\n",
      "steps = 143, loss = 2.079503059387207\n",
      "steps = 144, loss = 3.2365708351135254\n",
      "steps = 144, loss = 2.1409912109375\n",
      "steps = 144, loss = 2.655677556991577\n",
      "steps = 144, loss = 49.12668228149414\n",
      "steps = 144, loss = 3.3448116779327393\n",
      "steps = 144, loss = 5.598706245422363\n",
      "steps = 144, loss = 2.7211639881134033\n",
      "steps = 144, loss = 2.966468572616577\n",
      "steps = 144, loss = 2.526437997817993\n",
      "steps = 144, loss = 3.4471933841705322\n",
      "steps = 144, loss = 18.373369216918945\n",
      "steps = 144, loss = 3.0037343502044678\n",
      "steps = 144, loss = 3.355095624923706\n",
      "steps = 144, loss = 3.5051324367523193\n",
      "steps = 144, loss = 3.3868322372436523\n",
      "steps = 144, loss = 3.4921875\n",
      "steps = 144, loss = 2.725774049758911\n",
      "steps = 144, loss = 5.272122383117676\n",
      "steps = 144, loss = 2.971576452255249\n",
      "steps = 144, loss = 2.8547210693359375\n",
      "steps = 144, loss = 2.216590404510498\n",
      "steps = 144, loss = 4.245870113372803\n",
      "steps = 144, loss = 4.109542369842529\n",
      "steps = 144, loss = 6.564901828765869\n",
      "steps = 144, loss = 49.985382080078125\n",
      "steps = 144, loss = 4.0896735191345215\n",
      "steps = 144, loss = 2.641756296157837\n",
      "steps = 144, loss = 2.7413103580474854\n",
      "steps = 144, loss = 3.4532313346862793\n",
      "steps = 144, loss = 2.88236141204834\n",
      "steps = 144, loss = 3.4638047218322754\n",
      "steps = 144, loss = 2.0804309844970703\n",
      "steps = 145, loss = 2.741820812225342\n",
      "steps = 145, loss = 2.8539671897888184\n",
      "steps = 145, loss = 2.6729958057403564\n",
      "steps = 145, loss = 2.5469841957092285\n",
      "steps = 145, loss = 2.615501642227173\n",
      "steps = 145, loss = 3.6294829845428467\n",
      "steps = 145, loss = 2.062771797180176\n",
      "steps = 145, loss = 4.122000217437744\n",
      "steps = 145, loss = 18.526262283325195\n",
      "steps = 145, loss = 2.781782388687134\n",
      "steps = 145, loss = 3.5094428062438965\n",
      "steps = 145, loss = 3.3291306495666504\n",
      "steps = 145, loss = 2.1435487270355225\n",
      "steps = 145, loss = 49.985382080078125\n",
      "steps = 145, loss = 3.2353515625\n",
      "steps = 145, loss = 3.5518014430999756\n",
      "steps = 145, loss = 2.7200448513031006\n",
      "steps = 145, loss = 3.174084186553955\n",
      "steps = 145, loss = 2.970649003982544\n",
      "steps = 145, loss = 5.3722333908081055\n",
      "steps = 145, loss = 5.648928165435791\n",
      "steps = 145, loss = 2.8868520259857178\n",
      "steps = 145, loss = 4.2556915283203125\n",
      "steps = 145, loss = 2.6726810932159424\n",
      "steps = 145, loss = 3.360456943511963\n",
      "steps = 145, loss = 49.97658157348633\n",
      "steps = 145, loss = 3.428982734680176\n",
      "steps = 145, loss = 4.106081485748291\n",
      "steps = 145, loss = 6.247015476226807\n",
      "steps = 145, loss = 3.757016181945801\n",
      "steps = 145, loss = 3.1117706298828125\n",
      "steps = 145, loss = 2.293689250946045\n",
      "steps = 146, loss = 49.97684097290039\n",
      "steps = 146, loss = 3.4905641078948975\n",
      "steps = 146, loss = 2.723754405975342\n",
      "steps = 146, loss = 3.4060471057891846\n",
      "steps = 146, loss = 2.9635536670684814\n",
      "steps = 146, loss = 4.2503814697265625\n",
      "steps = 146, loss = 5.504636764526367\n",
      "steps = 146, loss = 2.645623207092285\n",
      "steps = 146, loss = 17.77008056640625\n",
      "steps = 146, loss = 3.4585773944854736\n",
      "steps = 146, loss = 4.1062421798706055\n",
      "steps = 146, loss = 2.0937674045562744\n",
      "steps = 146, loss = 3.541431427001953\n",
      "steps = 146, loss = 3.501981258392334\n",
      "steps = 146, loss = 5.355604648590088\n",
      "steps = 146, loss = 2.6068050861358643\n",
      "steps = 146, loss = 2.628702402114868\n",
      "steps = 146, loss = 2.9734244346618652\n",
      "steps = 146, loss = 2.851173162460327\n",
      "steps = 146, loss = 2.880500078201294\n",
      "steps = 146, loss = 2.0705461502075195\n",
      "steps = 146, loss = 3.2269446849823\n",
      "steps = 146, loss = 3.037780523300171\n",
      "steps = 146, loss = 4.119173049926758\n",
      "steps = 146, loss = 3.3535284996032715\n",
      "steps = 146, loss = 3.227966785430908\n",
      "steps = 146, loss = 2.055940628051758\n",
      "steps = 146, loss = 49.985382080078125\n",
      "steps = 146, loss = 3.3935153484344482\n",
      "steps = 146, loss = 2.7462143898010254\n",
      "steps = 146, loss = 6.393463611602783\n",
      "steps = 146, loss = 2.83524489402771\n",
      "steps = 147, loss = 4.139924049377441\n",
      "steps = 147, loss = 3.72344708442688\n",
      "steps = 147, loss = 2.132943868637085\n",
      "steps = 147, loss = 5.353275775909424\n",
      "steps = 147, loss = 2.1041419506073\n",
      "steps = 147, loss = 3.2363803386688232\n",
      "steps = 147, loss = 49.81163024902344\n",
      "steps = 147, loss = 2.741150379180908\n",
      "steps = 147, loss = 6.425489902496338\n",
      "steps = 147, loss = 49.985382080078125\n",
      "steps = 147, loss = 18.241559982299805\n",
      "steps = 147, loss = 3.641611099243164\n",
      "steps = 147, loss = 3.540921688079834\n",
      "steps = 147, loss = 2.63645601272583\n",
      "steps = 147, loss = 2.9529550075531006\n",
      "steps = 147, loss = 3.3696682453155518\n",
      "steps = 147, loss = 2.8699817657470703\n",
      "steps = 147, loss = 2.770339012145996\n",
      "steps = 147, loss = 5.620913028717041\n",
      "steps = 147, loss = 3.3568406105041504\n",
      "steps = 147, loss = 2.760983467102051\n",
      "steps = 147, loss = 2.9732208251953125\n",
      "steps = 147, loss = 3.5198943614959717\n",
      "steps = 147, loss = 3.0229947566986084\n",
      "steps = 147, loss = 3.2363674640655518\n",
      "steps = 147, loss = 4.276422500610352\n",
      "steps = 147, loss = 3.559464693069458\n",
      "steps = 147, loss = 2.5923163890838623\n",
      "steps = 147, loss = 4.139688014984131\n",
      "steps = 147, loss = 2.52325701713562\n",
      "steps = 147, loss = 2.1528167724609375\n",
      "steps = 147, loss = 2.89473032951355\n",
      "steps = 148, loss = 2.8575525283813477\n",
      "steps = 148, loss = 10.730225563049316\n",
      "steps = 148, loss = 4.152856349945068\n",
      "steps = 148, loss = 2.4928159713745117\n",
      "steps = 148, loss = 2.140462875366211\n",
      "steps = 148, loss = 2.969682216644287\n",
      "steps = 148, loss = 3.4112839698791504\n",
      "steps = 148, loss = 3.4293572902679443\n",
      "steps = 148, loss = 6.5460405349731445\n",
      "steps = 148, loss = 3.2335453033447266\n",
      "steps = 148, loss = 2.675173282623291\n",
      "steps = 148, loss = 2.5833899974823\n",
      "steps = 148, loss = 5.552725315093994\n",
      "steps = 148, loss = 2.778041362762451\n",
      "steps = 148, loss = 3.291957139968872\n",
      "steps = 148, loss = 18.320598602294922\n",
      "steps = 148, loss = 2.7211015224456787\n",
      "steps = 148, loss = 4.156001567840576\n",
      "steps = 148, loss = 2.0530765056610107\n",
      "steps = 148, loss = 3.654900074005127\n",
      "steps = 148, loss = 3.501386880874634\n",
      "steps = 148, loss = 3.42844295501709\n",
      "steps = 148, loss = 49.985382080078125\n",
      "steps = 148, loss = 49.97539520263672\n",
      "steps = 148, loss = 2.897905111312866\n",
      "steps = 148, loss = 4.283196449279785\n",
      "steps = 148, loss = 3.5554251670837402\n",
      "steps = 148, loss = 2.8546342849731445\n",
      "steps = 148, loss = 3.4457714557647705\n",
      "steps = 148, loss = 2.73305344581604\n",
      "steps = 148, loss = 2.2863614559173584\n",
      "steps = 148, loss = 2.9906365871429443\n",
      "steps = 149, loss = 3.5546886920928955\n",
      "steps = 149, loss = 49.978736877441406\n",
      "steps = 149, loss = 4.175806522369385\n",
      "steps = 149, loss = 49.985382080078125\n",
      "steps = 149, loss = 3.4119768142700195\n",
      "steps = 149, loss = 7.714649200439453\n",
      "steps = 149, loss = 2.8704402446746826\n",
      "steps = 149, loss = 4.166915416717529\n",
      "steps = 149, loss = 2.7811431884765625\n",
      "steps = 149, loss = 2.7665462493896484\n",
      "steps = 149, loss = 3.236239194869995\n",
      "steps = 149, loss = 2.0956194400787354\n",
      "steps = 149, loss = 3.458801746368408\n",
      "steps = 149, loss = 2.786848783493042\n",
      "steps = 149, loss = 2.6133668422698975\n",
      "steps = 149, loss = 3.6225903034210205\n",
      "steps = 149, loss = 18.342620849609375\n",
      "steps = 149, loss = 2.740288019180298\n",
      "steps = 149, loss = 3.07112455368042\n",
      "steps = 149, loss = 6.354557991027832\n",
      "steps = 149, loss = 2.8266725540161133\n",
      "steps = 149, loss = 2.90543532371521\n",
      "steps = 149, loss = 4.2981157302856445\n",
      "steps = 149, loss = 3.5802693367004395\n",
      "steps = 149, loss = 3.296877384185791\n",
      "steps = 149, loss = 3.3834216594696045\n",
      "steps = 149, loss = 2.770547389984131\n",
      "steps = 149, loss = 2.0757179260253906\n",
      "steps = 149, loss = 3.4957621097564697\n",
      "steps = 149, loss = 5.6177802085876465\n",
      "steps = 149, loss = 2.104097843170166\n",
      "steps = 149, loss = 2.973926305770874\n",
      "steps = 150, loss = 2.824506998062134\n",
      "steps = 150, loss = 18.651220321655273\n",
      "steps = 150, loss = 3.4764552116394043\n",
      "steps = 150, loss = 49.97551345825195\n",
      "steps = 150, loss = 2.092278480529785\n",
      "steps = 150, loss = 2.9531643390655518\n",
      "steps = 150, loss = 2.51550030708313\n",
      "steps = 150, loss = 3.654247999191284\n",
      "steps = 150, loss = 2.896524429321289\n",
      "steps = 150, loss = 4.287725448608398\n",
      "steps = 150, loss = 2.963465690612793\n",
      "steps = 150, loss = 3.3897316455841064\n",
      "steps = 150, loss = 2.6000382900238037\n",
      "steps = 150, loss = 3.151412010192871\n",
      "steps = 150, loss = 2.725529670715332\n",
      "steps = 150, loss = 3.4240334033966064\n",
      "steps = 150, loss = 2.7508931159973145\n",
      "steps = 150, loss = 3.435992956161499\n",
      "steps = 150, loss = 3.3170652389526367\n",
      "steps = 150, loss = 2.6958649158477783\n",
      "steps = 150, loss = 4.171618461608887\n",
      "steps = 150, loss = 6.508044242858887\n",
      "steps = 150, loss = 2.2159597873687744\n",
      "steps = 150, loss = 3.491924285888672\n",
      "steps = 150, loss = 3.2256269454956055\n",
      "steps = 150, loss = 3.4577841758728027\n",
      "steps = 150, loss = 2.8530495166778564\n",
      "steps = 150, loss = 4.165778160095215\n",
      "steps = 150, loss = 5.5881242752075195\n",
      "steps = 150, loss = 6.054056167602539\n",
      "steps = 150, loss = 2.1263718605041504\n",
      "steps = 150, loss = 49.985382080078125\n",
      "steps = 151, loss = 3.230513572692871\n",
      "steps = 151, loss = 3.082594633102417\n",
      "steps = 151, loss = 2.646591901779175\n",
      "steps = 151, loss = 2.8147318363189697\n",
      "steps = 151, loss = 2.6087305545806885\n",
      "steps = 151, loss = 4.308211803436279\n",
      "steps = 151, loss = 3.3880748748779297\n",
      "steps = 151, loss = 2.730635643005371\n",
      "steps = 151, loss = 2.721407413482666\n",
      "steps = 151, loss = 3.447892665863037\n",
      "steps = 151, loss = 49.985382080078125\n",
      "steps = 151, loss = 3.4986560344696045\n",
      "steps = 151, loss = 3.504814863204956\n",
      "steps = 151, loss = 4.187154769897461\n",
      "steps = 151, loss = 2.967618942260742\n",
      "steps = 151, loss = 6.374754428863525\n",
      "steps = 151, loss = 4.199746131896973\n",
      "steps = 151, loss = 18.270790100097656\n",
      "steps = 151, loss = 2.9074039459228516\n",
      "steps = 151, loss = 5.818355083465576\n",
      "steps = 151, loss = 3.323911666870117\n",
      "steps = 151, loss = 2.0535733699798584\n",
      "steps = 151, loss = 2.8542888164520264\n",
      "steps = 151, loss = 2.6377861499786377\n",
      "steps = 151, loss = 3.516719341278076\n",
      "steps = 151, loss = 2.1421096324920654\n",
      "steps = 151, loss = 5.658849716186523\n",
      "steps = 151, loss = 3.642181634902954\n",
      "steps = 151, loss = 2.5163536071777344\n",
      "steps = 151, loss = 3.213315725326538\n",
      "steps = 151, loss = 49.958133697509766\n",
      "steps = 151, loss = 1.782053828239441\n",
      "steps = 152, loss = 2.740433692932129\n",
      "steps = 152, loss = 1.8936163187026978\n",
      "steps = 152, loss = 2.9724857807159424\n",
      "steps = 152, loss = 18.09796905517578\n",
      "steps = 152, loss = 6.56761360168457\n",
      "steps = 152, loss = 3.501055955886841\n",
      "steps = 152, loss = 2.9158239364624023\n",
      "steps = 152, loss = 49.957862854003906\n",
      "steps = 152, loss = 3.410099983215332\n",
      "steps = 152, loss = 2.082859516143799\n",
      "steps = 152, loss = 3.5094339847564697\n",
      "steps = 152, loss = 3.2339046001434326\n",
      "steps = 152, loss = 3.1530332565307617\n",
      "steps = 152, loss = 3.561993360519409\n",
      "steps = 152, loss = 4.224171161651611\n",
      "steps = 152, loss = 5.524238586425781\n",
      "steps = 152, loss = 3.305786371231079\n",
      "steps = 152, loss = 3.1075797080993652\n",
      "steps = 152, loss = 49.985382080078125\n",
      "steps = 152, loss = 2.1034727096557617\n",
      "steps = 152, loss = 2.870175838470459\n",
      "steps = 152, loss = 3.107919931411743\n",
      "steps = 152, loss = 2.770490884780884\n",
      "steps = 152, loss = 2.607745885848999\n",
      "steps = 152, loss = 4.199014186859131\n",
      "steps = 152, loss = 2.680760622024536\n",
      "steps = 152, loss = 2.8698477745056152\n",
      "steps = 152, loss = 5.316486835479736\n",
      "steps = 152, loss = 4.323264122009277\n",
      "steps = 152, loss = 3.1224453449249268\n",
      "steps = 152, loss = 2.5062673091888428\n",
      "steps = 152, loss = 3.540087938308716\n",
      "steps = 153, loss = 2.3192052841186523\n",
      "steps = 153, loss = 2.8534488677978516\n",
      "steps = 153, loss = 3.30342960357666\n",
      "steps = 153, loss = 49.9489860534668\n",
      "steps = 153, loss = 3.1501126289367676\n",
      "steps = 153, loss = 18.697021484375\n",
      "steps = 153, loss = 4.220163345336914\n",
      "steps = 153, loss = 4.313106060028076\n",
      "steps = 153, loss = 3.568617105484009\n",
      "steps = 153, loss = 2.9624178409576416\n",
      "steps = 153, loss = 2.7267494201660156\n",
      "steps = 153, loss = 3.2616310119628906\n",
      "steps = 153, loss = 2.725883960723877\n",
      "steps = 153, loss = 3.028162956237793\n",
      "steps = 153, loss = 3.4769718647003174\n",
      "steps = 153, loss = 49.985382080078125\n",
      "steps = 153, loss = 2.5775372982025146\n",
      "steps = 153, loss = 2.635002613067627\n",
      "steps = 153, loss = 5.614192485809326\n",
      "steps = 153, loss = 49.95488357543945\n",
      "steps = 153, loss = 2.907320737838745\n",
      "steps = 153, loss = 3.2235894203186035\n",
      "steps = 153, loss = 49.95488739013672\n",
      "steps = 153, loss = 2.7509777545928955\n",
      "steps = 153, loss = 6.325162410736084\n",
      "steps = 153, loss = 2.6059489250183105\n",
      "steps = 153, loss = 2.092419385910034\n",
      "steps = 153, loss = 2.716435194015503\n",
      "steps = 153, loss = 3.354360580444336\n",
      "steps = 153, loss = 3.646578788757324\n",
      "steps = 153, loss = 4.199056625366211\n",
      "steps = 153, loss = 3.478583574295044\n",
      "steps = 154, loss = 2.7489562034606934\n",
      "steps = 154, loss = 2.028730630874634\n",
      "steps = 154, loss = 6.464053153991699\n",
      "steps = 154, loss = 2.721665382385254\n",
      "steps = 154, loss = 3.5402491092681885\n",
      "steps = 154, loss = 50.037662506103516\n",
      "steps = 154, loss = 4.220682621002197\n",
      "steps = 154, loss = 5.49943208694458\n",
      "steps = 154, loss = 3.7098348140716553\n",
      "steps = 154, loss = 2.9178221225738525\n",
      "steps = 154, loss = 49.96210861206055\n",
      "steps = 154, loss = 2.8894286155700684\n",
      "steps = 154, loss = 3.508402109146118\n",
      "steps = 154, loss = 2.8529560565948486\n",
      "steps = 154, loss = 49.855403900146484\n",
      "steps = 154, loss = 2.854375123977661\n",
      "steps = 154, loss = 2.142017364501953\n",
      "steps = 154, loss = 2.908414363861084\n",
      "steps = 154, loss = 3.3757643699645996\n",
      "steps = 154, loss = 3.036247730255127\n",
      "steps = 154, loss = 2.5969417095184326\n",
      "steps = 154, loss = 3.228543996810913\n",
      "steps = 154, loss = 4.333662509918213\n",
      "steps = 154, loss = 2.7312395572662354\n",
      "steps = 154, loss = 3.4542577266693115\n",
      "steps = 154, loss = 3.5045197010040283\n",
      "steps = 154, loss = 18.618099212646484\n",
      "steps = 154, loss = 2.966616153717041\n",
      "steps = 154, loss = 4.249037265777588\n",
      "steps = 154, loss = 49.985382080078125\n",
      "steps = 154, loss = 3.667670965194702\n",
      "steps = 154, loss = 3.1549417972564697\n",
      "steps = 155, loss = 2.9264092445373535\n",
      "steps = 155, loss = 2.103550434112549\n",
      "steps = 155, loss = 3.427616596221924\n",
      "steps = 155, loss = 3.457427978515625\n",
      "steps = 155, loss = 2.758401393890381\n",
      "steps = 155, loss = 4.273064613342285\n",
      "steps = 155, loss = 3.2225866317749023\n",
      "steps = 155, loss = 2.9715278148651123\n",
      "steps = 155, loss = 2.770763635635376\n",
      "steps = 155, loss = 5.559253215789795\n",
      "steps = 155, loss = 2.132051467895508\n",
      "steps = 155, loss = 3.231926679611206\n",
      "steps = 155, loss = 2.6566872596740723\n",
      "steps = 155, loss = 50.03568649291992\n",
      "steps = 155, loss = 4.348443031311035\n",
      "steps = 155, loss = 2.0928661823272705\n",
      "steps = 155, loss = 2.9511265754699707\n",
      "steps = 155, loss = 2.9197943210601807\n",
      "steps = 155, loss = 2.7405765056610107\n",
      "steps = 155, loss = 3.438660144805908\n",
      "steps = 155, loss = 3.5007176399230957\n",
      "steps = 155, loss = 6.493039131164551\n",
      "steps = 155, loss = 2.870304584503174\n",
      "steps = 155, loss = 3.6260175704956055\n",
      "steps = 155, loss = 4.232809066772461\n",
      "steps = 155, loss = 49.985382080078125\n",
      "steps = 155, loss = 2.6454224586486816\n",
      "steps = 155, loss = 3.6192359924316406\n",
      "steps = 155, loss = 18.743925094604492\n",
      "steps = 155, loss = 49.96164321899414\n",
      "steps = 155, loss = 2.7405076026916504\n",
      "steps = 155, loss = 3.5428740978240967\n",
      "steps = 156, loss = 2.09247088432312\n",
      "steps = 156, loss = 4.232270240783691\n",
      "steps = 156, loss = 2.726116895675659\n",
      "steps = 156, loss = 2.7831971645355225\n",
      "steps = 156, loss = 3.3146800994873047\n",
      "steps = 156, loss = 2.5123157501220703\n",
      "steps = 156, loss = 2.6486318111419678\n",
      "steps = 156, loss = 2.779729127883911\n",
      "steps = 156, loss = 3.6583304405212402\n",
      "steps = 156, loss = 2.7047512531280518\n",
      "steps = 156, loss = 4.337730884552002\n",
      "steps = 156, loss = 6.586334705352783\n",
      "steps = 156, loss = 3.4746477603912354\n",
      "steps = 156, loss = 3.0802273750305176\n",
      "steps = 156, loss = 3.4441306591033936\n",
      "steps = 156, loss = 4.2693705558776855\n",
      "steps = 156, loss = 3.2216806411743164\n",
      "steps = 156, loss = 3.4673874378204346\n",
      "steps = 156, loss = 3.441383123397827\n",
      "steps = 156, loss = 3.6661312580108643\n",
      "steps = 156, loss = 2.9615161418914795\n",
      "steps = 156, loss = 50.035213470458984\n",
      "steps = 156, loss = 5.5308003425598145\n",
      "steps = 156, loss = 3.4278061389923096\n",
      "steps = 156, loss = 2.9178295135498047\n",
      "steps = 156, loss = 2.1084842681884766\n",
      "steps = 156, loss = 19.044404983520508\n",
      "steps = 156, loss = 49.985382080078125\n",
      "steps = 156, loss = 2.8537745475769043\n",
      "steps = 156, loss = 2.751695156097412\n",
      "steps = 156, loss = 49.920955657958984\n",
      "steps = 156, loss = 2.076021909713745\n",
      "steps = 157, loss = 4.3031744956970215\n",
      "steps = 157, loss = 5.588047504425049\n",
      "steps = 157, loss = 3.4027786254882812\n",
      "steps = 157, loss = 50.029720306396484\n",
      "steps = 157, loss = 3.093808174133301\n",
      "steps = 157, loss = 2.7977590560913086\n",
      "steps = 157, loss = 2.871396780014038\n",
      "steps = 157, loss = 3.5184435844421387\n",
      "steps = 157, loss = 2.9709925651550293\n",
      "steps = 157, loss = 3.351205348968506\n",
      "steps = 157, loss = 3.6473186016082764\n",
      "steps = 157, loss = 3.231093645095825\n",
      "steps = 157, loss = 2.215308904647827\n",
      "steps = 157, loss = 2.931715726852417\n",
      "steps = 157, loss = 2.2724668979644775\n",
      "steps = 157, loss = 3.3590047359466553\n",
      "steps = 157, loss = 2.8726799488067627\n",
      "steps = 157, loss = 2.742648124694824\n",
      "steps = 157, loss = 2.77305006980896\n",
      "steps = 157, loss = 49.985382080078125\n",
      "steps = 157, loss = 2.5810859203338623\n",
      "steps = 157, loss = 3.4464540481567383\n",
      "steps = 157, loss = 18.44515037536621\n",
      "steps = 157, loss = 3.4680235385894775\n",
      "steps = 157, loss = 3.6052494049072266\n",
      "steps = 157, loss = 2.7328202724456787\n",
      "steps = 157, loss = 2.8853089809417725\n",
      "steps = 157, loss = 4.252727508544922\n",
      "steps = 157, loss = 6.431449890136719\n",
      "steps = 157, loss = 49.962459564208984\n",
      "steps = 157, loss = 2.1048636436462402\n",
      "steps = 157, loss = 4.363461017608643\n",
      "steps = 158, loss = 2.7305259704589844\n",
      "steps = 158, loss = 4.253674507141113\n",
      "steps = 158, loss = 3.395296573638916\n",
      "steps = 158, loss = 3.4732487201690674\n",
      "steps = 158, loss = 2.9614453315734863\n",
      "steps = 158, loss = 2.8552820682525635\n",
      "steps = 158, loss = 5.5535407066345215\n",
      "steps = 158, loss = 3.5237293243408203\n",
      "steps = 158, loss = 3.2191321849823\n",
      "steps = 158, loss = 2.5007336139678955\n",
      "steps = 158, loss = 49.985382080078125\n",
      "steps = 158, loss = 4.3005900382995605\n",
      "steps = 158, loss = 6.551989555358887\n",
      "steps = 158, loss = 2.7551398277282715\n",
      "steps = 158, loss = 2.092453956604004\n",
      "steps = 158, loss = 3.358649492263794\n",
      "steps = 158, loss = 2.466160535812378\n",
      "steps = 158, loss = 3.066439390182495\n",
      "steps = 158, loss = 2.924311876296997\n",
      "steps = 158, loss = 49.96397018432617\n",
      "steps = 158, loss = 3.48323130607605\n",
      "steps = 158, loss = 2.224250078201294\n",
      "steps = 158, loss = 50.03165817260742\n",
      "steps = 158, loss = 3.4757239818573\n",
      "steps = 158, loss = 3.5510900020599365\n",
      "steps = 158, loss = 2.4259824752807617\n",
      "steps = 158, loss = 2.6613481044769287\n",
      "steps = 158, loss = 2.7273504734039307\n",
      "steps = 158, loss = 4.3539605140686035\n",
      "steps = 158, loss = 3.2211644649505615\n",
      "steps = 158, loss = 3.131865978240967\n",
      "steps = 158, loss = 19.3234806060791\n",
      "steps = 159, loss = 2.2555840015411377\n",
      "steps = 159, loss = 2.7221202850341797\n",
      "steps = 159, loss = 2.672639846801758\n",
      "steps = 159, loss = 2.726120710372925\n",
      "steps = 159, loss = 2.8545610904693604\n",
      "steps = 159, loss = 3.3496923446655273\n",
      "steps = 159, loss = 3.5134847164154053\n",
      "steps = 159, loss = 50.019100189208984\n",
      "steps = 159, loss = 49.92914581298828\n",
      "steps = 159, loss = 2.9654946327209473\n",
      "steps = 159, loss = 3.48880672454834\n",
      "steps = 159, loss = 2.752582550048828\n",
      "steps = 159, loss = 3.3842432498931885\n",
      "steps = 159, loss = 4.275242805480957\n",
      "steps = 159, loss = 5.661495208740234\n",
      "steps = 159, loss = 4.374527931213379\n",
      "steps = 159, loss = 2.612543821334839\n",
      "steps = 159, loss = 3.4848387241363525\n",
      "steps = 159, loss = 2.8707542419433594\n",
      "steps = 159, loss = 2.935131311416626\n",
      "steps = 159, loss = 4.329656600952148\n",
      "steps = 159, loss = 3.6586711406707764\n",
      "steps = 159, loss = 2.8390603065490723\n",
      "steps = 159, loss = 18.35441017150879\n",
      "steps = 159, loss = 6.447628974914551\n",
      "steps = 159, loss = 3.7032933235168457\n",
      "steps = 159, loss = 2.9164042472839355\n",
      "steps = 159, loss = 3.3975765705108643\n",
      "steps = 159, loss = 3.225830316543579\n",
      "steps = 159, loss = 2.1462206840515137\n",
      "steps = 159, loss = 2.1422696113586426\n",
      "steps = 159, loss = 49.985382080078125\n",
      "steps = 160, loss = 5.486213207244873\n",
      "steps = 160, loss = 2.6247940063476562\n",
      "steps = 160, loss = 2.863177537918091\n",
      "steps = 160, loss = 3.570134401321411\n",
      "steps = 160, loss = 3.2483139038085938\n",
      "steps = 160, loss = 2.8541812896728516\n",
      "steps = 160, loss = 3.225304126739502\n",
      "steps = 160, loss = 49.985382080078125\n",
      "steps = 160, loss = 3.028874158859253\n",
      "steps = 160, loss = 3.388268232345581\n",
      "steps = 160, loss = 49.94886016845703\n",
      "steps = 160, loss = 4.288568019866943\n",
      "steps = 160, loss = 3.3536794185638428\n",
      "steps = 160, loss = 2.6484878063201904\n",
      "steps = 160, loss = 2.8172686100006104\n",
      "steps = 160, loss = 2.7435247898101807\n",
      "steps = 160, loss = 3.565598487854004\n",
      "steps = 160, loss = 6.564700126647949\n",
      "steps = 160, loss = 2.211536169052124\n",
      "steps = 160, loss = 3.576796531677246\n",
      "steps = 160, loss = 19.3873233795166\n",
      "steps = 160, loss = 4.384685039520264\n",
      "steps = 160, loss = 2.082564353942871\n",
      "steps = 160, loss = 49.9062614440918\n",
      "steps = 160, loss = 2.9404313564300537\n",
      "steps = 160, loss = 3.47804594039917\n",
      "steps = 160, loss = 50.00526428222656\n",
      "steps = 160, loss = 4.34904146194458\n",
      "steps = 160, loss = 2.1431427001953125\n",
      "steps = 160, loss = 2.9653642177581787\n",
      "steps = 160, loss = 2.721137046813965\n",
      "steps = 160, loss = 3.395369291305542\n",
      "steps = 161, loss = 2.8526217937469482\n",
      "steps = 161, loss = 3.18802809715271\n",
      "steps = 161, loss = 2.565244197845459\n",
      "steps = 161, loss = 18.73975944519043\n",
      "steps = 161, loss = 49.94708251953125\n",
      "steps = 161, loss = 2.0940303802490234\n",
      "steps = 161, loss = 2.7494449615478516\n",
      "steps = 161, loss = 3.35691499710083\n",
      "steps = 161, loss = 4.375148773193359\n",
      "steps = 161, loss = 3.217290163040161\n",
      "steps = 161, loss = 4.28578519821167\n",
      "steps = 161, loss = 2.6822381019592285\n",
      "steps = 161, loss = 3.318652868270874\n",
      "steps = 161, loss = 4.348529815673828\n",
      "steps = 161, loss = 2.5175373554229736\n",
      "steps = 161, loss = 6.456371784210205\n",
      "steps = 161, loss = 2.6479766368865967\n",
      "steps = 161, loss = 5.548059940338135\n",
      "steps = 161, loss = 49.951236724853516\n",
      "steps = 161, loss = 2.9589052200317383\n",
      "steps = 161, loss = 2.934521436691284\n",
      "steps = 161, loss = 3.5057826042175293\n",
      "steps = 161, loss = 1.8990304470062256\n",
      "steps = 161, loss = 3.632580518722534\n",
      "steps = 161, loss = 2.871992588043213\n",
      "steps = 161, loss = 49.985382080078125\n",
      "steps = 161, loss = 3.0493381023406982\n",
      "steps = 161, loss = 2.057832956314087\n",
      "steps = 161, loss = 50.003658294677734\n",
      "steps = 161, loss = 2.7244746685028076\n",
      "steps = 161, loss = 3.403080701828003\n",
      "steps = 161, loss = 3.4503588676452637\n",
      "steps = 162, loss = 2.868143081665039\n",
      "steps = 162, loss = 2.0607197284698486\n",
      "steps = 162, loss = 4.305490970611572\n",
      "steps = 162, loss = 3.5143730640411377\n",
      "steps = 162, loss = 18.96411895751953\n",
      "steps = 162, loss = 3.5079023838043213\n",
      "steps = 162, loss = 49.92845153808594\n",
      "steps = 162, loss = 3.4141297340393066\n",
      "steps = 162, loss = 5.552945137023926\n",
      "steps = 162, loss = 2.1520891189575195\n",
      "steps = 162, loss = 3.4886648654937744\n",
      "steps = 162, loss = 2.8705999851226807\n",
      "steps = 162, loss = 2.7717878818511963\n",
      "steps = 162, loss = 3.4137847423553467\n",
      "steps = 162, loss = 2.6690585613250732\n",
      "steps = 162, loss = 3.0843820571899414\n",
      "steps = 162, loss = 3.5551364421844482\n",
      "steps = 162, loss = 3.2265877723693848\n",
      "steps = 162, loss = 4.382736682891846\n",
      "steps = 162, loss = 2.7420425415039062\n",
      "steps = 162, loss = 2.607877492904663\n",
      "steps = 162, loss = 2.1047894954681396\n",
      "steps = 162, loss = 49.985382080078125\n",
      "steps = 162, loss = 2.6403186321258545\n",
      "steps = 162, loss = 2.8282759189605713\n",
      "steps = 162, loss = 2.9685006141662598\n",
      "steps = 162, loss = 6.634193420410156\n",
      "steps = 162, loss = 2.948478937149048\n",
      "steps = 162, loss = 3.5740699768066406\n",
      "steps = 162, loss = 4.400849342346191\n",
      "steps = 162, loss = 49.859806060791016\n",
      "steps = 162, loss = 23.3994083404541\n",
      "steps = 163, loss = 2.8907694816589355\n",
      "steps = 163, loss = 2.1069631576538086\n",
      "steps = 163, loss = 2.7746694087982178\n",
      "steps = 163, loss = 49.985382080078125\n",
      "steps = 163, loss = 3.6369593143463135\n",
      "steps = 163, loss = 4.402943134307861\n",
      "steps = 163, loss = 19.03373908996582\n",
      "steps = 163, loss = 2.9545388221740723\n",
      "steps = 163, loss = 3.057774066925049\n",
      "steps = 163, loss = 49.970115661621094\n",
      "steps = 163, loss = 2.7433743476867676\n",
      "steps = 163, loss = 3.3125319480895996\n",
      "steps = 163, loss = 4.319427967071533\n",
      "steps = 163, loss = 2.7626688480377197\n",
      "steps = 163, loss = 2.608267307281494\n",
      "steps = 163, loss = 3.4308488368988037\n",
      "steps = 163, loss = 1.9855809211730957\n",
      "steps = 163, loss = 3.515009641647339\n",
      "steps = 163, loss = 3.228325843811035\n",
      "steps = 163, loss = 4.4134392738342285\n",
      "steps = 163, loss = 2.8741443157196045\n",
      "steps = 163, loss = 2.653764247894287\n",
      "steps = 163, loss = 3.386538505554199\n",
      "steps = 163, loss = 2.8736400604248047\n",
      "steps = 163, loss = 3.7023842334747314\n",
      "steps = 163, loss = 17.78458595275879\n",
      "steps = 163, loss = 49.671112060546875\n",
      "steps = 163, loss = 5.5930256843566895\n",
      "steps = 163, loss = 2.9703874588012695\n",
      "steps = 163, loss = 6.490187644958496\n",
      "steps = 163, loss = 2.1433420181274414\n",
      "steps = 163, loss = 3.580871105194092\n",
      "steps = 164, loss = 2.6945574283599854\n",
      "steps = 164, loss = 3.322604179382324\n",
      "steps = 164, loss = 4.416611671447754\n",
      "steps = 164, loss = 2.722128391265869\n",
      "steps = 164, loss = 2.140146017074585\n",
      "steps = 164, loss = 3.5004770755767822\n",
      "steps = 164, loss = 3.0207407474517822\n",
      "steps = 164, loss = 3.4767391681671143\n",
      "steps = 164, loss = 2.1936495304107666\n",
      "steps = 164, loss = 4.419087886810303\n",
      "steps = 164, loss = 49.985382080078125\n",
      "steps = 164, loss = 49.91799545288086\n",
      "steps = 164, loss = 3.0118162631988525\n",
      "steps = 164, loss = 11.57939624786377\n",
      "steps = 164, loss = 49.99515914916992\n",
      "steps = 164, loss = 2.8698623180389404\n",
      "steps = 164, loss = 6.610603332519531\n",
      "steps = 164, loss = 2.73431396484375\n",
      "steps = 164, loss = 2.855628490447998\n",
      "steps = 164, loss = 19.168962478637695\n",
      "steps = 164, loss = 3.449554920196533\n",
      "steps = 164, loss = 4.333522319793701\n",
      "steps = 164, loss = 3.224508285522461\n",
      "steps = 164, loss = 2.6150362491607666\n",
      "steps = 164, loss = 2.046565532684326\n",
      "steps = 164, loss = 2.9659106731414795\n",
      "steps = 164, loss = 2.955977439880371\n",
      "steps = 164, loss = 2.8215651512145996\n",
      "steps = 164, loss = 3.363881826400757\n",
      "steps = 164, loss = 5.631131172180176\n",
      "steps = 164, loss = 3.359057664871216\n",
      "steps = 164, loss = 3.412679672241211\n",
      "steps = 165, loss = 2.853196144104004\n",
      "steps = 165, loss = 3.2159910202026367\n",
      "steps = 165, loss = 2.7504723072052\n",
      "steps = 165, loss = 4.329620838165283\n",
      "steps = 165, loss = 3.40460205078125\n",
      "steps = 165, loss = 40.53272247314453\n",
      "steps = 165, loss = 4.415271759033203\n",
      "steps = 165, loss = 2.7250120639801025\n",
      "steps = 165, loss = 49.985382080078125\n",
      "steps = 165, loss = 3.506959915161133\n",
      "steps = 165, loss = 3.395843029022217\n",
      "steps = 165, loss = 3.6856725215911865\n",
      "steps = 165, loss = 2.6887125968933105\n",
      "steps = 165, loss = 3.0924887657165527\n",
      "steps = 165, loss = 19.071924209594727\n",
      "steps = 165, loss = 2.093294858932495\n",
      "steps = 165, loss = 7.900510311126709\n",
      "steps = 165, loss = 2.1131973266601562\n",
      "steps = 165, loss = 3.4383301734924316\n",
      "steps = 165, loss = 49.966270446777344\n",
      "steps = 165, loss = 4.406153678894043\n",
      "steps = 165, loss = 2.858690023422241\n",
      "steps = 165, loss = 2.1620898246765137\n",
      "steps = 165, loss = 3.506179094314575\n",
      "steps = 165, loss = 6.505462646484375\n",
      "steps = 165, loss = 2.622495412826538\n",
      "steps = 165, loss = 2.949313163757324\n",
      "steps = 165, loss = 2.613757610321045\n",
      "steps = 165, loss = 3.3630402088165283\n",
      "steps = 165, loss = 2.74709153175354\n",
      "steps = 165, loss = 5.482795715332031\n",
      "steps = 165, loss = 2.95878005027771\n",
      "steps = 166, loss = 49.97385025024414\n",
      "steps = 166, loss = 3.4524950981140137\n",
      "steps = 166, loss = 3.1293797492980957\n",
      "steps = 166, loss = 2.7267467975616455\n",
      "steps = 166, loss = 2.254354953765869\n",
      "steps = 166, loss = 2.158383369445801\n",
      "steps = 166, loss = 2.968282699584961\n",
      "steps = 166, loss = 4.3492112159729\n",
      "steps = 166, loss = 3.4188296794891357\n",
      "steps = 166, loss = 3.510878801345825\n",
      "steps = 166, loss = 49.985382080078125\n",
      "steps = 166, loss = 2.96299147605896\n",
      "steps = 166, loss = 3.225463628768921\n",
      "steps = 166, loss = 3.4508073329925537\n",
      "steps = 166, loss = 18.677696228027344\n",
      "steps = 166, loss = 2.105088710784912\n",
      "steps = 166, loss = 3.5253891944885254\n",
      "steps = 166, loss = 2.9923043251037598\n",
      "steps = 166, loss = 4.449558734893799\n",
      "steps = 166, loss = 5.595810413360596\n",
      "steps = 166, loss = 2.87223482131958\n",
      "steps = 166, loss = 2.773800849914551\n",
      "steps = 166, loss = 2.541537284851074\n",
      "steps = 166, loss = 2.6383283138275146\n",
      "steps = 166, loss = 3.602309226989746\n",
      "steps = 166, loss = 3.4338977336883545\n",
      "steps = 166, loss = 2.8337182998657227\n",
      "steps = 166, loss = 2.742710590362549\n",
      "steps = 166, loss = 6.671959400177002\n",
      "steps = 166, loss = 5.596130847930908\n",
      "steps = 166, loss = 4.433206081390381\n",
      "steps = 166, loss = 7.845406532287598\n",
      "steps = 167, loss = 2.7070488929748535\n",
      "steps = 167, loss = 3.479652166366577\n",
      "steps = 167, loss = 2.9743990898132324\n",
      "steps = 167, loss = 19.446086883544922\n",
      "steps = 167, loss = 3.4776008129119873\n",
      "steps = 167, loss = 3.7137949466705322\n",
      "steps = 167, loss = 3.491227626800537\n",
      "steps = 167, loss = 3.3914592266082764\n",
      "steps = 167, loss = 2.8300275802612305\n",
      "steps = 167, loss = 4.364239692687988\n",
      "steps = 167, loss = 49.985382080078125\n",
      "steps = 167, loss = 2.9903461933135986\n",
      "steps = 167, loss = 2.965358257293701\n",
      "steps = 167, loss = 2.8553483486175537\n",
      "steps = 167, loss = 2.9645955562591553\n",
      "steps = 167, loss = 2.7351462841033936\n",
      "steps = 167, loss = 3.454705238342285\n",
      "steps = 167, loss = 49.978878021240234\n",
      "steps = 167, loss = 2.0705559253692627\n",
      "steps = 167, loss = 2.117497682571411\n",
      "steps = 167, loss = 2.7223660945892334\n",
      "steps = 167, loss = 4.464629173278809\n",
      "steps = 167, loss = 2.9434642791748047\n",
      "steps = 167, loss = 3.222433090209961\n",
      "steps = 167, loss = 2.140519142150879\n",
      "steps = 167, loss = 2.5853683948516846\n",
      "steps = 167, loss = 4.440217018127441\n",
      "steps = 167, loss = 6.419066905975342\n",
      "steps = 167, loss = 8.864920616149902\n",
      "steps = 167, loss = 5.6346659660339355\n",
      "steps = 167, loss = 3.422452926635742\n",
      "steps = 167, loss = 5.589507102966309\n",
      "steps = 168, loss = 2.8904976844787598\n",
      "steps = 168, loss = 3.397960901260376\n",
      "steps = 168, loss = 3.4391653537750244\n",
      "steps = 168, loss = 19.068790435791016\n",
      "steps = 168, loss = 2.9591832160949707\n",
      "steps = 168, loss = 49.97904586791992\n",
      "steps = 168, loss = 3.183023452758789\n",
      "steps = 168, loss = 6.574138164520264\n",
      "steps = 168, loss = 2.1094467639923096\n",
      "steps = 168, loss = 2.6915066242218018\n",
      "steps = 168, loss = 49.775142669677734\n",
      "steps = 168, loss = 2.7823145389556885\n",
      "steps = 168, loss = 2.0933074951171875\n",
      "steps = 168, loss = 3.3348886966705322\n",
      "steps = 168, loss = 4.360650539398193\n",
      "steps = 168, loss = 2.792506694793701\n",
      "steps = 168, loss = 2.853454351425171\n",
      "steps = 168, loss = 4.463312149047852\n",
      "steps = 168, loss = 3.214146375656128\n",
      "steps = 168, loss = 5.00455904006958\n",
      "steps = 168, loss = 3.192436695098877\n",
      "steps = 168, loss = 2.9577927589416504\n",
      "steps = 168, loss = 2.7356371879577637\n",
      "steps = 168, loss = 2.725057601928711\n",
      "steps = 168, loss = 2.5756354331970215\n",
      "steps = 168, loss = 2.751349687576294\n",
      "steps = 168, loss = 5.558182716369629\n",
      "steps = 168, loss = 4.427202224731445\n",
      "steps = 168, loss = 3.4393229484558105\n",
      "steps = 168, loss = 4.809065341949463\n",
      "steps = 168, loss = 3.413376569747925\n",
      "steps = 168, loss = 49.985382080078125\n",
      "steps = 169, loss = 3.3965060710906982\n",
      "steps = 169, loss = 2.9487478733062744\n",
      "steps = 169, loss = 2.275707721710205\n",
      "steps = 169, loss = 4.537499904632568\n",
      "steps = 169, loss = 2.7428271770477295\n",
      "steps = 169, loss = 3.5031604766845703\n",
      "steps = 169, loss = 2.7738704681396484\n",
      "steps = 169, loss = 2.6227190494537354\n",
      "steps = 169, loss = 49.94460678100586\n",
      "steps = 169, loss = 2.8156354427337646\n",
      "steps = 169, loss = 4.455103874206543\n",
      "steps = 169, loss = 3.4496889114379883\n",
      "steps = 169, loss = 2.872169017791748\n",
      "steps = 169, loss = 3.7034122943878174\n",
      "steps = 169, loss = 2.105252981185913\n",
      "steps = 169, loss = 3.423351526260376\n",
      "steps = 169, loss = 4.497350215911865\n",
      "steps = 169, loss = 3.649064540863037\n",
      "steps = 169, loss = 5.601105213165283\n",
      "steps = 169, loss = 18.95958137512207\n",
      "steps = 169, loss = 2.9728121757507324\n",
      "steps = 169, loss = 3.062145709991455\n",
      "steps = 169, loss = 49.985382080078125\n",
      "steps = 169, loss = 2.6382100582122803\n",
      "steps = 169, loss = 4.779638767242432\n",
      "steps = 169, loss = 2.9673707485198975\n",
      "steps = 169, loss = 4.379894256591797\n",
      "steps = 169, loss = 3.727677345275879\n",
      "steps = 169, loss = 3.223674774169922\n",
      "steps = 169, loss = 6.585582256317139\n",
      "steps = 169, loss = 2.6036324501037598\n",
      "steps = 169, loss = 3.423708200454712\n",
      "steps = 170, loss = 3.2745015621185303\n",
      "steps = 170, loss = 3.393592596054077\n",
      "steps = 170, loss = 2.1404216289520264\n",
      "steps = 170, loss = 2.8554232120513916\n",
      "steps = 170, loss = 2.953077793121338\n",
      "steps = 170, loss = 2.826235055923462\n",
      "steps = 170, loss = 2.054614782333374\n",
      "steps = 170, loss = 2.2424583435058594\n",
      "steps = 170, loss = 2.975236177444458\n",
      "steps = 170, loss = 3.7282450199127197\n",
      "steps = 170, loss = 5.67648983001709\n",
      "steps = 170, loss = 3.355924129486084\n",
      "steps = 170, loss = 2.9637696743011475\n",
      "steps = 170, loss = 3.50335955619812\n",
      "steps = 170, loss = 2.7952940464019775\n",
      "steps = 170, loss = 4.395320892333984\n",
      "steps = 170, loss = 49.96254348754883\n",
      "steps = 170, loss = 2.7362093925476074\n",
      "steps = 170, loss = 3.503638744354248\n",
      "steps = 170, loss = 3.2207930088043213\n",
      "steps = 170, loss = 2.6871206760406494\n",
      "steps = 170, loss = 6.6207122802734375\n",
      "steps = 170, loss = 6.243725299835205\n",
      "steps = 170, loss = 18.633228302001953\n",
      "steps = 170, loss = 2.535529136657715\n",
      "steps = 170, loss = 2.722383975982666\n",
      "steps = 170, loss = 49.985382080078125\n",
      "steps = 170, loss = 4.512257099151611\n",
      "steps = 170, loss = 2.6832451820373535\n",
      "steps = 170, loss = 3.53609037399292\n",
      "steps = 170, loss = 4.462535858154297\n",
      "steps = 170, loss = 3.4773991107940674\n",
      "steps = 171, loss = 49.985382080078125\n",
      "steps = 171, loss = 3.3502442836761475\n",
      "steps = 171, loss = 49.97292709350586\n",
      "steps = 171, loss = 4.5118794441223145\n",
      "steps = 171, loss = 3.371351957321167\n",
      "steps = 171, loss = 2.6063947677612305\n",
      "steps = 171, loss = 5.444243907928467\n",
      "steps = 171, loss = 6.687853813171387\n",
      "steps = 171, loss = 2.080183982849121\n",
      "steps = 171, loss = 2.7517971992492676\n",
      "steps = 171, loss = 2.6659934520721436\n",
      "steps = 171, loss = 2.9570577144622803\n",
      "steps = 171, loss = 2.853717088699341\n",
      "steps = 171, loss = 3.5725882053375244\n",
      "steps = 171, loss = 2.093259811401367\n",
      "steps = 171, loss = 4.44873571395874\n",
      "steps = 171, loss = 2.7250587940216064\n",
      "steps = 171, loss = 3.5125110149383545\n",
      "steps = 171, loss = 4.392039775848389\n",
      "steps = 171, loss = 3.000741481781006\n",
      "steps = 171, loss = 2.7372121810913086\n",
      "steps = 171, loss = 3.19199275970459\n",
      "steps = 171, loss = 3.4740116596221924\n",
      "steps = 171, loss = 2.9690070152282715\n",
      "steps = 171, loss = 2.786972761154175\n",
      "steps = 171, loss = 5.643367767333984\n",
      "steps = 171, loss = 3.212627410888672\n",
      "steps = 171, loss = 3.5922820568084717\n",
      "steps = 171, loss = 2.074272871017456\n",
      "steps = 171, loss = 3.0555713176727295\n",
      "steps = 171, loss = 19.295421600341797\n",
      "steps = 171, loss = 2.51863956451416\n",
      "steps = 172, loss = 2.6526575088500977\n",
      "steps = 172, loss = 2.7739603519439697\n",
      "steps = 172, loss = 2.9824461936950684\n",
      "steps = 172, loss = 49.985382080078125\n",
      "steps = 172, loss = 2.742915391921997\n",
      "steps = 172, loss = 3.6104915142059326\n",
      "steps = 172, loss = 3.58606219291687\n",
      "steps = 172, loss = 2.105433225631714\n",
      "steps = 172, loss = 5.554908275604248\n",
      "steps = 172, loss = 49.97270202636719\n",
      "steps = 172, loss = 2.061190366744995\n",
      "steps = 172, loss = 2.606367349624634\n",
      "steps = 172, loss = 2.653625726699829\n",
      "steps = 172, loss = 4.476650238037109\n",
      "steps = 172, loss = 6.589053153991699\n",
      "steps = 172, loss = 2.18896746635437\n",
      "steps = 172, loss = 3.6799559593200684\n",
      "steps = 172, loss = 4.545588970184326\n",
      "steps = 172, loss = 2.9622669219970703\n",
      "steps = 172, loss = 2.966595411300659\n",
      "steps = 172, loss = 3.558284044265747\n",
      "steps = 172, loss = 5.642137050628662\n",
      "steps = 172, loss = 3.4764747619628906\n",
      "steps = 172, loss = 19.22858428955078\n",
      "steps = 172, loss = 2.872246026992798\n",
      "steps = 172, loss = 2.914062738418579\n",
      "steps = 172, loss = 3.3986005783081055\n",
      "steps = 172, loss = 3.236111640930176\n",
      "steps = 172, loss = 4.410892963409424\n",
      "steps = 172, loss = 2.816279172897339\n",
      "steps = 172, loss = 3.6518983840942383\n",
      "steps = 172, loss = 3.2220990657806396\n",
      "steps = 173, loss = 49.957820892333984\n",
      "steps = 173, loss = 2.9630377292633057\n",
      "steps = 173, loss = 2.1403558254241943\n",
      "steps = 173, loss = 3.2573351860046387\n",
      "steps = 173, loss = 3.3704776763916016\n",
      "steps = 173, loss = 2.98486590385437\n",
      "steps = 173, loss = 2.866591453552246\n",
      "steps = 173, loss = 2.7540433406829834\n",
      "steps = 173, loss = 3.438845157623291\n",
      "steps = 173, loss = 2.855531692504883\n",
      "steps = 173, loss = 49.985382080078125\n",
      "steps = 173, loss = 3.4222757816314697\n",
      "steps = 173, loss = 4.484277725219727\n",
      "steps = 173, loss = 3.3435590267181396\n",
      "steps = 173, loss = 2.607590436935425\n",
      "steps = 173, loss = 3.438392400741577\n",
      "steps = 173, loss = 4.426726818084717\n",
      "steps = 173, loss = 2.7967140674591064\n",
      "steps = 173, loss = 2.7369096279144287\n",
      "steps = 173, loss = 6.489986419677734\n",
      "steps = 173, loss = 3.541714906692505\n",
      "steps = 173, loss = 19.755720138549805\n",
      "steps = 173, loss = 4.56102991104126\n",
      "steps = 173, loss = 2.7225937843322754\n",
      "steps = 173, loss = 3.4924683570861816\n",
      "steps = 173, loss = 6.646969795227051\n",
      "steps = 173, loss = 2.020848512649536\n",
      "steps = 173, loss = 2.7077443599700928\n",
      "steps = 173, loss = 5.576706409454346\n",
      "steps = 173, loss = 3.219294309616089\n",
      "steps = 173, loss = 2.1222963333129883\n",
      "steps = 173, loss = 3.010671377182007\n",
      "steps = 174, loss = 2.07871675491333\n",
      "steps = 174, loss = 5.562757968902588\n",
      "steps = 174, loss = 3.197556257247925\n",
      "steps = 174, loss = 3.268035411834717\n",
      "steps = 174, loss = 2.725104808807373\n",
      "steps = 174, loss = 2.6556334495544434\n",
      "steps = 174, loss = 49.985382080078125\n",
      "steps = 174, loss = 4.559952735900879\n",
      "steps = 174, loss = 2.9563798904418945\n",
      "steps = 174, loss = 3.007338523864746\n",
      "steps = 174, loss = 3.177863359451294\n",
      "steps = 174, loss = 4.46926212310791\n",
      "steps = 174, loss = 2.853970766067505\n",
      "steps = 174, loss = 2.7475624084472656\n",
      "steps = 174, loss = 2.7522385120391846\n",
      "steps = 174, loss = 3.0877366065979004\n",
      "steps = 174, loss = 3.0296361446380615\n",
      "steps = 174, loss = 2.6396658420562744\n",
      "steps = 174, loss = 2.978579044342041\n",
      "steps = 174, loss = 3.4493355751037598\n",
      "steps = 174, loss = 3.392087459564209\n",
      "steps = 174, loss = 3.5008106231689453\n",
      "steps = 174, loss = 6.5660400390625\n",
      "steps = 174, loss = 3.648594856262207\n",
      "steps = 174, loss = 2.093229293823242\n",
      "steps = 174, loss = 6.35654354095459\n",
      "steps = 174, loss = 49.95155334472656\n",
      "steps = 174, loss = 2.2284839153289795\n",
      "steps = 174, loss = 18.62295150756836\n",
      "steps = 174, loss = 4.423000812530518\n",
      "steps = 174, loss = 3.4012508392333984\n",
      "steps = 174, loss = 3.2110159397125244\n",
      "steps = 175, loss = 2.5560808181762695\n",
      "steps = 175, loss = 3.5023996829986572\n",
      "steps = 175, loss = 3.5254762172698975\n",
      "steps = 175, loss = 2.9607913494110107\n",
      "steps = 175, loss = 49.93695068359375\n",
      "steps = 175, loss = 2.717485189437866\n",
      "steps = 175, loss = 3.7553701400756836\n",
      "steps = 175, loss = 49.985382080078125\n",
      "steps = 175, loss = 2.7778420448303223\n",
      "steps = 175, loss = 2.9887661933898926\n",
      "steps = 175, loss = 3.008232593536377\n",
      "steps = 175, loss = 2.5130879878997803\n",
      "steps = 175, loss = 4.494773864746094\n",
      "steps = 175, loss = 2.8548152446746826\n",
      "steps = 175, loss = 3.330188274383545\n",
      "steps = 175, loss = 3.2166426181793213\n",
      "steps = 175, loss = 2.7403664588928223\n",
      "steps = 175, loss = 3.7046854496002197\n",
      "steps = 175, loss = 6.618016242980957\n",
      "steps = 175, loss = 2.722611427307129\n",
      "steps = 175, loss = 2.8094303607940674\n",
      "steps = 175, loss = 4.5887370109558105\n",
      "steps = 175, loss = 19.457904815673828\n",
      "steps = 175, loss = 2.1419742107391357\n",
      "steps = 175, loss = 4.443368911743164\n",
      "steps = 175, loss = 3.3962130546569824\n",
      "steps = 175, loss = 5.59444522857666\n",
      "steps = 175, loss = 2.075782299041748\n",
      "steps = 175, loss = 7.084597587585449\n",
      "steps = 175, loss = 3.698225736618042\n",
      "steps = 175, loss = 3.406728506088257\n",
      "steps = 175, loss = 2.0103542804718018\n",
      "steps = 176, loss = 2.146165609359741\n",
      "steps = 176, loss = 4.505947589874268\n",
      "steps = 176, loss = 2.105224847793579\n",
      "steps = 176, loss = 2.8706886768341064\n",
      "steps = 176, loss = 2.809792995452881\n",
      "steps = 176, loss = 19.327816009521484\n",
      "steps = 176, loss = 4.613009929656982\n",
      "steps = 176, loss = 49.959930419921875\n",
      "steps = 176, loss = 2.6346068382263184\n",
      "steps = 176, loss = 2.772646903991699\n",
      "steps = 176, loss = 3.4331021308898926\n",
      "steps = 176, loss = 3.219780445098877\n",
      "steps = 176, loss = 3.4264492988586426\n",
      "steps = 176, loss = 5.564101696014404\n",
      "steps = 176, loss = 3.0262625217437744\n",
      "steps = 176, loss = 2.9657390117645264\n",
      "steps = 176, loss = 3.3463521003723145\n",
      "steps = 176, loss = 3.524909257888794\n",
      "steps = 176, loss = 4.453654766082764\n",
      "steps = 176, loss = 3.5353996753692627\n",
      "steps = 176, loss = 3.0852556228637695\n",
      "steps = 176, loss = 2.996964454650879\n",
      "steps = 176, loss = 2.096789598464966\n",
      "steps = 176, loss = 2.7270736694335938\n",
      "steps = 176, loss = 2.579617738723755\n",
      "steps = 176, loss = 2.7416505813598633\n",
      "steps = 176, loss = 3.583224058151245\n",
      "steps = 176, loss = 6.7205424308776855\n",
      "steps = 176, loss = 49.985382080078125\n",
      "steps = 176, loss = 3.334014892578125\n",
      "steps = 176, loss = 3.516125202178955\n",
      "steps = 176, loss = 6.474649429321289\n",
      "steps = 177, loss = 2.104654312133789\n",
      "steps = 177, loss = 2.530449151992798\n",
      "steps = 177, loss = 3.4932708740234375\n",
      "steps = 177, loss = 2.7263095378875732\n",
      "steps = 177, loss = 2.988471746444702\n",
      "steps = 177, loss = 2.657850742340088\n",
      "steps = 177, loss = 2.7556920051574707\n",
      "steps = 177, loss = 2.611599922180176\n",
      "steps = 177, loss = 2.9562230110168457\n",
      "steps = 177, loss = 6.583009243011475\n",
      "steps = 177, loss = 19.166217803955078\n",
      "steps = 177, loss = 2.987969398498535\n",
      "steps = 177, loss = 4.453221797943115\n",
      "steps = 177, loss = 3.4355454444885254\n",
      "steps = 177, loss = 3.5734353065490723\n",
      "steps = 177, loss = 2.7648696899414062\n",
      "steps = 177, loss = 5.638566970825195\n",
      "steps = 177, loss = 4.490586280822754\n",
      "steps = 177, loss = 3.492020606994629\n",
      "steps = 177, loss = 49.985382080078125\n",
      "steps = 177, loss = 3.453303813934326\n",
      "steps = 177, loss = 2.855247735977173\n",
      "steps = 177, loss = 6.9692158699035645\n",
      "steps = 177, loss = 3.510720729827881\n",
      "steps = 177, loss = 2.221388578414917\n",
      "steps = 177, loss = 49.96214294433594\n",
      "steps = 177, loss = 4.608102321624756\n",
      "steps = 177, loss = 3.457761526107788\n",
      "steps = 177, loss = 2.092329263687134\n",
      "steps = 177, loss = 2.638730764389038\n",
      "steps = 177, loss = 3.2100629806518555\n",
      "steps = 177, loss = 3.7455263137817383\n",
      "steps = 178, loss = 6.67575216293335\n",
      "steps = 178, loss = 4.515541076660156\n",
      "steps = 178, loss = 3.3934152126312256\n",
      "steps = 178, loss = 2.723998546600342\n",
      "steps = 178, loss = 49.985382080078125\n",
      "steps = 178, loss = 3.215270519256592\n",
      "steps = 178, loss = 3.200913429260254\n",
      "steps = 178, loss = 3.514246940612793\n",
      "steps = 178, loss = 3.712063789367676\n",
      "steps = 178, loss = 7.294150352478027\n",
      "steps = 178, loss = 2.579991102218628\n",
      "steps = 178, loss = 2.722999095916748\n",
      "steps = 178, loss = 2.9603216648101807\n",
      "steps = 178, loss = 4.636500835418701\n",
      "steps = 178, loss = 3.659787893295288\n",
      "steps = 178, loss = 2.8552005290985107\n",
      "steps = 178, loss = 49.93891143798828\n",
      "steps = 178, loss = 3.437197208404541\n",
      "steps = 178, loss = 5.511368274688721\n",
      "steps = 178, loss = 2.0467586517333984\n",
      "steps = 178, loss = 3.395595073699951\n",
      "steps = 178, loss = 3.4773857593536377\n",
      "steps = 178, loss = 2.9983744621276855\n",
      "steps = 178, loss = 2.7373244762420654\n",
      "steps = 178, loss = 2.7684879302978516\n",
      "steps = 178, loss = 3.865412712097168\n",
      "steps = 178, loss = 2.1414942741394043\n",
      "steps = 178, loss = 2.786346912384033\n",
      "steps = 178, loss = 19.487619400024414\n",
      "steps = 178, loss = 4.473694324493408\n",
      "steps = 178, loss = 3.0592617988586426\n",
      "steps = 178, loss = 2.3125998973846436\n",
      "steps = 179, loss = 3.2185466289520264\n",
      "steps = 179, loss = 2.1023688316345215\n",
      "steps = 179, loss = 2.7729501724243164\n",
      "steps = 179, loss = 2.7488045692443848\n",
      "steps = 179, loss = 5.5934906005859375\n",
      "steps = 179, loss = 49.95417404174805\n",
      "steps = 179, loss = 2.871273994445801\n",
      "steps = 179, loss = 2.7419018745422363\n",
      "steps = 179, loss = 3.7508325576782227\n",
      "steps = 179, loss = 6.674365997314453\n",
      "steps = 179, loss = 2.101757287979126\n",
      "steps = 179, loss = 4.526607513427734\n",
      "steps = 179, loss = 2.105170965194702\n",
      "steps = 179, loss = 2.959320545196533\n",
      "steps = 179, loss = 6.6524224281311035\n",
      "steps = 179, loss = 3.6156697273254395\n",
      "steps = 179, loss = 2.9308621883392334\n",
      "steps = 179, loss = 3.507972240447998\n",
      "steps = 179, loss = 19.16781997680664\n",
      "steps = 179, loss = 4.660192966461182\n",
      "steps = 179, loss = 2.822915554046631\n",
      "steps = 179, loss = 3.5407369136810303\n",
      "steps = 179, loss = 2.965224504470825\n",
      "steps = 179, loss = 3.4730029106140137\n",
      "steps = 179, loss = 3.440950870513916\n",
      "steps = 179, loss = 3.757042407989502\n",
      "steps = 179, loss = 2.5828757286071777\n",
      "steps = 179, loss = 3.461792230606079\n",
      "steps = 179, loss = 2.7466652393341064\n",
      "steps = 179, loss = 49.985382080078125\n",
      "steps = 179, loss = 4.482883930206299\n",
      "steps = 179, loss = 3.006735324859619\n",
      "steps = 180, loss = 3.4251246452331543\n",
      "steps = 180, loss = 4.656309127807617\n",
      "steps = 180, loss = 3.2088499069213867\n",
      "steps = 180, loss = 6.740968227386475\n",
      "steps = 180, loss = 3.5045554637908936\n",
      "steps = 180, loss = 2.9557130336761475\n",
      "steps = 180, loss = 2.755157232284546\n",
      "steps = 180, loss = 3.440180540084839\n",
      "steps = 180, loss = 2.573204278945923\n",
      "steps = 180, loss = 7.032108783721924\n",
      "steps = 180, loss = 3.636537790298462\n",
      "steps = 180, loss = 2.998267650604248\n",
      "steps = 180, loss = 3.039257764816284\n",
      "steps = 180, loss = 4.509317874908447\n",
      "steps = 180, loss = 2.086296796798706\n",
      "steps = 180, loss = 3.3919591903686523\n",
      "steps = 180, loss = 2.8284292221069336\n",
      "steps = 180, loss = 19.44912338256836\n",
      "steps = 180, loss = 4.482588291168213\n",
      "steps = 180, loss = 3.3652820587158203\n",
      "steps = 180, loss = 2.4250597953796387\n",
      "steps = 180, loss = 49.985382080078125\n",
      "steps = 180, loss = 49.34134292602539\n",
      "steps = 180, loss = 2.092438220977783\n",
      "steps = 180, loss = 2.72670316696167\n",
      "steps = 180, loss = 3.4495344161987305\n",
      "steps = 180, loss = 2.6605701446533203\n",
      "steps = 180, loss = 2.71506404876709\n",
      "steps = 180, loss = 3.672414541244507\n",
      "steps = 180, loss = 2.694528818130493\n",
      "steps = 180, loss = 2.8556323051452637\n",
      "steps = 180, loss = 5.5612473487854\n",
      "steps = 181, loss = 3.527552604675293\n",
      "steps = 181, loss = 3.587782621383667\n",
      "steps = 181, loss = 3.5247716903686523\n",
      "steps = 181, loss = 6.6597700119018555\n",
      "steps = 181, loss = 2.2680726051330566\n",
      "steps = 181, loss = 3.386411428451538\n",
      "steps = 181, loss = 19.419456481933594\n",
      "steps = 181, loss = 6.664125442504883\n",
      "steps = 181, loss = 4.500743865966797\n",
      "steps = 181, loss = 3.2180886268615723\n",
      "steps = 181, loss = 2.553708791732788\n",
      "steps = 181, loss = 2.7751903533935547\n",
      "steps = 181, loss = 49.985382080078125\n",
      "steps = 181, loss = 2.8737094402313232\n",
      "steps = 181, loss = 2.740384578704834\n",
      "steps = 181, loss = 3.5243618488311768\n",
      "steps = 181, loss = 3.341909170150757\n",
      "steps = 181, loss = 2.6638317108154297\n",
      "steps = 181, loss = 49.97043228149414\n",
      "steps = 181, loss = 2.8246030807495117\n",
      "steps = 181, loss = 5.598666667938232\n",
      "steps = 181, loss = 2.743800163269043\n",
      "steps = 181, loss = 2.106149673461914\n",
      "steps = 181, loss = 4.690895080566406\n",
      "steps = 181, loss = 4.5392656326293945\n",
      "steps = 181, loss = 3.0112273693084717\n",
      "steps = 181, loss = 2.7469735145568848\n",
      "steps = 181, loss = 2.838768243789673\n",
      "steps = 181, loss = 2.1318581104278564\n",
      "steps = 181, loss = 2.964938163757324\n",
      "steps = 181, loss = 3.4611496925354004\n",
      "steps = 181, loss = 3.5102343559265137\n",
      "steps = 182, loss = 3.003641128540039\n",
      "steps = 182, loss = 2.7580716609954834\n",
      "steps = 182, loss = 3.074350595474243\n",
      "steps = 182, loss = 7.171195983886719\n",
      "steps = 182, loss = 4.688652515411377\n",
      "steps = 182, loss = 4.522765636444092\n",
      "steps = 182, loss = 2.7377912998199463\n",
      "steps = 182, loss = 49.96171951293945\n",
      "steps = 182, loss = 3.357494831085205\n",
      "steps = 182, loss = 4.501675128936768\n",
      "steps = 182, loss = 2.103952169418335\n",
      "steps = 182, loss = 2.751936674118042\n",
      "steps = 182, loss = 2.8838307857513428\n",
      "steps = 182, loss = 3.3736677169799805\n",
      "steps = 182, loss = 2.72798752784729\n",
      "steps = 182, loss = 3.847687005996704\n",
      "steps = 182, loss = 3.2086398601531982\n",
      "steps = 182, loss = 3.4536545276641846\n",
      "steps = 182, loss = 2.7674312591552734\n",
      "steps = 182, loss = 2.092482089996338\n",
      "steps = 182, loss = 2.9558441638946533\n",
      "steps = 182, loss = 49.985382080078125\n",
      "steps = 182, loss = 3.357834815979004\n",
      "steps = 182, loss = 2.8571269512176514\n",
      "steps = 182, loss = 3.356001615524292\n",
      "steps = 182, loss = 3.349231243133545\n",
      "steps = 182, loss = 6.808963298797607\n",
      "steps = 182, loss = 2.6033992767333984\n",
      "steps = 182, loss = 5.627556800842285\n",
      "steps = 182, loss = 47.382835388183594\n",
      "steps = 182, loss = 3.454864025115967\n",
      "steps = 182, loss = 19.60198211669922\n",
      "steps = 183, loss = 2.705077648162842\n",
      "steps = 183, loss = 7.476062297821045\n",
      "steps = 183, loss = 3.5135397911071777\n",
      "steps = 183, loss = 2.1417884826660156\n",
      "steps = 183, loss = 3.4808547496795654\n",
      "steps = 183, loss = 3.4996235370635986\n",
      "steps = 183, loss = 4.7181901931762695\n",
      "steps = 183, loss = 2.7234814167022705\n",
      "steps = 183, loss = 3.4450924396514893\n",
      "steps = 183, loss = 6.544395446777344\n",
      "steps = 183, loss = 2.955203056335449\n",
      "steps = 183, loss = 2.666515827178955\n",
      "steps = 183, loss = 3.013566017150879\n",
      "steps = 183, loss = 3.0369322299957275\n",
      "steps = 183, loss = 3.7031126022338867\n",
      "steps = 183, loss = 2.7321221828460693\n",
      "steps = 183, loss = 2.650047779083252\n",
      "steps = 183, loss = 3.761945962905884\n",
      "steps = 183, loss = 2.8554327487945557\n",
      "steps = 183, loss = 2.8069632053375244\n",
      "steps = 183, loss = 5.596044063568115\n",
      "steps = 183, loss = 1.8233948945999146\n",
      "steps = 183, loss = 2.9597513675689697\n",
      "steps = 183, loss = 49.985382080078125\n",
      "steps = 183, loss = 49.970340728759766\n",
      "steps = 183, loss = 3.213409185409546\n",
      "steps = 183, loss = 3.3921396732330322\n",
      "steps = 183, loss = 2.2663025856018066\n",
      "steps = 183, loss = 19.769981384277344\n",
      "steps = 183, loss = 3.421145439147949\n",
      "steps = 183, loss = 4.521634101867676\n",
      "steps = 183, loss = 4.5476508140563965\n",
      "steps = 184, loss = 2.964719295501709\n",
      "steps = 184, loss = 2.8722100257873535\n",
      "steps = 184, loss = 3.4063479900360107\n",
      "steps = 184, loss = 3.028050661087036\n",
      "steps = 184, loss = 2.1034746170043945\n",
      "steps = 184, loss = 6.7032470703125\n",
      "steps = 184, loss = 2.200324773788452\n",
      "steps = 184, loss = 3.997584581375122\n",
      "steps = 184, loss = 2.9138946533203125\n",
      "steps = 184, loss = 3.627206802368164\n",
      "steps = 184, loss = 2.602198839187622\n",
      "steps = 184, loss = 3.2168142795562744\n",
      "steps = 184, loss = 3.530207633972168\n",
      "steps = 184, loss = 49.985382080078125\n",
      "steps = 184, loss = 6.481213092803955\n",
      "steps = 184, loss = 2.7425167560577393\n",
      "steps = 184, loss = 3.4894039630889893\n",
      "steps = 184, loss = 3.471341133117676\n",
      "steps = 184, loss = 5.635428428649902\n",
      "steps = 184, loss = 19.994611740112305\n",
      "steps = 184, loss = 3.1053102016448975\n",
      "steps = 184, loss = 49.908851623535156\n",
      "steps = 184, loss = 3.6421995162963867\n",
      "steps = 184, loss = 4.530797958374023\n",
      "steps = 184, loss = 2.104949712753296\n",
      "steps = 184, loss = 2.7738254070281982\n",
      "steps = 184, loss = 4.7425312995910645\n",
      "steps = 184, loss = 3.021895408630371\n",
      "steps = 184, loss = 2.6490519046783447\n",
      "steps = 184, loss = 3.0923612117767334\n",
      "steps = 184, loss = 4.559168338775635\n",
      "steps = 184, loss = 3.5957977771759033\n",
      "steps = 185, loss = 49.985382080078125\n",
      "steps = 185, loss = 5.617187023162842\n",
      "steps = 185, loss = 2.8565099239349365\n",
      "steps = 185, loss = 3.2070817947387695\n",
      "steps = 185, loss = 7.128618240356445\n",
      "steps = 185, loss = 2.7562432289123535\n",
      "steps = 185, loss = 3.013317584991455\n",
      "steps = 185, loss = 4.542217254638672\n",
      "steps = 185, loss = 3.6952126026153564\n",
      "steps = 185, loss = 2.627243757247925\n",
      "steps = 185, loss = 2.69328236579895\n",
      "steps = 185, loss = 3.006228446960449\n",
      "steps = 185, loss = 2.1297152042388916\n",
      "steps = 185, loss = 2.092686653137207\n",
      "steps = 185, loss = 3.214662551879883\n",
      "steps = 185, loss = 2.617389678955078\n",
      "steps = 185, loss = 3.449547290802002\n",
      "steps = 185, loss = 2.7273569107055664\n",
      "steps = 185, loss = 4.530767917633057\n",
      "steps = 185, loss = 4.109712600708008\n",
      "steps = 185, loss = 3.4252045154571533\n",
      "steps = 185, loss = 2.040299892425537\n",
      "steps = 185, loss = 3.5422959327697754\n",
      "steps = 185, loss = 6.670559406280518\n",
      "steps = 185, loss = 19.4143009185791\n",
      "steps = 185, loss = 49.97916793823242\n",
      "steps = 185, loss = 2.63788104057312\n",
      "steps = 185, loss = 4.7382378578186035\n",
      "steps = 185, loss = 2.9552295207977295\n",
      "steps = 185, loss = 3.418001413345337\n",
      "steps = 185, loss = 3.477520704269409\n",
      "steps = 185, loss = 2.9950528144836426\n",
      "steps = 186, loss = 2.6539745330810547\n",
      "steps = 186, loss = 49.985382080078125\n",
      "steps = 186, loss = 2.7707619667053223\n",
      "steps = 186, loss = 2.7357027530670166\n",
      "steps = 186, loss = 3.023228645324707\n",
      "steps = 186, loss = 3.3904757499694824\n",
      "steps = 186, loss = 2.987138509750366\n",
      "steps = 186, loss = 2.855388641357422\n",
      "steps = 186, loss = 49.98351287841797\n",
      "steps = 186, loss = 3.682387113571167\n",
      "steps = 186, loss = 2.1951239109039307\n",
      "steps = 186, loss = 4.568614959716797\n",
      "steps = 186, loss = 3.3646178245544434\n",
      "steps = 186, loss = 3.3988196849823\n",
      "steps = 186, loss = 3.891413927078247\n",
      "steps = 186, loss = 2.154419183731079\n",
      "steps = 186, loss = 2.959132194519043\n",
      "steps = 186, loss = 2.1417479515075684\n",
      "steps = 186, loss = 2.7246339321136475\n",
      "steps = 186, loss = 4.766288757324219\n",
      "steps = 186, loss = 3.597266674041748\n",
      "steps = 186, loss = 2.723452091217041\n",
      "steps = 186, loss = 7.157644748687744\n",
      "steps = 186, loss = 5.760258197784424\n",
      "steps = 186, loss = 4.550485134124756\n",
      "steps = 186, loss = 6.749951362609863\n",
      "steps = 186, loss = 19.43362045288086\n",
      "steps = 186, loss = 3.2120094299316406\n",
      "steps = 186, loss = 2.9454758167266846\n",
      "steps = 186, loss = 3.433039426803589\n",
      "steps = 186, loss = 3.6839284896850586\n",
      "steps = 186, loss = 2.548912286758423\n",
      "steps = 187, loss = 3.031327247619629\n",
      "steps = 187, loss = 3.2875776290893555\n",
      "steps = 187, loss = 3.0212440490722656\n",
      "steps = 187, loss = 2.161750555038452\n",
      "steps = 187, loss = 3.411355972290039\n",
      "steps = 187, loss = 2.8720970153808594\n",
      "steps = 187, loss = 2.1052422523498535\n",
      "steps = 187, loss = 2.8149664402008057\n",
      "steps = 187, loss = 2.96407151222229\n",
      "steps = 187, loss = 6.726577281951904\n",
      "steps = 187, loss = 5.489602088928223\n",
      "steps = 187, loss = 2.5804548263549805\n",
      "steps = 187, loss = 4.790236473083496\n",
      "steps = 187, loss = 3.9297492504119873\n",
      "steps = 187, loss = 3.4697837829589844\n",
      "steps = 187, loss = 4.5584635734558105\n",
      "steps = 187, loss = 3.657292604446411\n",
      "steps = 187, loss = 3.5203239917755127\n",
      "steps = 187, loss = 2.833226442337036\n",
      "steps = 187, loss = 2.9588098526000977\n",
      "steps = 187, loss = 49.97257614135742\n",
      "steps = 187, loss = 4.5786285400390625\n",
      "steps = 187, loss = 3.7626492977142334\n",
      "steps = 187, loss = 2.742501974105835\n",
      "steps = 187, loss = 2.2479217052459717\n",
      "steps = 187, loss = 49.985382080078125\n",
      "steps = 187, loss = 7.140434265136719\n",
      "steps = 187, loss = 2.7074129581451416\n",
      "steps = 187, loss = 2.774092674255371\n",
      "steps = 187, loss = 3.215334892272949\n",
      "steps = 187, loss = 3.6000120639801025\n",
      "steps = 187, loss = 19.762943267822266\n",
      "steps = 188, loss = 49.985382080078125\n",
      "steps = 188, loss = 2.0946671962738037\n",
      "steps = 188, loss = 2.8565261363983154\n",
      "steps = 188, loss = 3.4487645626068115\n",
      "steps = 188, loss = 2.756246328353882\n",
      "steps = 188, loss = 6.871869087219238\n",
      "steps = 188, loss = 2.582789421081543\n",
      "steps = 188, loss = 4.5587477684021\n",
      "steps = 188, loss = 3.2056093215942383\n",
      "steps = 188, loss = 49.9743766784668\n",
      "steps = 188, loss = 2.0927515029907227\n",
      "steps = 188, loss = 2.884207010269165\n",
      "steps = 188, loss = 2.727236032485962\n",
      "steps = 188, loss = 2.9546144008636475\n",
      "steps = 188, loss = 3.4287784099578857\n",
      "steps = 188, loss = 3.0103600025177\n",
      "steps = 188, loss = 3.41402006149292\n",
      "steps = 188, loss = 3.7912371158599854\n",
      "steps = 188, loss = 2.802060842514038\n",
      "steps = 188, loss = 5.53947639465332\n",
      "steps = 188, loss = 2.9872303009033203\n",
      "steps = 188, loss = 4.785974025726318\n",
      "steps = 188, loss = 3.3340258598327637\n",
      "steps = 188, loss = 19.590295791625977\n",
      "steps = 188, loss = 2.818546772003174\n",
      "steps = 188, loss = 3.0227599143981934\n",
      "steps = 188, loss = 4.5610737800598145\n",
      "steps = 188, loss = 2.2319540977478027\n",
      "steps = 188, loss = 3.3690054416656494\n",
      "steps = 188, loss = 3.3820347785949707\n",
      "steps = 188, loss = 7.192605972290039\n",
      "steps = 188, loss = 3.6962649822235107\n",
      "steps = 189, loss = 3.4282095432281494\n",
      "steps = 189, loss = 2.0732009410858154\n",
      "steps = 189, loss = 2.9184153079986572\n",
      "steps = 189, loss = 3.4837515354156494\n",
      "steps = 189, loss = 2.8500776290893555\n",
      "steps = 189, loss = 3.394390106201172\n",
      "steps = 189, loss = 49.83424758911133\n",
      "steps = 189, loss = 3.2106165885925293\n",
      "steps = 189, loss = 2.6734373569488525\n",
      "steps = 189, loss = 3.5206961631774902\n",
      "steps = 189, loss = 2.777460813522339\n",
      "steps = 189, loss = 3.0321030616760254\n",
      "steps = 189, loss = 49.985382080078125\n",
      "steps = 189, loss = 4.578028678894043\n",
      "steps = 189, loss = 2.0501768589019775\n",
      "steps = 189, loss = 5.559299945831299\n",
      "steps = 189, loss = 4.010096549987793\n",
      "steps = 189, loss = 6.564516067504883\n",
      "steps = 189, loss = 4.587456226348877\n",
      "steps = 189, loss = 3.669426918029785\n",
      "steps = 189, loss = 2.736842155456543\n",
      "steps = 189, loss = 3.404543161392212\n",
      "steps = 189, loss = 3.509575605392456\n",
      "steps = 189, loss = 7.703303337097168\n",
      "steps = 189, loss = 2.656454563140869\n",
      "steps = 189, loss = 19.300073623657227\n",
      "steps = 189, loss = 4.814314842224121\n",
      "steps = 189, loss = 2.1416406631469727\n",
      "steps = 189, loss = 2.9585213661193848\n",
      "steps = 189, loss = 2.7233920097351074\n",
      "steps = 189, loss = 2.8554701805114746\n",
      "steps = 189, loss = 2.654806137084961\n",
      "steps = 190, loss = 3.4318783283233643\n",
      "steps = 190, loss = 3.1259419918060303\n",
      "steps = 190, loss = 2.8257641792297363\n",
      "steps = 190, loss = 3.2138638496398926\n",
      "steps = 190, loss = 2.963454484939575\n",
      "steps = 190, loss = 6.971519470214844\n",
      "steps = 190, loss = 2.632624626159668\n",
      "steps = 190, loss = 4.5977959632873535\n",
      "steps = 190, loss = 2.872138738632202\n",
      "steps = 190, loss = 3.4192605018615723\n",
      "steps = 190, loss = 6.727133750915527\n",
      "steps = 190, loss = 49.97488784790039\n",
      "steps = 190, loss = 3.0402286052703857\n",
      "steps = 190, loss = 4.838575839996338\n",
      "steps = 190, loss = 2.742645263671875\n",
      "steps = 190, loss = 2.142899751663208\n",
      "steps = 190, loss = 5.579812049865723\n",
      "steps = 190, loss = 49.985382080078125\n",
      "steps = 190, loss = 2.7741429805755615\n",
      "steps = 190, loss = 3.4312164783477783\n",
      "steps = 190, loss = 2.5779929161071777\n",
      "steps = 190, loss = 2.1053757667541504\n",
      "steps = 190, loss = 3.3848626613616943\n",
      "steps = 190, loss = 2.904937267303467\n",
      "steps = 190, loss = 2.1589744091033936\n",
      "steps = 190, loss = 3.743000030517578\n",
      "steps = 190, loss = 3.23130464553833\n",
      "steps = 190, loss = 2.917287826538086\n",
      "steps = 190, loss = 19.464004516601562\n",
      "steps = 190, loss = 3.649123430252075\n",
      "steps = 190, loss = 4.586498737335205\n",
      "steps = 190, loss = 3.5923287868499756\n",
      "steps = 191, loss = 2.7425777912139893\n",
      "steps = 191, loss = 3.0307483673095703\n",
      "steps = 191, loss = 3.497743844985962\n",
      "steps = 191, loss = 8.014276504516602\n",
      "steps = 191, loss = 2.7874693870544434\n",
      "steps = 191, loss = 3.4373927116394043\n",
      "steps = 191, loss = 3.420896530151367\n",
      "steps = 191, loss = 19.477195739746094\n",
      "steps = 191, loss = 2.641073226928711\n",
      "steps = 191, loss = 3.3760759830474854\n",
      "steps = 191, loss = 2.803473949432373\n",
      "steps = 191, loss = 4.8524909019470215\n",
      "steps = 191, loss = 6.71141242980957\n",
      "steps = 191, loss = 3.376539468765259\n",
      "steps = 191, loss = 2.5813305377960205\n",
      "steps = 191, loss = 2.95959734916687\n",
      "steps = 191, loss = 48.862125396728516\n",
      "steps = 191, loss = 3.041081190109253\n",
      "steps = 191, loss = 5.600520133972168\n",
      "steps = 191, loss = 3.777531623840332\n",
      "steps = 191, loss = 4.604137420654297\n",
      "steps = 191, loss = 49.985382080078125\n",
      "steps = 191, loss = 2.1402997970581055\n",
      "steps = 191, loss = 4.600436210632324\n",
      "steps = 191, loss = 2.26591420173645\n",
      "steps = 191, loss = 3.340771198272705\n",
      "steps = 191, loss = 3.7453453540802\n",
      "steps = 191, loss = 2.7230491638183594\n",
      "steps = 191, loss = 2.5984418392181396\n",
      "steps = 191, loss = 3.2109861373901367\n",
      "steps = 191, loss = 2.8560876846313477\n",
      "steps = 191, loss = 2.0609443187713623\n",
      "steps = 192, loss = 4.583031177520752\n",
      "steps = 192, loss = 3.431950092315674\n",
      "steps = 192, loss = 3.0959787368774414\n",
      "steps = 192, loss = 5.538780689239502\n",
      "steps = 192, loss = 49.985382080078125\n",
      "steps = 192, loss = 2.728888511657715\n",
      "steps = 192, loss = 2.7253034114837646\n",
      "steps = 192, loss = 4.595564842224121\n",
      "steps = 192, loss = 7.2339277267456055\n",
      "steps = 192, loss = 2.069586753845215\n",
      "steps = 192, loss = 6.805208206176758\n",
      "steps = 192, loss = 3.314908742904663\n",
      "steps = 192, loss = 3.20295786857605\n",
      "steps = 192, loss = 3.92149019241333\n",
      "steps = 192, loss = 2.4674928188323975\n",
      "steps = 192, loss = 3.034808874130249\n",
      "steps = 192, loss = 3.3962104320526123\n",
      "steps = 192, loss = 2.093609094619751\n",
      "steps = 192, loss = 3.3993723392486572\n",
      "steps = 192, loss = 3.3995630741119385\n",
      "steps = 192, loss = 2.1203153133392334\n",
      "steps = 192, loss = 3.129664421081543\n",
      "steps = 192, loss = 19.175127029418945\n",
      "steps = 192, loss = 3.293914556503296\n",
      "steps = 192, loss = 49.960201263427734\n",
      "steps = 192, loss = 4.850401401519775\n",
      "steps = 192, loss = 2.6520233154296875\n",
      "steps = 192, loss = 3.702484607696533\n",
      "steps = 192, loss = 2.7521042823791504\n",
      "steps = 192, loss = 2.9529600143432617\n",
      "steps = 192, loss = 2.547290563583374\n",
      "steps = 192, loss = 2.8548433780670166\n",
      "steps = 193, loss = 2.692319869995117\n",
      "steps = 193, loss = 2.7573888301849365\n",
      "steps = 193, loss = 49.961204528808594\n",
      "steps = 193, loss = 2.947619676589966\n",
      "steps = 193, loss = 3.2085862159729004\n",
      "steps = 193, loss = 2.8583364486694336\n",
      "steps = 193, loss = 3.5225930213928223\n",
      "steps = 193, loss = 3.4227380752563477\n",
      "steps = 193, loss = 4.879611492156982\n",
      "steps = 193, loss = 3.544603109359741\n",
      "steps = 193, loss = 2.1416897773742676\n",
      "steps = 193, loss = 3.7966508865356445\n",
      "steps = 193, loss = 3.527233123779297\n",
      "steps = 193, loss = 2.737379789352417\n",
      "steps = 193, loss = 2.20259428024292\n",
      "steps = 193, loss = 4.611194610595703\n",
      "steps = 193, loss = 4.614618301391602\n",
      "steps = 193, loss = 19.493621826171875\n",
      "steps = 193, loss = 2.610189199447632\n",
      "steps = 193, loss = 7.4963555335998535\n",
      "steps = 193, loss = 2.723233699798584\n",
      "steps = 193, loss = 2.0658702850341797\n",
      "steps = 193, loss = 2.85538911819458\n",
      "steps = 193, loss = 3.3572998046875\n",
      "steps = 193, loss = 2.9572649002075195\n",
      "steps = 193, loss = 3.043999195098877\n",
      "steps = 193, loss = 49.985382080078125\n",
      "steps = 193, loss = 6.698705673217773\n",
      "steps = 193, loss = 3.4023032188415527\n",
      "steps = 193, loss = 5.628204822540283\n",
      "steps = 193, loss = 2.7429778575897217\n",
      "steps = 193, loss = 3.311211347579956\n",
      "steps = 194, loss = 2.774075746536255\n",
      "steps = 194, loss = 6.871213912963867\n",
      "steps = 194, loss = 2.6592776775360107\n",
      "steps = 194, loss = 3.3552863597869873\n",
      "steps = 194, loss = 2.8396291732788086\n",
      "steps = 194, loss = 3.0524773597717285\n",
      "steps = 194, loss = 3.7338573932647705\n",
      "steps = 194, loss = 3.3780293464660645\n",
      "steps = 194, loss = 3.5054900646209717\n",
      "steps = 194, loss = 4.903088569641113\n",
      "steps = 194, loss = 7.228635311126709\n",
      "steps = 194, loss = 49.94210433959961\n",
      "steps = 194, loss = 5.559666156768799\n",
      "steps = 194, loss = 2.702765941619873\n",
      "steps = 194, loss = 3.457653760910034\n",
      "steps = 194, loss = 4.620925426483154\n",
      "steps = 194, loss = 2.1058349609375\n",
      "steps = 194, loss = 3.597839832305908\n",
      "steps = 194, loss = 2.8713772296905518\n",
      "steps = 194, loss = 3.524010419845581\n",
      "steps = 194, loss = 2.7428030967712402\n",
      "steps = 194, loss = 2.920168161392212\n",
      "steps = 194, loss = 4.623042106628418\n",
      "steps = 194, loss = 2.742368459701538\n",
      "steps = 194, loss = 49.960487365722656\n",
      "steps = 194, loss = 3.734726667404175\n",
      "steps = 194, loss = 3.2116191387176514\n",
      "steps = 194, loss = 49.985382080078125\n",
      "steps = 194, loss = 2.962172508239746\n",
      "steps = 194, loss = 19.248943328857422\n",
      "steps = 194, loss = 1.9816737174987793\n",
      "steps = 194, loss = 2.8683156967163086\n",
      "steps = 195, loss = 5.584286689758301\n",
      "steps = 195, loss = 3.0435869693756104\n",
      "steps = 195, loss = 7.931371212005615\n",
      "steps = 195, loss = 3.4827797412872314\n",
      "steps = 195, loss = 19.15452766418457\n",
      "steps = 195, loss = 4.899506092071533\n",
      "steps = 195, loss = 49.985382080078125\n",
      "steps = 195, loss = 3.4004170894622803\n",
      "steps = 195, loss = 2.726818323135376\n",
      "steps = 195, loss = 3.326709032058716\n",
      "steps = 195, loss = 2.8354532718658447\n",
      "steps = 195, loss = 2.9145588874816895\n",
      "steps = 195, loss = 2.8563406467437744\n",
      "steps = 195, loss = 2.017237424850464\n",
      "steps = 195, loss = 2.5456066131591797\n",
      "steps = 195, loss = 4.622802257537842\n",
      "steps = 195, loss = 3.027329683303833\n",
      "steps = 195, loss = 4.600460052490234\n",
      "steps = 195, loss = 3.3849687576293945\n",
      "steps = 195, loss = 2.5804684162139893\n",
      "steps = 195, loss = 2.092621088027954\n",
      "steps = 195, loss = 2.7544212341308594\n",
      "steps = 195, loss = 2.057440996170044\n",
      "steps = 195, loss = 3.418261766433716\n",
      "steps = 195, loss = 2.7562308311462402\n",
      "steps = 195, loss = 3.41542387008667\n",
      "steps = 195, loss = 6.594998836517334\n",
      "steps = 195, loss = 49.96663284301758\n",
      "steps = 195, loss = 2.9528934955596924\n",
      "steps = 195, loss = 2.759244441986084\n",
      "steps = 195, loss = 3.202151298522949\n",
      "steps = 195, loss = 3.3928515911102295\n",
      "steps = 196, loss = 4.628713130950928\n",
      "steps = 196, loss = 2.8580758571624756\n",
      "steps = 196, loss = 3.471604347229004\n",
      "steps = 196, loss = 4.911370754241943\n",
      "steps = 196, loss = 3.718528985977173\n",
      "steps = 196, loss = 3.2011916637420654\n",
      "steps = 196, loss = 2.0357558727264404\n",
      "steps = 196, loss = 2.836620807647705\n",
      "steps = 196, loss = 2.7594220638275146\n",
      "steps = 196, loss = 2.7542829513549805\n",
      "steps = 196, loss = 19.388038635253906\n",
      "steps = 196, loss = 49.985382080078125\n",
      "steps = 196, loss = 2.553976058959961\n",
      "steps = 196, loss = 3.2663142681121826\n",
      "steps = 196, loss = 2.725057363510132\n",
      "steps = 196, loss = 2.784911870956421\n",
      "steps = 196, loss = 6.7560811042785645\n",
      "steps = 196, loss = 5.5552263259887695\n",
      "steps = 196, loss = 3.0436818599700928\n",
      "steps = 196, loss = 2.0824880599975586\n",
      "steps = 196, loss = 2.416377544403076\n",
      "steps = 196, loss = 3.2637453079223633\n",
      "steps = 196, loss = 49.96388626098633\n",
      "steps = 196, loss = 2.951854944229126\n",
      "steps = 196, loss = 4.603492736816406\n",
      "steps = 196, loss = 2.7279279232025146\n",
      "steps = 196, loss = 2.09371280670166\n",
      "steps = 196, loss = 7.247350215911865\n",
      "steps = 196, loss = 3.4314515590667725\n",
      "steps = 196, loss = 3.593296527862549\n",
      "steps = 196, loss = 2.6425271034240723\n",
      "steps = 196, loss = 3.4246509075164795\n",
      "steps = 197, loss = 3.2103662490844727\n",
      "steps = 197, loss = 4.636197566986084\n",
      "steps = 197, loss = 6.739261150360107\n",
      "steps = 197, loss = 3.456014394760132\n",
      "steps = 197, loss = 2.1051173210144043\n",
      "steps = 197, loss = 2.6559410095214844\n",
      "steps = 197, loss = 3.114661455154419\n",
      "steps = 197, loss = 3.6425530910491943\n",
      "steps = 197, loss = 3.5452935695648193\n",
      "steps = 197, loss = 2.7758727073669434\n",
      "steps = 197, loss = 3.0655148029327393\n",
      "steps = 197, loss = 19.342979431152344\n",
      "steps = 197, loss = 49.89059066772461\n",
      "steps = 197, loss = 4.645907878875732\n",
      "steps = 197, loss = 3.4722347259521484\n",
      "steps = 197, loss = 5.631259441375732\n",
      "steps = 197, loss = 2.744680404663086\n",
      "steps = 197, loss = 2.125321626663208\n",
      "steps = 197, loss = 3.0579919815063477\n",
      "steps = 197, loss = 2.121382474899292\n",
      "steps = 197, loss = 2.9386069774627686\n",
      "steps = 197, loss = 3.4633989334106445\n",
      "steps = 197, loss = 3.5638909339904785\n",
      "steps = 197, loss = 49.985382080078125\n",
      "steps = 197, loss = 2.961353063583374\n",
      "steps = 197, loss = 2.612170696258545\n",
      "steps = 197, loss = 6.855672359466553\n",
      "steps = 197, loss = 2.87369966506958\n",
      "steps = 197, loss = 2.9481406211853027\n",
      "steps = 197, loss = 2.625633478164673\n",
      "steps = 197, loss = 4.946260452270508\n",
      "steps = 197, loss = 3.4004859924316406\n",
      "steps = 198, loss = 19.612096786499023\n",
      "steps = 198, loss = 3.6766865253448486\n",
      "steps = 198, loss = 3.4503908157348633\n",
      "steps = 198, loss = 5.570133209228516\n",
      "steps = 198, loss = 4.962434768676758\n",
      "steps = 198, loss = 2.7367351055145264\n",
      "steps = 198, loss = 3.0599513053894043\n",
      "steps = 198, loss = 2.575895309448242\n",
      "steps = 198, loss = 2.252389669418335\n",
      "steps = 198, loss = 2.741058111190796\n",
      "steps = 198, loss = 8.487356185913086\n",
      "steps = 198, loss = 49.963932037353516\n",
      "steps = 198, loss = 2.1401047706604004\n",
      "steps = 198, loss = 2.7238993644714355\n",
      "steps = 198, loss = 3.0149946212768555\n",
      "steps = 198, loss = 2.0366668701171875\n",
      "steps = 198, loss = 4.642519950866699\n",
      "steps = 198, loss = 3.4668796062469482\n",
      "steps = 198, loss = 49.985382080078125\n",
      "steps = 198, loss = 3.207777261734009\n",
      "steps = 198, loss = 6.791932106018066\n",
      "steps = 198, loss = 2.9580581188201904\n",
      "steps = 198, loss = 2.8302266597747803\n",
      "steps = 198, loss = 3.3438992500305176\n",
      "steps = 198, loss = 3.6908609867095947\n",
      "steps = 198, loss = 2.764613151550293\n",
      "steps = 198, loss = 2.670198440551758\n",
      "steps = 198, loss = 3.5153098106384277\n",
      "steps = 198, loss = 3.4785284996032715\n",
      "steps = 198, loss = 3.413916826248169\n",
      "steps = 198, loss = 4.660920143127441\n",
      "steps = 198, loss = 2.8563389778137207\n",
      "steps = 199, loss = 4.648103713989258\n",
      "steps = 199, loss = 4.670298099517822\n",
      "steps = 199, loss = 3.5364997386932373\n",
      "steps = 199, loss = 2.957421064376831\n",
      "steps = 199, loss = 3.5526983737945557\n",
      "steps = 199, loss = 2.9157872200012207\n",
      "steps = 199, loss = 6.814154624938965\n",
      "steps = 199, loss = 2.8557486534118652\n",
      "steps = 199, loss = 2.8646252155303955\n",
      "steps = 199, loss = 2.142610549926758\n",
      "steps = 199, loss = 49.959007263183594\n",
      "steps = 199, loss = 2.7227251529693604\n",
      "steps = 199, loss = 3.4470088481903076\n",
      "steps = 199, loss = 5.6624298095703125\n",
      "steps = 199, loss = 3.503880739212036\n",
      "steps = 199, loss = 2.0911099910736084\n",
      "steps = 199, loss = 2.68538498878479\n",
      "steps = 199, loss = 49.985382080078125\n",
      "steps = 199, loss = 19.915870666503906\n",
      "steps = 199, loss = 2.6222987174987793\n",
      "steps = 199, loss = 4.980209827423096\n",
      "steps = 199, loss = 3.005476713180542\n",
      "steps = 199, loss = 3.063861131668091\n",
      "steps = 199, loss = 2.8032939434051514\n",
      "steps = 199, loss = 2.746823787689209\n",
      "steps = 199, loss = 3.206988573074341\n",
      "steps = 199, loss = 2.3240740299224854\n",
      "steps = 199, loss = 3.513531446456909\n",
      "steps = 199, loss = 3.4548144340515137\n",
      "steps = 199, loss = 3.32861590385437\n",
      "steps = 199, loss = 8.135098457336426\n",
      "steps = 199, loss = 3.4033520221710205\n",
      "steps = 200, loss = 2.6891465187072754\n",
      "steps = 200, loss = 2.7422988414764404\n",
      "steps = 200, loss = 49.985382080078125\n",
      "steps = 200, loss = 19.594419479370117\n",
      "steps = 200, loss = 3.451159715652466\n",
      "steps = 200, loss = 2.7744712829589844\n",
      "steps = 200, loss = 3.53037166595459\n",
      "steps = 200, loss = 2.961698055267334\n",
      "steps = 200, loss = 4.676443576812744\n",
      "steps = 200, loss = 3.663933753967285\n",
      "steps = 200, loss = 3.172332763671875\n",
      "steps = 200, loss = 2.8718087673187256\n",
      "steps = 200, loss = 49.95890808105469\n",
      "steps = 200, loss = 4.657039165496826\n",
      "steps = 200, loss = 5.001543998718262\n",
      "steps = 200, loss = 2.0446834564208984\n",
      "steps = 200, loss = 2.1063950061798096\n",
      "steps = 200, loss = 3.3734545707702637\n",
      "steps = 200, loss = 3.6428797245025635\n",
      "steps = 200, loss = 2.2905023097991943\n",
      "steps = 200, loss = 3.000662088394165\n",
      "steps = 200, loss = 2.570701837539673\n",
      "steps = 200, loss = 2.973604917526245\n",
      "steps = 200, loss = 6.807355880737305\n",
      "steps = 200, loss = 3.7025108337402344\n",
      "steps = 200, loss = 2.8332464694976807\n",
      "steps = 200, loss = 2.771144151687622\n",
      "steps = 200, loss = 3.2097113132476807\n",
      "steps = 200, loss = 3.071011543273926\n",
      "steps = 200, loss = 6.74507999420166\n",
      "steps = 200, loss = 3.4435341358184814\n",
      "steps = 200, loss = 5.544910907745361\n",
      "steps = 201, loss = 4.996453762054443\n",
      "steps = 201, loss = 3.441129684448242\n",
      "steps = 201, loss = 2.952179431915283\n",
      "steps = 201, loss = 2.550773859024048\n",
      "steps = 201, loss = 6.957350254058838\n",
      "steps = 201, loss = 3.4872779846191406\n",
      "steps = 201, loss = 4.633152961730957\n",
      "steps = 201, loss = 3.0395941734313965\n",
      "steps = 201, loss = 3.0682249069213867\n",
      "steps = 201, loss = 5.57136869430542\n",
      "steps = 201, loss = 2.8311898708343506\n",
      "steps = 201, loss = 2.092726469039917\n",
      "steps = 201, loss = 2.1590731143951416\n",
      "steps = 201, loss = 3.200007677078247\n",
      "steps = 201, loss = 4.675595760345459\n",
      "steps = 201, loss = 2.979869842529297\n",
      "steps = 201, loss = 19.93486785888672\n",
      "steps = 201, loss = 3.061937093734741\n",
      "steps = 201, loss = 2.7267072200775146\n",
      "steps = 201, loss = 7.351661205291748\n",
      "steps = 201, loss = 3.6494603157043457\n",
      "steps = 201, loss = 3.2580504417419434\n",
      "steps = 201, loss = 2.0144567489624023\n",
      "steps = 201, loss = 2.7561898231506348\n",
      "steps = 201, loss = 2.856490135192871\n",
      "steps = 201, loss = 3.4507224559783936\n",
      "steps = 201, loss = 3.466318130493164\n",
      "steps = 201, loss = 3.4889471530914307\n",
      "steps = 201, loss = 2.5844969749450684\n",
      "steps = 201, loss = 49.95591354370117\n",
      "steps = 201, loss = 49.985382080078125\n",
      "steps = 201, loss = 3.385470390319824\n",
      "steps = 202, loss = 3.499316453933716\n",
      "steps = 202, loss = 3.4748177528381348\n",
      "steps = 202, loss = 2.1066396236419678\n",
      "steps = 202, loss = 2.5453524589538574\n",
      "steps = 202, loss = 2.0997400283813477\n",
      "steps = 202, loss = 3.4938302040100098\n",
      "steps = 202, loss = 19.774383544921875\n",
      "steps = 202, loss = 3.0747199058532715\n",
      "steps = 202, loss = 4.6671528816223145\n",
      "steps = 202, loss = 2.7445569038391113\n",
      "steps = 202, loss = 7.251788139343262\n",
      "steps = 202, loss = 3.2090351581573486\n",
      "steps = 202, loss = 5.598847389221191\n",
      "steps = 202, loss = 2.776832342147827\n",
      "steps = 202, loss = 2.040483236312866\n",
      "steps = 202, loss = 3.6773462295532227\n",
      "steps = 202, loss = 2.874382734298706\n",
      "steps = 202, loss = 2.8375399112701416\n",
      "steps = 202, loss = 49.96379470825195\n",
      "steps = 202, loss = 2.7079975605010986\n",
      "steps = 202, loss = 3.0021862983703613\n",
      "steps = 202, loss = 2.7767157554626465\n",
      "steps = 202, loss = 4.690598487854004\n",
      "steps = 202, loss = 3.3100805282592773\n",
      "steps = 202, loss = 3.6504082679748535\n",
      "steps = 202, loss = 3.510505437850952\n",
      "steps = 202, loss = 2.923307418823242\n",
      "steps = 202, loss = 6.725400924682617\n",
      "steps = 202, loss = 49.985382080078125\n",
      "steps = 202, loss = 3.1733481884002686\n",
      "steps = 202, loss = 5.029903411865234\n",
      "steps = 202, loss = 2.9611780643463135\n",
      "steps = 203, loss = 2.8778398036956787\n",
      "steps = 203, loss = 3.629732847213745\n",
      "steps = 203, loss = 6.810629367828369\n",
      "steps = 203, loss = 3.4303781986236572\n",
      "steps = 203, loss = 2.0926566123962402\n",
      "steps = 203, loss = 3.6194145679473877\n",
      "steps = 203, loss = 2.6358304023742676\n",
      "steps = 203, loss = 2.7594454288482666\n",
      "steps = 203, loss = 3.3702502250671387\n",
      "steps = 203, loss = 49.985382080078125\n",
      "steps = 203, loss = 5.570026874542236\n",
      "steps = 203, loss = 3.5569844245910645\n",
      "steps = 203, loss = 1.9193423986434937\n",
      "steps = 203, loss = 5.027089595794678\n",
      "steps = 203, loss = 2.7410812377929688\n",
      "steps = 203, loss = 3.5012900829315186\n",
      "steps = 203, loss = 49.94295120239258\n",
      "steps = 203, loss = 20.115583419799805\n",
      "steps = 203, loss = 2.623023509979248\n",
      "steps = 203, loss = 7.3982133865356445\n",
      "steps = 203, loss = 49.89310073852539\n",
      "steps = 203, loss = 3.4551620483398438\n",
      "steps = 203, loss = 3.5469107627868652\n",
      "steps = 203, loss = 3.00270938873291\n",
      "steps = 203, loss = 2.7283167839050293\n",
      "steps = 203, loss = 3.1998884677886963\n",
      "steps = 203, loss = 2.8582348823547363\n",
      "steps = 203, loss = 4.691679954528809\n",
      "steps = 203, loss = 4.643676280975342\n",
      "steps = 203, loss = 2.9522950649261475\n",
      "steps = 203, loss = 3.0667221546173096\n",
      "steps = 203, loss = 2.5863702297210693\n",
      "steps = 204, loss = 3.4464430809020996\n",
      "steps = 204, loss = 2.8560571670532227\n",
      "steps = 204, loss = 2.82481050491333\n",
      "steps = 204, loss = 5.0576043128967285\n",
      "steps = 204, loss = 2.7990543842315674\n",
      "steps = 204, loss = 2.5382556915283203\n",
      "steps = 204, loss = 4.673393249511719\n",
      "steps = 204, loss = 5.633762836456299\n",
      "steps = 204, loss = 3.697486639022827\n",
      "steps = 204, loss = 2.733628511428833\n",
      "steps = 204, loss = 3.4660160541534424\n",
      "steps = 204, loss = 2.707305669784546\n",
      "steps = 204, loss = 3.0759596824645996\n",
      "steps = 204, loss = 3.3966832160949707\n",
      "steps = 204, loss = 1.9986988306045532\n",
      "steps = 204, loss = 3.372574806213379\n",
      "steps = 204, loss = 19.797603607177734\n",
      "steps = 204, loss = 3.7450814247131348\n",
      "steps = 204, loss = 2.9561007022857666\n",
      "steps = 204, loss = 2.1154160499572754\n",
      "steps = 204, loss = 4.7098069190979\n",
      "steps = 204, loss = 2.7243380546569824\n",
      "steps = 204, loss = 6.812307834625244\n",
      "steps = 204, loss = 3.2047624588012695\n",
      "steps = 204, loss = 2.824301242828369\n",
      "steps = 204, loss = 8.134780883789062\n",
      "steps = 204, loss = 3.4118897914886475\n",
      "steps = 204, loss = 3.5086207389831543\n",
      "steps = 204, loss = 49.73408126831055\n",
      "steps = 204, loss = 49.985382080078125\n",
      "steps = 204, loss = 2.7361834049224854\n",
      "steps = 204, loss = 2.1415250301361084\n",
      "steps = 205, loss = 2.8557322025299072\n",
      "steps = 205, loss = 5.541853427886963\n",
      "steps = 205, loss = 2.723052501678467\n",
      "steps = 205, loss = 2.956110715866089\n",
      "steps = 205, loss = 3.4082629680633545\n",
      "steps = 205, loss = 3.466763496398926\n",
      "steps = 205, loss = 2.1317319869995117\n",
      "steps = 205, loss = 2.6041812896728516\n",
      "steps = 205, loss = 49.985382080078125\n",
      "steps = 205, loss = 3.4983937740325928\n",
      "steps = 205, loss = 49.950775146484375\n",
      "steps = 205, loss = 2.0458312034606934\n",
      "steps = 205, loss = 2.747208833694458\n",
      "steps = 205, loss = 2.7379040718078613\n",
      "steps = 205, loss = 2.729508638381958\n",
      "steps = 205, loss = 3.3564422130584717\n",
      "steps = 205, loss = 3.5097999572753906\n",
      "steps = 205, loss = 2.900160789489746\n",
      "steps = 205, loss = 5.076428413391113\n",
      "steps = 205, loss = 2.142446756362915\n",
      "steps = 205, loss = 3.2045581340789795\n",
      "steps = 205, loss = 3.080507278442383\n",
      "steps = 205, loss = 6.912351131439209\n",
      "steps = 205, loss = 2.7230327129364014\n",
      "steps = 205, loss = 4.7205610275268555\n",
      "steps = 205, loss = 4.679976463317871\n",
      "steps = 205, loss = 2.7542684078216553\n",
      "steps = 205, loss = 3.4752414226531982\n",
      "steps = 205, loss = 7.844902992248535\n",
      "steps = 205, loss = 3.0281176567077637\n",
      "steps = 205, loss = 19.814464569091797\n",
      "steps = 205, loss = 3.467005968093872\n",
      "steps = 206, loss = 19.837543487548828\n",
      "steps = 206, loss = 3.4442384243011475\n",
      "steps = 206, loss = 49.985382080078125\n",
      "steps = 206, loss = 3.438795804977417\n",
      "steps = 206, loss = 2.663376808166504\n",
      "steps = 206, loss = 6.649622917175293\n",
      "steps = 206, loss = 3.396070718765259\n",
      "steps = 206, loss = 2.8557043075561523\n",
      "steps = 206, loss = 3.3810956478118896\n",
      "steps = 206, loss = 2.7543370723724365\n",
      "steps = 206, loss = 49.95099639892578\n",
      "steps = 206, loss = 4.655123233795166\n",
      "steps = 206, loss = 2.0311315059661865\n",
      "steps = 206, loss = 3.695796012878418\n",
      "steps = 206, loss = 2.9501538276672363\n",
      "steps = 206, loss = 6.925787925720215\n",
      "steps = 206, loss = 2.8034605979919434\n",
      "steps = 206, loss = 2.189181089401245\n",
      "steps = 206, loss = 3.0939056873321533\n",
      "steps = 206, loss = 2.465235948562622\n",
      "steps = 206, loss = 2.7240333557128906\n",
      "steps = 206, loss = 3.0748469829559326\n",
      "steps = 206, loss = 3.5219874382019043\n",
      "steps = 206, loss = 5.073852062225342\n",
      "steps = 206, loss = 3.1969428062438965\n",
      "steps = 206, loss = 5.624236106872559\n",
      "steps = 206, loss = 3.1408259868621826\n",
      "steps = 206, loss = 3.2647812366485596\n",
      "steps = 206, loss = 2.7254953384399414\n",
      "steps = 206, loss = 2.094064474105835\n",
      "steps = 206, loss = 4.716404914855957\n",
      "steps = 206, loss = 2.61421275138855\n",
      "steps = 207, loss = 3.4338459968566895\n",
      "steps = 207, loss = 3.087655544281006\n",
      "steps = 207, loss = 2.1792593002319336\n",
      "steps = 207, loss = 2.903564929962158\n",
      "steps = 207, loss = 3.205998659133911\n",
      "steps = 207, loss = 2.7439608573913574\n",
      "steps = 207, loss = 7.342108726501465\n",
      "steps = 207, loss = 2.8725104331970215\n",
      "steps = 207, loss = 3.4977915287017822\n",
      "steps = 207, loss = 2.0747761726379395\n",
      "steps = 207, loss = 2.5913033485412598\n",
      "steps = 207, loss = 3.5619006156921387\n",
      "steps = 207, loss = 2.775524616241455\n",
      "steps = 207, loss = 3.6515772342681885\n",
      "steps = 207, loss = 5.575807571411133\n",
      "steps = 207, loss = 2.6724798679351807\n",
      "steps = 207, loss = 4.9791717529296875\n",
      "steps = 207, loss = 6.833079814910889\n",
      "steps = 207, loss = 49.985382080078125\n",
      "steps = 207, loss = 3.4554085731506348\n",
      "steps = 207, loss = 3.53873872756958\n",
      "steps = 207, loss = 3.1422388553619385\n",
      "steps = 207, loss = 5.108314514160156\n",
      "steps = 207, loss = 2.741008996963501\n",
      "steps = 207, loss = 19.502931594848633\n",
      "steps = 207, loss = 3.498323678970337\n",
      "steps = 207, loss = 2.9597039222717285\n",
      "steps = 207, loss = 2.1063592433929443\n",
      "steps = 207, loss = 4.731524467468262\n",
      "steps = 207, loss = 4.690199851989746\n",
      "steps = 207, loss = 2.9594006538391113\n",
      "steps = 207, loss = 49.95559310913086\n",
      "steps = 208, loss = 3.677323818206787\n",
      "steps = 208, loss = 3.4777214527130127\n",
      "steps = 208, loss = 2.87604022026062\n",
      "steps = 208, loss = 2.108198404312134\n",
      "steps = 208, loss = 3.503331422805786\n",
      "steps = 208, loss = 3.6455767154693604\n",
      "steps = 208, loss = 2.1264827251434326\n",
      "steps = 208, loss = 2.692321538925171\n",
      "steps = 208, loss = 2.792280912399292\n",
      "steps = 208, loss = 2.2772650718688965\n",
      "steps = 208, loss = 2.075164556503296\n",
      "steps = 208, loss = 3.5217056274414062\n",
      "steps = 208, loss = 3.7168586254119873\n",
      "steps = 208, loss = 2.94232439994812\n",
      "steps = 208, loss = 3.207789897918701\n",
      "steps = 208, loss = 5.630908012390137\n",
      "steps = 208, loss = 4.743107318878174\n",
      "steps = 208, loss = 5.130140781402588\n",
      "steps = 208, loss = 7.132824897766113\n",
      "steps = 208, loss = 3.3439104557037354\n",
      "steps = 208, loss = 3.093153238296509\n",
      "steps = 208, loss = 49.96375274658203\n",
      "steps = 208, loss = 2.631331205368042\n",
      "steps = 208, loss = 4.701563358306885\n",
      "steps = 208, loss = 19.741029739379883\n",
      "steps = 208, loss = 49.985382080078125\n",
      "steps = 208, loss = 2.961385726928711\n",
      "steps = 208, loss = 2.745328903198242\n",
      "steps = 208, loss = 2.7386043071746826\n",
      "steps = 208, loss = 2.9370148181915283\n",
      "steps = 208, loss = 2.77854323387146\n",
      "steps = 208, loss = 6.811844825744629\n",
      "steps = 209, loss = 2.6658976078033447\n",
      "steps = 209, loss = 5.53720760345459\n",
      "steps = 209, loss = 3.4804227352142334\n",
      "steps = 209, loss = 3.084182024002075\n",
      "steps = 209, loss = 4.674667835235596\n",
      "steps = 209, loss = 6.897334575653076\n",
      "steps = 209, loss = 2.068924903869629\n",
      "steps = 209, loss = 4.743460178375244\n",
      "steps = 209, loss = 3.47655987739563\n",
      "steps = 209, loss = 2.7599990367889404\n",
      "steps = 209, loss = 2.858548879623413\n",
      "steps = 209, loss = 49.985382080078125\n",
      "steps = 209, loss = 3.396092176437378\n",
      "steps = 209, loss = 7.46363639831543\n",
      "steps = 209, loss = 2.092315912246704\n",
      "steps = 209, loss = 2.7281947135925293\n",
      "steps = 209, loss = 2.6592752933502197\n",
      "steps = 209, loss = 2.9518356323242188\n",
      "steps = 209, loss = 3.4855823516845703\n",
      "steps = 209, loss = 2.6093852519989014\n",
      "steps = 209, loss = 3.1979663372039795\n",
      "steps = 209, loss = 2.9951817989349365\n",
      "steps = 209, loss = 3.2918272018432617\n",
      "steps = 209, loss = 3.1054961681365967\n",
      "steps = 209, loss = 19.588775634765625\n",
      "steps = 209, loss = 2.257267951965332\n",
      "steps = 209, loss = 5.125558853149414\n",
      "steps = 209, loss = 3.3789708614349365\n",
      "steps = 209, loss = 49.973384857177734\n",
      "steps = 209, loss = 2.365636110305786\n",
      "steps = 209, loss = 2.7715296745300293\n",
      "steps = 209, loss = 3.411377191543579\n",
      "steps = 210, loss = 5.153805255889893\n",
      "steps = 210, loss = 3.72782301902771\n",
      "steps = 210, loss = 2.8818209171295166\n",
      "steps = 210, loss = 2.866591453552246\n",
      "steps = 210, loss = 2.473884344100952\n",
      "steps = 210, loss = 3.202693462371826\n",
      "steps = 210, loss = 2.7375473976135254\n",
      "steps = 210, loss = 3.093045234680176\n",
      "steps = 210, loss = 3.506657838821411\n",
      "steps = 210, loss = 2.6597654819488525\n",
      "steps = 210, loss = 2.8562891483306885\n",
      "steps = 210, loss = 2.7244012355804443\n",
      "steps = 210, loss = 7.925617218017578\n",
      "steps = 210, loss = 5.637558937072754\n",
      "steps = 210, loss = 2.1415250301361084\n",
      "steps = 210, loss = 6.73970890045166\n",
      "steps = 210, loss = 49.985382080078125\n",
      "steps = 210, loss = 4.760942459106445\n",
      "steps = 210, loss = 3.510791540145874\n",
      "steps = 210, loss = 2.715579032897949\n",
      "steps = 210, loss = 2.0508813858032227\n",
      "steps = 210, loss = 3.4188358783721924\n",
      "steps = 210, loss = 2.7412705421447754\n",
      "steps = 210, loss = 4.705006122589111\n",
      "steps = 210, loss = 3.434577226638794\n",
      "steps = 210, loss = 3.1567070484161377\n",
      "steps = 210, loss = 2.955421209335327\n",
      "steps = 210, loss = 49.951637268066406\n",
      "steps = 210, loss = 19.89568519592285\n",
      "steps = 210, loss = 3.480592727661133\n",
      "steps = 210, loss = 2.987858772277832\n",
      "steps = 210, loss = 2.1256680488586426\n",
      "steps = 211, loss = 3.4113848209381104\n",
      "steps = 211, loss = 2.0779080390930176\n",
      "steps = 211, loss = 49.92959976196289\n",
      "steps = 211, loss = 3.5075926780700684\n",
      "steps = 211, loss = 7.02581787109375\n",
      "steps = 211, loss = 5.153569221496582\n",
      "steps = 211, loss = 19.461687088012695\n",
      "steps = 211, loss = 4.679783821105957\n",
      "steps = 211, loss = 5.503411293029785\n",
      "steps = 211, loss = 3.503873109817505\n",
      "steps = 211, loss = 2.192917585372925\n",
      "steps = 211, loss = 3.350543975830078\n",
      "steps = 211, loss = 2.8571979999542236\n",
      "steps = 211, loss = 6.862257957458496\n",
      "steps = 211, loss = 2.756815195083618\n",
      "steps = 211, loss = 4.758081912994385\n",
      "steps = 211, loss = 3.2243492603302\n",
      "steps = 211, loss = 3.032581329345703\n",
      "steps = 211, loss = 2.3922057151794434\n",
      "steps = 211, loss = 2.7902591228485107\n",
      "steps = 211, loss = 49.985382080078125\n",
      "steps = 211, loss = 2.7267420291900635\n",
      "steps = 211, loss = 2.0938758850097656\n",
      "steps = 211, loss = 3.404057025909424\n",
      "steps = 211, loss = 3.494795083999634\n",
      "steps = 211, loss = 2.6402297019958496\n",
      "steps = 211, loss = 2.613111972808838\n",
      "steps = 211, loss = 3.0885863304138184\n",
      "steps = 211, loss = 2.9499452114105225\n",
      "steps = 211, loss = 3.1956233978271484\n",
      "steps = 211, loss = 2.4930150508880615\n",
      "steps = 211, loss = 2.7983899116516113\n",
      "steps = 212, loss = 3.0977814197540283\n",
      "steps = 212, loss = 7.929012775421143\n",
      "steps = 212, loss = 2.6112358570098877\n",
      "steps = 212, loss = 4.712143421173096\n",
      "steps = 212, loss = 3.5676441192626953\n",
      "steps = 212, loss = 2.346245765686035\n",
      "steps = 212, loss = 2.724034547805786\n",
      "steps = 212, loss = 2.8190276622772217\n",
      "steps = 212, loss = 49.985382080078125\n",
      "steps = 212, loss = 2.7085318565368652\n",
      "steps = 212, loss = 2.7825326919555664\n",
      "steps = 212, loss = 3.7296295166015625\n",
      "steps = 212, loss = 3.358492851257324\n",
      "steps = 212, loss = 4.775945663452148\n",
      "steps = 212, loss = 2.1416540145874023\n",
      "steps = 212, loss = 3.3775484561920166\n",
      "steps = 212, loss = 2.1897895336151123\n",
      "steps = 212, loss = 6.774899482727051\n",
      "steps = 212, loss = 19.829946517944336\n",
      "steps = 212, loss = 49.94367980957031\n",
      "steps = 212, loss = 3.4566431045532227\n",
      "steps = 212, loss = 2.9542603492736816\n",
      "steps = 212, loss = 5.558296203613281\n",
      "steps = 212, loss = 3.442997694015503\n",
      "steps = 212, loss = 2.86130952835083\n",
      "steps = 212, loss = 3.04911470413208\n",
      "steps = 212, loss = 5.183154106140137\n",
      "steps = 212, loss = 2.7432587146759033\n",
      "steps = 212, loss = 2.8559041023254395\n",
      "steps = 212, loss = 3.6122324466705322\n",
      "steps = 212, loss = 3.2011704444885254\n",
      "steps = 212, loss = 2.044905185699463\n",
      "steps = 213, loss = 3.5106799602508545\n",
      "steps = 213, loss = 3.593263626098633\n",
      "steps = 213, loss = 2.1056807041168213\n",
      "steps = 213, loss = 2.872079372406006\n",
      "steps = 213, loss = 2.639453172683716\n",
      "steps = 213, loss = 4.720963954925537\n",
      "steps = 213, loss = 2.7433974742889404\n",
      "steps = 213, loss = 4.782138824462891\n",
      "steps = 213, loss = 49.96860885620117\n",
      "steps = 213, loss = 3.6530838012695312\n",
      "steps = 213, loss = 3.5153157711029053\n",
      "steps = 213, loss = 3.1463208198547363\n",
      "steps = 213, loss = 2.2367894649505615\n",
      "steps = 213, loss = 2.9672927856445312\n",
      "steps = 213, loss = 49.985382080078125\n",
      "steps = 213, loss = 2.634077787399292\n",
      "steps = 213, loss = 6.863091945648193\n",
      "steps = 213, loss = 2.67521595954895\n",
      "steps = 213, loss = 2.8257856369018555\n",
      "steps = 213, loss = 3.20424222946167\n",
      "steps = 213, loss = 19.83780288696289\n",
      "steps = 213, loss = 5.5771260261535645\n",
      "steps = 213, loss = 3.4259443283081055\n",
      "steps = 213, loss = 2.7755753993988037\n",
      "steps = 213, loss = 2.695359706878662\n",
      "steps = 213, loss = 5.206427574157715\n",
      "steps = 213, loss = 2.0965497493743896\n",
      "steps = 213, loss = 2.9591879844665527\n",
      "steps = 213, loss = 3.1062302589416504\n",
      "steps = 213, loss = 3.5238540172576904\n",
      "steps = 213, loss = 3.383394956588745\n",
      "steps = 213, loss = 7.148379325866699\n",
      "steps = 214, loss = 2.108409881591797\n",
      "steps = 214, loss = 5.598087310791016\n",
      "steps = 214, loss = 6.857152462005615\n",
      "steps = 214, loss = 3.4568934440612793\n",
      "steps = 214, loss = 4.730777263641357\n",
      "steps = 214, loss = 49.961585998535156\n",
      "steps = 214, loss = 3.3869500160217285\n",
      "steps = 214, loss = 3.2057807445526123\n",
      "steps = 214, loss = 49.985382080078125\n",
      "steps = 214, loss = 2.7655699253082275\n",
      "steps = 214, loss = 3.4562978744506836\n",
      "steps = 214, loss = 2.7451274394989014\n",
      "steps = 214, loss = 2.740948438644409\n",
      "steps = 214, loss = 6.9612650871276855\n",
      "steps = 214, loss = 3.4499497413635254\n",
      "steps = 214, loss = 4.7930450439453125\n",
      "steps = 214, loss = 2.0985989570617676\n",
      "steps = 214, loss = 3.1108028888702393\n",
      "steps = 214, loss = 2.1125330924987793\n",
      "steps = 214, loss = 2.8795251846313477\n",
      "steps = 214, loss = 2.8760335445404053\n",
      "steps = 214, loss = 2.742957592010498\n",
      "steps = 214, loss = 2.601170539855957\n",
      "steps = 214, loss = 2.960763454437256\n",
      "steps = 214, loss = 2.778822898864746\n",
      "steps = 214, loss = 3.4642281532287598\n",
      "steps = 214, loss = 3.670166015625\n",
      "steps = 214, loss = 2.79305362701416\n",
      "steps = 214, loss = 3.498385190963745\n",
      "steps = 214, loss = 20.183563232421875\n",
      "steps = 214, loss = 2.938511848449707\n",
      "steps = 214, loss = 5.226159572601318\n",
      "steps = 215, loss = 2.5412991046905518\n",
      "steps = 215, loss = 3.6572024822235107\n",
      "steps = 215, loss = 2.8573238849639893\n",
      "steps = 215, loss = 2.058232545852661\n",
      "steps = 215, loss = 49.971275329589844\n",
      "steps = 215, loss = 6.938252925872803\n",
      "steps = 215, loss = 3.4741218090057373\n",
      "steps = 215, loss = 2.3551721572875977\n",
      "steps = 215, loss = 2.8168609142303467\n",
      "steps = 215, loss = 2.9564785957336426\n",
      "steps = 215, loss = 3.420477867126465\n",
      "steps = 215, loss = 3.0711097717285156\n",
      "steps = 215, loss = 3.198612689971924\n",
      "steps = 215, loss = 2.139828681945801\n",
      "steps = 215, loss = 7.19065523147583\n",
      "steps = 215, loss = 2.724189281463623\n",
      "steps = 215, loss = 19.98984146118164\n",
      "steps = 215, loss = 3.5085337162017822\n",
      "steps = 215, loss = 2.535703182220459\n",
      "steps = 215, loss = 4.733488082885742\n",
      "steps = 215, loss = 5.240488529205322\n",
      "steps = 215, loss = 3.5265626907348633\n",
      "steps = 215, loss = 2.8055331707000732\n",
      "steps = 215, loss = 5.591014862060547\n",
      "steps = 215, loss = 3.5072083473205566\n",
      "steps = 215, loss = 2.744196653366089\n",
      "steps = 215, loss = 49.985382080078125\n",
      "steps = 215, loss = 4.806553840637207\n",
      "steps = 215, loss = 3.1106979846954346\n",
      "steps = 215, loss = 3.371880292892456\n",
      "steps = 215, loss = 3.202420473098755\n",
      "steps = 215, loss = 3.0864686965942383\n",
      "steps = 216, loss = 2.685363531112671\n",
      "steps = 216, loss = 19.46817398071289\n",
      "steps = 216, loss = 3.3207597732543945\n",
      "steps = 216, loss = 2.7259347438812256\n",
      "steps = 216, loss = 2.2585835456848145\n",
      "steps = 216, loss = 3.434612512588501\n",
      "steps = 216, loss = 2.9498252868652344\n",
      "steps = 216, loss = 2.7548959255218506\n",
      "steps = 216, loss = 2.8564515113830566\n",
      "steps = 216, loss = 5.2357025146484375\n",
      "steps = 216, loss = 3.194371461868286\n",
      "steps = 216, loss = 49.93872833251953\n",
      "steps = 216, loss = 4.704550743103027\n",
      "steps = 216, loss = 2.0933380126953125\n",
      "steps = 216, loss = 3.714820146560669\n",
      "steps = 216, loss = 3.056245803833008\n",
      "steps = 216, loss = 3.104424238204956\n",
      "steps = 216, loss = 3.4269776344299316\n",
      "steps = 216, loss = 49.9659538269043\n",
      "steps = 216, loss = 3.6953089237213135\n",
      "steps = 216, loss = 3.5104551315307617\n",
      "steps = 216, loss = 49.985382080078125\n",
      "steps = 216, loss = 3.3270504474639893\n",
      "steps = 216, loss = 5.61247444152832\n",
      "steps = 216, loss = 7.567899227142334\n",
      "steps = 216, loss = 2.6425750255584717\n",
      "steps = 216, loss = 2.7585031986236572\n",
      "steps = 216, loss = 6.726579189300537\n",
      "steps = 216, loss = 2.7385668754577637\n",
      "steps = 216, loss = 2.5528907775878906\n",
      "steps = 216, loss = 4.800025463104248\n",
      "steps = 216, loss = 2.919495105743408\n",
      "steps = 217, loss = 2.1417853832244873\n",
      "steps = 217, loss = 3.1129579544067383\n",
      "steps = 217, loss = 3.345115900039673\n",
      "steps = 217, loss = 3.7416820526123047\n",
      "steps = 217, loss = 2.7984232902526855\n",
      "steps = 217, loss = 2.95389723777771\n",
      "steps = 217, loss = 4.817793846130371\n",
      "steps = 217, loss = 2.7448270320892334\n",
      "steps = 217, loss = 2.343832492828369\n",
      "steps = 217, loss = 19.962844848632812\n",
      "steps = 217, loss = 2.723879098892212\n",
      "steps = 217, loss = 2.6190953254699707\n",
      "steps = 217, loss = 2.856492519378662\n",
      "steps = 217, loss = 5.264946937561035\n",
      "steps = 217, loss = 5.5838117599487305\n",
      "steps = 217, loss = 2.926302194595337\n",
      "steps = 217, loss = 2.8561439514160156\n",
      "steps = 217, loss = 3.4743857383728027\n",
      "steps = 217, loss = 49.985382080078125\n",
      "steps = 217, loss = 4.738038539886475\n",
      "steps = 217, loss = 3.3875010013580322\n",
      "steps = 217, loss = 2.967460870742798\n",
      "steps = 217, loss = 49.938297271728516\n",
      "steps = 217, loss = 2.7700295448303223\n",
      "steps = 217, loss = 3.4403445720672607\n",
      "steps = 217, loss = 3.4276227951049805\n",
      "steps = 217, loss = 6.853567600250244\n",
      "steps = 217, loss = 3.199969530105591\n",
      "steps = 217, loss = 5.179550647735596\n",
      "steps = 217, loss = 3.463172674179077\n",
      "steps = 217, loss = 8.053597450256348\n",
      "steps = 217, loss = 3.046433925628662\n",
      "steps = 218, loss = 3.7449676990509033\n",
      "steps = 218, loss = 3.108264207839966\n",
      "steps = 218, loss = 3.524148941040039\n",
      "steps = 218, loss = 4.8136305809021\n",
      "steps = 218, loss = 19.53059196472168\n",
      "steps = 218, loss = 4.711117744445801\n",
      "steps = 218, loss = 3.3852176666259766\n",
      "steps = 218, loss = 2.6549932956695557\n",
      "steps = 218, loss = 5.640719890594482\n",
      "steps = 218, loss = 3.59248948097229\n",
      "steps = 218, loss = 5.263338088989258\n",
      "steps = 218, loss = 2.7566304206848145\n",
      "steps = 218, loss = 3.2028391361236572\n",
      "steps = 218, loss = 2.513185739517212\n",
      "steps = 218, loss = 2.726013422012329\n",
      "steps = 218, loss = 49.985382080078125\n",
      "steps = 218, loss = 2.8149890899658203\n",
      "steps = 218, loss = 3.5186381340026855\n",
      "steps = 218, loss = 2.6728546619415283\n",
      "steps = 218, loss = 2.913318157196045\n",
      "steps = 218, loss = 3.441622018814087\n",
      "steps = 218, loss = 2.3094706535339355\n",
      "steps = 218, loss = 2.856811761856079\n",
      "steps = 218, loss = 2.7357141971588135\n",
      "steps = 218, loss = 49.973331451416016\n",
      "steps = 218, loss = 2.3995487689971924\n",
      "steps = 218, loss = 3.1929268836975098\n",
      "steps = 218, loss = 6.834414482116699\n",
      "steps = 218, loss = 7.447761535644531\n",
      "steps = 218, loss = 2.9485256671905518\n",
      "steps = 218, loss = 2.0936808586120605\n",
      "steps = 218, loss = 2.9923901557922363\n",
      "steps = 219, loss = 3.4048519134521484\n",
      "steps = 219, loss = 2.957756757736206\n",
      "steps = 219, loss = 4.747535705566406\n",
      "steps = 219, loss = 49.985382080078125\n",
      "steps = 219, loss = 3.2019026279449463\n",
      "steps = 219, loss = 6.888696193695068\n",
      "steps = 219, loss = 2.7549166679382324\n",
      "steps = 219, loss = 2.7768473625183105\n",
      "steps = 219, loss = 3.2610108852386475\n",
      "steps = 219, loss = 2.1058599948883057\n",
      "steps = 219, loss = 3.1867659091949463\n",
      "steps = 219, loss = 2.873157024383545\n",
      "steps = 219, loss = 2.936368227005005\n",
      "steps = 219, loss = 3.124997138977051\n",
      "steps = 219, loss = 5.587826728820801\n",
      "steps = 219, loss = 3.1216256618499756\n",
      "steps = 219, loss = 3.456465482711792\n",
      "steps = 219, loss = 3.5636370182037354\n",
      "steps = 219, loss = 2.716062307357788\n",
      "steps = 219, loss = 19.453449249267578\n",
      "steps = 219, loss = 3.641557455062866\n",
      "steps = 219, loss = 3.401594877243042\n",
      "steps = 219, loss = 4.828611850738525\n",
      "steps = 219, loss = 11.677473068237305\n",
      "steps = 219, loss = 49.77096939086914\n",
      "steps = 219, loss = 2.7446234226226807\n",
      "steps = 219, loss = 2.4097678661346436\n",
      "steps = 219, loss = 3.488353967666626\n",
      "steps = 219, loss = 2.9644558429718018\n",
      "steps = 219, loss = 5.29702615737915\n",
      "steps = 219, loss = 3.518345594406128\n",
      "steps = 219, loss = 2.7052907943725586\n",
      "steps = 220, loss = 2.7349116802215576\n",
      "steps = 220, loss = 2.857457160949707\n",
      "steps = 220, loss = 2.8766160011291504\n",
      "steps = 220, loss = 2.108274221420288\n",
      "steps = 220, loss = 3.081488609313965\n",
      "steps = 220, loss = 2.1272075176239014\n",
      "steps = 220, loss = 6.893012523651123\n",
      "steps = 220, loss = 3.4765148162841797\n",
      "steps = 220, loss = 3.3088061809539795\n",
      "steps = 220, loss = 4.8401618003845215\n",
      "steps = 220, loss = 2.926652193069458\n",
      "steps = 220, loss = 3.2365314960479736\n",
      "steps = 220, loss = 3.681640386581421\n",
      "steps = 220, loss = 3.2037429809570312\n",
      "steps = 220, loss = 2.959791421890259\n",
      "steps = 220, loss = 2.779430866241455\n",
      "steps = 220, loss = 2.7458951473236084\n",
      "steps = 220, loss = 49.9372444152832\n",
      "steps = 220, loss = 8.211645126342773\n",
      "steps = 220, loss = 2.6236648559570312\n",
      "steps = 220, loss = 5.662042617797852\n",
      "steps = 220, loss = 4.757967948913574\n",
      "steps = 220, loss = 3.0428342819213867\n",
      "steps = 220, loss = 20.132831573486328\n",
      "steps = 220, loss = 3.1270272731781006\n",
      "steps = 220, loss = 3.551542282104492\n",
      "steps = 220, loss = 3.4884228706359863\n",
      "steps = 220, loss = 3.583087205886841\n",
      "steps = 220, loss = 3.414304256439209\n",
      "steps = 220, loss = 49.985382080078125\n",
      "steps = 220, loss = 2.762918472290039\n",
      "steps = 220, loss = 5.318037509918213\n",
      "steps = 221, loss = 3.1599793434143066\n",
      "steps = 221, loss = 5.520689487457275\n",
      "steps = 221, loss = 3.131171941757202\n",
      "steps = 221, loss = 19.584829330444336\n",
      "steps = 221, loss = 2.09208083152771\n",
      "steps = 221, loss = 3.118150472640991\n",
      "steps = 221, loss = 49.95864486694336\n",
      "steps = 221, loss = 3.430121898651123\n",
      "steps = 221, loss = 3.024604320526123\n",
      "steps = 221, loss = 4.840075492858887\n",
      "steps = 221, loss = 3.1064019203186035\n",
      "steps = 221, loss = 5.313990592956543\n",
      "steps = 221, loss = 2.225696563720703\n",
      "steps = 221, loss = 2.6616768836975098\n",
      "steps = 221, loss = 2.9503915309906006\n",
      "steps = 221, loss = 3.442100763320923\n",
      "steps = 221, loss = 2.728424072265625\n",
      "steps = 221, loss = 6.490332126617432\n",
      "steps = 221, loss = 2.760826826095581\n",
      "steps = 221, loss = 3.41963791847229\n",
      "steps = 221, loss = 4.728320598602295\n",
      "steps = 221, loss = 3.3819000720977783\n",
      "steps = 221, loss = 3.194138765335083\n",
      "steps = 221, loss = 6.9870991706848145\n",
      "steps = 221, loss = 2.7356882095336914\n",
      "steps = 221, loss = 2.823331117630005\n",
      "steps = 221, loss = 2.595311403274536\n",
      "steps = 221, loss = 2.859323263168335\n",
      "steps = 221, loss = 3.526733875274658\n",
      "steps = 221, loss = 49.985382080078125\n",
      "steps = 221, loss = 3.398277759552002\n",
      "steps = 221, loss = 3.3243041038513184\n",
      "steps = 222, loss = 2.9539201259613037\n",
      "steps = 222, loss = 6.727504730224609\n",
      "steps = 222, loss = 2.724639892578125\n",
      "steps = 222, loss = 5.622949123382568\n",
      "steps = 222, loss = 3.1988043785095215\n",
      "steps = 222, loss = 5.342844009399414\n",
      "steps = 222, loss = 2.1415724754333496\n",
      "steps = 222, loss = 2.4186131954193115\n",
      "steps = 222, loss = 3.5420727729797363\n",
      "steps = 222, loss = 2.6182711124420166\n",
      "steps = 222, loss = 49.22945022583008\n",
      "steps = 222, loss = 2.6873557567596436\n",
      "steps = 222, loss = 2.905050277709961\n",
      "steps = 222, loss = 3.40035343170166\n",
      "steps = 222, loss = 3.7051992416381836\n",
      "steps = 222, loss = 3.501673460006714\n",
      "steps = 222, loss = 2.752851724624634\n",
      "steps = 222, loss = 49.985382080078125\n",
      "steps = 222, loss = 3.38920521736145\n",
      "steps = 222, loss = 2.2125720977783203\n",
      "steps = 222, loss = 6.246274471282959\n",
      "steps = 222, loss = 3.0197784900665283\n",
      "steps = 222, loss = 4.761645317077637\n",
      "steps = 222, loss = 20.071117401123047\n",
      "steps = 222, loss = 3.126614809036255\n",
      "steps = 222, loss = 2.739454507827759\n",
      "steps = 222, loss = 2.856616258621216\n",
      "steps = 222, loss = 3.4692249298095703\n",
      "steps = 222, loss = 2.9814882278442383\n",
      "steps = 222, loss = 4.856530666351318\n",
      "steps = 222, loss = 3.460920810699463\n",
      "steps = 222, loss = 3.536001682281494\n",
      "steps = 223, loss = 4.77019739151001\n",
      "steps = 223, loss = 2.1991097927093506\n",
      "steps = 223, loss = 3.486891746520996\n",
      "steps = 223, loss = 3.1351394653320312\n",
      "steps = 223, loss = 49.9641227722168\n",
      "steps = 223, loss = 49.900474548339844\n",
      "steps = 223, loss = 3.0695769786834717\n",
      "steps = 223, loss = 49.985382080078125\n",
      "steps = 223, loss = 3.708115577697754\n",
      "steps = 223, loss = 3.4925527572631836\n",
      "steps = 223, loss = 2.937849998474121\n",
      "steps = 223, loss = 3.6281726360321045\n",
      "steps = 223, loss = 2.6276721954345703\n",
      "steps = 223, loss = 2.105605125427246\n",
      "steps = 223, loss = 3.546450138092041\n",
      "steps = 223, loss = 2.126136064529419\n",
      "steps = 223, loss = 3.288567304611206\n",
      "steps = 223, loss = 20.275264739990234\n",
      "steps = 223, loss = 3.0915048122406006\n",
      "steps = 223, loss = 3.4344868659973145\n",
      "steps = 223, loss = 2.8738279342651367\n",
      "steps = 223, loss = 2.7440884113311768\n",
      "steps = 223, loss = 4.862593173980713\n",
      "steps = 223, loss = 5.365678787231445\n",
      "steps = 223, loss = 2.9587769508361816\n",
      "steps = 223, loss = 5.558163166046143\n",
      "steps = 223, loss = 3.202091693878174\n",
      "steps = 223, loss = 2.5578155517578125\n",
      "steps = 223, loss = 6.910490036010742\n",
      "steps = 223, loss = 2.7772364616394043\n",
      "steps = 223, loss = 4.862929821014404\n",
      "steps = 223, loss = 2.7474796772003174\n",
      "steps = 224, loss = 5.0689544677734375\n",
      "steps = 224, loss = 19.845245361328125\n",
      "steps = 224, loss = 4.862360954284668\n",
      "steps = 224, loss = 2.0508410930633545\n",
      "steps = 224, loss = 49.960838317871094\n",
      "steps = 224, loss = 2.7599401473999023\n",
      "steps = 224, loss = 3.1265127658843994\n",
      "steps = 224, loss = 2.949690341949463\n",
      "steps = 224, loss = 2.5449182987213135\n",
      "steps = 224, loss = 3.3350799083709717\n",
      "steps = 224, loss = 5.360764503479004\n",
      "steps = 224, loss = 2.5756733417510986\n",
      "steps = 224, loss = 49.985382080078125\n",
      "steps = 224, loss = 4.740335941314697\n",
      "steps = 224, loss = 2.0925323963165283\n",
      "steps = 224, loss = 49.95988845825195\n",
      "steps = 224, loss = 3.448132038116455\n",
      "steps = 224, loss = 3.663457155227661\n",
      "steps = 224, loss = 3.478890895843506\n",
      "steps = 224, loss = 5.578007221221924\n",
      "steps = 224, loss = 3.70345401763916\n",
      "steps = 224, loss = 6.862014293670654\n",
      "steps = 224, loss = 3.055527925491333\n",
      "steps = 224, loss = 3.2488908767700195\n",
      "steps = 224, loss = 2.1098763942718506\n",
      "steps = 224, loss = 2.72747802734375\n",
      "steps = 224, loss = 2.727907657623291\n",
      "steps = 224, loss = 2.734483480453491\n",
      "steps = 224, loss = 3.192899465560913\n",
      "steps = 224, loss = 2.7591803073883057\n",
      "steps = 224, loss = 2.8587872982025146\n",
      "steps = 224, loss = 3.441340208053589\n",
      "steps = 225, loss = 2.9595580101013184\n",
      "steps = 225, loss = 20.22389030456543\n",
      "steps = 225, loss = 2.856511354446411\n",
      "steps = 225, loss = 2.7245302200317383\n",
      "steps = 225, loss = 2.55403995513916\n",
      "steps = 225, loss = 2.7414023876190186\n",
      "steps = 225, loss = 2.1416120529174805\n",
      "steps = 225, loss = 3.4749972820281982\n",
      "steps = 225, loss = 49.947601318359375\n",
      "steps = 225, loss = 3.1977620124816895\n",
      "steps = 225, loss = 4.87872838973999\n",
      "steps = 225, loss = 3.5118398666381836\n",
      "steps = 225, loss = 3.006380796432495\n",
      "steps = 225, loss = 2.7968924045562744\n",
      "steps = 225, loss = 2.1190547943115234\n",
      "steps = 225, loss = 49.985382080078125\n",
      "steps = 225, loss = 3.134779214859009\n",
      "steps = 225, loss = 12.630901336669922\n",
      "steps = 225, loss = 3.030221700668335\n",
      "steps = 225, loss = 3.417510509490967\n",
      "steps = 225, loss = 5.390085220336914\n",
      "steps = 225, loss = 49.94842529296875\n",
      "steps = 225, loss = 3.6907119750976562\n",
      "steps = 225, loss = 6.950827598571777\n",
      "steps = 225, loss = 3.4640438556671143\n",
      "steps = 225, loss = 3.4726316928863525\n",
      "steps = 225, loss = 2.9533920288085938\n",
      "steps = 225, loss = 5.563767910003662\n",
      "steps = 225, loss = 2.032177448272705\n",
      "steps = 225, loss = 4.774098873138428\n",
      "steps = 225, loss = 2.7613537311553955\n",
      "steps = 225, loss = 3.3944008350372314\n",
      "steps = 226, loss = 8.47591495513916\n",
      "steps = 226, loss = 5.388981819152832\n",
      "steps = 226, loss = 49.94111251831055\n",
      "steps = 226, loss = 2.0938234329223633\n",
      "steps = 226, loss = 3.7088217735290527\n",
      "steps = 226, loss = 49.9604606628418\n",
      "steps = 226, loss = 3.440608501434326\n",
      "steps = 226, loss = 3.1305062770843506\n",
      "steps = 226, loss = 4.875208854675293\n",
      "steps = 226, loss = 3.1910085678100586\n",
      "steps = 226, loss = 2.9480879306793213\n",
      "steps = 226, loss = 2.7578492164611816\n",
      "steps = 226, loss = 3.423283576965332\n",
      "steps = 226, loss = 3.2539055347442627\n",
      "steps = 226, loss = 1.999849557876587\n",
      "steps = 226, loss = 49.985382080078125\n",
      "steps = 226, loss = 2.0908710956573486\n",
      "steps = 226, loss = 19.55364418029785\n",
      "steps = 226, loss = 2.5477757453918457\n",
      "steps = 226, loss = 3.073420286178589\n",
      "steps = 226, loss = 2.726747989654541\n",
      "steps = 226, loss = 2.826665163040161\n",
      "steps = 226, loss = 2.5473849773406982\n",
      "steps = 226, loss = 3.3830528259277344\n",
      "steps = 226, loss = 2.6997945308685303\n",
      "steps = 226, loss = 5.598812103271484\n",
      "steps = 226, loss = 4.745744705200195\n",
      "steps = 226, loss = 2.857908248901367\n",
      "steps = 226, loss = 3.5336897373199463\n",
      "steps = 226, loss = 3.4917633533477783\n",
      "steps = 226, loss = 3.0243029594421387\n",
      "steps = 226, loss = 6.860188007354736\n",
      "steps = 227, loss = 2.9213709831237793\n",
      "steps = 227, loss = 3.657050848007202\n",
      "steps = 227, loss = 2.7451791763305664\n",
      "steps = 227, loss = 2.6704297065734863\n",
      "steps = 227, loss = 3.4639973640441895\n",
      "steps = 227, loss = 3.0876388549804688\n",
      "steps = 227, loss = 3.3940317630767822\n",
      "steps = 227, loss = 2.874002456665039\n",
      "steps = 227, loss = 49.87049102783203\n",
      "steps = 227, loss = 6.972034454345703\n",
      "steps = 227, loss = 5.422784328460693\n",
      "steps = 227, loss = 2.6570687294006348\n",
      "steps = 227, loss = 1.8952072858810425\n",
      "steps = 227, loss = 2.8534324169158936\n",
      "steps = 227, loss = 3.4995274543762207\n",
      "steps = 227, loss = 19.664569854736328\n",
      "steps = 227, loss = 3.1999619007110596\n",
      "steps = 227, loss = 3.522047281265259\n",
      "steps = 227, loss = 4.784049987792969\n",
      "steps = 227, loss = 3.3651626110076904\n",
      "steps = 227, loss = 2.9572603702545166\n",
      "steps = 227, loss = 3.630692481994629\n",
      "steps = 227, loss = 2.869514226913452\n",
      "steps = 227, loss = 4.490576267242432\n",
      "steps = 227, loss = 4.8892011642456055\n",
      "steps = 227, loss = 49.985382080078125\n",
      "steps = 227, loss = 3.143491506576538\n",
      "steps = 227, loss = 49.92584228515625\n",
      "steps = 227, loss = 5.602873802185059\n",
      "steps = 227, loss = 2.0927038192749023\n",
      "steps = 227, loss = 2.105156660079956\n",
      "steps = 227, loss = 2.777712106704712\n",
      "steps = 228, loss = 3.323570728302002\n",
      "steps = 228, loss = 5.440880298614502\n",
      "steps = 228, loss = 3.607146978378296\n",
      "steps = 228, loss = 2.7244558334350586\n",
      "steps = 228, loss = 49.985382080078125\n",
      "steps = 228, loss = 49.9619140625\n",
      "steps = 228, loss = 2.7498812675476074\n",
      "steps = 228, loss = 4.903128623962402\n",
      "steps = 228, loss = 20.34598731994629\n",
      "steps = 228, loss = 3.1976895332336426\n",
      "steps = 228, loss = 3.4417755603790283\n",
      "steps = 228, loss = 2.7887232303619385\n",
      "steps = 228, loss = 2.2284200191497803\n",
      "steps = 228, loss = 3.4905924797058105\n",
      "steps = 228, loss = 2.1915011405944824\n",
      "steps = 228, loss = 3.6592655181884766\n",
      "steps = 228, loss = 5.608282566070557\n",
      "steps = 228, loss = 2.6264827251434326\n",
      "steps = 228, loss = 2.7910141944885254\n",
      "steps = 228, loss = 3.4426021575927734\n",
      "steps = 228, loss = 2.8570783138275146\n",
      "steps = 228, loss = 49.91543197631836\n",
      "steps = 228, loss = 3.1446704864501953\n",
      "steps = 228, loss = 3.439276695251465\n",
      "steps = 228, loss = 2.7439424991607666\n",
      "steps = 228, loss = 4.822056770324707\n",
      "steps = 228, loss = 2.759467124938965\n",
      "steps = 228, loss = 2.531280755996704\n",
      "steps = 228, loss = 2.954075336456299\n",
      "steps = 228, loss = 6.812057971954346\n",
      "steps = 228, loss = 2.140143632888794\n",
      "steps = 228, loss = 4.789484024047852\n",
      "steps = 229, loss = 4.77151346206665\n",
      "steps = 229, loss = 5.638489723205566\n",
      "steps = 229, loss = 2.5968854427337646\n",
      "steps = 229, loss = 2.0808353424072266\n",
      "steps = 229, loss = 2.106614112854004\n",
      "steps = 229, loss = 2.7268011569976807\n",
      "steps = 229, loss = 3.516590118408203\n",
      "steps = 229, loss = 3.4373087882995605\n",
      "steps = 229, loss = 2.9448635578155518\n",
      "steps = 229, loss = 4.907908916473389\n",
      "steps = 229, loss = 3.4621331691741943\n",
      "steps = 229, loss = 2.8741798400878906\n",
      "steps = 229, loss = 2.7776753902435303\n",
      "steps = 229, loss = 2.958352565765381\n",
      "steps = 229, loss = 3.526711940765381\n",
      "steps = 229, loss = 49.985382080078125\n",
      "steps = 229, loss = 3.102202892303467\n",
      "steps = 229, loss = 3.200505018234253\n",
      "steps = 229, loss = 2.771655559539795\n",
      "steps = 229, loss = 3.152315378189087\n",
      "steps = 229, loss = 3.4370651245117188\n",
      "steps = 229, loss = 49.95685958862305\n",
      "steps = 229, loss = 2.7440242767333984\n",
      "steps = 229, loss = 3.3339974880218506\n",
      "steps = 229, loss = 3.598010301589966\n",
      "steps = 229, loss = 5.459708213806152\n",
      "steps = 229, loss = 2.5445406436920166\n",
      "steps = 229, loss = 2.1707522869110107\n",
      "steps = 229, loss = 49.48621368408203\n",
      "steps = 229, loss = 6.984871864318848\n",
      "steps = 229, loss = 19.977506637573242\n",
      "steps = 229, loss = 4.7965192794799805\n",
      "steps = 230, loss = 2.858893871307373\n",
      "steps = 230, loss = 2.9969124794006348\n",
      "steps = 230, loss = 2.5406715869903564\n",
      "steps = 230, loss = 2.0246009826660156\n",
      "steps = 230, loss = 4.907461166381836\n",
      "steps = 230, loss = 3.547407627105713\n",
      "steps = 230, loss = 49.92073059082031\n",
      "steps = 230, loss = 3.421823024749756\n",
      "steps = 230, loss = 4.758261203765869\n",
      "steps = 230, loss = 4.76387357711792\n",
      "steps = 230, loss = 3.4815104007720947\n",
      "steps = 230, loss = 2.681819438934326\n",
      "steps = 230, loss = 49.96303176879883\n",
      "steps = 230, loss = 3.191234827041626\n",
      "steps = 230, loss = 2.7875802516937256\n",
      "steps = 230, loss = 6.870373249053955\n",
      "steps = 230, loss = 5.584176540374756\n",
      "steps = 230, loss = 3.639820098876953\n",
      "steps = 230, loss = 5.455342769622803\n",
      "steps = 230, loss = 2.7275187969207764\n",
      "steps = 230, loss = 49.985382080078125\n",
      "steps = 230, loss = 20.062122344970703\n",
      "steps = 230, loss = 2.759127616882324\n",
      "steps = 230, loss = 3.449099540710449\n",
      "steps = 230, loss = 3.143043041229248\n",
      "steps = 230, loss = 2.092421293258667\n",
      "steps = 230, loss = 2.9491100311279297\n",
      "steps = 230, loss = 3.682605266571045\n",
      "steps = 230, loss = 2.165635585784912\n",
      "steps = 230, loss = 2.527074098587036\n",
      "steps = 230, loss = 3.414708375930786\n",
      "steps = 230, loss = 2.8408946990966797\n",
      "steps = 231, loss = 49.96146011352539\n",
      "steps = 231, loss = 3.397392749786377\n",
      "steps = 231, loss = 49.948387145996094\n",
      "steps = 231, loss = 3.5355429649353027\n",
      "steps = 231, loss = 3.4485766887664795\n",
      "steps = 231, loss = 3.0205252170562744\n",
      "steps = 231, loss = 3.4694833755493164\n",
      "steps = 231, loss = 2.7290890216827393\n",
      "steps = 231, loss = 2.1416149139404297\n",
      "steps = 231, loss = 49.985382080078125\n",
      "steps = 231, loss = 3.196162700653076\n",
      "steps = 231, loss = 2.7429702281951904\n",
      "steps = 231, loss = 6.99716854095459\n",
      "steps = 231, loss = 1.9941405057907104\n",
      "steps = 231, loss = 4.947038173675537\n",
      "steps = 231, loss = 3.1515088081359863\n",
      "steps = 231, loss = 3.5792479515075684\n",
      "steps = 231, loss = 3.4925594329833984\n",
      "steps = 231, loss = 5.681463241577148\n",
      "steps = 231, loss = 2.852245330810547\n",
      "steps = 231, loss = 2.077755928039551\n",
      "steps = 231, loss = 4.923731803894043\n",
      "steps = 231, loss = 4.800333023071289\n",
      "steps = 231, loss = 2.934006690979004\n",
      "steps = 231, loss = 2.749706745147705\n",
      "steps = 231, loss = 2.952744245529175\n",
      "steps = 231, loss = 2.6176352500915527\n",
      "steps = 231, loss = 3.470564842224121\n",
      "steps = 231, loss = 2.856696605682373\n",
      "steps = 231, loss = 5.4842305183410645\n",
      "steps = 231, loss = 20.122344970703125\n",
      "steps = 231, loss = 2.724561929702759\n",
      "steps = 232, loss = 3.743032932281494\n",
      "steps = 232, loss = 2.237192392349243\n",
      "steps = 232, loss = 49.95001220703125\n",
      "steps = 232, loss = 3.2679789066314697\n",
      "steps = 232, loss = 4.932699203491211\n",
      "steps = 232, loss = 2.872127056121826\n",
      "steps = 232, loss = 5.502492904663086\n",
      "steps = 232, loss = 2.6953439712524414\n",
      "steps = 232, loss = 2.77754282951355\n",
      "steps = 232, loss = 3.400205373764038\n",
      "steps = 232, loss = 3.521880865097046\n",
      "steps = 232, loss = 49.985382080078125\n",
      "steps = 232, loss = 2.142426013946533\n",
      "steps = 232, loss = 4.805300235748291\n",
      "steps = 232, loss = 2.9527640342712402\n",
      "steps = 232, loss = 3.1558339595794678\n",
      "steps = 232, loss = 2.723477602005005\n",
      "steps = 232, loss = 3.4542453289031982\n",
      "steps = 232, loss = 19.94391441345215\n",
      "steps = 232, loss = 2.0033233165740967\n",
      "steps = 232, loss = 5.490778923034668\n",
      "steps = 232, loss = 2.7373509407043457\n",
      "steps = 232, loss = 2.856482744216919\n",
      "steps = 232, loss = 49.966697692871094\n",
      "steps = 232, loss = 2.6268203258514404\n",
      "steps = 232, loss = 6.813631534576416\n",
      "steps = 232, loss = 3.5147757530212402\n",
      "steps = 232, loss = 3.4539406299591064\n",
      "steps = 232, loss = 5.10228967666626\n",
      "steps = 232, loss = 3.196082830429077\n",
      "steps = 232, loss = 2.750072717666626\n",
      "steps = 232, loss = 3.784574508666992\n",
      "steps = 233, loss = 3.1633589267730713\n",
      "steps = 233, loss = 3.053205966949463\n",
      "steps = 233, loss = 49.962608337402344\n",
      "steps = 233, loss = 3.393031120300293\n",
      "steps = 233, loss = 24.27046775817871\n",
      "steps = 233, loss = 3.5753979682922363\n",
      "steps = 233, loss = 2.674070358276367\n",
      "steps = 233, loss = 49.985382080078125\n",
      "steps = 233, loss = 2.8724987506866455\n",
      "steps = 233, loss = 49.979530334472656\n",
      "steps = 233, loss = 3.640288829803467\n",
      "steps = 233, loss = 3.430905342102051\n",
      "steps = 233, loss = 4.937270641326904\n",
      "steps = 233, loss = 3.5158498287200928\n",
      "steps = 233, loss = 2.851001024246216\n",
      "steps = 233, loss = 2.1067042350769043\n",
      "steps = 233, loss = 2.6638023853302\n",
      "steps = 233, loss = 4.811317443847656\n",
      "steps = 233, loss = 3.6673476696014404\n",
      "steps = 233, loss = 2.743382692337036\n",
      "steps = 233, loss = 2.1215693950653076\n",
      "steps = 233, loss = 3.5109050273895264\n",
      "steps = 233, loss = 20.152000427246094\n",
      "steps = 233, loss = 3.1987550258636475\n",
      "steps = 233, loss = 6.933296203613281\n",
      "steps = 233, loss = 2.1937811374664307\n",
      "steps = 233, loss = 3.1204030513763428\n",
      "steps = 233, loss = 5.520784378051758\n",
      "steps = 233, loss = 5.5865397453308105\n",
      "steps = 233, loss = 2.957139492034912\n",
      "steps = 233, loss = 2.895204782485962\n",
      "steps = 233, loss = 2.7766621112823486\n",
      "steps = 234, loss = 3.6651484966278076\n",
      "steps = 234, loss = 3.384871244430542\n",
      "steps = 234, loss = 19.95372772216797\n",
      "steps = 234, loss = 4.936647415161133\n",
      "steps = 234, loss = 3.4442670345306396\n",
      "steps = 234, loss = 3.189655303955078\n",
      "steps = 234, loss = 2.72622013092041\n",
      "steps = 234, loss = 3.0717525482177734\n",
      "steps = 234, loss = 3.4349756240844727\n",
      "steps = 234, loss = 49.97481155395508\n",
      "steps = 234, loss = 2.727045774459839\n",
      "steps = 234, loss = 3.212313413619995\n",
      "steps = 234, loss = 2.0597825050354004\n",
      "steps = 234, loss = 3.154266119003296\n",
      "steps = 234, loss = 2.605504035949707\n",
      "steps = 234, loss = 4.777429580688477\n",
      "steps = 234, loss = 2.5795392990112305\n",
      "steps = 234, loss = 3.5167715549468994\n",
      "steps = 234, loss = 5.566287040710449\n",
      "steps = 234, loss = 49.985382080078125\n",
      "steps = 234, loss = 3.494601249694824\n",
      "steps = 234, loss = 2.9480981826782227\n",
      "steps = 234, loss = 2.8327367305755615\n",
      "steps = 234, loss = 49.95998764038086\n",
      "steps = 234, loss = 2.0925121307373047\n",
      "steps = 234, loss = 2.2510292530059814\n",
      "steps = 234, loss = 2.8582379817962646\n",
      "steps = 234, loss = 2.757672071456909\n",
      "steps = 234, loss = 2.999452829360962\n",
      "steps = 234, loss = 5.515872001647949\n",
      "steps = 234, loss = 6.98598051071167\n",
      "steps = 234, loss = 10.055255889892578\n",
      "steps = 235, loss = 3.750600814819336\n",
      "steps = 235, loss = 2.875251293182373\n",
      "steps = 235, loss = 2.778796672821045\n",
      "steps = 235, loss = 6.886270046234131\n",
      "steps = 235, loss = 3.413322687149048\n",
      "steps = 235, loss = 3.1984381675720215\n",
      "steps = 235, loss = 49.985382080078125\n",
      "steps = 235, loss = 3.054262399673462\n",
      "steps = 235, loss = 2.6389780044555664\n",
      "steps = 235, loss = 5.593571662902832\n",
      "steps = 235, loss = 19.873153686523438\n",
      "steps = 235, loss = 2.102121353149414\n",
      "steps = 235, loss = 5.547366142272949\n",
      "steps = 235, loss = 2.970010995864868\n",
      "steps = 235, loss = 49.96059799194336\n",
      "steps = 235, loss = 5.882307529449463\n",
      "steps = 235, loss = 4.8171539306640625\n",
      "steps = 235, loss = 3.1665759086608887\n",
      "steps = 235, loss = 2.745596408843994\n",
      "steps = 235, loss = 4.949616432189941\n",
      "steps = 235, loss = 2.2737162113189697\n",
      "steps = 235, loss = 2.7896535396575928\n",
      "steps = 235, loss = 49.54914855957031\n",
      "steps = 235, loss = 3.3120322227478027\n",
      "steps = 235, loss = 3.5486831665039062\n",
      "steps = 235, loss = 2.569751024246216\n",
      "steps = 235, loss = 3.4350242614746094\n",
      "steps = 235, loss = 2.1057279109954834\n",
      "steps = 235, loss = 3.4258289337158203\n",
      "steps = 235, loss = 2.9568779468536377\n",
      "steps = 235, loss = 3.4715349674224854\n",
      "steps = 235, loss = 3.023838758468628\n",
      "steps = 236, loss = 49.985382080078125\n",
      "steps = 236, loss = 2.833897352218628\n",
      "steps = 236, loss = 4.951095104217529\n",
      "steps = 236, loss = 49.163063049316406\n",
      "steps = 236, loss = 2.728555679321289\n",
      "steps = 236, loss = 2.96954083442688\n",
      "steps = 236, loss = 2.2627406120300293\n",
      "steps = 236, loss = 3.354642629623413\n",
      "steps = 236, loss = 49.95090866088867\n",
      "steps = 236, loss = 2.6390953063964844\n",
      "steps = 236, loss = 3.5367772579193115\n",
      "steps = 236, loss = 3.449354648590088\n",
      "steps = 236, loss = 4.785099029541016\n",
      "steps = 236, loss = 3.189762830734253\n",
      "steps = 236, loss = 19.31694221496582\n",
      "steps = 236, loss = 4.404789447784424\n",
      "steps = 236, loss = 3.1588399410247803\n",
      "steps = 236, loss = 2.673502206802368\n",
      "steps = 236, loss = 3.3621666431427\n",
      "steps = 236, loss = 2.6742544174194336\n",
      "steps = 236, loss = 2.092381238937378\n",
      "steps = 236, loss = 7.023379325866699\n",
      "steps = 236, loss = 5.5972700119018555\n",
      "steps = 236, loss = 2.948375701904297\n",
      "steps = 236, loss = 3.4240357875823975\n",
      "steps = 236, loss = 3.4326858520507812\n",
      "steps = 236, loss = 5.544332027435303\n",
      "steps = 236, loss = 2.120765447616577\n",
      "steps = 236, loss = 3.415302038192749\n",
      "steps = 236, loss = 2.859996795654297\n",
      "steps = 236, loss = 2.7609522342681885\n",
      "steps = 236, loss = 2.7192084789276123\n",
      "steps = 237, loss = 2.141533374786377\n",
      "steps = 237, loss = 2.2045886516571045\n",
      "steps = 237, loss = 2.658900022506714\n",
      "steps = 237, loss = 2.946110963821411\n",
      "steps = 237, loss = 19.87204360961914\n",
      "steps = 237, loss = 3.074190855026245\n",
      "steps = 237, loss = 49.730613708496094\n",
      "steps = 237, loss = 3.165219783782959\n",
      "steps = 237, loss = 4.822543621063232\n",
      "steps = 237, loss = 49.985382080078125\n",
      "steps = 237, loss = 3.463179111480713\n",
      "steps = 237, loss = 2.724951982498169\n",
      "steps = 237, loss = 3.1945929527282715\n",
      "steps = 237, loss = 3.4259564876556396\n",
      "steps = 237, loss = 49.96376037597656\n",
      "steps = 237, loss = 4.967161655426025\n",
      "steps = 237, loss = 2.741481065750122\n",
      "steps = 237, loss = 3.387660026550293\n",
      "steps = 237, loss = 2.8146722316741943\n",
      "steps = 237, loss = 2.068507194519043\n",
      "steps = 237, loss = 2.809556245803833\n",
      "steps = 237, loss = 5.574714183807373\n",
      "steps = 237, loss = 6.856060981750488\n",
      "steps = 237, loss = 5.615889072418213\n",
      "steps = 237, loss = 4.795057773590088\n",
      "steps = 237, loss = 2.8569488525390625\n",
      "steps = 237, loss = 3.4625024795532227\n",
      "steps = 237, loss = 3.5026211738586426\n",
      "steps = 237, loss = 2.9520530700683594\n",
      "steps = 237, loss = 3.1669435501098633\n",
      "steps = 237, loss = 3.485018730163574\n",
      "steps = 237, loss = 3.6747310161590576\n",
      "steps = 238, loss = 20.43562126159668\n",
      "steps = 238, loss = 49.88389587402344\n",
      "steps = 238, loss = 6.963337421417236\n",
      "steps = 238, loss = 2.9933900833129883\n",
      "steps = 238, loss = 5.592686176300049\n",
      "steps = 238, loss = 2.74965500831604\n",
      "steps = 238, loss = 5.10664701461792\n",
      "steps = 238, loss = 3.171529531478882\n",
      "steps = 238, loss = 4.975988388061523\n",
      "steps = 238, loss = 3.1944775581359863\n",
      "steps = 238, loss = 2.729107618331909\n",
      "steps = 238, loss = 3.5065605640411377\n",
      "steps = 238, loss = 49.985382080078125\n",
      "steps = 238, loss = 2.1424007415771484\n",
      "steps = 238, loss = 3.463855743408203\n",
      "steps = 238, loss = 3.5601727962493896\n",
      "steps = 238, loss = 2.1836252212524414\n",
      "steps = 238, loss = 2.952115058898926\n",
      "steps = 238, loss = 3.5503599643707275\n",
      "steps = 238, loss = 2.8566505908966064\n",
      "steps = 238, loss = 3.751490831375122\n",
      "steps = 238, loss = 2.57757568359375\n",
      "steps = 238, loss = 2.8365817070007324\n",
      "steps = 238, loss = 5.612634181976318\n",
      "steps = 238, loss = 3.5694546699523926\n",
      "steps = 238, loss = 3.427004337310791\n",
      "steps = 238, loss = 2.083911180496216\n",
      "steps = 238, loss = 49.94102478027344\n",
      "steps = 238, loss = 4.827332496643066\n",
      "steps = 238, loss = 2.723602771759033\n",
      "steps = 238, loss = 2.7961483001708984\n",
      "steps = 238, loss = 2.9997830390930176\n",
      "steps = 239, loss = 6.920799255371094\n",
      "steps = 239, loss = 3.7648727893829346\n",
      "steps = 239, loss = 5.602624893188477\n",
      "steps = 239, loss = 3.2773666381835938\n",
      "steps = 239, loss = 49.96113204956055\n",
      "steps = 239, loss = 49.94558334350586\n",
      "steps = 239, loss = 5.194079875946045\n",
      "steps = 239, loss = 2.627873420715332\n",
      "steps = 239, loss = 3.1873252391815186\n",
      "steps = 239, loss = 2.756835699081421\n",
      "steps = 239, loss = 19.33022117614746\n",
      "steps = 239, loss = 2.791745662689209\n",
      "steps = 239, loss = 3.846834421157837\n",
      "steps = 239, loss = 3.0006442070007324\n",
      "steps = 239, loss = 3.3630599975585938\n",
      "steps = 239, loss = 3.0934834480285645\n",
      "steps = 239, loss = 4.792851448059082\n",
      "steps = 239, loss = 49.985382080078125\n",
      "steps = 239, loss = 2.0939555168151855\n",
      "steps = 239, loss = 3.1665759086608887\n",
      "steps = 239, loss = 3.4513418674468994\n",
      "steps = 239, loss = 4.971179008483887\n",
      "steps = 239, loss = 2.725844621658325\n",
      "steps = 239, loss = 5.5887675285339355\n",
      "steps = 239, loss = 2.622380256652832\n",
      "steps = 239, loss = 2.5840227603912354\n",
      "steps = 239, loss = 2.857461452484131\n",
      "steps = 239, loss = 2.368274211883545\n",
      "steps = 239, loss = 3.5856072902679443\n",
      "steps = 239, loss = 3.5092763900756836\n",
      "steps = 239, loss = 3.636383295059204\n",
      "steps = 239, loss = 2.946420907974243\n",
      "steps = 240, loss = 2.744878053665161\n",
      "steps = 240, loss = 2.8734352588653564\n",
      "steps = 240, loss = 3.1960487365722656\n",
      "steps = 240, loss = 49.95263671875\n",
      "steps = 240, loss = 49.93284606933594\n",
      "steps = 240, loss = 3.0426323413848877\n",
      "steps = 240, loss = 2.254626750946045\n",
      "steps = 240, loss = 5.624500274658203\n",
      "steps = 240, loss = 5.256324768066406\n",
      "steps = 240, loss = 3.1790566444396973\n",
      "steps = 240, loss = 3.5138633251190186\n",
      "steps = 240, loss = 3.40336537361145\n",
      "steps = 240, loss = 6.9718146324157715\n",
      "steps = 240, loss = 2.8789753913879395\n",
      "steps = 240, loss = 2.8773350715637207\n",
      "steps = 240, loss = 2.9554193019866943\n",
      "steps = 240, loss = 2.6459717750549316\n",
      "steps = 240, loss = 19.45794677734375\n",
      "steps = 240, loss = 2.3220198154449463\n",
      "steps = 240, loss = 3.4197182655334473\n",
      "steps = 240, loss = 2.6790781021118164\n",
      "steps = 240, loss = 2.7776153087615967\n",
      "steps = 240, loss = 2.737238645553589\n",
      "steps = 240, loss = 4.8315653800964355\n",
      "steps = 240, loss = 3.440765380859375\n",
      "steps = 240, loss = 3.4551053047180176\n",
      "steps = 240, loss = 49.985382080078125\n",
      "steps = 240, loss = 5.620345592498779\n",
      "steps = 240, loss = 3.1700563430786133\n",
      "steps = 240, loss = 4.983688831329346\n",
      "steps = 240, loss = 3.5071089267730713\n",
      "steps = 240, loss = 2.1051852703094482\n",
      "steps = 241, loss = 47.404964447021484\n",
      "steps = 241, loss = 5.637231349945068\n",
      "steps = 241, loss = 3.0963897705078125\n",
      "steps = 241, loss = 4.994503021240234\n",
      "steps = 241, loss = 4.561318397521973\n",
      "steps = 241, loss = 2.957441806793213\n",
      "steps = 241, loss = 3.197876453399658\n",
      "steps = 241, loss = 2.761611223220825\n",
      "steps = 241, loss = 49.95217514038086\n",
      "steps = 241, loss = 2.932915687561035\n",
      "steps = 241, loss = 3.783809185028076\n",
      "steps = 241, loss = 2.8770148754119873\n",
      "steps = 241, loss = 2.272538423538208\n",
      "steps = 241, loss = 49.985382080078125\n",
      "steps = 241, loss = 6.961647033691406\n",
      "steps = 241, loss = 20.070556640625\n",
      "steps = 241, loss = 2.7517588138580322\n",
      "steps = 241, loss = 3.184150457382202\n",
      "steps = 241, loss = 3.503999948501587\n",
      "steps = 241, loss = 2.108259677886963\n",
      "steps = 241, loss = 3.4180009365081787\n",
      "steps = 241, loss = 4.84134578704834\n",
      "steps = 241, loss = 2.5364716053009033\n",
      "steps = 241, loss = 2.7805912494659424\n",
      "steps = 241, loss = 2.7001631259918213\n",
      "steps = 241, loss = 3.5962820053100586\n",
      "steps = 241, loss = 3.500904083251953\n",
      "steps = 241, loss = 2.200544595718384\n",
      "steps = 241, loss = 5.640560150146484\n",
      "steps = 241, loss = 3.476077079772949\n",
      "steps = 241, loss = 2.7462158203125\n",
      "steps = 241, loss = 3.7117743492126465\n",
      "steps = 242, loss = 2.9482498168945312\n",
      "steps = 242, loss = 2.72847056388855\n",
      "steps = 242, loss = 2.548861026763916\n",
      "steps = 242, loss = 5.290807723999023\n",
      "steps = 242, loss = 3.4929628372192383\n",
      "steps = 242, loss = 3.4110512733459473\n",
      "steps = 242, loss = 3.175450563430786\n",
      "steps = 242, loss = 2.76892352104187\n",
      "steps = 242, loss = 7.084896564483643\n",
      "steps = 242, loss = 49.985382080078125\n",
      "steps = 242, loss = 2.43310809135437\n",
      "steps = 242, loss = 2.291185140609741\n",
      "steps = 242, loss = 2.108625888824463\n",
      "steps = 242, loss = 3.3883039951324463\n",
      "steps = 242, loss = 4.806128978729248\n",
      "steps = 242, loss = 2.0920755863189697\n",
      "steps = 242, loss = 2.7613565921783447\n",
      "steps = 242, loss = 19.709989547729492\n",
      "steps = 242, loss = 49.90785598754883\n",
      "steps = 242, loss = 49.962215423583984\n",
      "steps = 242, loss = 5.647562503814697\n",
      "steps = 242, loss = 3.431330680847168\n",
      "steps = 242, loss = 3.4686124324798584\n",
      "steps = 242, loss = 4.99418830871582\n",
      "steps = 242, loss = 3.434842109680176\n",
      "steps = 242, loss = 3.1886215209960938\n",
      "steps = 242, loss = 5.637349605560303\n",
      "steps = 242, loss = 3.072523832321167\n",
      "steps = 242, loss = 2.5721633434295654\n",
      "steps = 242, loss = 3.4398727416992188\n",
      "steps = 242, loss = 2.8603286743164062\n",
      "steps = 242, loss = 2.632642984390259\n",
      "steps = 243, loss = 2.7250959873199463\n",
      "steps = 243, loss = 4.843992233276367\n",
      "steps = 243, loss = 6.849392414093018\n",
      "steps = 243, loss = 2.8571784496307373\n",
      "steps = 243, loss = 20.138513565063477\n",
      "steps = 243, loss = 2.724929094314575\n",
      "steps = 243, loss = 3.193251371383667\n",
      "steps = 243, loss = 2.1415653228759766\n",
      "steps = 243, loss = 3.416656255722046\n",
      "steps = 243, loss = 3.7579519748687744\n",
      "steps = 243, loss = 3.5051562786102295\n",
      "steps = 243, loss = 3.0167884826660156\n",
      "steps = 243, loss = 2.7321650981903076\n",
      "steps = 243, loss = 5.665799140930176\n",
      "steps = 243, loss = 3.443211793899536\n",
      "steps = 243, loss = 49.985382080078125\n",
      "steps = 243, loss = 2.7421951293945312\n",
      "steps = 243, loss = 3.5395095348358154\n",
      "steps = 243, loss = 49.955718994140625\n",
      "steps = 243, loss = 2.68005108833313\n",
      "steps = 243, loss = 5.009679317474365\n",
      "steps = 243, loss = 2.0942628383636475\n",
      "steps = 243, loss = 2.951700210571289\n",
      "steps = 243, loss = 2.80271577835083\n",
      "steps = 243, loss = 2.155693292617798\n",
      "steps = 243, loss = 5.619404315948486\n",
      "steps = 243, loss = 3.495634078979492\n",
      "steps = 243, loss = 49.93288040161133\n",
      "steps = 243, loss = 3.1834187507629395\n",
      "steps = 243, loss = 5.629772663116455\n",
      "steps = 243, loss = 2.8119921684265137\n",
      "steps = 243, loss = 3.426746368408203\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IncrementalSearchCV(decay_rate=0,\n",
       "                    estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                    max_iter=243, n_initial_parameters=32,\n",
       "                    parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                                &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;,\n",
       "                                                       &#x27;ELU&#x27;, &#x27;PReLU&#x27;],\n",
       "                                &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                                 &#x27;xavier_normal_&#x27;,\n",
       "                                                 &#x27;kaiming_uniform_&#x27;,\n",
       "                                                 &#x27;kaiming_norm...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                                &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                                &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, ...],\n",
       "                                &#x27;train_split&#x27;: [None]},\n",
       "                    random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;IncrementalSearchCV<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>IncrementalSearchCV(decay_rate=0,\n",
       "                    estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                    max_iter=243, n_initial_parameters=32,\n",
       "                    parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                                &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;,\n",
       "                                                       &#x27;ELU&#x27;, &#x27;PReLU&#x27;],\n",
       "                                &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                                 &#x27;xavier_normal_&#x27;,\n",
       "                                                 &#x27;kaiming_uniform_&#x27;,\n",
       "                                                 &#x27;kaiming_norm...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                                &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                                &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, ...],\n",
       "                                &#x27;train_split&#x27;: [None]},\n",
       "                    random_state=42)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "IncrementalSearchCV(decay_rate=0,\n",
       "                    estimator=<class '__main__.TrimParams'>[uninitialized](\n",
       "  module=<class 'autoencoder.Autoencoder'>,\n",
       "),\n",
       "                    max_iter=243, n_initial_parameters=32,\n",
       "                    parameters={'batch_size': [32, 64, 128, 256, 512],\n",
       "                                'module__activation': ['ReLU', 'LeakyReLU',\n",
       "                                                       'ELU', 'PReLU'],\n",
       "                                'module__init': ['xavier_uniform_',\n",
       "                                                 'xavier_normal_',\n",
       "                                                 'kaiming_uniform_',\n",
       "                                                 'kaiming_norm...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                                'optimizer__nesterov': [True],\n",
       "                                'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, ...],\n",
       "                                'train_split': [None]},\n",
       "                    random_state=42)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passive_search.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 209.35 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 44.30 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "save_search(passive_search, today, \"passive\", X_test.compute(), y_test.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_stats = client.profile()\n",
    "\n",
    "with open(\"final-final-timings.json\", \"w\") as f:\n",
    "    json.dump(timing_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* patience: `max_iter // 8` (10 epochs)\n",
    "* n_initial: `2 * num_models`\n",
    "\n",
    "This requires choosing\n",
    "\n",
    "* the explore/exploit tradeoff (`patience` vs `n_initial`)\n",
    "* some estimate on many models will take advantage of `patience` to get total number of partial fit calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import IncrementalSearchCV\n",
    "\n",
    "patience_search = IncrementalSearchCV(\n",
    "    model,\n",
    "    params,\n",
    "    decay_rate=0,\n",
    "    patience=max_iter // 10,\n",
    "    n_initial_parameters=2 * num_models,\n",
    "    max_iter=num_calls,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/dask_ml/model_selection/_incremental.py:1047: FutureWarning: decay_rate is deprecated in InverseDecaySearchCV. Use InverseDecaySearchCV to use decay_rate=0\n",
      "  warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 418.70 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 1, loss = 1.977994441986084\n",
      "steps = 1, loss = 1.3603099584579468\n",
      "steps = 1, loss = 2.838804006576538\n",
      "steps = 1, loss = 2.043135643005371\n",
      "steps = 1, loss = 1.6521202325820923\n",
      "steps = 1, loss = 50.01109313964844\n",
      "steps = 1, loss = 2.415562391281128\n",
      "steps = 1, loss = 1.870324969291687\n",
      "steps = 1, loss = 49.96464538574219\n",
      "steps = 1, loss = 2.2370715141296387\n",
      "steps = 1, loss = 1.9614841938018799\n",
      "steps = 1, loss = 2.0443553924560547\n",
      "steps = 1, loss = 1.9816150665283203\n",
      "steps = 1, loss = 2.5164997577667236\n",
      "steps = 1, loss = 2.497925281524658\n",
      "steps = 1, loss = 2.5560879707336426\n",
      "steps = 1, loss = 2.4865505695343018\n",
      "steps = 1, loss = 49.804935455322266\n",
      "steps = 1, loss = 1.912054181098938\n",
      "steps = 1, loss = 2.76041841506958\n",
      "steps = 1, loss = 3.190953016281128\n",
      "steps = 1, loss = 2.2113561630249023\n",
      "steps = 1, loss = 0.8000869154930115\n",
      "steps = 1, loss = 2.2022204399108887\n",
      "steps = 1, loss = 1.941310167312622\n",
      "steps = 1, loss = 2.932598829269409\n",
      "steps = 1, loss = 0.7037603259086609\n",
      "steps = 1, loss = 2.0006825923919678\n",
      "steps = 1, loss = 0.7184212803840637\n",
      "steps = 1, loss = 3.0951783657073975\n",
      "steps = 1, loss = 2.324460983276367\n",
      "steps = 1, loss = 2.3322062492370605\n",
      "steps = 1, loss = 2.006298303604126\n",
      "steps = 1, loss = 1.5403401851654053\n",
      "steps = 1, loss = 2.3946335315704346\n",
      "steps = 1, loss = 2.0212912559509277\n",
      "steps = 1, loss = 2.952493906021118\n",
      "steps = 1, loss = 3.0519697666168213\n",
      "steps = 1, loss = 1.8121695518493652\n",
      "steps = 1, loss = 0.746051549911499\n",
      "steps = 1, loss = 4.036801338195801\n",
      "steps = 1, loss = 49.954139709472656\n",
      "steps = 1, loss = 2.2716519832611084\n",
      "steps = 1, loss = 1.885867714881897\n",
      "steps = 1, loss = 1.771657109260559\n",
      "steps = 1, loss = 1.1579663753509521\n",
      "steps = 1, loss = 50.036739349365234\n",
      "steps = 1, loss = 2.0085082054138184\n",
      "steps = 1, loss = 2.0158419609069824\n",
      "steps = 1, loss = 2.2599852085113525\n",
      "steps = 1, loss = 2.0252788066864014\n",
      "steps = 1, loss = 2.259545087814331\n",
      "steps = 1, loss = 49.96394348144531\n",
      "steps = 1, loss = 2.292583703994751\n",
      "steps = 1, loss = 2.414008378982544\n",
      "steps = 1, loss = 2.941617250442505\n",
      "steps = 1, loss = 1.161966323852539\n",
      "steps = 1, loss = 1.5079712867736816\n",
      "steps = 1, loss = 2.412308692932129\n",
      "steps = 1, loss = 2.9393296241760254\n",
      "steps = 1, loss = 2.021217107772827\n",
      "steps = 1, loss = 3.9109604358673096\n",
      "steps = 1, loss = 1.9616116285324097\n",
      "steps = 1, loss = 1.9427739381790161\n",
      "steps = 2, loss = 2.2423770427703857\n",
      "steps = 2, loss = 2.6824567317962646\n",
      "steps = 2, loss = 2.1852619647979736\n",
      "steps = 2, loss = 50.00917434692383\n",
      "steps = 2, loss = 2.2025296688079834\n",
      "steps = 2, loss = 2.727801561355591\n",
      "steps = 2, loss = 2.3244123458862305\n",
      "steps = 2, loss = 49.901649475097656\n",
      "steps = 2, loss = 2.046281576156616\n",
      "steps = 2, loss = 2.65340518951416\n",
      "steps = 2, loss = 1.8346657752990723\n",
      "steps = 2, loss = 2.6433682441711426\n",
      "steps = 2, loss = 23.026071548461914\n",
      "steps = 2, loss = 2.5953173637390137\n",
      "steps = 2, loss = 2.079664945602417\n",
      "steps = 2, loss = 2.4055447578430176\n",
      "steps = 2, loss = 2.6412603855133057\n",
      "steps = 2, loss = 1.3943268060684204\n",
      "steps = 2, loss = 2.767336368560791\n",
      "steps = 2, loss = 2.3057682514190674\n",
      "steps = 2, loss = 2.950493097305298\n",
      "steps = 2, loss = 2.885573148727417\n",
      "steps = 2, loss = 2.987656354904175\n",
      "steps = 2, loss = 2.256288766860962\n",
      "steps = 2, loss = 3.049968719482422\n",
      "steps = 2, loss = 2.470289945602417\n",
      "steps = 2, loss = 2.6478168964385986\n",
      "steps = 2, loss = 2.343691825866699\n",
      "steps = 2, loss = 2.121609926223755\n",
      "steps = 2, loss = 1.3991059064865112\n",
      "steps = 2, loss = 2.271973133087158\n",
      "steps = 2, loss = 3.1412525177001953\n",
      "steps = 2, loss = 0.8093917369842529\n",
      "steps = 2, loss = 4.4699835777282715\n",
      "steps = 2, loss = 1.1348555088043213\n",
      "steps = 2, loss = 2.9098753929138184\n",
      "steps = 2, loss = 5.587151527404785\n",
      "steps = 2, loss = 1.9991998672485352\n",
      "steps = 2, loss = 2.4852216243743896\n",
      "steps = 2, loss = 2.0766592025756836\n",
      "steps = 2, loss = 3.753962993621826\n",
      "steps = 2, loss = 1.9610443115234375\n",
      "steps = 2, loss = 1.9613159894943237\n",
      "steps = 2, loss = 49.94446563720703\n",
      "steps = 2, loss = 1.8245487213134766\n",
      "steps = 2, loss = 50.036739349365234\n",
      "steps = 2, loss = 1.6565150022506714\n",
      "steps = 2, loss = 2.5458178520202637\n",
      "steps = 2, loss = 2.7245242595672607\n",
      "steps = 2, loss = 2.2645559310913086\n",
      "steps = 2, loss = 2.0196475982666016\n",
      "steps = 2, loss = 3.1789517402648926\n",
      "steps = 2, loss = 2.097236156463623\n",
      "steps = 2, loss = 2.1321523189544678\n",
      "steps = 2, loss = 2.085528612136841\n",
      "steps = 2, loss = 2.544827938079834\n",
      "steps = 2, loss = 49.962135314941406\n",
      "steps = 2, loss = 2.0630433559417725\n",
      "steps = 2, loss = 2.941762685775757\n",
      "steps = 2, loss = 1.567878007888794\n",
      "steps = 2, loss = 2.541213035583496\n",
      "steps = 2, loss = 2.6607348918914795\n",
      "steps = 2, loss = 3.1983706951141357\n",
      "steps = 2, loss = 2.112300395965576\n",
      "steps = 3, loss = 2.9258668422698975\n",
      "steps = 3, loss = 2.726710796356201\n",
      "steps = 3, loss = 2.587764024734497\n",
      "steps = 3, loss = 2.585850238800049\n",
      "steps = 3, loss = 2.1416432857513428\n",
      "steps = 3, loss = 2.291645050048828\n",
      "steps = 3, loss = 2.1698362827301025\n",
      "steps = 3, loss = 2.2271130084991455\n",
      "steps = 3, loss = 3.5907552242279053\n",
      "steps = 3, loss = 2.052799701690674\n",
      "steps = 3, loss = 3.1649107933044434\n",
      "steps = 3, loss = 2.2712562084198\n",
      "steps = 3, loss = 2.85284686088562\n",
      "steps = 3, loss = 49.97662353515625\n",
      "steps = 3, loss = 1.1263988018035889\n",
      "steps = 3, loss = 2.8274686336517334\n",
      "steps = 3, loss = 49.532691955566406\n",
      "steps = 3, loss = 3.151118040084839\n",
      "steps = 3, loss = 1.9797396659851074\n",
      "steps = 3, loss = 50.0100212097168\n",
      "steps = 3, loss = 2.3559699058532715\n",
      "steps = 3, loss = 1.9426344633102417\n",
      "steps = 3, loss = 2.8911025524139404\n",
      "steps = 3, loss = 2.6873228549957275\n",
      "steps = 3, loss = 2.8376224040985107\n",
      "steps = 3, loss = 2.6422836780548096\n",
      "steps = 3, loss = 2.0145504474639893\n",
      "steps = 3, loss = 2.850067615509033\n",
      "steps = 3, loss = 2.9370408058166504\n",
      "steps = 3, loss = 2.47830867767334\n",
      "steps = 3, loss = 2.8056187629699707\n",
      "steps = 3, loss = 2.872300863265991\n",
      "steps = 3, loss = 2.725950002670288\n",
      "steps = 3, loss = 2.513045072555542\n",
      "steps = 3, loss = 1.7169644832611084\n",
      "steps = 3, loss = 1.509967565536499\n",
      "steps = 3, loss = 13.854316711425781\n",
      "steps = 3, loss = 1.6941646337509155\n",
      "steps = 3, loss = 2.331071615219116\n",
      "steps = 3, loss = 1.624025821685791\n",
      "steps = 3, loss = 2.028019428253174\n",
      "steps = 3, loss = 2.603590726852417\n",
      "steps = 3, loss = 2.1388094425201416\n",
      "steps = 3, loss = 3.274916410446167\n",
      "steps = 3, loss = 2.0333786010742188\n",
      "steps = 3, loss = 2.4347360134124756\n",
      "steps = 3, loss = 50.036739349365234\n",
      "steps = 3, loss = 49.9510612487793\n",
      "steps = 3, loss = 2.607816219329834\n",
      "steps = 3, loss = 2.846242904663086\n",
      "steps = 3, loss = 1.8896430730819702\n",
      "steps = 3, loss = 4.610028266906738\n",
      "steps = 3, loss = 2.4677529335021973\n",
      "steps = 3, loss = 2.621941089630127\n",
      "steps = 3, loss = 2.8571794033050537\n",
      "steps = 3, loss = 3.1596429347991943\n",
      "steps = 3, loss = 2.0937585830688477\n",
      "steps = 3, loss = 2.208881378173828\n",
      "steps = 3, loss = 2.4037187099456787\n",
      "steps = 3, loss = 2.2038514614105225\n",
      "steps = 3, loss = 5.7373833656311035\n",
      "steps = 3, loss = 1.7933027744293213\n",
      "steps = 3, loss = 2.418419599533081\n",
      "steps = 3, loss = 2.4498372077941895\n",
      "steps = 4, loss = 2.4694981575012207\n",
      "steps = 4, loss = 2.0144546031951904\n",
      "steps = 4, loss = 1.6784296035766602\n",
      "steps = 4, loss = 49.947364807128906\n",
      "steps = 4, loss = 1.953586220741272\n",
      "steps = 4, loss = 2.9902093410491943\n",
      "steps = 4, loss = 2.1602401733398438\n",
      "steps = 4, loss = 2.9690802097320557\n",
      "steps = 4, loss = 2.397367000579834\n",
      "steps = 4, loss = 2.695958137512207\n",
      "steps = 4, loss = 3.160614252090454\n",
      "steps = 4, loss = 2.6865437030792236\n",
      "steps = 4, loss = 4.032916069030762\n",
      "steps = 4, loss = 2.978248119354248\n",
      "steps = 4, loss = 2.760654926300049\n",
      "steps = 4, loss = 2.6516613960266113\n",
      "steps = 4, loss = 1.4135137796401978\n",
      "steps = 4, loss = 3.750169277191162\n",
      "steps = 4, loss = 2.0603785514831543\n",
      "steps = 4, loss = 2.405555486679077\n",
      "steps = 4, loss = 3.024423837661743\n",
      "steps = 4, loss = 2.805875062942505\n",
      "steps = 4, loss = 2.279665231704712\n",
      "steps = 4, loss = 3.319690704345703\n",
      "steps = 4, loss = 1.8248192071914673\n",
      "steps = 4, loss = 2.4530248641967773\n",
      "steps = 4, loss = 2.7388932704925537\n",
      "steps = 4, loss = 2.71218204498291\n",
      "steps = 4, loss = 2.4067699909210205\n",
      "steps = 4, loss = 2.858050584793091\n",
      "steps = 4, loss = 49.953208923339844\n",
      "steps = 4, loss = 2.598263740539551\n",
      "steps = 4, loss = 2.093824863433838\n",
      "steps = 4, loss = 2.3316378593444824\n",
      "steps = 4, loss = 3.159813404083252\n",
      "steps = 4, loss = 3.0671088695526123\n",
      "steps = 4, loss = 2.99273419380188\n",
      "steps = 4, loss = 2.928272008895874\n",
      "steps = 4, loss = 7.276143550872803\n",
      "steps = 4, loss = 2.563037395477295\n",
      "steps = 4, loss = 2.9608850479125977\n",
      "steps = 4, loss = 5.693477153778076\n",
      "steps = 4, loss = 2.601498603820801\n",
      "steps = 4, loss = 2.9064090251922607\n",
      "steps = 4, loss = 2.534970283508301\n",
      "steps = 4, loss = 1.885491967201233\n",
      "steps = 4, loss = 2.164395332336426\n",
      "steps = 4, loss = 5.480606555938721\n",
      "steps = 4, loss = 2.7104170322418213\n",
      "steps = 4, loss = 2.745028495788574\n",
      "steps = 4, loss = 1.814347505569458\n",
      "steps = 4, loss = 2.2889249324798584\n",
      "steps = 4, loss = 2.5574262142181396\n",
      "steps = 4, loss = 49.831085205078125\n",
      "steps = 4, loss = 50.0100212097168\n",
      "steps = 4, loss = 2.356247663497925\n",
      "steps = 4, loss = 3.1664538383483887\n",
      "steps = 4, loss = 2.9083333015441895\n",
      "steps = 4, loss = 50.036739349365234\n",
      "steps = 4, loss = 2.8198089599609375\n",
      "steps = 4, loss = 2.1003880500793457\n",
      "steps = 4, loss = 1.747594952583313\n",
      "steps = 4, loss = 2.5422027111053467\n",
      "steps = 4, loss = 2.1579601764678955\n",
      "steps = 5, loss = 4.383218288421631\n",
      "steps = 5, loss = 3.0920348167419434\n",
      "steps = 5, loss = 3.1342883110046387\n",
      "steps = 5, loss = 2.9817705154418945\n",
      "steps = 5, loss = 2.6335935592651367\n",
      "steps = 5, loss = 2.89444899559021\n",
      "steps = 5, loss = 1.8684386014938354\n",
      "steps = 5, loss = 2.455489158630371\n",
      "steps = 5, loss = 2.879889965057373\n",
      "steps = 5, loss = 3.522576093673706\n",
      "steps = 5, loss = 2.8823082447052\n",
      "steps = 5, loss = 1.5596777200698853\n",
      "steps = 5, loss = 2.172886371612549\n",
      "steps = 5, loss = 1.931275486946106\n",
      "steps = 5, loss = 2.845160722732544\n",
      "steps = 5, loss = 3.277207136154175\n",
      "steps = 5, loss = 49.9449577331543\n",
      "steps = 5, loss = 49.9427375793457\n",
      "steps = 5, loss = 3.034292221069336\n",
      "steps = 5, loss = 2.07060170173645\n",
      "steps = 5, loss = 2.8456039428710938\n",
      "steps = 5, loss = 2.502384662628174\n",
      "steps = 5, loss = 3.169517755508423\n",
      "steps = 5, loss = 2.250290870666504\n",
      "steps = 5, loss = 2.0835399627685547\n",
      "steps = 5, loss = 2.5193536281585693\n",
      "steps = 5, loss = 2.056832790374756\n",
      "steps = 5, loss = 2.107062578201294\n",
      "steps = 5, loss = 2.4872283935546875\n",
      "steps = 5, loss = 4.14890718460083\n",
      "steps = 5, loss = 2.5566654205322266\n",
      "steps = 5, loss = 2.3224105834960938\n",
      "steps = 5, loss = 50.011260986328125\n",
      "steps = 5, loss = 2.405390977859497\n",
      "steps = 5, loss = 5.650601863861084\n",
      "steps = 5, loss = 2.9370272159576416\n",
      "steps = 5, loss = 3.189788341522217\n",
      "steps = 5, loss = 2.083397626876831\n",
      "steps = 5, loss = 1.764374852180481\n",
      "steps = 5, loss = 2.9533584117889404\n",
      "steps = 5, loss = 48.54106140136719\n",
      "steps = 5, loss = 2.330925226211548\n",
      "steps = 5, loss = 50.036739349365234\n",
      "steps = 5, loss = 2.6933841705322266\n",
      "steps = 5, loss = 2.678727149963379\n",
      "steps = 5, loss = 2.3965399265289307\n",
      "steps = 5, loss = 1.8137085437774658\n",
      "steps = 5, loss = 2.609670400619507\n",
      "steps = 5, loss = 1.8936811685562134\n",
      "steps = 5, loss = 2.73193097114563\n",
      "steps = 5, loss = 3.045558452606201\n",
      "steps = 5, loss = 3.032932996749878\n",
      "steps = 5, loss = 2.7136075496673584\n",
      "steps = 5, loss = 2.8923158645629883\n",
      "steps = 5, loss = 2.8341822624206543\n",
      "steps = 5, loss = 2.7491846084594727\n",
      "steps = 5, loss = 2.751976728439331\n",
      "steps = 5, loss = 3.2195093631744385\n",
      "steps = 5, loss = 2.5140295028686523\n",
      "steps = 5, loss = 2.48052716255188\n",
      "steps = 5, loss = 2.720104217529297\n",
      "steps = 5, loss = 1.957813024520874\n",
      "steps = 5, loss = 2.664680004119873\n",
      "steps = 5, loss = 5.9155144691467285\n",
      "steps = 6, loss = 1.7239148616790771\n",
      "steps = 6, loss = 3.8300960063934326\n",
      "steps = 6, loss = 2.6231491565704346\n",
      "steps = 6, loss = 3.0548861026763916\n",
      "steps = 6, loss = 49.85740280151367\n",
      "steps = 6, loss = 2.8950624465942383\n",
      "steps = 6, loss = 2.4630165100097656\n",
      "steps = 6, loss = 3.181989908218384\n",
      "steps = 6, loss = 3.295133113861084\n",
      "steps = 6, loss = 5.804987907409668\n",
      "steps = 6, loss = 1.966200828552246\n",
      "steps = 6, loss = 2.7608180046081543\n",
      "steps = 6, loss = 1.8567543029785156\n",
      "steps = 6, loss = 2.358018398284912\n",
      "steps = 6, loss = 2.942483901977539\n",
      "steps = 6, loss = 2.964445114135742\n",
      "steps = 6, loss = 2.515049457550049\n",
      "steps = 6, loss = 2.744614839553833\n",
      "steps = 6, loss = 3.1965410709381104\n",
      "steps = 6, loss = 1.9323058128356934\n",
      "steps = 6, loss = 1.9048219919204712\n",
      "steps = 6, loss = 49.94874572753906\n",
      "steps = 6, loss = 2.5481255054473877\n",
      "steps = 6, loss = 2.0801732540130615\n",
      "steps = 6, loss = 2.0467793941497803\n",
      "steps = 6, loss = 50.036739349365234\n",
      "steps = 6, loss = 3.055478811264038\n",
      "steps = 6, loss = 3.0478553771972656\n",
      "steps = 6, loss = 2.5046257972717285\n",
      "steps = 6, loss = 2.6314949989318848\n",
      "steps = 6, loss = 2.9045932292938232\n",
      "steps = 6, loss = 1.6419484615325928\n",
      "steps = 6, loss = 2.931382894515991\n",
      "steps = 6, loss = 1.963150143623352\n",
      "steps = 6, loss = 2.9399452209472656\n",
      "steps = 6, loss = 2.7374625205993652\n",
      "steps = 6, loss = 2.463581085205078\n",
      "steps = 6, loss = 49.97129440307617\n",
      "steps = 6, loss = 2.11822509765625\n",
      "steps = 6, loss = 2.320018768310547\n",
      "steps = 6, loss = 2.8052401542663574\n",
      "steps = 6, loss = 5.589700222015381\n",
      "steps = 6, loss = 3.161669969558716\n",
      "steps = 6, loss = 3.1354129314422607\n",
      "steps = 6, loss = 3.0889511108398438\n",
      "steps = 6, loss = 1.8170503377914429\n",
      "steps = 6, loss = 3.8256165981292725\n",
      "steps = 6, loss = 2.593254566192627\n",
      "steps = 6, loss = 2.4353652000427246\n",
      "steps = 6, loss = 2.4056875705718994\n",
      "steps = 6, loss = 2.6009256839752197\n",
      "steps = 6, loss = 3.064809799194336\n",
      "steps = 6, loss = 2.764910936355591\n",
      "steps = 6, loss = 3.0937368869781494\n",
      "steps = 6, loss = 2.926847457885742\n",
      "steps = 6, loss = 3.295287847518921\n",
      "steps = 6, loss = 2.5396697521209717\n",
      "steps = 6, loss = 2.0496459007263184\n",
      "steps = 6, loss = 49.75054931640625\n",
      "steps = 6, loss = 2.8532700538635254\n",
      "steps = 6, loss = 2.755390167236328\n",
      "steps = 6, loss = 2.047325372695923\n",
      "steps = 6, loss = 2.54097843170166\n",
      "steps = 6, loss = 3.018305778503418\n",
      "steps = 7, loss = 2.1198713779449463\n",
      "steps = 7, loss = 2.989314317703247\n",
      "steps = 7, loss = 2.410959243774414\n",
      "steps = 7, loss = 3.03246808052063\n",
      "steps = 7, loss = 2.4817240238189697\n",
      "steps = 7, loss = 2.928492784500122\n",
      "steps = 7, loss = 2.436609983444214\n",
      "steps = 7, loss = 1.7034143209457397\n",
      "steps = 7, loss = 1.8610094785690308\n",
      "steps = 7, loss = 3.4231486320495605\n",
      "steps = 7, loss = 2.5979976654052734\n",
      "steps = 7, loss = 3.072195291519165\n",
      "steps = 7, loss = 2.9077205657958984\n",
      "steps = 7, loss = 2.9372754096984863\n",
      "steps = 7, loss = 1.9826173782348633\n",
      "steps = 7, loss = 3.1065986156463623\n",
      "steps = 7, loss = 3.095618724822998\n",
      "steps = 7, loss = 2.63750958442688\n",
      "steps = 7, loss = 2.837618589401245\n",
      "steps = 7, loss = 3.2269484996795654\n",
      "steps = 7, loss = 2.90630841255188\n",
      "steps = 7, loss = 3.8130624294281006\n",
      "steps = 7, loss = 2.5964651107788086\n",
      "steps = 7, loss = 2.6858069896698\n",
      "steps = 7, loss = 2.3830955028533936\n",
      "steps = 7, loss = 2.5575859546661377\n",
      "steps = 7, loss = 2.396624803543091\n",
      "steps = 7, loss = 2.914276361465454\n",
      "steps = 7, loss = 1.9699925184249878\n",
      "steps = 7, loss = 2.8171956539154053\n",
      "steps = 7, loss = 2.758988618850708\n",
      "steps = 7, loss = 3.1534368991851807\n",
      "steps = 7, loss = 5.893864154815674\n",
      "steps = 7, loss = 6.472270965576172\n",
      "steps = 7, loss = 3.168163776397705\n",
      "steps = 7, loss = 2.8911824226379395\n",
      "steps = 7, loss = 49.09409713745117\n",
      "steps = 7, loss = 3.262685537338257\n",
      "steps = 7, loss = 3.1103086471557617\n",
      "steps = 7, loss = 2.0853781700134277\n",
      "steps = 7, loss = 49.92168045043945\n",
      "steps = 7, loss = 3.192920684814453\n",
      "steps = 7, loss = 2.070594549179077\n",
      "steps = 7, loss = 2.606874465942383\n",
      "steps = 7, loss = 1.9488083124160767\n",
      "steps = 7, loss = 2.3933544158935547\n",
      "steps = 7, loss = 2.906886339187622\n",
      "steps = 7, loss = 3.91447114944458\n",
      "steps = 7, loss = 50.036739349365234\n",
      "steps = 7, loss = 1.997374415397644\n",
      "steps = 7, loss = 2.60734224319458\n",
      "steps = 7, loss = 2.6841256618499756\n",
      "steps = 7, loss = 49.345703125\n",
      "steps = 7, loss = 2.148293972015381\n",
      "steps = 7, loss = 3.021738290786743\n",
      "steps = 7, loss = 49.99876403808594\n",
      "steps = 7, loss = 2.049299716949463\n",
      "steps = 7, loss = 1.9390958547592163\n",
      "steps = 7, loss = 2.7938592433929443\n",
      "steps = 7, loss = 2.6262118816375732\n",
      "steps = 7, loss = 1.8940755128860474\n",
      "steps = 7, loss = 3.1649179458618164\n",
      "steps = 7, loss = 2.6838951110839844\n",
      "steps = 7, loss = 2.9811019897460938\n",
      "steps = 8, loss = 3.2330210208892822\n",
      "steps = 8, loss = 3.1541600227355957\n",
      "steps = 8, loss = 3.0632617473602295\n",
      "steps = 8, loss = 3.3971874713897705\n",
      "steps = 8, loss = 2.3098080158233643\n",
      "steps = 8, loss = 3.377166986465454\n",
      "steps = 8, loss = 2.7912209033966064\n",
      "steps = 8, loss = 2.188209056854248\n",
      "steps = 8, loss = 2.4804868698120117\n",
      "steps = 8, loss = 2.676234483718872\n",
      "steps = 8, loss = 1.9415149688720703\n",
      "steps = 8, loss = 3.0477285385131836\n",
      "steps = 8, loss = 3.2522175312042236\n",
      "steps = 8, loss = 2.7810182571411133\n",
      "steps = 8, loss = 2.7944223880767822\n",
      "steps = 8, loss = 2.8372581005096436\n",
      "steps = 8, loss = 3.0952017307281494\n",
      "steps = 8, loss = 2.7888786792755127\n",
      "steps = 8, loss = 3.1737418174743652\n",
      "steps = 8, loss = 3.022827625274658\n",
      "steps = 8, loss = 3.6128993034362793\n",
      "steps = 8, loss = 2.6370527744293213\n",
      "steps = 8, loss = 2.9692914485931396\n",
      "steps = 8, loss = 3.190960645675659\n",
      "steps = 8, loss = 2.930349588394165\n",
      "steps = 8, loss = 2.9580395221710205\n",
      "steps = 8, loss = 2.5872578620910645\n",
      "steps = 8, loss = 2.0355758666992188\n",
      "steps = 8, loss = 1.7669025659561157\n",
      "steps = 8, loss = 2.188399314880371\n",
      "steps = 8, loss = 1.9139695167541504\n",
      "steps = 8, loss = 2.9539589881896973\n",
      "steps = 8, loss = 2.4382412433624268\n",
      "steps = 8, loss = 3.658825635910034\n",
      "steps = 8, loss = 2.839869260787964\n",
      "steps = 8, loss = 49.90791320800781\n",
      "steps = 8, loss = 3.4431793689727783\n",
      "steps = 8, loss = 2.617586612701416\n",
      "steps = 8, loss = 2.758082389831543\n",
      "steps = 8, loss = 2.1982712745666504\n",
      "steps = 8, loss = 49.999324798583984\n",
      "steps = 8, loss = 2.7410359382629395\n",
      "steps = 8, loss = 2.696230888366699\n",
      "steps = 8, loss = 49.75442123413086\n",
      "steps = 8, loss = 4.684375286102295\n",
      "steps = 8, loss = 2.819911003112793\n",
      "steps = 8, loss = 6.730591773986816\n",
      "steps = 8, loss = 2.0910937786102295\n",
      "steps = 8, loss = 2.9908337593078613\n",
      "steps = 8, loss = 3.2587010860443115\n",
      "steps = 8, loss = 2.3943159580230713\n",
      "steps = 8, loss = 2.762404441833496\n",
      "steps = 8, loss = 2.052389144897461\n",
      "steps = 8, loss = 1.9862374067306519\n",
      "steps = 8, loss = 2.022862195968628\n",
      "steps = 8, loss = 2.429152488708496\n",
      "steps = 8, loss = 50.036739349365234\n",
      "steps = 8, loss = 2.8419623374938965\n",
      "steps = 8, loss = 2.1023337841033936\n",
      "steps = 8, loss = 2.9231910705566406\n",
      "steps = 8, loss = 5.2641167640686035\n",
      "steps = 8, loss = 49.9653434753418\n",
      "steps = 8, loss = 3.1428821086883545\n",
      "steps = 8, loss = 3.0777761936187744\n",
      "steps = 9, loss = 2.011751174926758\n",
      "steps = 9, loss = 3.020799398422241\n",
      "steps = 9, loss = 49.459632873535156\n",
      "steps = 9, loss = 3.1850128173828125\n",
      "steps = 9, loss = 2.2357661724090576\n",
      "steps = 9, loss = 49.91965103149414\n",
      "steps = 9, loss = 3.273236036300659\n",
      "steps = 9, loss = 3.0974748134613037\n",
      "steps = 9, loss = 6.879836559295654\n",
      "steps = 9, loss = 2.860933780670166\n",
      "steps = 9, loss = 50.036739349365234\n",
      "steps = 9, loss = 3.0600080490112305\n",
      "steps = 9, loss = 2.480156660079956\n",
      "steps = 9, loss = 1.8047120571136475\n",
      "steps = 9, loss = 3.1602070331573486\n",
      "steps = 9, loss = 2.836198568344116\n",
      "steps = 9, loss = 2.0653269290924072\n",
      "steps = 9, loss = 2.837449312210083\n",
      "steps = 9, loss = 2.4220399856567383\n",
      "steps = 9, loss = 3.035764694213867\n",
      "steps = 9, loss = 3.3704066276550293\n",
      "steps = 9, loss = 2.4148142337799072\n",
      "steps = 9, loss = 2.078747272491455\n",
      "steps = 9, loss = 2.04634690284729\n",
      "steps = 9, loss = 2.9307069778442383\n",
      "steps = 9, loss = 3.2226319313049316\n",
      "steps = 9, loss = 2.5977306365966797\n",
      "steps = 9, loss = 2.8208022117614746\n",
      "steps = 9, loss = 2.4496865272521973\n",
      "steps = 9, loss = 3.1845200061798096\n",
      "steps = 9, loss = 2.7967686653137207\n",
      "steps = 9, loss = 3.028721332550049\n",
      "steps = 9, loss = 48.89390182495117\n",
      "steps = 9, loss = 2.8908917903900146\n",
      "steps = 9, loss = 2.897165536880493\n",
      "steps = 9, loss = 2.5304224491119385\n",
      "steps = 9, loss = 2.643517255783081\n",
      "steps = 9, loss = 3.0658881664276123\n",
      "steps = 9, loss = 2.7920303344726562\n",
      "steps = 9, loss = 1.939810037612915\n",
      "steps = 9, loss = 2.049316883087158\n",
      "steps = 9, loss = 2.8778696060180664\n",
      "steps = 9, loss = 2.6963603496551514\n",
      "steps = 9, loss = 3.6704959869384766\n",
      "steps = 9, loss = 2.3653066158294678\n",
      "steps = 9, loss = 2.8315908908843994\n",
      "steps = 9, loss = 4.371048927307129\n",
      "steps = 9, loss = 6.781729698181152\n",
      "steps = 9, loss = 2.7553720474243164\n",
      "steps = 9, loss = 2.1898598670959473\n",
      "steps = 9, loss = 4.571042537689209\n",
      "steps = 9, loss = 3.071551561355591\n",
      "steps = 9, loss = 3.1792094707489014\n",
      "steps = 9, loss = 1.9640154838562012\n",
      "steps = 9, loss = 3.701719284057617\n",
      "steps = 9, loss = 3.3003368377685547\n",
      "steps = 9, loss = 3.341705322265625\n",
      "steps = 9, loss = 2.1518304347991943\n",
      "steps = 9, loss = 50.000179290771484\n",
      "steps = 9, loss = 3.2116658687591553\n",
      "steps = 9, loss = 2.6349823474884033\n",
      "steps = 9, loss = 2.758265733718872\n",
      "steps = 9, loss = 2.116668224334717\n",
      "steps = 9, loss = 1.8070027828216553\n",
      "steps = 10, loss = 2.6980199813842773\n",
      "steps = 10, loss = 3.110539674758911\n",
      "steps = 10, loss = 3.169064998626709\n",
      "steps = 10, loss = 2.1125504970550537\n",
      "steps = 10, loss = 3.918379545211792\n",
      "steps = 10, loss = 6.914861679077148\n",
      "steps = 10, loss = 49.669376373291016\n",
      "steps = 10, loss = 2.759770631790161\n",
      "steps = 10, loss = 2.9180490970611572\n",
      "steps = 10, loss = 3.202474355697632\n",
      "steps = 10, loss = 2.480602502822876\n",
      "steps = 10, loss = 2.6216189861297607\n",
      "steps = 10, loss = 3.083296060562134\n",
      "steps = 10, loss = 2.437541961669922\n",
      "steps = 10, loss = 2.226630449295044\n",
      "steps = 10, loss = 2.122283458709717\n",
      "steps = 10, loss = 2.92329478263855\n",
      "steps = 10, loss = 2.0781290531158447\n",
      "steps = 10, loss = 2.9565770626068115\n",
      "steps = 10, loss = 49.75401306152344\n",
      "steps = 10, loss = 2.7725679874420166\n",
      "steps = 10, loss = 2.589040756225586\n",
      "steps = 10, loss = 2.4565775394439697\n",
      "steps = 10, loss = 2.866398811340332\n",
      "steps = 10, loss = 2.7155725955963135\n",
      "steps = 10, loss = 3.1598997116088867\n",
      "steps = 10, loss = 1.972366213798523\n",
      "steps = 10, loss = 2.1029815673828125\n",
      "steps = 10, loss = 2.106466054916382\n",
      "steps = 10, loss = 3.2242271900177\n",
      "steps = 10, loss = 2.9232535362243652\n",
      "steps = 10, loss = 1.844709873199463\n",
      "steps = 10, loss = 3.0254173278808594\n",
      "steps = 10, loss = 2.644934892654419\n",
      "steps = 10, loss = 2.0473928451538086\n",
      "steps = 10, loss = 3.0296595096588135\n",
      "steps = 10, loss = 2.999807357788086\n",
      "steps = 10, loss = 2.2895021438598633\n",
      "steps = 10, loss = 3.0233752727508545\n",
      "steps = 10, loss = 1.9944814443588257\n",
      "steps = 10, loss = 3.7354485988616943\n",
      "steps = 10, loss = 2.88006591796875\n",
      "steps = 10, loss = 3.151724338531494\n",
      "steps = 10, loss = 6.928299427032471\n",
      "steps = 10, loss = 3.1971089839935303\n",
      "steps = 10, loss = 3.162609338760376\n",
      "steps = 10, loss = 3.2766177654266357\n",
      "steps = 10, loss = 2.6049227714538574\n",
      "steps = 10, loss = 2.578016757965088\n",
      "steps = 10, loss = 3.3315062522888184\n",
      "steps = 10, loss = 50.0014533996582\n",
      "steps = 10, loss = 2.896130323410034\n",
      "steps = 10, loss = 3.8145813941955566\n",
      "steps = 10, loss = 50.036739349365234\n",
      "steps = 10, loss = 2.755784034729004\n",
      "steps = 10, loss = 3.344480514526367\n",
      "steps = 10, loss = 2.1088528633117676\n",
      "steps = 10, loss = 49.88893127441406\n",
      "steps = 10, loss = 2.7937095165252686\n",
      "steps = 10, loss = 3.1696364879608154\n",
      "steps = 10, loss = 3.061741828918457\n",
      "steps = 10, loss = 3.5276834964752197\n",
      "steps = 10, loss = 3.088775157928467\n",
      "steps = 10, loss = 2.1694962978363037\n",
      "steps = 11, loss = 50.036739349365234\n",
      "steps = 11, loss = 2.8930294513702393\n",
      "steps = 11, loss = 2.1073319911956787\n",
      "steps = 11, loss = 2.120239496231079\n",
      "steps = 11, loss = 3.106269359588623\n",
      "steps = 11, loss = 3.238255500793457\n",
      "steps = 11, loss = 2.7610228061676025\n",
      "steps = 11, loss = 50.00254440307617\n",
      "steps = 11, loss = 1.868908166885376\n",
      "steps = 11, loss = 2.209559917449951\n",
      "steps = 11, loss = 2.0698928833007812\n",
      "steps = 11, loss = 2.564669609069824\n",
      "steps = 11, loss = 3.0213382244110107\n",
      "steps = 11, loss = 49.97789764404297\n",
      "steps = 11, loss = 3.162285089492798\n",
      "steps = 11, loss = 3.459711790084839\n",
      "steps = 11, loss = 3.156646728515625\n",
      "steps = 11, loss = 2.093961238861084\n",
      "steps = 11, loss = 2.0080697536468506\n",
      "steps = 11, loss = 2.551239490509033\n",
      "steps = 11, loss = 3.147284507751465\n",
      "steps = 11, loss = 3.12447452545166\n",
      "steps = 11, loss = 2.9360263347625732\n",
      "steps = 11, loss = 50.00962448120117\n",
      "steps = 11, loss = 7.543130874633789\n",
      "steps = 11, loss = 3.5055880546569824\n",
      "steps = 11, loss = 2.8499574661254883\n",
      "steps = 11, loss = 2.96858549118042\n",
      "steps = 11, loss = 2.625816822052002\n",
      "steps = 11, loss = 3.0102696418762207\n",
      "steps = 11, loss = 3.4621779918670654\n",
      "steps = 11, loss = 2.128906011581421\n",
      "steps = 11, loss = 2.7662110328674316\n",
      "steps = 11, loss = 2.4151253700256348\n",
      "steps = 11, loss = 2.1441526412963867\n",
      "steps = 11, loss = 2.66689395904541\n",
      "steps = 11, loss = 2.3382904529571533\n",
      "steps = 11, loss = 3.7112293243408203\n",
      "steps = 11, loss = 3.0990705490112305\n",
      "steps = 11, loss = 2.8819665908813477\n",
      "steps = 11, loss = 2.9257211685180664\n",
      "steps = 11, loss = 2.6273648738861084\n",
      "steps = 11, loss = 2.4465088844299316\n",
      "steps = 11, loss = 3.2256786823272705\n",
      "steps = 11, loss = 2.8696885108947754\n",
      "steps = 11, loss = 3.01115083694458\n",
      "steps = 11, loss = 3.207770824432373\n",
      "steps = 11, loss = 3.8438148498535156\n",
      "steps = 11, loss = 3.0683505535125732\n",
      "steps = 11, loss = 2.147688388824463\n",
      "steps = 11, loss = 2.1071438789367676\n",
      "steps = 11, loss = 2.527482032775879\n",
      "steps = 11, loss = 1.9882382154464722\n",
      "steps = 11, loss = 2.8749852180480957\n",
      "steps = 11, loss = 3.172178268432617\n",
      "steps = 11, loss = 3.307800054550171\n",
      "steps = 11, loss = 2.728090763092041\n",
      "steps = 11, loss = 49.892147064208984\n",
      "steps = 11, loss = 2.6382389068603516\n",
      "steps = 11, loss = 2.8200905323028564\n",
      "steps = 11, loss = 3.122164249420166\n",
      "steps = 11, loss = 2.812087297439575\n",
      "steps = 11, loss = 2.6382570266723633\n",
      "steps = 11, loss = 7.55438232421875\n",
      "steps = 12, loss = 2.6156373023986816\n",
      "steps = 12, loss = 2.920485258102417\n",
      "steps = 12, loss = 2.064652681350708\n",
      "steps = 12, loss = 2.168567657470703\n",
      "steps = 12, loss = 50.00226593017578\n",
      "steps = 12, loss = 48.877864837646484\n",
      "steps = 12, loss = 3.0856223106384277\n",
      "steps = 12, loss = 2.155838966369629\n",
      "steps = 12, loss = 2.6733150482177734\n",
      "steps = 12, loss = 3.121190309524536\n",
      "steps = 12, loss = 2.216522455215454\n",
      "steps = 12, loss = 2.410794973373413\n",
      "steps = 12, loss = 2.627030611038208\n",
      "steps = 12, loss = 3.4646363258361816\n",
      "steps = 12, loss = 2.453464984893799\n",
      "steps = 12, loss = 2.098299741744995\n",
      "steps = 12, loss = 2.995795249938965\n",
      "steps = 12, loss = 2.8579459190368652\n",
      "steps = 12, loss = 3.338535785675049\n",
      "steps = 12, loss = 2.6159908771514893\n",
      "steps = 12, loss = 2.62654447555542\n",
      "steps = 12, loss = 6.932652950286865\n",
      "steps = 12, loss = 3.0192694664001465\n",
      "steps = 12, loss = 2.952026844024658\n",
      "steps = 12, loss = 2.5512654781341553\n",
      "steps = 12, loss = 2.071314811706543\n",
      "steps = 12, loss = 3.070513963699341\n",
      "steps = 12, loss = 3.378554105758667\n",
      "steps = 12, loss = 50.036739349365234\n",
      "steps = 12, loss = 2.6518120765686035\n",
      "steps = 12, loss = 49.974552154541016\n",
      "steps = 12, loss = 2.893319845199585\n",
      "steps = 12, loss = 49.868160247802734\n",
      "steps = 12, loss = 3.2013895511627197\n",
      "steps = 12, loss = 3.8509066104888916\n",
      "steps = 12, loss = 7.276576042175293\n",
      "steps = 12, loss = 2.9230284690856934\n",
      "steps = 12, loss = 2.97678542137146\n",
      "steps = 12, loss = 3.361009120941162\n",
      "steps = 12, loss = 2.706749677658081\n",
      "steps = 12, loss = 3.1744251251220703\n",
      "steps = 12, loss = 1.8836495876312256\n",
      "steps = 12, loss = 3.1573407649993896\n",
      "steps = 12, loss = 2.4650211334228516\n",
      "steps = 12, loss = 3.346107006072998\n",
      "steps = 12, loss = 3.0908594131469727\n",
      "steps = 12, loss = 2.9749743938446045\n",
      "steps = 12, loss = 3.068739414215088\n",
      "steps = 12, loss = 1.9960957765579224\n",
      "steps = 12, loss = 3.2576444149017334\n",
      "steps = 12, loss = 3.144388437271118\n",
      "steps = 12, loss = 2.759507656097412\n",
      "steps = 12, loss = 3.061446189880371\n",
      "steps = 12, loss = 2.01513409614563\n",
      "steps = 12, loss = 2.6832640171051025\n",
      "steps = 12, loss = 2.154680013656616\n",
      "steps = 12, loss = 3.595219373703003\n",
      "steps = 12, loss = 2.945554494857788\n",
      "steps = 12, loss = 3.2825753688812256\n",
      "steps = 12, loss = 2.08760929107666\n",
      "steps = 12, loss = 2.4461028575897217\n",
      "steps = 12, loss = 2.2674005031585693\n",
      "steps = 12, loss = 2.4467105865478516\n",
      "steps = 12, loss = 2.3656511306762695\n",
      "steps = 13, loss = 2.2001583576202393\n",
      "steps = 13, loss = 2.0103936195373535\n",
      "steps = 13, loss = 3.0829248428344727\n",
      "steps = 13, loss = 2.587580919265747\n",
      "steps = 13, loss = 2.0697529315948486\n",
      "steps = 13, loss = 2.23934006690979\n",
      "steps = 13, loss = 2.6798593997955322\n",
      "steps = 13, loss = 3.046308994293213\n",
      "steps = 13, loss = 3.1374926567077637\n",
      "steps = 13, loss = 2.1918516159057617\n",
      "steps = 13, loss = 2.693079948425293\n",
      "steps = 13, loss = 2.86788272857666\n",
      "steps = 13, loss = 2.687169075012207\n",
      "steps = 13, loss = 49.85222625732422\n",
      "steps = 13, loss = 2.413248300552368\n",
      "steps = 13, loss = 3.0287163257598877\n",
      "steps = 13, loss = 2.203371286392212\n",
      "steps = 13, loss = 2.897049903869629\n",
      "steps = 13, loss = 2.6210100650787354\n",
      "steps = 13, loss = 49.965484619140625\n",
      "steps = 13, loss = 2.6304550170898438\n",
      "steps = 13, loss = 3.264091968536377\n",
      "steps = 13, loss = 2.7547481060028076\n",
      "steps = 13, loss = 2.5715973377227783\n",
      "steps = 13, loss = 2.2229292392730713\n",
      "steps = 13, loss = 49.83757781982422\n",
      "steps = 13, loss = 3.1987826824188232\n",
      "steps = 13, loss = 2.1146445274353027\n",
      "steps = 13, loss = 2.6629505157470703\n",
      "steps = 13, loss = 3.275583505630493\n",
      "steps = 13, loss = 3.345207929611206\n",
      "steps = 13, loss = 1.901581048965454\n",
      "steps = 13, loss = 3.1043171882629395\n",
      "steps = 13, loss = 2.893310546875\n",
      "steps = 13, loss = 2.6228854656219482\n",
      "steps = 13, loss = 2.1128594875335693\n",
      "steps = 13, loss = 3.00347900390625\n",
      "steps = 13, loss = 2.746765375137329\n",
      "steps = 13, loss = 2.3845314979553223\n",
      "steps = 13, loss = 2.4114158153533936\n",
      "steps = 13, loss = 3.4167022705078125\n",
      "steps = 13, loss = 2.9290623664855957\n",
      "steps = 13, loss = 3.7467775344848633\n",
      "steps = 13, loss = 50.036739349365234\n",
      "steps = 13, loss = 2.072127103805542\n",
      "steps = 13, loss = 3.511394500732422\n",
      "steps = 13, loss = 3.4421229362487793\n",
      "steps = 13, loss = 3.1024177074432373\n",
      "steps = 13, loss = 2.0269265174865723\n",
      "steps = 13, loss = 2.8653621673583984\n",
      "steps = 13, loss = 2.77781343460083\n",
      "steps = 13, loss = 3.230165481567383\n",
      "steps = 13, loss = 49.982826232910156\n",
      "steps = 13, loss = 3.04384446144104\n",
      "steps = 13, loss = 7.137396335601807\n",
      "steps = 13, loss = 3.2229769229888916\n",
      "steps = 13, loss = 3.8976967334747314\n",
      "steps = 13, loss = 2.7921581268310547\n",
      "steps = 13, loss = 2.453495740890503\n",
      "steps = 13, loss = 7.023890495300293\n",
      "steps = 13, loss = 2.5805349349975586\n",
      "steps = 13, loss = 3.3617796897888184\n",
      "steps = 13, loss = 3.070087432861328\n",
      "steps = 13, loss = 3.1122069358825684\n",
      "steps = 14, loss = 2.691815137863159\n",
      "steps = 14, loss = 50.00356674194336\n",
      "steps = 14, loss = 3.1744813919067383\n",
      "steps = 14, loss = 3.514740467071533\n",
      "steps = 14, loss = 50.036739349365234\n",
      "steps = 14, loss = 2.7421298027038574\n",
      "steps = 14, loss = 3.0777833461761475\n",
      "steps = 14, loss = 2.6569392681121826\n",
      "steps = 14, loss = 3.146761655807495\n",
      "steps = 14, loss = 3.1182775497436523\n",
      "steps = 14, loss = 7.809165954589844\n",
      "steps = 14, loss = 2.648944854736328\n",
      "steps = 14, loss = 49.98196792602539\n",
      "steps = 14, loss = 3.2373573780059814\n",
      "steps = 14, loss = 3.299175262451172\n",
      "steps = 14, loss = 2.2559311389923096\n",
      "steps = 14, loss = 2.1555659770965576\n",
      "steps = 14, loss = 2.6963350772857666\n",
      "steps = 14, loss = 3.4275388717651367\n",
      "steps = 14, loss = 2.104400157928467\n",
      "steps = 14, loss = 2.252717971801758\n",
      "steps = 14, loss = 3.552546501159668\n",
      "steps = 14, loss = 1.9337289333343506\n",
      "steps = 14, loss = 3.532111644744873\n",
      "steps = 14, loss = 49.95450210571289\n",
      "steps = 14, loss = 3.5031309127807617\n",
      "steps = 14, loss = 3.380937337875366\n",
      "steps = 14, loss = 3.3427979946136475\n",
      "steps = 14, loss = 2.4388363361358643\n",
      "steps = 14, loss = 3.1465888023376465\n",
      "steps = 14, loss = 2.156796455383301\n",
      "steps = 14, loss = 7.084455966949463\n",
      "steps = 14, loss = 2.2541139125823975\n",
      "steps = 14, loss = 2.2994325160980225\n",
      "steps = 14, loss = 7.555584907531738\n",
      "steps = 14, loss = 2.7911884784698486\n",
      "steps = 14, loss = 2.1979284286499023\n",
      "steps = 14, loss = 2.769733190536499\n",
      "steps = 14, loss = 3.151642084121704\n",
      "steps = 14, loss = 3.359208106994629\n",
      "steps = 14, loss = 2.5361876487731934\n",
      "steps = 14, loss = 3.204901695251465\n",
      "steps = 14, loss = 2.7686476707458496\n",
      "steps = 14, loss = 2.7272579669952393\n",
      "steps = 14, loss = 3.995837450027466\n",
      "steps = 14, loss = 2.857020616531372\n",
      "steps = 14, loss = 49.80304718017578\n",
      "steps = 14, loss = 2.058483362197876\n",
      "steps = 14, loss = 2.0435588359832764\n",
      "steps = 14, loss = 2.907756805419922\n",
      "steps = 14, loss = 2.8499114513397217\n",
      "steps = 14, loss = 2.9600226879119873\n",
      "steps = 14, loss = 3.103364944458008\n",
      "steps = 14, loss = 2.891838550567627\n",
      "steps = 14, loss = 3.153437376022339\n",
      "steps = 14, loss = 2.055575370788574\n",
      "steps = 14, loss = 2.479896306991577\n",
      "steps = 14, loss = 2.7405333518981934\n",
      "steps = 14, loss = 2.6707184314727783\n",
      "steps = 14, loss = 2.4845194816589355\n",
      "steps = 14, loss = 2.7390339374542236\n",
      "steps = 14, loss = 3.1575703620910645\n",
      "steps = 14, loss = 3.1789302825927734\n",
      "steps = 14, loss = 3.5857722759246826\n",
      "steps = 15, loss = 7.213588237762451\n",
      "steps = 15, loss = 49.98196792602539\n",
      "steps = 15, loss = 2.5282490253448486\n",
      "steps = 15, loss = 49.97734832763672\n",
      "steps = 15, loss = 2.675689458847046\n",
      "steps = 15, loss = 2.7353525161743164\n",
      "steps = 15, loss = 2.365933656692505\n",
      "steps = 15, loss = 2.122493267059326\n",
      "steps = 15, loss = 2.8767127990722656\n",
      "steps = 15, loss = 3.0650415420532227\n",
      "steps = 15, loss = 50.036739349365234\n",
      "steps = 15, loss = 2.9633522033691406\n",
      "steps = 15, loss = 2.993163585662842\n",
      "steps = 15, loss = 3.9916210174560547\n",
      "steps = 15, loss = 2.7644777297973633\n",
      "steps = 15, loss = 3.2730343341827393\n",
      "steps = 15, loss = 3.1406421661376953\n",
      "steps = 15, loss = 3.210216522216797\n",
      "steps = 15, loss = 2.872053623199463\n",
      "steps = 15, loss = 2.538053035736084\n",
      "steps = 15, loss = 1.9487172365188599\n",
      "steps = 15, loss = 3.514000177383423\n",
      "steps = 15, loss = 2.0560717582702637\n",
      "steps = 15, loss = 7.535501956939697\n",
      "steps = 15, loss = 3.2329163551330566\n",
      "steps = 15, loss = 49.86967468261719\n",
      "steps = 15, loss = 3.279083728790283\n",
      "steps = 15, loss = 2.125365734100342\n",
      "steps = 15, loss = 3.1355860233306885\n",
      "steps = 15, loss = 2.6966118812561035\n",
      "steps = 15, loss = 3.113422155380249\n",
      "steps = 15, loss = 2.8853089809417725\n",
      "steps = 15, loss = 2.2855007648468018\n",
      "steps = 15, loss = 2.1315255165100098\n",
      "steps = 15, loss = 2.064340114593506\n",
      "steps = 15, loss = 2.6356801986694336\n",
      "steps = 15, loss = 2.847179651260376\n",
      "steps = 15, loss = 3.364198684692383\n",
      "steps = 15, loss = 3.114820957183838\n",
      "steps = 15, loss = 3.5539357662200928\n",
      "steps = 15, loss = 2.8151588439941406\n",
      "steps = 15, loss = 6.396501541137695\n",
      "steps = 15, loss = 2.4687719345092773\n",
      "steps = 15, loss = 1.9993152618408203\n",
      "steps = 15, loss = 2.55234694480896\n",
      "steps = 15, loss = 2.2708451747894287\n",
      "steps = 15, loss = 2.4168248176574707\n",
      "steps = 15, loss = 2.7942848205566406\n",
      "steps = 15, loss = 2.6951420307159424\n",
      "steps = 15, loss = 3.268436908721924\n",
      "steps = 15, loss = 49.78529739379883\n",
      "steps = 15, loss = 3.129981517791748\n",
      "steps = 15, loss = 2.181177854537964\n",
      "steps = 15, loss = 2.2282588481903076\n",
      "steps = 15, loss = 3.4244978427886963\n",
      "steps = 15, loss = 2.1758506298065186\n",
      "steps = 15, loss = 3.3800268173217773\n",
      "steps = 15, loss = 3.483690023422241\n",
      "steps = 15, loss = 2.5794317722320557\n",
      "steps = 15, loss = 3.5741517543792725\n",
      "steps = 15, loss = 3.1445398330688477\n",
      "steps = 15, loss = 3.1487324237823486\n",
      "steps = 15, loss = 3.1956734657287598\n",
      "steps = 15, loss = 3.1773178577423096\n",
      "steps = 16, loss = 2.573875665664673\n",
      "steps = 16, loss = 2.4375314712524414\n",
      "steps = 16, loss = 3.104529619216919\n",
      "steps = 16, loss = 3.24696946144104\n",
      "steps = 16, loss = 2.493403434753418\n",
      "steps = 16, loss = 2.258352756500244\n",
      "steps = 16, loss = 7.692595481872559\n",
      "steps = 16, loss = 2.079214334487915\n",
      "steps = 16, loss = 3.2687649726867676\n",
      "steps = 16, loss = 3.1560850143432617\n",
      "steps = 16, loss = 3.4664480686187744\n",
      "steps = 16, loss = 3.269463539123535\n",
      "steps = 16, loss = 49.340572357177734\n",
      "steps = 16, loss = 2.3251423835754395\n",
      "steps = 16, loss = 2.7101728916168213\n",
      "steps = 16, loss = 3.154542922973633\n",
      "steps = 16, loss = 49.98353958129883\n",
      "steps = 16, loss = 2.7514021396636963\n",
      "steps = 16, loss = 2.811779499053955\n",
      "steps = 16, loss = 3.1968953609466553\n",
      "steps = 16, loss = 1.9701794385910034\n",
      "steps = 16, loss = 49.875301361083984\n",
      "steps = 16, loss = 50.036739349365234\n",
      "steps = 16, loss = 3.4208333492279053\n",
      "steps = 16, loss = 3.0324864387512207\n",
      "steps = 16, loss = 2.0488200187683105\n",
      "steps = 16, loss = 2.524462938308716\n",
      "steps = 16, loss = 3.413696050643921\n",
      "steps = 16, loss = 3.4401113986968994\n",
      "steps = 16, loss = 3.5613439083099365\n",
      "steps = 16, loss = 7.5223307609558105\n",
      "steps = 16, loss = 2.7355334758758545\n",
      "steps = 16, loss = 2.083789110183716\n",
      "steps = 16, loss = 2.328460931777954\n",
      "steps = 16, loss = 2.5112760066986084\n",
      "steps = 16, loss = 2.8976938724517822\n",
      "steps = 16, loss = 2.8322925567626953\n",
      "steps = 16, loss = 2.731174945831299\n",
      "steps = 16, loss = 2.2155139446258545\n",
      "steps = 16, loss = 49.764652252197266\n",
      "steps = 16, loss = 4.047629356384277\n",
      "steps = 16, loss = 2.1040542125701904\n",
      "steps = 16, loss = 3.1996426582336426\n",
      "steps = 16, loss = 2.214764356613159\n",
      "steps = 16, loss = 3.0946099758148193\n",
      "steps = 16, loss = 2.7412326335906982\n",
      "steps = 16, loss = 3.230320930480957\n",
      "steps = 16, loss = 3.2970187664031982\n",
      "steps = 16, loss = 2.7999279499053955\n",
      "steps = 16, loss = 7.845466613769531\n",
      "steps = 16, loss = 2.7335264682769775\n",
      "steps = 16, loss = 2.9630441665649414\n",
      "steps = 16, loss = 2.8610239028930664\n",
      "steps = 16, loss = 2.698065996170044\n",
      "steps = 16, loss = 3.0942232608795166\n",
      "steps = 16, loss = 3.1934568881988525\n",
      "steps = 16, loss = 2.221944570541382\n",
      "steps = 16, loss = 2.2093701362609863\n",
      "steps = 16, loss = 3.164116144180298\n",
      "steps = 16, loss = 2.9031620025634766\n",
      "steps = 16, loss = 3.1963655948638916\n",
      "steps = 16, loss = 3.414464235305786\n",
      "steps = 16, loss = 3.1696221828460693\n",
      "steps = 16, loss = 3.6036839485168457\n",
      "steps = 17, loss = 2.4540059566497803\n",
      "steps = 17, loss = 2.2547593116760254\n",
      "steps = 17, loss = 2.9767463207244873\n",
      "steps = 17, loss = 3.363765239715576\n",
      "steps = 17, loss = 3.410026788711548\n",
      "steps = 17, loss = 3.126352548599243\n",
      "steps = 17, loss = 2.0777220726013184\n",
      "steps = 17, loss = 2.8840551376342773\n",
      "steps = 17, loss = 3.6151845455169678\n",
      "steps = 17, loss = 5.440336227416992\n",
      "steps = 17, loss = 49.84209060668945\n",
      "steps = 17, loss = 2.678757905960083\n",
      "steps = 17, loss = 2.216167688369751\n",
      "steps = 17, loss = 2.6037299633026123\n",
      "steps = 17, loss = 2.798908233642578\n",
      "steps = 17, loss = 2.880319595336914\n",
      "steps = 17, loss = 3.534982204437256\n",
      "steps = 17, loss = 3.2065961360931396\n",
      "steps = 17, loss = 3.282344102859497\n",
      "steps = 17, loss = 2.822263240814209\n",
      "steps = 17, loss = 2.0423641204833984\n",
      "steps = 17, loss = 3.152050018310547\n",
      "steps = 17, loss = 2.8155386447906494\n",
      "steps = 17, loss = 7.245482444763184\n",
      "steps = 17, loss = 49.989418029785156\n",
      "steps = 17, loss = 3.1541407108306885\n",
      "steps = 17, loss = 3.01558256149292\n",
      "steps = 17, loss = 2.6983141899108887\n",
      "steps = 17, loss = 3.1442182064056396\n",
      "steps = 17, loss = 49.73413848876953\n",
      "steps = 17, loss = 2.2274398803710938\n",
      "steps = 17, loss = 2.4734745025634766\n",
      "steps = 17, loss = 3.0762510299682617\n",
      "steps = 17, loss = 2.2363696098327637\n",
      "steps = 17, loss = 2.9289438724517822\n",
      "steps = 17, loss = 3.0762779712677\n",
      "steps = 17, loss = 3.1980042457580566\n",
      "steps = 17, loss = 3.2726194858551025\n",
      "steps = 17, loss = 3.9790525436401367\n",
      "steps = 17, loss = 50.036739349365234\n",
      "steps = 17, loss = 1.970804214477539\n",
      "steps = 17, loss = 2.80505108833313\n",
      "steps = 17, loss = 2.339768409729004\n",
      "steps = 17, loss = 2.336796283721924\n",
      "steps = 17, loss = 1.8724842071533203\n",
      "steps = 17, loss = 2.618887186050415\n",
      "steps = 17, loss = 3.275747299194336\n",
      "steps = 17, loss = 2.0799777507781982\n",
      "steps = 17, loss = 2.0777056217193604\n",
      "steps = 17, loss = 2.4029014110565186\n",
      "steps = 17, loss = 2.522716522216797\n",
      "steps = 17, loss = 2.5942530632019043\n",
      "steps = 17, loss = 3.434577703475952\n",
      "steps = 17, loss = 3.24433970451355\n",
      "steps = 17, loss = 3.2516679763793945\n",
      "steps = 17, loss = 48.47196578979492\n",
      "steps = 17, loss = 2.4131176471710205\n",
      "steps = 17, loss = 2.570875644683838\n",
      "steps = 17, loss = 3.2013208866119385\n",
      "steps = 17, loss = 2.6019179821014404\n",
      "steps = 17, loss = 3.4663937091827393\n",
      "steps = 17, loss = 6.943919658660889\n",
      "steps = 17, loss = 2.919771194458008\n",
      "steps = 17, loss = 2.712019443511963\n",
      "steps = 18, loss = 2.628391981124878\n",
      "steps = 18, loss = 3.2410666942596436\n",
      "steps = 18, loss = 2.4752626419067383\n",
      "steps = 18, loss = 2.9633944034576416\n",
      "steps = 18, loss = 2.754092216491699\n",
      "steps = 18, loss = 2.3828542232513428\n",
      "steps = 18, loss = 2.098510265350342\n",
      "steps = 18, loss = 3.2623538970947266\n",
      "steps = 18, loss = 49.98785400390625\n",
      "steps = 18, loss = 3.248312473297119\n",
      "steps = 18, loss = 3.421069860458374\n",
      "steps = 18, loss = 2.5741539001464844\n",
      "steps = 18, loss = 2.8806755542755127\n",
      "steps = 18, loss = 3.220259666442871\n",
      "steps = 18, loss = 2.324997901916504\n",
      "steps = 18, loss = 6.489242076873779\n",
      "steps = 18, loss = 49.95975112915039\n",
      "steps = 18, loss = 2.2648978233337402\n",
      "steps = 18, loss = 3.0308353900909424\n",
      "steps = 18, loss = 2.480802059173584\n",
      "steps = 18, loss = 2.580497980117798\n",
      "steps = 18, loss = 2.8791005611419678\n",
      "steps = 18, loss = 7.140199184417725\n",
      "steps = 18, loss = 2.7591488361358643\n",
      "steps = 18, loss = 3.021413564682007\n",
      "steps = 18, loss = 2.972978115081787\n",
      "steps = 18, loss = 49.56696319580078\n",
      "steps = 18, loss = 3.180095911026001\n",
      "steps = 18, loss = 2.101226329803467\n",
      "steps = 18, loss = 8.053365707397461\n",
      "steps = 18, loss = 3.450087785720825\n",
      "steps = 18, loss = 2.654571294784546\n",
      "steps = 18, loss = 2.706808090209961\n",
      "steps = 18, loss = 2.8379483222961426\n",
      "steps = 18, loss = 3.398027181625366\n",
      "steps = 18, loss = 3.291799306869507\n",
      "steps = 18, loss = 3.184027671813965\n",
      "steps = 18, loss = 1.9891051054000854\n",
      "steps = 18, loss = 3.613675117492676\n",
      "steps = 18, loss = 2.512760877609253\n",
      "steps = 18, loss = 2.371333122253418\n",
      "steps = 18, loss = 49.96223449707031\n",
      "steps = 18, loss = 50.036739349365234\n",
      "steps = 18, loss = 2.8800461292266846\n",
      "steps = 18, loss = 2.1303181648254395\n",
      "steps = 18, loss = 3.2870702743530273\n",
      "steps = 18, loss = 1.449602723121643\n",
      "steps = 18, loss = 2.2314329147338867\n",
      "steps = 18, loss = 2.5314998626708984\n",
      "steps = 18, loss = 2.41682505607605\n",
      "steps = 18, loss = 3.1474008560180664\n",
      "steps = 18, loss = 3.612152576446533\n",
      "steps = 18, loss = 3.366457223892212\n",
      "steps = 18, loss = 2.8646328449249268\n",
      "steps = 18, loss = 3.134192705154419\n",
      "steps = 18, loss = 2.7091143131256104\n",
      "steps = 18, loss = 2.0451314449310303\n",
      "steps = 18, loss = 4.0609941482543945\n",
      "steps = 18, loss = 3.30780291557312\n",
      "steps = 18, loss = 3.3121016025543213\n",
      "steps = 18, loss = 2.9089934825897217\n",
      "steps = 18, loss = 2.2625234127044678\n",
      "steps = 18, loss = 3.1861865520477295\n",
      "steps = 18, loss = 3.3667569160461426\n",
      "steps = 19, loss = 3.0825278759002686\n",
      "steps = 19, loss = 3.4695374965667725\n",
      "steps = 19, loss = 2.897949695587158\n",
      "steps = 19, loss = 49.457515716552734\n",
      "steps = 19, loss = 2.6970226764678955\n",
      "steps = 19, loss = 2.071155071258545\n",
      "steps = 19, loss = 3.2235188484191895\n",
      "steps = 19, loss = 2.5037271976470947\n",
      "steps = 19, loss = 3.3370561599731445\n",
      "steps = 19, loss = 3.164097547531128\n",
      "steps = 19, loss = 2.0081799030303955\n",
      "steps = 19, loss = 3.1613516807556152\n",
      "steps = 19, loss = 49.83036804199219\n",
      "steps = 19, loss = 3.3059046268463135\n",
      "steps = 19, loss = 2.9059219360351562\n",
      "steps = 19, loss = 49.94924545288086\n",
      "steps = 19, loss = 3.320662021636963\n",
      "steps = 19, loss = 3.294705390930176\n",
      "steps = 19, loss = 2.862863779067993\n",
      "steps = 19, loss = 3.3611600399017334\n",
      "steps = 19, loss = 2.3138415813446045\n",
      "steps = 19, loss = 2.989327907562256\n",
      "steps = 19, loss = 2.2613894939422607\n",
      "steps = 19, loss = 2.520477771759033\n",
      "steps = 19, loss = 3.243091106414795\n",
      "steps = 19, loss = 2.8129451274871826\n",
      "steps = 19, loss = 2.762275218963623\n",
      "steps = 19, loss = 5.520616054534912\n",
      "steps = 19, loss = 2.423706293106079\n",
      "steps = 19, loss = 4.099461078643799\n",
      "steps = 19, loss = 2.1256840229034424\n",
      "steps = 19, loss = 2.963017463684082\n",
      "steps = 19, loss = 3.324503183364868\n",
      "steps = 19, loss = 7.847439289093018\n",
      "steps = 19, loss = 3.2400407791137695\n",
      "steps = 19, loss = 2.435281276702881\n",
      "steps = 19, loss = 3.158342123031616\n",
      "steps = 19, loss = 2.295036554336548\n",
      "steps = 19, loss = 2.667391777038574\n",
      "steps = 19, loss = 3.112112045288086\n",
      "steps = 19, loss = 2.84494686126709\n",
      "steps = 19, loss = 7.749220848083496\n",
      "steps = 19, loss = 49.98970413208008\n",
      "steps = 19, loss = 3.8338820934295654\n",
      "steps = 19, loss = 2.6742286682128906\n",
      "steps = 19, loss = 3.409065008163452\n",
      "steps = 19, loss = 2.6371073722839355\n",
      "steps = 19, loss = 2.1200270652770996\n",
      "steps = 19, loss = 3.615081310272217\n",
      "steps = 19, loss = 2.7901194095611572\n",
      "steps = 19, loss = 2.86637806892395\n",
      "steps = 19, loss = 2.4375500679016113\n",
      "steps = 19, loss = 3.2941555976867676\n",
      "steps = 19, loss = 50.036739349365234\n",
      "steps = 19, loss = 2.734388589859009\n",
      "steps = 19, loss = 2.252329111099243\n",
      "steps = 19, loss = 2.784769058227539\n",
      "steps = 19, loss = 3.1575138568878174\n",
      "steps = 19, loss = 2.0654549598693848\n",
      "steps = 19, loss = 3.7972702980041504\n",
      "steps = 19, loss = 2.1037397384643555\n",
      "steps = 19, loss = 3.073812246322632\n",
      "steps = 19, loss = 2.7462027072906494\n",
      "steps = 19, loss = 3.3006184101104736\n",
      "steps = 20, loss = 2.7911946773529053\n",
      "steps = 20, loss = 49.96694564819336\n",
      "steps = 20, loss = 2.730156183242798\n",
      "steps = 20, loss = 3.209918737411499\n",
      "steps = 20, loss = 49.9415397644043\n",
      "steps = 20, loss = 32.270164489746094\n",
      "steps = 20, loss = 4.021426200866699\n",
      "steps = 20, loss = 2.1187877655029297\n",
      "steps = 20, loss = 3.2413389682769775\n",
      "steps = 20, loss = 2.7482612133026123\n",
      "steps = 20, loss = 2.8193390369415283\n",
      "steps = 20, loss = 2.124898910522461\n",
      "steps = 20, loss = 2.745270252227783\n",
      "steps = 20, loss = 2.3069190979003906\n",
      "steps = 20, loss = 3.321361780166626\n",
      "steps = 20, loss = 2.6809682846069336\n",
      "steps = 20, loss = 2.767742395401001\n",
      "steps = 20, loss = 2.4319939613342285\n",
      "steps = 20, loss = 2.210582971572876\n",
      "steps = 20, loss = 3.480778694152832\n",
      "steps = 20, loss = 3.2951090335845947\n",
      "steps = 20, loss = 2.713350772857666\n",
      "steps = 20, loss = 3.142608165740967\n",
      "steps = 20, loss = 3.1961069107055664\n",
      "steps = 20, loss = 2.2390031814575195\n",
      "steps = 20, loss = 1.9395776987075806\n",
      "steps = 20, loss = 3.132704734802246\n",
      "steps = 20, loss = 3.181317090988159\n",
      "steps = 20, loss = 3.2948532104492188\n",
      "steps = 20, loss = 2.00627064704895\n",
      "steps = 20, loss = 2.767841100692749\n",
      "steps = 20, loss = 2.9304962158203125\n",
      "steps = 20, loss = 2.9280924797058105\n",
      "steps = 20, loss = 3.643390417098999\n",
      "steps = 20, loss = 3.096011161804199\n",
      "steps = 20, loss = 2.466336965560913\n",
      "steps = 20, loss = 2.0814976692199707\n",
      "steps = 20, loss = 3.297481060028076\n",
      "steps = 20, loss = 2.482649326324463\n",
      "steps = 20, loss = 3.5826122760772705\n",
      "steps = 20, loss = 2.540220022201538\n",
      "steps = 20, loss = 5.440648078918457\n",
      "steps = 20, loss = 2.537776470184326\n",
      "steps = 20, loss = 3.337310791015625\n",
      "steps = 20, loss = 50.036739349365234\n",
      "steps = 20, loss = 2.8658504486083984\n",
      "steps = 20, loss = 7.421857833862305\n",
      "steps = 20, loss = 2.3340330123901367\n",
      "steps = 20, loss = 2.8867807388305664\n",
      "steps = 20, loss = 2.0506012439727783\n",
      "steps = 20, loss = 2.7570888996124268\n",
      "steps = 20, loss = 47.08092498779297\n",
      "steps = 20, loss = 3.3181049823760986\n",
      "steps = 20, loss = 2.4139623641967773\n",
      "steps = 20, loss = 2.7189877033233643\n",
      "steps = 20, loss = 8.238872528076172\n",
      "steps = 20, loss = 3.333667278289795\n",
      "steps = 20, loss = 2.4478728771209717\n",
      "steps = 20, loss = 3.3571460247039795\n",
      "steps = 20, loss = 2.8798630237579346\n",
      "steps = 20, loss = 2.728593111038208\n",
      "steps = 20, loss = 3.0794942378997803\n",
      "steps = 20, loss = 3.4943788051605225\n",
      "steps = 20, loss = 49.99140548706055\n",
      "steps = 21, loss = 4.992173194885254\n",
      "steps = 21, loss = 3.6558244228363037\n",
      "steps = 21, loss = 7.319479942321777\n",
      "steps = 21, loss = 2.904991388320923\n",
      "steps = 21, loss = 49.9603271484375\n",
      "steps = 21, loss = 3.3367247581481934\n",
      "steps = 21, loss = 2.4981069564819336\n",
      "steps = 21, loss = 49.99077606201172\n",
      "steps = 21, loss = 2.162029981613159\n",
      "steps = 21, loss = 2.8564505577087402\n",
      "steps = 21, loss = 2.881354808807373\n",
      "steps = 21, loss = 3.5406296253204346\n",
      "steps = 21, loss = 49.9901008605957\n",
      "steps = 21, loss = 2.1330642700195312\n",
      "steps = 21, loss = 2.232837677001953\n",
      "steps = 21, loss = 2.1502773761749268\n",
      "steps = 21, loss = 2.8871817588806152\n",
      "steps = 21, loss = 7.686549186706543\n",
      "steps = 21, loss = 3.7140719890594482\n",
      "steps = 21, loss = 3.3601372241973877\n",
      "steps = 21, loss = 2.3408260345458984\n",
      "steps = 21, loss = 3.3407626152038574\n",
      "steps = 21, loss = 3.4337899684906006\n",
      "steps = 21, loss = 3.1770012378692627\n",
      "steps = 21, loss = 3.5542685985565186\n",
      "steps = 21, loss = 2.226383686065674\n",
      "steps = 21, loss = 3.0910181999206543\n",
      "steps = 21, loss = 3.4369277954101562\n",
      "steps = 21, loss = 2.5670881271362305\n",
      "steps = 21, loss = 3.3097763061523438\n",
      "steps = 21, loss = 2.5231025218963623\n",
      "steps = 21, loss = 2.1424005031585693\n",
      "steps = 21, loss = 3.3313114643096924\n",
      "steps = 21, loss = 49.94766616821289\n",
      "steps = 21, loss = 2.9630823135375977\n",
      "steps = 21, loss = 3.2759101390838623\n",
      "steps = 21, loss = 2.833024263381958\n",
      "steps = 21, loss = 2.892573595046997\n",
      "steps = 21, loss = 2.0225398540496826\n",
      "steps = 21, loss = 4.106201171875\n",
      "steps = 21, loss = 2.9192841053009033\n",
      "steps = 21, loss = 2.0198400020599365\n",
      "steps = 21, loss = 2.894026279449463\n",
      "steps = 21, loss = 3.148829460144043\n",
      "steps = 21, loss = 2.417273998260498\n",
      "steps = 21, loss = 3.1662471294403076\n",
      "steps = 21, loss = 2.3963232040405273\n",
      "steps = 21, loss = 3.6475424766540527\n",
      "steps = 21, loss = 2.471683979034424\n",
      "steps = 21, loss = 2.697256326675415\n",
      "steps = 21, loss = 2.783379077911377\n",
      "steps = 21, loss = 3.216642379760742\n",
      "steps = 21, loss = 2.5956826210021973\n",
      "steps = 21, loss = 2.469420909881592\n",
      "steps = 21, loss = 2.8779420852661133\n",
      "steps = 21, loss = 2.399174690246582\n",
      "steps = 21, loss = 2.7135446071624756\n",
      "steps = 21, loss = 30.79792022705078\n",
      "steps = 21, loss = 2.769279718399048\n",
      "steps = 21, loss = 3.412334442138672\n",
      "steps = 21, loss = 2.4888510704040527\n",
      "steps = 21, loss = 3.5340538024902344\n",
      "steps = 21, loss = 2.7917306423187256\n",
      "steps = 21, loss = 50.036739349365234\n",
      "steps = 22, loss = 3.631288528442383\n",
      "steps = 22, loss = 3.226283311843872\n",
      "steps = 22, loss = 50.008243560791016\n",
      "steps = 22, loss = 23.53184700012207\n",
      "steps = 22, loss = 50.036739349365234\n",
      "steps = 22, loss = 7.939105987548828\n",
      "steps = 22, loss = 3.229186773300171\n",
      "steps = 22, loss = 3.3547165393829346\n",
      "steps = 22, loss = 2.4470536708831787\n",
      "steps = 22, loss = 2.5824012756347656\n",
      "steps = 22, loss = 2.1357531547546387\n",
      "steps = 22, loss = 2.6736559867858887\n",
      "steps = 22, loss = 3.397291421890259\n",
      "steps = 22, loss = 8.648119926452637\n",
      "steps = 22, loss = 3.3354907035827637\n",
      "steps = 22, loss = 2.890681505203247\n",
      "steps = 22, loss = 2.2329235076904297\n",
      "steps = 22, loss = 2.0244076251983643\n",
      "steps = 22, loss = 3.13299560546875\n",
      "steps = 22, loss = 2.8768084049224854\n",
      "steps = 22, loss = 3.145691156387329\n",
      "steps = 22, loss = 2.161284923553467\n",
      "steps = 22, loss = 3.0916237831115723\n",
      "steps = 22, loss = 2.46134352684021\n",
      "steps = 22, loss = 2.8387937545776367\n",
      "steps = 22, loss = 2.7109997272491455\n",
      "steps = 22, loss = 2.2247889041900635\n",
      "steps = 22, loss = 3.1312098503112793\n",
      "steps = 22, loss = 2.0474679470062256\n",
      "steps = 22, loss = 2.501215696334839\n",
      "steps = 22, loss = 2.713139057159424\n",
      "steps = 22, loss = 3.918055534362793\n",
      "steps = 22, loss = 2.792478322982788\n",
      "steps = 22, loss = 2.807157278060913\n",
      "steps = 22, loss = 3.3517558574676514\n",
      "steps = 22, loss = 2.4162957668304443\n",
      "steps = 22, loss = 3.3889319896698\n",
      "steps = 22, loss = 2.034162759780884\n",
      "steps = 22, loss = 3.145714282989502\n",
      "steps = 22, loss = 5.147255897521973\n",
      "steps = 22, loss = 2.5536346435546875\n",
      "steps = 22, loss = 2.9083173274993896\n",
      "steps = 22, loss = 2.5050129890441895\n",
      "steps = 22, loss = 2.1694905757904053\n",
      "steps = 22, loss = 49.93397521972656\n",
      "steps = 22, loss = 3.164602518081665\n",
      "steps = 22, loss = 2.8888962268829346\n",
      "steps = 22, loss = 3.336841583251953\n",
      "steps = 22, loss = 3.6564667224884033\n",
      "steps = 22, loss = 2.4952054023742676\n",
      "steps = 22, loss = 49.9692497253418\n",
      "steps = 22, loss = 2.8333466053009033\n",
      "steps = 22, loss = 4.117701530456543\n",
      "steps = 22, loss = 2.884798765182495\n",
      "steps = 22, loss = 2.490292549133301\n",
      "steps = 22, loss = 2.8817520141601562\n",
      "steps = 22, loss = 3.044342041015625\n",
      "steps = 22, loss = 3.242478370666504\n",
      "steps = 22, loss = 3.3541998863220215\n",
      "steps = 22, loss = 2.926952600479126\n",
      "steps = 22, loss = 2.3675692081451416\n",
      "steps = 22, loss = 2.464566707611084\n",
      "steps = 22, loss = 49.96885681152344\n",
      "steps = 22, loss = 3.176654577255249\n",
      "steps = 23, loss = 3.163428783416748\n",
      "steps = 23, loss = 3.850391149520874\n",
      "steps = 23, loss = 2.0497686862945557\n",
      "steps = 23, loss = 2.928565263748169\n",
      "steps = 23, loss = 2.6754629611968994\n",
      "steps = 23, loss = 3.250060796737671\n",
      "steps = 23, loss = 3.361837387084961\n",
      "steps = 23, loss = 8.011724472045898\n",
      "steps = 23, loss = 3.406601905822754\n",
      "steps = 23, loss = 2.0610649585723877\n",
      "steps = 23, loss = 2.9126765727996826\n",
      "steps = 23, loss = 2.5847811698913574\n",
      "steps = 23, loss = 2.7331840991973877\n",
      "steps = 23, loss = 3.482330322265625\n",
      "steps = 23, loss = 2.8080577850341797\n",
      "steps = 23, loss = 3.319532632827759\n",
      "steps = 23, loss = 4.144885063171387\n",
      "steps = 23, loss = 3.5276660919189453\n",
      "steps = 23, loss = 2.6160061359405518\n",
      "steps = 23, loss = 3.3649187088012695\n",
      "steps = 23, loss = 7.682755947113037\n",
      "steps = 23, loss = 3.360513687133789\n",
      "steps = 23, loss = 2.556910991668701\n",
      "steps = 23, loss = 2.1940250396728516\n",
      "steps = 23, loss = 3.1877591609954834\n",
      "steps = 23, loss = 2.4372286796569824\n",
      "steps = 23, loss = 2.103111743927002\n",
      "steps = 23, loss = 2.2622647285461426\n",
      "steps = 23, loss = 2.20568585395813\n",
      "steps = 23, loss = 2.5530827045440674\n",
      "steps = 23, loss = 2.956143617630005\n",
      "steps = 23, loss = 2.772667169570923\n",
      "steps = 23, loss = 2.747509717941284\n",
      "steps = 23, loss = 50.036739349365234\n",
      "steps = 23, loss = 2.2631728649139404\n",
      "steps = 23, loss = 3.381373405456543\n",
      "steps = 23, loss = 3.3279001712799072\n",
      "steps = 23, loss = 3.386199474334717\n",
      "steps = 23, loss = 3.1568281650543213\n",
      "steps = 23, loss = 3.5999133586883545\n",
      "steps = 23, loss = 2.8903980255126953\n",
      "steps = 23, loss = 2.3965418338775635\n",
      "steps = 23, loss = 49.989166259765625\n",
      "steps = 23, loss = 2.9793593883514404\n",
      "steps = 23, loss = 19.57548713684082\n",
      "steps = 23, loss = 3.562671661376953\n",
      "steps = 23, loss = 2.7535061836242676\n",
      "steps = 23, loss = 2.1860055923461914\n",
      "steps = 23, loss = 49.69269943237305\n",
      "steps = 23, loss = 2.5368027687072754\n",
      "steps = 23, loss = 3.0918216705322266\n",
      "steps = 23, loss = 2.963003635406494\n",
      "steps = 23, loss = 2.513920307159424\n",
      "steps = 23, loss = 2.5027594566345215\n",
      "steps = 23, loss = 2.680676221847534\n",
      "steps = 23, loss = 2.734156370162964\n",
      "steps = 23, loss = 2.843264579772949\n",
      "steps = 23, loss = 2.892690658569336\n",
      "steps = 23, loss = 50.020050048828125\n",
      "steps = 23, loss = 2.8430426120758057\n",
      "steps = 23, loss = 49.939388275146484\n",
      "steps = 23, loss = 3.2318921089172363\n",
      "steps = 23, loss = 4.946835994720459\n",
      "steps = 23, loss = 3.191348075866699\n",
      "steps = 24, loss = 3.4006779193878174\n",
      "steps = 24, loss = 2.5402982234954834\n",
      "steps = 24, loss = 3.370067596435547\n",
      "steps = 24, loss = 3.1896920204162598\n",
      "steps = 24, loss = 2.4925334453582764\n",
      "steps = 24, loss = 3.3689167499542236\n",
      "steps = 24, loss = 2.4710798263549805\n",
      "steps = 24, loss = 2.909822702407837\n",
      "steps = 24, loss = 3.379154682159424\n",
      "steps = 24, loss = 50.036739349365234\n",
      "steps = 24, loss = 3.2898671627044678\n",
      "steps = 24, loss = 3.233158588409424\n",
      "steps = 24, loss = 2.6173365116119385\n",
      "steps = 24, loss = 2.0453782081604004\n",
      "steps = 24, loss = 2.5566742420196533\n",
      "steps = 24, loss = 11.290292739868164\n",
      "steps = 24, loss = 3.1685080528259277\n",
      "steps = 24, loss = 2.5191898345947266\n",
      "steps = 24, loss = 2.0852363109588623\n",
      "steps = 24, loss = 3.9770593643188477\n",
      "steps = 24, loss = 3.103849172592163\n",
      "steps = 24, loss = 49.984718322753906\n",
      "steps = 24, loss = 2.1940598487854004\n",
      "steps = 24, loss = 2.6740918159484863\n",
      "steps = 24, loss = 3.0632636547088623\n",
      "steps = 24, loss = 2.406423330307007\n",
      "steps = 24, loss = 2.849015951156616\n",
      "steps = 24, loss = 4.0445237159729\n",
      "steps = 24, loss = 2.2413687705993652\n",
      "steps = 24, loss = 2.5276639461517334\n",
      "steps = 24, loss = 3.1259255409240723\n",
      "steps = 24, loss = 2.7569222450256348\n",
      "steps = 24, loss = 2.8302159309387207\n",
      "steps = 24, loss = 49.94272994995117\n",
      "steps = 24, loss = 3.456394672393799\n",
      "steps = 24, loss = 1.835769772529602\n",
      "steps = 24, loss = 3.5173990726470947\n",
      "steps = 24, loss = 3.3397960662841797\n",
      "steps = 24, loss = 3.0740325450897217\n",
      "steps = 24, loss = 2.853212356567383\n",
      "steps = 24, loss = 3.2535009384155273\n",
      "steps = 24, loss = 3.206911087036133\n",
      "steps = 24, loss = 2.188075065612793\n",
      "steps = 24, loss = 2.891564130783081\n",
      "steps = 24, loss = 3.0118937492370605\n",
      "steps = 24, loss = 3.165975332260132\n",
      "steps = 24, loss = 2.4814376831054688\n",
      "steps = 24, loss = 3.0273756980895996\n",
      "steps = 24, loss = 2.2244083881378174\n",
      "steps = 24, loss = 2.720137596130371\n",
      "steps = 24, loss = 2.9226534366607666\n",
      "steps = 24, loss = 2.6134021282196045\n",
      "steps = 24, loss = 4.882054805755615\n",
      "steps = 24, loss = 49.9831428527832\n",
      "steps = 24, loss = 7.320405006408691\n",
      "steps = 24, loss = 39.03253173828125\n",
      "steps = 24, loss = 7.667562484741211\n",
      "steps = 24, loss = 3.2829811573028564\n",
      "steps = 24, loss = 2.875647783279419\n",
      "steps = 24, loss = 3.468170166015625\n",
      "steps = 24, loss = 2.0680174827575684\n",
      "steps = 24, loss = 2.9296071529388428\n",
      "steps = 24, loss = 3.3124234676361084\n",
      "steps = 24, loss = 2.414591073989868\n",
      "steps = 25, loss = 4.647321701049805\n",
      "steps = 25, loss = 2.4593377113342285\n",
      "steps = 25, loss = 1.9024713039398193\n",
      "steps = 25, loss = 2.447068452835083\n",
      "steps = 25, loss = 2.2086477279663086\n",
      "steps = 25, loss = 49.95280838012695\n",
      "steps = 25, loss = 2.932380437850952\n",
      "steps = 25, loss = 49.96952819824219\n",
      "steps = 25, loss = 2.918733835220337\n",
      "steps = 25, loss = 49.94561767578125\n",
      "steps = 25, loss = 3.5295164585113525\n",
      "steps = 25, loss = 2.683720827102661\n",
      "steps = 25, loss = 49.98893356323242\n",
      "steps = 25, loss = 3.937419891357422\n",
      "steps = 25, loss = 3.2954511642456055\n",
      "steps = 25, loss = 4.334072589874268\n",
      "steps = 25, loss = 3.199753999710083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IncrementalSearchCV(decay_rate=0,\n",
       "                    estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                    max_iter=243, n_initial_parameters=64,\n",
       "                    parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                                &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;,\n",
       "                                                       &#x27;ELU&#x27;, &#x27;PReLU&#x27;],\n",
       "                                &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                                 &#x27;xavier_normal_&#x27;,\n",
       "                                                 &#x27;kaiming_uniform_&#x27;,\n",
       "                                                 &#x27;kaiming_norm...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                                &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                                &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, ...],\n",
       "                                &#x27;train_split&#x27;: [None]},\n",
       "                    patience=24, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;IncrementalSearchCV<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>IncrementalSearchCV(decay_rate=0,\n",
       "                    estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                    max_iter=243, n_initial_parameters=64,\n",
       "                    parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                                &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;,\n",
       "                                                       &#x27;ELU&#x27;, &#x27;PReLU&#x27;],\n",
       "                                &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                                 &#x27;xavier_normal_&#x27;,\n",
       "                                                 &#x27;kaiming_uniform_&#x27;,\n",
       "                                                 &#x27;kaiming_norm...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                                &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                                &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, ...],\n",
       "                                &#x27;train_split&#x27;: [None]},\n",
       "                    patience=24, random_state=42)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "IncrementalSearchCV(decay_rate=0,\n",
       "                    estimator=<class '__main__.TrimParams'>[uninitialized](\n",
       "  module=<class 'autoencoder.Autoencoder'>,\n",
       "),\n",
       "                    max_iter=243, n_initial_parameters=64,\n",
       "                    parameters={'batch_size': [32, 64, 128, 256, 512],\n",
       "                                'module__activation': ['ReLU', 'LeakyReLU',\n",
       "                                                       'ELU', 'PReLU'],\n",
       "                                'module__init': ['xavier_uniform_',\n",
       "                                                 'xavier_normal_',\n",
       "                                                 'kaiming_uniform_',\n",
       "                                                 'kaiming_norm...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                                'optimizer__nesterov': [True],\n",
       "                                'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, ...],\n",
       "                                'train_split': [None]},\n",
       "                    patience=24, random_state=42)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patience_search.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9024713039398193"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patience_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 209.35 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/home/barradd/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/distributed/client.py:3163: UserWarning: Sending large graph of size 32.84 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "save_search(patience_search, today, \"patience\", X_test.compute(), y_test.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_stats = client.profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final-timings.json\", \"w\") as f:\n",
    "    json.dump(timing_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, fig = client.get_task_stream(plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19232"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: table;\"><div style=\"display: table-row;\"><div style=\"display: table-cell;\"><b title=\"bokeh.plotting._figure.figure\">figure</b>(</div><div style=\"display: table-cell;\">id&nbsp;=&nbsp;'p2109941', <span id=\"p2110008\" style=\"cursor: pointer;\">&hellip;)</span></div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">above&nbsp;=&nbsp;[],</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">align&nbsp;=&nbsp;'auto',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">aspect_ratio&nbsp;=&nbsp;None,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">aspect_scale&nbsp;=&nbsp;1,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">background_fill_alpha&nbsp;=&nbsp;1.0,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">background_fill_color&nbsp;=&nbsp;'#ffffff',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">below&nbsp;=&nbsp;[DatetimeAxis(id='p2109953', ...)],</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">border_fill_alpha&nbsp;=&nbsp;1.0,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">border_fill_color&nbsp;=&nbsp;'#ffffff',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">center&nbsp;=&nbsp;[Grid(id='p2109969', ...), Grid(id='p2109974', ...)],</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">context_menu&nbsp;=&nbsp;None,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">css_classes&nbsp;=&nbsp;[],</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">disabled&nbsp;=&nbsp;False,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">extra_x_ranges&nbsp;=&nbsp;{},</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">extra_x_scales&nbsp;=&nbsp;{},</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">extra_y_ranges&nbsp;=&nbsp;{},</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">extra_y_scales&nbsp;=&nbsp;{},</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">flow_mode&nbsp;=&nbsp;'block',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">frame_align&nbsp;=&nbsp;True,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">frame_height&nbsp;=&nbsp;None,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">frame_width&nbsp;=&nbsp;None,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">height&nbsp;=&nbsp;600,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">height_policy&nbsp;=&nbsp;'auto',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">hidpi&nbsp;=&nbsp;True,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">hold_render&nbsp;=&nbsp;False,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_event_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_property_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">left&nbsp;=&nbsp;[],</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_factor&nbsp;=&nbsp;10,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_interval&nbsp;=&nbsp;300,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_threshold&nbsp;=&nbsp;2000,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_timeout&nbsp;=&nbsp;500,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">margin&nbsp;=&nbsp;None,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">match_aspect&nbsp;=&nbsp;False,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">max_height&nbsp;=&nbsp;None,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">max_width&nbsp;=&nbsp;None,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border&nbsp;=&nbsp;5,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_bottom&nbsp;=&nbsp;50,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_left&nbsp;=&nbsp;None,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_right&nbsp;=&nbsp;None,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_top&nbsp;=&nbsp;None,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_height&nbsp;=&nbsp;None,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_width&nbsp;=&nbsp;None,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">name&nbsp;=&nbsp;'task_stream',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_alpha&nbsp;=&nbsp;1.0,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_cap&nbsp;=&nbsp;'butt',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_color&nbsp;=&nbsp;'#e5e5e5',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_dash&nbsp;=&nbsp;[],</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_dash_offset&nbsp;=&nbsp;0,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_join&nbsp;=&nbsp;'bevel',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_width&nbsp;=&nbsp;1,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">output_backend&nbsp;=&nbsp;'canvas',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">renderers&nbsp;=&nbsp;[GlyphRenderer(id='p2109981', ...)],</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">reset_policy&nbsp;=&nbsp;'standard',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">resizable&nbsp;=&nbsp;False,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">right&nbsp;=&nbsp;[],</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">sizing_mode&nbsp;=&nbsp;'stretch_both',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">styles&nbsp;=&nbsp;{},</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">stylesheets&nbsp;=&nbsp;[],</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">subscribed_events&nbsp;=&nbsp;PropertyValueSet(),</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">syncable&nbsp;=&nbsp;True,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">tags&nbsp;=&nbsp;[],</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">title&nbsp;=&nbsp;Title(id='p2109944', ...),</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">title_location&nbsp;=&nbsp;'above',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">toolbar&nbsp;=&nbsp;Toolbar(id='p2109950', ...),</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">toolbar_inner&nbsp;=&nbsp;False,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">toolbar_location&nbsp;=&nbsp;'above',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">toolbar_sticky&nbsp;=&nbsp;True,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">visible&nbsp;=&nbsp;True,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">width&nbsp;=&nbsp;600,</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">width_policy&nbsp;=&nbsp;'auto',</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">x_range&nbsp;=&nbsp;DataRange1d(id='p2109939', ...),</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">x_scale&nbsp;=&nbsp;LinearScale(id='p2109951', ...),</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">y_range&nbsp;=&nbsp;DataRange1d(id='p2109940', ...),</div></div><div class=\"p2110007\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">y_scale&nbsp;=&nbsp;LinearScale(id='p2109952', ...))</div></div></div>\n",
       "<script>\n",
       "(function() {\n",
       "  let expanded = false;\n",
       "  const ellipsis = document.getElementById(\"p2110008\");\n",
       "  ellipsis.addEventListener(\"click\", function() {\n",
       "    const rows = document.getElementsByClassName(\"p2110007\");\n",
       "    for (let i = 0; i < rows.length; i++) {\n",
       "      const el = rows[i];\n",
       "      el.style.display = expanded ? \"none\" : \"table-row\";\n",
       "    }\n",
       "    ellipsis.innerHTML = expanded ? \"&hellip;)\" : \"&lsaquo;&lsaquo;&lsaquo;\";\n",
       "    expanded = !expanded;\n",
       "  });\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "figure(id='p2109941', ...)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_ = Out[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>worker</th>\n",
       "      <th>nbytes</th>\n",
       "      <th>type</th>\n",
       "      <th>typename</th>\n",
       "      <th>metadata</th>\n",
       "      <th>thread</th>\n",
       "      <th>startstops</th>\n",
       "      <th>status</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>task-finished-1706678416.9221444</td>\n",
       "      <td>tcp://127.0.0.1:41087</td>\n",
       "      <td>2153</td>\n",
       "      <td>b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>tuple</td>\n",
       "      <td>{}</td>\n",
       "      <td>139900515796736</td>\n",
       "      <td>({'action': 'compute', 'start': 1706678318.965...</td>\n",
       "      <td>OK</td>\n",
       "      <td>_partial_fit-bf707c36-2316-4ab5-aa62-fa3d4a417c1a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>task-finished-1706678513.142254</td>\n",
       "      <td>tcp://127.0.0.1:41087</td>\n",
       "      <td>2153</td>\n",
       "      <td>b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>tuple</td>\n",
       "      <td>{}</td>\n",
       "      <td>139900515796736</td>\n",
       "      <td>({'action': 'compute', 'start': 1706678416.979...</td>\n",
       "      <td>OK</td>\n",
       "      <td>_partial_fit-2d2b8d50-e8bd-44a6-b33c-8f8a98047c18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>task-finished-1706678613.2071962</td>\n",
       "      <td>tcp://127.0.0.1:39149</td>\n",
       "      <td>2153</td>\n",
       "      <td>b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>tuple</td>\n",
       "      <td>{}</td>\n",
       "      <td>140578566170368</td>\n",
       "      <td>({'action': 'transfer', 'start': 1706678513.03...</td>\n",
       "      <td>OK</td>\n",
       "      <td>_partial_fit-c3350e1e-48dc-4a28-8dc6-f081a97aa134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>task-finished-1706678711.4727633</td>\n",
       "      <td>tcp://127.0.0.1:41087</td>\n",
       "      <td>2153</td>\n",
       "      <td>b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>tuple</td>\n",
       "      <td>{}</td>\n",
       "      <td>139900515796736</td>\n",
       "      <td>({'action': 'transfer', 'start': 1706678613.21...</td>\n",
       "      <td>OK</td>\n",
       "      <td>_partial_fit-e79c3045-b3e9-4002-896f-6d5688cf6357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>task-finished-1706678810.3910055</td>\n",
       "      <td>tcp://127.0.0.1:41087</td>\n",
       "      <td>2153</td>\n",
       "      <td>b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>tuple</td>\n",
       "      <td>{}</td>\n",
       "      <td>139900515796736</td>\n",
       "      <td>({'action': 'compute', 'start': 1706678711.526...</td>\n",
       "      <td>OK</td>\n",
       "      <td>_partial_fit-80b04eb1-fd1c-4e3f-a2b3-4c3eee1a78b9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        stimulus_id                 worker  nbytes  \\\n",
       "0  task-finished-1706678416.9221444  tcp://127.0.0.1:41087    2153   \n",
       "1   task-finished-1706678513.142254  tcp://127.0.0.1:41087    2153   \n",
       "2  task-finished-1706678613.2071962  tcp://127.0.0.1:39149    2153   \n",
       "3  task-finished-1706678711.4727633  tcp://127.0.0.1:41087    2153   \n",
       "4  task-finished-1706678810.3910055  tcp://127.0.0.1:41087    2153   \n",
       "\n",
       "                                                type typename metadata  \\\n",
       "0  b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...    tuple       {}   \n",
       "1  b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...    tuple       {}   \n",
       "2  b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...    tuple       {}   \n",
       "3  b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...    tuple       {}   \n",
       "4  b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...    tuple       {}   \n",
       "\n",
       "            thread                                         startstops status  \\\n",
       "0  139900515796736  ({'action': 'compute', 'start': 1706678318.965...     OK   \n",
       "1  139900515796736  ({'action': 'compute', 'start': 1706678416.979...     OK   \n",
       "2  140578566170368  ({'action': 'transfer', 'start': 1706678513.03...     OK   \n",
       "3  139900515796736  ({'action': 'transfer', 'start': 1706678613.21...     OK   \n",
       "4  139900515796736  ({'action': 'compute', 'start': 1706678711.526...     OK   \n",
       "\n",
       "                                                 key  \n",
       "0  _partial_fit-bf707c36-2316-4ab5-aa62-fa3d4a417c1a  \n",
       "1  _partial_fit-2d2b8d50-e8bd-44a6-b33c-8f8a98047c18  \n",
       "2  _partial_fit-c3350e1e-48dc-4a28-8dc6-f081a97aa134  \n",
       "3  _partial_fit-e79c3045-b3e9-4002-896f-6d5688cf6357  \n",
       "4  _partial_fit-80b04eb1-fd1c-4e3f-a2b3-4c3eee1a78b9  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_msgpack'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26937/3296423393.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_msgpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"times.msgpack\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/rapids-23.12/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5898\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5899\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_msgpack'"
     ]
    }
   ],
   "source": [
    "df.to_.to_msgpack(\"times.msgpack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19232"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
