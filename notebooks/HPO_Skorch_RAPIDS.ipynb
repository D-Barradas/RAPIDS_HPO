{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D-Barradas/RAPIDS_HPO/blob/main/notebooks/HPO_Skorch_RAPIDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Required Libraries\n",
        "We start by installing and importing all the necessary libraries for this tutorial. This includes RAPIDS libraries (cuDF, cuML), PyTorch, Skorch, and other utilities for data handling and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7Jelpaqf8wB"
      },
      "outputs": [],
      "source": [
        "!pip install skorch\n",
        "!pip install scikit-learn pandas\n",
        "!pip install optuna\n",
        "!pip install optuna-integration[skorch]\n",
        "!pip3 install torch torchaudio torchvision torchtext torchdata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I3sKr65dGpW"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore') # Reduce number of messages/warnings displayed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate Model Performance\n",
        "After training, we evaluate the model's performance on the test set. We display key metrics such as accuracy and visualize the results to better understand the model's effectiveness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Optimization (HPO) with Skorch and RAPIDS\n",
        "This tutorial demonstrates how to use RAPIDS for GPU-accelerated data processing and Skorch for PyTorch-based models with scikit-learn compatible HPO tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 1. Import Required Libraries\n",
        "Import RAPIDS libraries (cuDF, cuML), PyTorch, Skorch, and any other required libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from skorch import NeuralNetClassifier\n",
        "import cudf\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "from cuml.datasets import make_classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 2. Set Up RAPIDS Environment\n",
        "Configure the RAPIDS environment and check GPU availability.\n",
        "\n",
        "### Set Up RAPIDS Environment\n",
        "Here, we configure the RAPIDS environment and check if a GPU is available. This ensures that all RAPIDS and PyTorch operations will run on the GPU for maximum acceleration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "\n",
        "import cuml\n",
        "print('cuML version:', cuml.__version__)\n",
        "\n",
        "import cudf\n",
        "print('cuDF version:', cudf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare Dataset\n",
        "Load a dataset using cuDF, perform preprocessing, and split into training and test sets.\n",
        "\n",
        "\n",
        "We generate a synthetic classification dataset using RAPIDS cuML, convert it to numpy arrays for PyTorch/Skorch compatibility, and split it into training and test sets. This step demonstrates RAPIDS-accelerated data handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic data on GPU with cuML\n",
        "# You can change the n_samples and n_features to stress test the HPO algorithm\n",
        "df_X, df_y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_classes=2, random_state=42)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y , test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Define Neural Network with Skorch\n",
        "Create a PyTorch neural network model and wrap it with Skorch for scikit-learn compatibility.\n",
        "\n",
        "### Define Neural Network with Skorch\n",
        "We define a simple feedforward neural network in PyTorch and wrap it with Skorch's `NeuralNetClassifier`. This makes the model compatible with scikit-learn tools and enables easy integration with hyperparameter optimization libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClassifierModule(nn.Module):\n",
        "    def __init__(self, num_units=10, nonlin=F.relu):\n",
        "        super().__init__()\n",
        "        self.dense0 = nn.Linear(20, num_units)\n",
        "        self.nonlin = nonlin\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.dense1 = nn.Linear(num_units, 2)\n",
        "    def forward(self, X):\n",
        "        X = self.nonlin(self.dense0(X))\n",
        "        X = self.dropout(X)\n",
        "        X = self.dense1(X)\n",
        "        return X\n",
        "\n",
        "net = NeuralNetClassifier(\n",
        "    ClassifierModule,\n",
        "    max_epochs=10,\n",
        "    lr=0.1,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Model Using RAPIDS Data Structures\n",
        "Train the Skorch model using RAPIDS-accelerated data structures and monitor training progress. We'll use GridSearchCV for HPO.\n",
        "\n",
        "### Train Model Using RAPIDS Data Structures\n",
        "We train the Skorch-wrapped PyTorch model using the RAPIDS-accelerated data structures. Training progress and metrics are monitored to ensure the model is learning effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "    'lr': [0.01, 0.1],\n",
        "    'max_epochs': [10, 20],\n",
        "    'module__num_units': [10, 20], # this parameter is inside of the ClassifierModule also this is Neurons in each layer\n",
        "}\n",
        "\n",
        "gs = GridSearchCV(net, params, refit=True, cv=3, scoring='accuracy', verbose=2)\n",
        "# the property .get() is used to retrieve the underlying data from the cuDF DataFrame and convert it to a NumPy array\n",
        "gs.fit(X_train.get(), y_train.get())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate Model Performance\n",
        "Evaluate the trained model on the test set and display performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Best parameters:', gs.best_params_)\n",
        "print('Best CV accuracy:', gs.best_score_)\n",
        "test_acc = gs.score(X_test.get(), y_test.get())\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "# Visualize grid search results\n",
        "import pandas as pd\n",
        "results = pd.DataFrame(gs.cv_results_)\n",
        "pivot = results.pivot_table(index='param_module__num_units', columns='param_lr', values='mean_test_score')\n",
        "pivot.plot(kind='bar')\n",
        "plt.ylabel('Mean CV Accuracy')\n",
        "plt.title('Grid Search Results')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Optimization with Optuna and Skorch (RAPIDS Accelerated)\n",
        "This section demonstrates how to use Optuna for hyperparameter optimization of a Skorch-wrapped PyTorch model, leveraging RAPIDS for data handling and GPU acceleration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install and Import Optuna\n",
        "We install Optuna (if not already installed) and import the necessary modules, including the SkorchPruningCallback. This callback allows Optuna to stop unpromising trials early, saving compute time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.integration import SkorchPruningCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a Flexible PyTorch Model\n",
        "We define a PyTorch neural network class (`OptunaClassifierModule`) whose architecture (number of layers, units per layer, dropout) can be controlled by hyperparameters. This flexibility allows Optuna to search over different model architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptunaClassifierModule(nn.Module):\n",
        "    def __init__(self, n_layers, dropout, hidden_units):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        input_dim = 20  # matches synthetic dataset features\n",
        "        for i in range(n_layers):\n",
        "            layers.append(nn.Linear(input_dim, hidden_units[i]))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            input_dim = hidden_units[i]\n",
        "        layers.append(nn.Linear(input_dim, 2))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Optuna Objective Function\n",
        "The `optuna_objective` function is called by Optuna for each trial. It:\n",
        "- Suggests hyperparameters (number of layers, units, dropout, learning rate, epochs)\n",
        "- Instantiates the model and wraps it with Skorch's `NeuralNetClassifier`\n",
        "- Trains the model on the training set\n",
        "- Evaluates accuracy on the test set\n",
        "- Returns the accuracy as the objective to maximize\n",
        "\n",
        "The SkorchPruningCallback is used to enable early stopping of bad trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def optuna_objective(trial, X_train, X_test, y_train, y_test):\n",
        "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
        "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
        "    hidden_units = [trial.suggest_int(f'n_units_l{i}', 16, 128) for i in range(n_layers)] # Neurons in each layer\n",
        "    max_epochs = trial.suggest_int('max_epochs', 10, 30)\n",
        "    lr = trial.suggest_float('lr', 1e-4, 1e-1, log=True)\n",
        "\n",
        "    model = OptunaClassifierModule(n_layers, dropout, hidden_units)\n",
        "    net = NeuralNetClassifier(\n",
        "        model,\n",
        "        criterion=nn.CrossEntropyLoss,\n",
        "        max_epochs=max_epochs,\n",
        "        lr=lr,\n",
        "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "        verbose=0,\n",
        "        callbacks=[SkorchPruningCallback(trial, 'valid_acc')],\n",
        "    )\n",
        "    net.fit(X_train.astype(np.float32), y_train)\n",
        "    y_pred = net.predict(X_test.astype(np.float32))\n",
        "    return (y_pred == y_test).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the Optuna Study\n",
        "We create an Optuna study and run the optimization for a set number of trials or until a timeout. Optuna will try different hyperparameter combinations, train the model, and keep track of the best results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Optuna HPO\n",
        "\n",
        "# This line tells Optuna: “Use the MedianPruner to automatically stop trials that are not performing well,\n",
        "#  so we don’t waste time on bad hyperparameter combinations.”\n",
        "\n",
        "pruner = optuna.pruners.MedianPruner()\n",
        "\n",
        "# This line creates an Optuna study that will search for the best hyperparameters to maximize your chosen metric,\n",
        "#  and will use the pruner to stop bad trials early. The study object will manage all the optimization work for you.\n",
        "\n",
        "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
        "\n",
        "# The Following line tells Optuna: “Try up to 20 different sets of hyperparameters (or stop after 10 minutes). \n",
        "# For each set, call the optuna_objective function with the current trial and the training/testing data. Use the results to find the best model.”\n",
        "# The lambda is just a quick way to pass the trial and data to your objective function.\n",
        "\n",
        "study.optimize(lambda trial: optuna_objective(trial, X_train.get(), X_test.get(), y_train.get(), y_test.get()), n_trials=20, timeout=600)\n",
        "\n",
        "print(f\"Number of finished trials: {len(study.trials)}\")\n",
        "print(f\"Best trial value: {study.best_trial.value}\")\n",
        "print(\"Best trial parameters:\")\n",
        "for key, value in study.best_trial.params.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Review the Results\n",
        "After optimization, we print the number of trials, the best accuracy found, and the best hyperparameters. This helps us understand which model configuration performed best on our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Optuna optimization history and parameter importances\n",
        "import optuna.visualization as vis\n",
        "\n",
        "# Plot optimization history\n",
        "vis.plot_optimization_history(study).show()\n",
        "\n",
        "# Plot parameter importances\n",
        "vis.plot_param_importances(study).show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
