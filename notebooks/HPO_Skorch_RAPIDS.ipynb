{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Hyperparameter Optimization with Dask-ML\n",
    "\n",
    "This notebook demonstrates Hyperband, a model selection algorithm now part of Dask-ML. It showcases:\n",
    "\n",
    "* **Problem:** Tuning hyperparameters for good model performance.\n",
    "* **Realistic Use Case:** Image denoising with a deep learning model.\n",
    "* **Hyperparameter Optimization:**\n",
    "    * Explores parameter space with Hyperband.\n",
    "    * Compares with early stopping techniques.\n",
    "* **Results Visualization:**\n",
    "    * Analyzes input/output data.\n",
    "    * Visualizes best model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of Hyperparameter Tuning\n",
    "\n",
    "Any machine learning model requires tuning hyperparameters for optimal performance. This notebook focuses on image denoising, where crucial parameters include:\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    'module__init': ['xavier_uniform_', 'xavier_normal_', 'kaiming_uniform_', 'kaiming_normal_'],\n",
    "    'module__activation': ['ReLU', 'LeakyReLU', 'ELU', 'PReLU'],\n",
    "    'optimizer': [\"SGD\"] * 5 + [\"Adam\"] * 2,\n",
    "    'batch_size': [32, 64, 128, 256, 512],\n",
    "    'optimizer__lr': np.logspace(1, -1.5, num=1000),\n",
    "    'optimizer__weight_decay': [0]*200 + np.logspace(-5, -3, num=1000).tolist(),\n",
    "    'optimizer__nesterov': [True],\n",
    "    'optimizer__momentum': np.linspace(0, 1, num=1000),\n",
    "    'train_split': [None],\n",
    "}\n",
    "\n",
    "\n",
    "These parameters control:\n",
    "\n",
    "* Model initialization\n",
    "* Activation function\n",
    "* Optimizer and its hyperparameters (learning rate, momentum, etc.)\n",
    "* Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Reduce number of messages/warnings displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PYTHONPATH=../src:$PYTHONPATH\n",
    "# !echo $PYTHONPATH\n",
    "!cp -u ../src/noisy_mnist.py . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date: 2024-02-07\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "print(\"Today's date:\", today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"/\".join(os.getcwd().split(\"/\")[:-1])\n",
    "results_folder = f\"data/results/skorch_run/{today}\"\n",
    "absolutepath_to_results = os.path.join(path,results_folder)\n",
    "os.makedirs(absolutepath_to_results,exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "s=socket.socket()\n",
    "s.bind((\"\", 0))\n",
    "port = s.getsockname()[1]\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, wait\n",
    "from dask_cuda import LocalCUDACluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-2af7bbc1-c5a9-11ee-914e-ac1f6b685fd7</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_cuda.LocalCUDACluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:33325/status\" target=\"_blank\">http://127.0.0.1:33325/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCUDACluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">3c2976bb</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:33325/status\" target=\"_blank\">http://127.0.0.1:33325/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 2\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 2\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 187.57 GiB\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "    <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
       "    <td style=\"text-align: left;\"><strong>Using processes:</strong> True</td>\n",
       "</tr>\n",
       "\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-fe918e87-9d8d-4973-aa82-328cd0c2cd3d</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://127.0.0.1:45399\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 2\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:33325/status\" target=\"_blank\">http://127.0.0.1:33325/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 2\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 187.57 GiB\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:40671\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:33789/status\" target=\"_blank\">http://127.0.0.1:33789/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 93.78 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:42051\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /tmp/dask-scratch-space/worker-55bloyxu\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>GPU: </strong>Quadro GV100\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>GPU memory: </strong> 31.74 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 1</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:35239\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 1\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:41265/status\" target=\"_blank\">http://127.0.0.1:41265/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 93.78 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:44341\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /tmp/dask-scratch-space/worker-4qzwcpg8\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>GPU: </strong>Quadro GV100\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>GPU memory: </strong> 31.75 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:45399' processes=2 threads=2, memory=187.57 GiB>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = LocalCUDACluster(dashboard_address=f\"127.0.0.1:{port}\")\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.64 s, sys: 343 ms, total: 1.98 s\n",
      "Wall time: 3.93 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:35239': {'status': 'OK'},\n",
       " 'tcp://127.0.0.1:40671': {'status': 'OK'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time client.upload_file('../models/autoencoder.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask ml version : 2023.3.24\n"
     ]
    }
   ],
   "source": [
    "import dask_ml\n",
    "print (f'Dask ml version : {dask_ml.__version__}' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Model\n",
    "\n",
    "* **Dataset:** Noisy MNIST images for denoising.\n",
    "* **Model:** Deep learning autoencoder with a latent dimension of 49."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import noisy_mnist\n",
    "chunk_size = 70_000 // 3\n",
    "_X, _y = noisy_mnist.dataset()\n",
    "_X = _X[:chunk_size * 3]\n",
    "_y = _y[:chunk_size * 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69999, 784), dtype('float32'), 0.0, 1.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_X.shape, _X.dtype, _X.min(), _X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69999, 784), dtype('float32'), 0.0, 1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_y.shape, _y.dtype, _y.min(), _y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dask.array<array, shape=(69999, 784), dtype=float32, chunksize=(23333, 784), chunktype=numpy.ndarray>,\n",
       " dask.array<array, shape=(69999, 784), dtype=float32, chunksize=(23333, 784), chunktype=numpy.ndarray>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.array as da\n",
    "n, d = _X.shape\n",
    "X = da.from_array(_X, chunks=(n // 3, d))\n",
    "y = da.from_array(_y, chunks=n // 3)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsEAAACuCAYAAADXjNDSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABRIklEQVR4nO2dedwd4/n/37FLEVsITQlFBa3YiS1SS4uvfVdF1K4osVRERGyVSKQltpRQ1C60qKULUUqtrdhbVFs/2mo1VVTV7w+vz9zXzLmfk3PO82R7zuf9enk5mTnPnDlz7rln5rquz+fq8cknn3yCMcYYY4wxbcRcs3oHjDHGGGOMmdn4JtgYY4wxxrQdvgk2xhhjjDFth2+CjTHGGGNM2+GbYGOMMcYY03b4JtgYY4wxxrQdvgk2xhhjjDFth2+CjTHGGGNM2+GbYGOMMcYY03b4JtgYY4wxxrQdvgk2xhhjjDFth2+CjTHGGGNM2+GbYGOMMcYY03b4JtgYY4wxxrQdvgk2xhhjjDFth2+CjTHGGGNM2+GbYGOMMcYY03Z0+5vgSZMmseiii87q3TDGtDHTpk0r/ttqq63YaqutmGuuuZhrrrm46aabiv+MMcbMPLr9TbAxxhhjjDFV5pnRH/Cf//yH+eabb0Z/TFvwxz/+EYDHH38cgKeffrpY99ZbbwFw4403AvDOO+8U61ZaaSUAXn755Zmxm7M1p556KgBnnXVWsWyfffYBYJ111imWHXfccTN3x0y35pRTTile/+xnPwOgR48eAHz/+98v1u2+++4zd8e6AS+99BIAd955JwBXX311se43v/kNAP/73/8AmGuuFPdZfvnlAfjJT34CwCqrrDLjd9YYM1vRdCR42rRp7LvvvnzmM59hmWWWYdy4cQwaNIhjjz0WgH79+nHmmWdywAEH0KtXLw4++GAAbrnlFlZffXXmn39++vXrx/nnn1/abo8ePZg8eXJp2aKLLsqkSZMAeO211+jRowe33norW2yxBT179mTNNdfkkUceKf3NpEmTWG655ejZsyc777wzf/vb35r9isYYY4wxppvT9E3wcccdxy9/+UvuuOMO7rvvPqZMmcKTTz5Zes/o0aNZY401eOKJJxg+fDhPPPEEe+yxB3vttRe//e1vOf300xk+fHhxg9sMw4YNY+jQoTz99NOsssoq7L333vz3v/8F4NFHH2XIkCEcccQRPP3002yxxRaceeaZTX+GMcYYY4zp3jRVDjFt2jSuuuoqrrvuOr785S8DcOWVV7LsssuW3jd48GCGDh1a/Hvffffly1/+MsOHDwc+TTs999xzjB49mgMOOKCpHR46dCjbbbcdACNHjmT11VfnlVdeYdVVV2X8+PFss802nHzyycXnPPzww0W6a05kzJgxxWul+x588MHp/l1M+ynt2s787ne/A+Caa64Bysfn+uuvL/0fUsnJVlttBcBuu+1WrFt44YVn7M6absPEiRMBuOyyyzp8z6677jqzdqfboLIv+DQwAvDqq6/WvE9z36qrrgrA17/+9WJdr169APjc5z43w/bTGDN701Qk+Pe//z0fffQR66+/frGsV69efOELXyi9b9111y39+/nnn2fjjTcuLdt44415+eWX+fjjj5va4S996UvF62WWWQaAt99+u/icjTbaqPT+6r+NMcYYY4xpKhL8ySefALWRRS0Xn/nMZ2rWT+9vevToUbPso48+qtmHeeedt/Q3kEQP1b/vDsRI+UknndTSNhQhWWKJJQA477zzinUHHXRQ6zs3B7HHHnsA8MYbbzT0/htuuKH0/3POOadYd/PNNwPlB7J2Zdy4cUBZSDjPPJ9OKz//+c8B2GSTTWb+js1ifv/73wMwYsQIgKJkK7LlllsCFLoJ0zEffvghAKeffjpQnsN0HVhjjTUA6Nu3b7FO2cc+ffoASQw3OzF27Fhg9hPjSm8zfvz4YpkCYLPbvhrTKk1Fgj//+c8z77zz8thjjxXL/vnPf07XdWC11VbjoYceKi17+OGHWWWVVZh77rkB6N27N2+++Wax/uWXX+bf//53M7vHaqutxq9+9avSsuq/jTHGGGOMaSoSvPDCC7P//vtzwgknsPjii7PUUksxYsQI5pprrrp1p8cffzzrrbceo0aNYs899+SRRx7hwgsvZMKECcV7Bg8ezIUXXsiGG27I//73P0466aRS1LcRjj76aAYOHMh5553HTjvtxL333jtH1wMbY4wxxpgZQ9M+wWPHjuWwww5j++23Z5FFFuHEE0/kjTfeYIEFFujwb9Zee21uvPFGTjvtNEaNGsUyyyzDGWecUUr1n3/++Rx44IFsttlmLLvssowfP54nnniiqX3bcMMNmThxIiNGjOD0009nyy235NRTT2XUqFHNfs3ZhmppSWTIkCHF6yuuuKLD96lc5B//+Afw6cOCWGihhQDYc889O7ObsxUqeVApA8Bzzz3X4fvlYz1gwIBiWcx2QBLWAZxxxhkAXH755QAstthindvhOYR4DDTeZHUYhYZRM9BdUDr+X//6V7FM5UU5Ro8eDVDKbgmJtFpxx2kHlAGMrkMqf7jrrruAsjh10KBBQPJbrve7zI7MTqUF0XJUZV+xk6EEiRrXGudzGi+88ELx+oMPPgDg1ltvBeAvf/lLQ9vQsbjvvvuKZWuvvXZX7eIcRQw2qnRwypQpQLl89qqrrgJgl112AaBnz54dbjP+Diprev7554tlGp+9e/fu1L43fRO88MILc+211xb/fu+99xg5ciSHHHII8Kmfb45dd921rgp62WWX5Z577ikt000bfOo/XK35XXTRRWuWDRkypHRzCJ9Goo0xxhhjjBFN3wQ/9dRTvPDCC6y//vq8++67RVRsxx137PKdMzD//PMXr6u117Kpy7H00ksXr2UFpM5KevIFOPTQQ4EUPZFYZ05GEdrYFU4suOCCQHIWgRRRP/HEE4tlt99+O0DRBOZPf/pTse62224D4MADDwQoLPu6OxIXQrlbIZSFSsrwKAPRHZCwrV4keNq0acXr6gN9jIbst99+QHkMmoSsIPfee+8O36PzE2DzzTef4fvU3VEEOB5zZdRilkfn9Jxgu6nOjJCyWFdeeSUAzzzzTLHu/fff79TnRGu+dokEK0ory8F77723WKexUf0/wP777w98qt+C/PHS/Bh1ZH/4wx9qtqXrsAKwrdJS2+QxY8bw4osvMt9887HOOuswZcoUllxyyU7tiDHGGGOMMTOLpm+C11prraZrdY0xxhhjjJmdaCkSbGYeMRW14oorAqmA/89//nPN+9W9T53RAD772c8C1DQ1gZTCbdaObnZE9eESMeWQkEtlIB2hwn2JmLbYYoti3V//+lcgpWPapRxCTWkiK6+8MpBSWDDnCZMaQQLVekJVedgCvP7666V1yy23XPG62S6Z7YY0HFHvISGcyiBcAtE1qAxi4MCBQPl6o+Mfy5qk64nlT7ML6qS68847A/Duu+8W69SUS+n0OLZUOrjIIosA5fS6jodKdCD5n3fHvgT1iJ1qJUbNHU9Rb5nKKeJ9isoYc4I6jcGlllqqWLbppps2/yUyNOUTPKP5xS9+QY8ePUqCOGOMMcYYY7qapiLBgwYNYsCAAVxwwQWd/uCu3FZ3JorYNttsMyAV4ue6UF133XVA+SlJ2zjmmGOAcgeg7oQsU8aMGVOzTt3ddtppp6a2qQL+3XbbrVh2ySWXAO3TiEURlVyL88MOOwwoP6G3G+oOJ4uuHEcddVTx2oK4Wr75zW8Wr9966y2gHAlSBscR4M4TbdAkhFPEMyeCi+5KypDNjihTpciurC8hda1UpnSHHXYo1vXv37+0Lke061IkeE4QB3YFsiWTqA1qRW/xfkOR+JxOTEI6bStGi//2t79ltw3p+nL33XcXy/S7dZYujQR/8skn2RszY4wxxhhjZicavgk+4IADeOCBBxg/fjw9evSgR48eTJo0iR49enDPPfew7rrrMv/88zNlyhQOOOCAmojbscceW9SR5LYV/YWfeOIJ1l13XXr27MnAgQN58cUXu+K7GmOMMcYYAzRRDjF+/Hheeukl1lhjjcIbeOrUqcCn/qpjxoxhxRVXZNFFF21pW7179y5uhIcNG8b5559P7969OeywwxgyZAi//OUvm/xqczbPPvssAD/96U+LZVVXjii22X777YGU9o+om58EchGlFPr169e5HZ5FKBUNsM8++3T4Pglqon9yMwwePLh4rXKIduHSSy8FUpoa0riJZSLtymmnnQbAP//5z5p1iy++OEBNAx9TZsKECcVrpUE1p0HzZUwmIb9flW9Fv2+VP6j0YaONNirWqfRhdupoVw/NRV/96leBcrmgvPJbRf7CER27et1yuwMS+cqrF5LfvsbIt7/97WJdtUwhlpJInK/AZiyHqIrsJEqHVOrYVSUQkYZvgnv16sV8881Hz5496dOnD5BaD55xxhlstdVWDX9obluRs846q6j9Ovnkk9luu+344IMPuv1gM8YYY4wxM4cusUhbd911u2IzBTGaKRHJ22+/XYp8dlckYlMEOPcUroL/Nddcs1j2ve99b7rbVjRl0qRJxTJ1wIoCvDkB2ZRJmAW1HcoOOuig4nUuCm4aQ5HOSN++fUv/b2fUmSonlDnnnHMAWGyxxTr8+xhhmThxIgDPPfdcsUydIQ8//PDO7+xsgqJ0yjJEFI2MQsPuaLs3s9hrr70AeOyxx4Cy+E2vdcyvv/76Yt2cem7XszFsldz1UVme7m6ROXnyZKA8vykCfPXVV3f4d4oAR2GlIsC5ubIaVY6R4J49e7ay6w3RJcK46qCba665ajziPvroo4a3N++88xavdbC6UwtWY4wxxhgza2nqJni++ebL2iRV6d27N2+++WZp2dNPP93StowxxhhjjOlqmiqH6NevH48++iivvfYaCy20UIfR2cGDBzN69GiuvvpqNtpoI6655hqeffZZ1lprrQ63pdRCuyMv4HqtqVXOsOeeeza1bdVwx0J1+eDOaeUC8hWNwkFx5JFHAnDSSScVy+aee+5Ofd5LL73Uqb+fE1EHtGayOO2EyiDkb5lj7bXXrln23nvvATB27FiAkld6rlGQOhNKeBfH9ZyKgiTyLo8oDe8SiOaRCE4lEJB8gXPdvTbYYAMAHnrooZm1i2YOJY6bRoSq6kdw7bXX1mxDXsLxnnDYsGFdsZtN01QkeOjQocw999ysttpq9O7du1TLFtlmm20YPnw4J554Iuuttx7Tpk0rTJKb3ZYxxhhjjDFdTVOR4FVWWaXUbQY+9fzNMXLkSEaOHNnUtvr161dTSzxgwIBu36P71ltvLV6rG9yMIHccH3jgASBFnmf3iLBsfu69996adYr2SljZFd9FAryLL76409ua06hXhz9gwICZtyOzKVdddRWQ79y45ZZbArDOOuvUrDvxxBOBZLWXswnKoU5V3SESLLGMvnscazfeeGPp/5A6Tenc/ta3vjVT9nNOoyqCgzSmJIJT9BfKQjjTHM04YnUH4txUzwZN56qu0fHvTjnlFCCJ32ak4K1RurRjnDHGGGOMMXMCvgk2xhhjjDFtR5f4BJvWmDZtGgA/+tGPimXvvPNO6T1R0CWhUrNdutRtT+mw3r17F+uUxpCf6Q477NDUtmc2v/vd74C8gEhWfd/4xje67PPkVfqnP/2py7bZHdh7771n9S7Mcn7wgx90uG7o0KGlf3/ta18rXl933XUd/l290q85vSwsin3vvvtuoDZVH5dFqsd63LhxxevLLrsMgK985Stdt7NzALFcROejxkg8hlpmEVzz1CsJW2mllWbinsw6dD29/PLLi2VqZqbyBp2DUNsNLvr9jho1asbubAs4EmyMMcYYY9oOR4JnIXqKqtd1Rd3h4vubRV3VYheqKrH39+zM2Wef3eG6ztqgxUjbXXfdBcCIESNq3qeolWxeTHtTT8wmG7Qbbrihw/dHuyFlbaLQRMzpNpIvv/xy8fr9998vrVPHMsgfzw8//BCAJ598EihnZiRQbLdIsCyoIM1JilzGyLoiwBbBNY86scqeMKJoaHdHNqrx/kPXYWW8cuJeRYB//etfz5T9bBVHgo0xxhhjTNvhm2BjjDHGGNN2uBxiJjF16lSAUtOQZ599tsP3y3/5rLPOaupz1KHq7bffLpZVU6uLLrpozT4stthiTX3O7ELcb4ltWkUlEFBfIHjaaacBMHz48E593pxG9HScd955Z+GezP7ccsstANx0001AXmAjL+FddtmlWPbjH/+4w21WxXZzGu+++26H66Yn1vr3v/8NJAFYvePUXal2g3v44YeLdSp/kDf6wIEDi3Uug2iOv//978XrCRMm1KxfbrnlgHw3yO6I5v3Pfe5zxbKqSDf++5BDDgHmHG99R4KNMcYYY0zb4UjwTOK2224DykK3XKepI444AoDtt98egD59+nS4TVmmAXz88cdAshD717/+VaxbYoklADj22GMBWHDBBYt1yyyzTMPfYVZxzTXXFK9feuml0roYCV5vvfUa3maMJI0ePRqARx99tMP3R0HenB6Ra5Udd9yxeC2xhMkjO6GcZZW4//77AbjvvvuKZbn3rbjiikC++9ychLq9tYKiUSuvvHLNumeeeQaA119/HYDll1++5c+Znal2g4viN71WBNjR39Z58803i9dRzCmOPPJIoJxR7c48//zzQLJRhTRP5eYrvX9OwZFgY4wxxhjTdvgm2BhjjDHGtB0uh5gBvPLKK8Vr+QyqSFxlC5E99tijeH3ooYcCsMACC9Rsa8qUKUDyHo1pB6Vd+/XrB8DSSy9drLvkkksAGDx4cCtfZ5bz0UcfFa/rdfCpx5133gnAb37zG6AsOKx6lkLyOBwyZAgARx99dLGus37Epnuw8cYbA8nbN0cjXd5y74ldln7605+2sHezH8cff3zxutXud/q7OA+ozKI7lUGoG1zszFj1AJYIDlwG0ZVMnjy57vo11lhj5uzIbMJ7770HJHEqwDHHHAOke5DYDVL3Kfvttx9Qv7Pm7IAjwcYYY4wxpu1wJHgGcMIJJxSv77jjjum+P/aAj6+bQXZLsmRaZJFFWtrOnMYHH3xQvH7ggQdK62KE7owzzgDKUWUx//zzAymyB3DllVcC0Ldv367bWdOtkLhSncokWMpRr6tcZJNNNgHg2muvLZbNCeLVesjaK1pPNXo8hOwPv//97wPlSPmGG27Y2V2c7VA3uCh+E1URHDgC3JXkxHBRBPeFL3xhJu7NrKd3795AykBDEtdr2T333FOs22677YBke/jXv/61WLfkkkvO2J1tAUeCjTHGGGNM2+GbYGOMMcYY03a4HGIGcPDBBxevlSb48MMPu2z7I0aMAMpCt89//vNA+5RBiD//+c/F62aEf+r6A3DUUUcBZeGOqUVpbYB//OMfQPt4Zebo1asXAD/72c+Acgcp+VnrPbFTpESvK6ywAlBOa8t/eZ55us/ULI/fVVZZpVhWTTm/9tprxWuJe3/7298Wy5R2lf/54YcfXqyLotU5kWonOIBHHnkEKJeNSAinkrnuWAYyK5HPdE7Iteaaaxav5dvdnXnwwQeL1/I8rydwi3PfWmutBSSxXPT5V6+C2QlHgo0xxhhjTNvRfcINsxHbbrtt8frCCy8EUpeZ//znP01ta5tttileL7vsskAS4qy//vqd2s85BYmFIEXKFEnKCd1yLL744gAcd9xxAOy///7FOh1XU5/YrU9ih3aOBAuJRJRRgBSdfO6554A5X9zWGdSx8oorriiWyXpQESfNaQAbbbQRAL/+9a+LZS+88AKQxtucHv2N5Lpv6XUUxjkCPGNR58acDeewYcNm9u7MUqJNXP/+/af7fonnIGXCn3zySQBefPHFrt25LsaRYGOMMcYY03b4JtgYY4wxxrQdLoeYwSjtp3Roo+l7odQglFMO7cTKK69cvJZYZtCgQUDqTgOwyy67ALDBBhvUbOOwww4DYKGFFppRu9ntkKejUrNKa0MSO5mESp6qr82nxPFz++23A8nb+6mnnirWSUB3yCGHFMsmTpwIdM/jKi9y+aqaWcMf//jHDtett956M3FPZj3xunr11VdP9/0SFUIyA1B3x9nRGzjiSLAxxhhjjGk7enzSahN3Y0xboEhAjFRNmDAB6F5WXsaY9uPjjz8GktA8ZiXUmVB2X5CEsN2Zs88+u3i98847A3mB3K233gqkDoeQOrWqs14UuM6OGURHgo0xxhhjTNvhm2BjjDHGGNN2OJdpjKmLup1Fr8ypU6cC5U5KxhgzpyE/21gGIQYMGAC0RwlERCUQAKeeeipQFuZfeumlQPKxjlW1et8+++wDzJ4lEBFHgo0xxhhjTNvhSLAxpiHeeOONWb0LxhgzQ1lsscWK192pM2EzRBGcosISvEHq3KrsYLRUU4fXfffdd4bvZ1fgSLAxxhhjjGk7fBNsjDHGGGPaDvsEG2OMMcaYtsORYGOMMcYY03b4JtgYY4wxxrQdvgk2xhhjjDFth2+CjTHGGGNM2+GbYGOMMcYY03b4JtgYY4wxxrQdvgk2xhhjjDFth2+CjTHGGGNM2+GbYGOMMcYY03b4JtgYY4wxxrQdvgk2xhhjjDFth2+CjTHGGGNM2+GbYGOMMcYY03b4JtgYY4wxxrQdvgk2xhhjjDFtxzyzege6Az//+c8B2GKLLQB49NFHi3UbbLABAE8++SQAa6+9drHur3/9KwBLLrlksexvf/sbAEsssUSHn/fiiy+Wtgmw9957A/DCCy8AsOqqq9b83VtvvQXAf//732LZZz/72TrfrJYrrrgCgCFDhjT1dwAPPPAAACuuuGKx7HOf+xwAxx13XLFs2WWXBeCoo44CYIEFFqjZ1tSpUwFYffXVi2W/+MUvABg0aBAAH374YbFu8uTJAOy+++4AvP/++8W6z3zmMzXbP/roowH47ne/C8Bdd91VrFt66aUBWGeddQB47bXXinVvvPEGAP/4xz8AWHjhhYt12q/Oon358pe/DMD8889frPvPf/4DwHzzzVfzd88++ywAv/vd7wDYcccda96jsQXwhS98Ifv3AGussUZp3b///e/idc+ePRv4Fp/yySefFK979OgBpDEGaZxp7OrYN0M8T377298CsP/++9e8Lze29bkag3PPPXexbqGFFir9/cknn1y81nm/8847A2lcQBrzm2yyCQAPPfRQzb5o/AA888wzQPptt9pqq5p1a665JgC/+c1vinU6RzQ3dBW//OUvi9f6DvF3rHLuuecC5eNTnTMjN998MwC77bZbsUzz2gcffACUx6mOx+KLL97hPuTOC50Hn//85wG4/PLLi3X/93//B8Dbb79dLPvSl77U4fY7QueqzpfFFlusWKe54S9/+QsAvXv3rvn7eF5dcsklACyzzDJA/nd95JFHav5u5ZVXBmC55ZYD0jGE/NwqHn/8caA8v3zxi18svefXv/518fqll14CYN999+1wm12FriWrrLJKsUzH5c033yz9O8cf/vCH4rWOSz3efffd4rXG+qKLLgqUj/XHH38MlOd9ofGl8/8rX/lKzXvee++90nsBjj322OnuX0fEa73mLo3pueZKMdDc2BOaA+N9iuazXr16AeVxtOCCC5b+/t577y1eP/300wAceeSRQP7aq+P73HPPFct0z/CnP/2pWKZz6Y9//CMAiyyySLFO+1VvThCOBBtjjDHGmLbDN8HGGGOMMabt6PFJvTyW6ZD77ruveB3Tkx3xyiuvALDSSivVrFOKFlK6SWmtjTbaqOb9Sgk8/PDDxTKl+Vvl7LPPBuD4448vll111VUAHHLIIcWycePGAfCtb32r6c9QGnXjjTeuWac0EqSSkPvvvx+AffbZp1g3duxYIKVhVVICsO222za8L7k0fERp5ttuuw2A008/vVh38cUXA3D44YcD6beF2t/3lltuKV4r/ZVLAU2Pf/7zn8XrmPYBePDBB4vXm222GQDvvPMOUE4Hvf766wBcf/31QPl3VRoulnboGIuYElS66ac//SmQSjMAXn31VQBWWGGFmn1XCk6/W0wbqgTozDPPLJadeuqpdJbTTjuteH3GGWcAKR130UUXFetUgqD0H6SSkF/96lcAbLjhhg19po61UqX9+/cv1v39738HUhnEWmutVazr27cvUD4GKs9QmVBE42vXXXcF4M9//nOxLvf+zvC9730PKJd06VxW2nWeeTqusPvXv/5VvK6WkuSYNm1a8bqaXlbKG1La++WXXwbK5SXbb789kNK9q622WrFO6WzNo3vssUex7sYbb5zu/jWDxsPyyy9fLNM59/zzzwPl46pzRvsIaS7S+2KZgq4XKnG64447inVLLbUUkEoA4pyw9dZbl/Yznv/9+vUD0nwM6RzV/lXnoq5E85rmGkhlPyKWxYhqGVe9bUM633U8Bw4cWKzTfHHOOecUyzRPqJRQcwM0Nj80cuyuueaa4vXXvva16W6zyv/7f/8PKJ839a47KpFQKVs8P6vzOaR5pt4co7KbXMlNvCYIHY///e9/QLlcI4fuwXL3Xx999BEA8847b91tgCPBxhhjjDGmDXEkuEVyUQqJNxTNgfREJcGKBCEAX//614GyWKJexLiKhB2QxB16So3ipJxIalYwYcIEAI444oi671MEeMstt6xZpyjmXnvtBaRjCHD11VcDKbKuInyA/fbbr8PP01O3oqhQK5bQbwtJdLjnnnsCZcGPonoSjKy33nrFOkXwmxUjQlkspsiOTl2JeLoaRUgUKZPIK5L7Ts1ETaPQIXdczjrrLCBF8HL70AqKSuYikhJkAlx44YUdbkPRdomuclEXRVj0m0GKqMcIl6iXAcpx0EEHAXDppZfWrKsXlW0GiZAkQIkZKEU4dX7pnIDaCJL+HspzJJRFrNp+zDzcfffdQPqecf7Vb6mIcE6EKKZMmVK83nTTTUvroqhQIjiJvaAswmqVOP/rOyuqFqOy1eMT0XGMY0pzvCLyyuJBOnckwJOgF9I5p0xcFH7q+qKIMMDQoUOBND9GcW0UPHcliupBiuxpboqRcs37Oi5xDtfvKHFXjIYrMxOvw+InP/kJUBaxVUWWMdu30047ASlTEYWViuDnrg267isaGn/brkbnR4zixt+9iiKz8ZzLzV2imoWMQtpcFrgjnnrqqeJ1zJY1grIsjWRdHQk2xhhjjDFth2+CjTHGGGNM2+FyiCY59NBDgbLPrERizYbslWaIyBtP6b9ddtmlWKeUVSNpuZxARt6DBx98cLFu/PjxABxzzDHA9FMXo0aNAmD48OHT3YcqjabJc4X4jaBSkJgiq5IrplcK8Qc/+EGx7MADD+xwG1deeSWQvGZzBfw6ThLPQdlnsVkk1IOUdtT4UUoa0rFTylQCMEjprJyHpZCgAqBPnz6ldTE9pTICeZDGsgH5hGq81vvesRxC24ypNpUHKMW23XbbdbitmUHOK1nnkERYkMqTRPSZVlnQnXfeCcA3v/nNYp2mYwlyIJ+m7YhYVhCFU52hngimOqaigFQpXZU/TZw4sVinOUwlSEpfQj6FqeOuY54TadZDqX3Nq3EftO+5EhQJP6Es/mwWeTZH0ZZEXdHrvBninKDz8J577gHKZUMqF1HKu+rjCvD73/8eKPvKqpQkvl8iOY2tRgSOrfL9738fSCU/UOvtHKnntyzxYRSoVlHZRTwGuWMlJHqN9wIqqcn5X1f7AqisIn6mBGPVubdZtO1Yxqe5QWUYuXsE3X/E0pZ6XtLyYI8e2jqXdV7myll07GJJZ9WDOsePfvSj4rXKWCQcjCL5ZspJHAk2xhhjjDFthzvGNYmEGdFeqvo0HJ/wVDx/wAEHADBixIhinYrm1V0KUqRDT3DxiVeRI1l6xYL8b3/726V9WXfddYt16qQm+xtFJaDWfqVe5xhIndRaIRcBVuQrRsPqiZGqxEirrMskkIuiOaHfJtqiyVotRvJ1rKIoRChKLJudKKhTxE9P3fWirs0QOwAqCqPoQYxuSAyV+1wJjrbZZhsgRY0hRd1jVENIVFIv07H55psXrxW51DiLkWBFhxWZietuv/12AL761a8WyxoViNUjjnGJgvSbx8icIrk//vGPO9xWtNjS+atzOxd5VWSoXgQ7dpzSGI7WPpprLrvsspq/VeRekaNoGSWbrM7azCnToY5lhx12WLFO0RhFt2MXMx1b7VPMvkSrMChHizTPxXOv2qEwRo71PZXJied27FZVRX+nuTZm5qrWb62iSFcu2qtlimBGy6xGovhVuzBIIlnNgQCDBw8G6ovtcqI2CYujBaSs+GLGQShKrGhdtLZsRZwdI8CiXuS53rWrmlWsZ2uZi/7qGgqp86fE51GEXo0ARyswCcWqWQ1Ix0eZsc5GgrW9mL3RXKHPyGXo9J4YJVZUNUZ7FQHWdSmKcBUBVjYyHk9Z7D322GOlf0+Pn/3sZ0BZBC4hpOanRrrD5XAk2BhjjDHGtB2+CTbGGGOMMW2HhXEziXqisJiulBBDAq7owyiRUC5lrXSWUg+x5EEp4AsuuKD0nriuUXKpnK5GpR1K36ljFtSmquRhCiklL/FLTKvruOpYxy4/IpaeqEh/8uTJQLnDnDqQRfGSUDpYaZ5Y2nHKKacAqTtfq6ikQAK9b3zjG8U6pZKVsosiI3mkVv1R47ZiijV2sAJ4/PHHi9cqwdD2ozerjr/S+LFsR+lUpaJ32GGHmn3RMYfku5nzMe1q9B2q3rKQUuYxratzTp6lcUzpvFUqOqZxleZ/4okngHJ3L6UQ47hWCdK5554LlP1k9Rvl/EzrdWxqBqW3c37UKmuRGFVevdND51OuW6M6o8V0bXXMxq5+midUuhA9wVUioeMfvU712doXlXbE7UfRqcqIWkHp3/XXX7/lbQh1xotzoY7Bo48+CpRFRkpta7zGc7ValpITAuZKAXI00kWsGdShMJZq6LpT3e8csQupxoZKPGI5RyxngzSmAZZeemmgnGpXiZPKA+L5KxFiLAVpBIkc9ffN+Ol2BeomKKLHcj3PXZ0nOk4R/Z1El5DGZe4aXZ0Tcr0YoiBW5UN6//TEtR3hSLAxxhhjjGk7HAlukXpdiSISZlR7tEN9SxehCAIkAZ2euKMYSdGBesg2Z4899iiWqfOanozVEQjSk3hXo4go1I+KKvIVo+eKRORs1PQdFKFVtzFIgqbvfOc7QDkCqchz7A9fZbfddite33TTTUASRERhkyJ48bcRel8UUnQGCdaiZZ7GgbIFseOVhG0ab1GIU08IqP2OUe0TTzwRSEKusWPHFuv0FK7xE38/RfX1/tilS1G7SZMmFcsURVYEuRVktQTpHFJEKVrIKXofxXiK0KgrYPyt9TvmOk7JrkeR0ThOJXoaOHAgUO72lovo1kNRFom6cp0mNSZko9Us6qioKHicP4REl/pOOWKkddiwYUDKPERx8IABAzrchizYJMSDNI/qUhbPB0XnFc2MXSQ1T9TrGJYTOXc13/3ud4Hpi471G+csGUW1mxmk7NeZZ57Z4d/V66AYo/CKyEXLv46I869+785SzW5EgZuihIo2RrvPnGWZUAc3RbBjhF3RRYn+oP74FPUyHcqmxgi35pJ6kdWuJtcNUfNdzLjpO0RxZ/X7xXO7eh7FDKIE+5r3ozBO3QurYt/4eTESXM8KtRG7VOFIsDHGGGOMaTt8E2yMMcYYY9oOl0O0SBTBKN1bLZiH1IFF3nnyQoXUSSlHrjOORDZKzUbv2HpCAYnfJMSJQip1kROxm5x8UKPfrrrNxML5RlEqOXabk2/y/fffXyyTIEOlGdV9bJSYbpL4QaKbmDJVqix6gv7whz8EUmpmyy23LNYpLaQSiU022aTms2+44QYA9txzz5b2vUo8TXMptlbIeWWqLAJSmnD33XcHyoIRpe9y5RPyuFWaNx47fY48HpWGha7xBJ4eOq+U4o++3fXIpZmPPfZYIAlOY8pXIkQJauJ5r7ISlWLELkg58ZmOkVLA0ZNTPqQqbYnlTEcddRRQ+zs2i0oucl6yb775JpDKPnLiFP3WuY5Q8j5W90VI31ddLCGNVQm/Ikq/aj9zKVCVa8QxLO9mpV3rdUucVURRUe74N4KOXa5URmgMRl9lCZOuuuqqmverdEPdEiF1ddSYr+fF2wj1xIS5lHmVXIq+Xsc5XYOiX7TmsDh2VVqjsa9rPCQx3kMPPQSUS8E09uqVDImZPRbl061Sy3g/Ue/817kaS090zFQyE6+1mjN1vseOdrqu6hqt8ghIc0H8HTQu9XupBAKa8wx2JNgYY4wxxrQd7hjXIjmrM0WAo5hCUTE91cSnonrknlTVNUV861vfqnm/nnTjU5siwIroxqhr1SZoelHX2JGqWVR0Hy2bFMGK21XBvvZFdnFQ7joFqWsbpCfyUaNGAeUnSUV2FbGKkScJTWJkXpZDxx9/PFA+Znr6lWgrCoXOP/98oOtsgkR8Ytb+SpwZI616itbTu/YfYOLEiUDKBOSEPjGyq+yFiN3Sqlx00UXFa4nfFCmpdmuCJGKM0V9FI2K3NEWDJD5t1IIrovEAKQsROzkJiV9yXYw0fuLYrc4BUQik80qZgCh+lXhSkeAY/c2JHdWhTfZ7Es9B6rSo82jMmDHFultvvRVoPQIsNJeMGzcOKM871d8jnquaDxW9idaOypppjMUOaRK9RbGv5g6d07KSi2jMy0oSkihHv3uMwkmIKUs1Rc4hiTpbGW85lFWLwspqFkURc0gZmVxmoB45OyvNUyeddFLN+xX503wX57k451TReasIO6QsoWgl+htRZi52L9O1NZeBEooAR5tQRVY1T6njGaT5PNdlU/ugLmuQxJUS18brsqzNZP0VM2sau4oSxy6Jul6oy2Jno78333wzUBZ05+ZXoWOgeSueq7kIsL6z5iSNb6id76MAT9dYvT9289QcoG6H8XfXNuK4rgo+W+3u6EiwMcYYY4xpO3wTbIwxxhhj2g4L45rkxhtvBMrF+vVSM0KpJXkEQkpVxLSNisOVrql27YLUbSX67Ub/y+o6pQCVbog+rEp/bb/99kBKmUO50Lz6/phm6QpiWYNEbDkRgzrb5IR5Ks7XtuIxkO+hBEoSxUSUPgbYZZddSuuib2NM/UBZtCa6qjuciGnHqrDi5z//efFaIr+c32z12MXUtdJvUXhQ9YKMKTqVVKgEIB4DeTkq1S3BDKQxpRRbTvAXS2MkhIrizM6gNLzKGnLlSRI1xv1UCjF6hEoQJ7FHLBPQOpVBxE5b6oSmtH9M7SslHjsp6XhIEBu3FcVjVSRQ0bZa9QkW9c69zhLT0yr3Oeigg2rW17tcjR49GoATTjihZl09T3b5SMfSHM2x8f1dMQZV1gHJI1nHNabj5Rcbxc/1qAokG0WlIxJdy4cXkkA5dpE7/PDDgeQvH7+PSnM0huO83VXe6FXi9VTp81xnP5UiqKwh+uDqN9a8Fa97ErHGDnPxugKpBA3SvKLPy4lBRSybVJmMRJ2xg9uMON+q1PMSly9+nCtV1qD9jHOY7llUChLnf30X3UfE30/zp46FzllI/sLxWOgave+++zbyFTukqUjwoEGDignfGGOMMcaYOZWmIsHvvPMO8847b7aAfEZx+umnM3ny5JpI55xKtFGKQhohIYee6OPTV9UaKwqVJArL/Zx6UpIwQhGIuE3ZjESrEz3RxW45ElC0Qr3C/ByylOnfv3+xrF5HLXWDU8QsIrHWaaedBpSL6BWxiBFeRTPUZU0iCEiCL0XWY/REKHIrqyyAnXbaCUjih1a58sorgSSM03eC2u5TEupBEskpyhSttiRCPPTQQ4tlshW64447ANhxxx2LdRLj6XeIAjyhcaPsCaRIdaMRLqHfK+5zZ8jZ/uh7xgiNOhMquhiDABLgXHHFFR3um8RhsTOexk9O/CLhThTuqaOhxJzRskrbUgQq7ruWVQWOraJoT9w3ddd6/fXXgSRWgSRYiZHrjvjxj39cvM6JwXTe65zT7wJw4IEHlv6v8wNShzmdD1GAF0U2Mwp1DlT0PxfhUzQ17o/EzK1aLEZRoeZRRSdjVFbHR9eBGNHTtSBeL2QNqrEVxUsSd0lkFaOtsQtboyjCGoWqyhKKmDXSvJbr1qlzO9fRU+NNGdI4z2kOVIdMSIJKie3iWKwndlRUWNH3nCVkznZNEflmkHBNEf6I7qUa6XwHKTussQLp/kQR8riPEgxr3ESRuK61Gg9x/yTmyyHRpawtoTaLp8xx/MxGxl1TV5RmvNeMMcYYY4yZXWm5HKJfv36cffbZDBkyhIUXXpjllluuMD6HT+tBevTowfXXX8/AgQNZYIEFWH311Qujafj0yatqbD558uTiqXTSpEmMHDmSZ555hh49etCjR49SPasxxhhjjDGt0FQ5xKBBgxgwYAAXXHAB/fr1Y9q0aYwaNYqtt96am2++mWHDhjF16lRWXXVVXnvtNVZYYQX69u3LBRdcwGqrrcbYsWO54YYbePXVV1liiSWYNGkSxx57bKkQffLkyey888588sknvP/++wwfPpyf/OQnRSF1r169WkqtdBXqgrLtttsWyyQIUMotV1yutEFMx02ePBlIafLI9ddfD5TT0zlvU6EyCqW6YgmBuOeee4ByKYFSrUr3xJILiXsOOeSQYllnfDOV8lSJAcB1110HlEUwSglJJBg7aqmU4tJLLwXKQj6lHHNIYFVNx0ASLUYRooQxeuiLKUSlDFUGoc5K8bW+Q0zZVLtrtYp+a4kFGkXlE0rjVX09O0JdBGMXI/2WuenjzjvvLK2L5R86N1QuEv0oc2k/pWTlv9kK55xzTvG6qmmIYkj9xjvvvHPNNvTwHlP78pVVijZ6rKq8RALaXKczpUpjOY1+E80NkMaixnwOeVWrBAEa9ySfHirl0JzSaDlTVdgaU/TVOXLTTTctXstH9bzzziuW6RjJ7zeWfeT8roXKvFSKpe55kDo9SiBa9SCfEcQyLAla9bm5DmdRVKRMrK4l1157bbFO5V7qWBbHlMosJNKNJXk6BiphiOVumpNjBrhajhDRvKQ0eKPjpCM0z3amBE80U06lsgpI4yd6HkevZyj7kMv3Vp+n3wPSPJgTMWqbEufFMrroQ98ssbxRc2i9Y5E75k899RRQvj5qrlSJjeZ8qD23o9+0RIua96PgXMdA94OxDEY+wSq7gtTVrirghlQm00jpYacs0rbddluOOOIIVlppJU466SSWXHLJUqQXPr1Q7LrrrvTv35+LL76YXr16FWrc6bHggguy0EILMc8889CnTx/69OkzS2+AjTHGGGNM96BTKhOJPuDTu/A+ffrU2EfF4u955pmHddddt9S/fU5DFluxAF7dhWStFJ+EJIzQE6Ge9CBFgLVNSE+JekKqF/1VpyNIT096uoxRAkVBJBxRBypIUTJ1r1pooYWKdSNGjOjws1tBT6IxSpGzM4pPz1CO9OhJUlGGaI+iiIWiJzF6qIic7H5i1DFGW4RspSRaif3kJe6SVZkivJCivBLA5OzLWmH8+PHF63322We67692w4LUdU0R8xjR0JN9rmd9jLYLRYdzkdqq/Zy6rUE6VopK5+xtYsQwRmVaJX7P6kN0jEBKqBSjMBqritbFsaLOSPXES7LtyqHuXrmuW7mIZc4yUOeKxIfqtgQpmhxFPa2w9dZbA2lsxOOp6KIi0HGsyEZP5DJkynjFeUdEy7lq1CrOCeoqWO1iBRQlehIhx2uWyEWAFWmuF2VuBP2OGkdxvOlz62WI4vVCEV1lExX9hVqLq2g9pShdPLeFou4SW8ZomubInBBeYrDYHUyZqWpnylZRNDIeA10PJbqLoiuJ9HLdOjV+dM5FQZ0inLmMgO5nYiawGgmOYtBGiFkMoe13lZWchHVxXtbx0bFYaqmlav5Ox1zCTMhft3SfoQhtrgOfor1RyKtrT3y/0Bwiu9aqEQCk6G/klVdeAcq/SzPmDZ2KBFfTHT169CjdLHSEvtxcc81Vk06NrQSNMcYYY4yZEczwjnGyVIFPa1GeeOKJwh6pd+/eTJs2rXg6g9qmD/PNN19NZNAYY4wxxpjO0DWmm3W46KKLWHnllenfvz/jxo3j73//e5HG2WCDDejZsyennHIK3/zmN3nsscdq3B/69evHq6++ytNPP03fvn1ZeOGFS+KymY3EKbGuWakVpUVjpxOF5eVPGLvZyEc1+hIqXS8f1ohSFUolK+0AKZ2o1EwUyFT3JXZ7k0+fUm3RY7OrG6MoXaFiekjpZXXygSQQUSefXBpVxyf64Er4J6/cXGopJ77KHWulUeU5vMMOOxTrorAE8mlMifr0/84SU4uN+JvGMogqKgnQcYK0nzEFVfUEjV30JPyKjjBC41+CiugJfMsttwBpDMSHXu1D/L2jR3WrROGK9k0P3rEDkYRtcTxIlJnz1hw7diwAxx13HFBO+yntLSGRui5BSgnmPDzlhxu7+Ql1UtNxhdr0fld11ovou2ueiynJako1jlOd2ypTimUmQscwln+oS1c9EZNSppBKHfTdo9dt1Yu9Xke1WEKgDGW9jl+NoHKeDTbYACgfA42RekLZWG5Q3Wb0f9Y5o+3HLmbjxo0DaruZQSrr0/eMomWVPMTUuISh2pZKUCCNZ5U4yYsbyn7czRK/p65zuVS5vIBjuZ9QhlpdzHL3ELmyGI1vlTVC+r3qlS5ofsnN1Tovoo+yXLL0vTojhoO8MF7fJXdeVee3OM9p3o/zm0pARfTRVwmRyil1jkP6ziqdUe8CSPcluseKpbQ65rG8UMI5jeF4but+q5FxN8Mjweeeey7f+c53WHPNNZkyZQq33357cRFYfPHFueaaa7jrrrv44he/yA9/+MOSITPArrvuyle+8hW22GILevfuXaovNMYYY4wxphWaigRH5wc9UUVyXd369+9fKomostNOO9VYhEl0A58+sdXrJDKzkd1HLLyWpdKDDz4IpI4wkJ50cv3M9SQjcQmUI5tVqj3E45OkLKsUAVb3tLgPsf95dRuKAEcxiiIA66yzTof71Ax6QszZtsSnOAkMNW6izYnsmrTfuU5livbGjnpCAsLoT62IuiIJkGzhJNyLUSJtXxGnWMeubEC03uoKFAWEJHqQvVyM/skWqxFLIFn6RWJEXsIqiXr0eVDuhFRFv42Of7TbUaRJEdm4f9IYRDFoPRucVpCQQ+MufpYsnmIkONe9TCjKoEiHOiVBOmc0F0SxjrI9Er/GyKrW5VCkJJ7HyiJpDOy66641f6fv3KplmiJIigzG87E6hqLgSBFr2QZGMbE6PykCHOcYWc/FMaaxIRFZ7ryX8Hfo0KHFMolXJT6sJ1SN0SyNz3jtiRm0RlHUT1kt/T+S615YjaJDmo81luKcoMiX5q0YgdT3lLBS9muQhHE5FIGNtoASO2nui2K7qnCzM9FfSIKnaE8Wr3lVcmNfaP7QcYljUSItbTteGzTvS1gNaTzLGjUnjJb4LVpAKviXWxd/E6BUItpKZ0PN43HurR67eK5Wu8fFLng5dM+jUtWYQT7llFOA9PvHbIYyRTpHc8JTVQpEzZmEnxIJx/W6PsWOcc00dpvhkWBjjDHGGGNmN3wTbIwxxhhj2o6mOsaZ5EkZO50onSkRT0yxymc21xVOvn0Sw0ASPai0RP6fkIrdlQ6PncpUnqEOcLFLlhg5ciRQ9uhTCYCEFUqddoQ6tuT8+qZHrrwi19lFx0ydh6JYUqnZqhdtRCmamL5XJx+lt2Lq89577wWSHyqkFIu2FYWQxxxzDJDSVFGgWCWu07jI+VhOj5wIUsRuZPIC1mfEVJ3Gp9LM0U81h1JkKlWJQpcrrrgCSOMldiGM6Usol4uoTEPjPKYBowCmK4nuMkoJqrQripI0Bi+66KJimcR6MSUsDjjgACAJRqInp0p6JEKSkAhSWYJ+hyhilbA1ilB0jur8P+OMM4p1KrdQ2n/q1Kk1+95M96R6aN6InaPk+Z4T4ggd/1hupHNcc5I8qSGJOqMAMHbjq6KumpdcckmH78mV04wZMwYol08I+TvnfMxbIVfyoJKLXIlEPZTqjn77KgVRmUJMfWuMy/s8dh7U8dA8EfdFJRaxZGnzzTcH0lwS087VUqV4a9FKGZPEaLETqETT1fKBHLEkTeIuCXNzpROaRyVijJ8XBaCapySQjmLQqj9t9ACX+FBlEDmfXoneNY+3iq61UZSsUk79ZjnRpcoNo+BWYyNetyRaU+lCvEar3FHCwXjvc/HFFwN5gWru+FfRmIQ0FnPkyok6wpFgY4wxxhjTdjgS3CLqCAa1grXYrS3XEUsoAqwn9OmhiJoEY7ExiSKXMZopZFWiCFIU8Oh7KJoSrVn0FHnaaacVy/T024pNnWyfYiRJoqIYLZTgQJZq8aldkS6JrnKiOUWE4lN4/A5VctFtWX/puET0dKmndllkQXrqjZHRriD2X9fvou8bIyXVCHA9+6XpoSdtCT5j9FzfWRmE+DsomlQv6pJDYs54zGWXJXFntUFPI0SRicabojfx+GgMRoutXLREbLvttgDcfffdNetkHZWzQVOGI9f576677gLK0TdFdSTqrDdl5zIG+h1jFKUzxOio7OdipLsR6kUGdQ7FqJo6INZDx0VRZkiZOwmT41yt31uR8rhOnfck0msViXX02+VExrlOgCIKcuP5B+VOZbKCk2g3RhK1Xa2L1lNVclmTesTrYDXC2Ui0tlkkxFOENmYglBnVuIkZWR1HReKjAFjfWeK32HlMblW5zo/afhyn1c59MRup8132dVG4reOoCGm9aGgjNCKMjigrrXMi3ltoDjz33HOLZcpQ6/eI400WnBLwyroU0vfTMY7iTmVGtK34GymjJstKSNdYXRNaFbE6EmyMMcYYY9oO3wQbY4wxxpi2w+UQLRKFK/vvv3+H75P4RX6YsTOSvPiid6U8BFVEr5Q3pBSQwv9RUCdhjFJm0bcvJ8oTu+++O5DSG9NLh6kUI5ZUdIa333675rPkn1z9TKhN5UWfQaVfVKQf/VRVlqKi/Zxvc2TChAlAElBFEYq2r9Tm7bffXqyL3f+g3C0nlk10hmrKLXp2y/9S+x07QNUTnkmEEjsjSbClbmcam5DKb1QWE/1tVe6iqSWKExrx+40pr3o+va2gtKNS3/IihSToix0fq6IliVIhpTol6JDYD2D48OFAGhuxRErpU6Vfo+hLxyV6lepYK50aRZoiJxhSqYL2pavIiVMk0IvpcZXw6D2x9ETiLM2jUaylVKZEvgB77bUXABMnTgTKZR86H5u9lF1wwQVAvjOm5om4z531vYX6YytH9JLVHKlSnngt0bHW7x9LkPT9VDYVzz2JlzR+pifIUlmB9mu99dbrcJ+VFm8VeUlH3+4ooIzvgSR+FrnSIJV2xVKJ2H0QyuUQWhdT8yrB0H41WzKTE22p05/mkFY8qXNEH2h57ObKbzTn5uZbCVSjsLXa1CyibZx33nlAWZwn1NcgHmsJhlUyEYV4uv7G8akyRm2/FT9lcCTYGGOMMca0IY4EdwESriiSFCOQEgmok0/k8ssvr1kmUZeepnJ2WuoqJ6EZpH7p6uSSQ09KMQoqKzVZXSkCDSlqoT7fkOyjjjzyyA4/pyP05CbBFOSjo1Ubs9ifXuIOoQgR0FRLbUWBIP1e8YlV0XP9PwoBJeDS8YlCv1jo35XE7j6NWF212mEtZgL0nRVdisesKviMEVUJxTQWY9eiKtEGK0atq3QmuhSjsDp2cUw3gjopjhs3rlgmEaqiN9FCrmqbF6N+1fNKneoisSOaIkaKBMduV4oq6f1RKCTbLIk8JWptFkVFZQEXhYBVUW+8nMhGT9mmmDFRhzvNRbKUgySyi+ecvqds/aIobLPNNgPSfFhPEBvnR9mzyQ4qdj+rRhxbJWfzVUVCzBgZlJgzno/KOMrSMaLrTC46KXFULmqrCHIuWieiaFlZSGXwJDyEdO3Q7xCtw7oK7YsiutXrwfSQACxeV6tdRCUkhyQml5gVkgixnk2n0HGCvCWa0Jyg79eKjWZEGeiYKa133VA2S9avcY4RMdKq/dO8HCPlOp8kiItZLZ3byl7Gc1xZl3o2rDGC/+677wL1hXSN4EiwMcYYY4xpO3wTbIwxxhhj2g6XQzSJ0kcSJ0EShZx99tkAnHzyycU6hewlYoppThHTi1V/2eh9p9RYLAEQSnnJTzXun9IRUXwi1JlOqRJ11qnulzj88MOB1PllRqC0htIj6koDKQ2mFJYEHZBSsdXyFEiiMIloYrpQgq+Y9ttqq62AJAqLojIJCyQCUpE/pDIBfXauM05XoQ5e0dd2hx12ANJ+Ry9KdRCTaC52QdM4jX7ESiUr/Rp9IlUeoLR/rjOPfJRjarkRn98owIvpvFaJ5TRKCWtZ9Lednliyikp5dPx1zCNKYUchrUR5SpVK+AZJ9BbTfhI2qawhloRIMKIxGYUmEsJoTugqYtmHxGISRsZ9k1hXaA6EVD6j8anzGmqFsRGVUeR8gyUGVie/6XHnnXcCqcxE50BXkutQKep5eassLpbKqGRE40VesHFd9HlvBPlwq7wtjsVc6ZHS2CrziGVFOg/0d+oqB0n01Ao5gZsEXxrjkEqrVFalcwJqPZhVLgDlEsC4bUilPFF0p1IAnZexK6fW5cpMdH3SORtLHnSN1j5Hb90jjjiCZqk3tvQb5koecuj+JpaJqGxC5TCx7EO/l47d9773vWJd9dyOom6Vi0ismfPIjv7F9UoPJRrNlQBVcSTYGGOMMca0HY4EN4kipzFCpSisokrqNgTpaUZP6lEIJvus2Kdb3dIOPvjgms9WQb6iWPGJVeKurvw5FdGOEcCuID7lyiYqPuFVnxb19A9JMCKLFnXYgnRcFDGXxRckMaGOtX5HSHY0cZmioPvttx8A119/fbFOkQ6JLPTEC+mJXlGaZjtpzSoUuYjjRyIriTqjXZMicTrGUaikCLWyC/Wsa6LwRxH1qs3cjECfFe2R9JtFmzjZoOm8jetyllNVFImQ2APKURMod3XSPuTOYx1/zQOQjn8u8tOI6KkRvvvd7wLpPIk2Ssq6iGjNqN9d3yVaQVbtDGPULmdFpgyLfo9oPSh0zubs0xSRl1gP0jyj8RazZ4paxyhfZ7ovijgOFOXNdVTUmIqRVkWqNX5idFKZGUU841w2YMAAIP2ORx99dIf7FyPP+uxo0xmvVVWq4tXYqbFV+6oqykpJDBWFoMrMKcJar1tazNZWr28SPsf3RZGWrjP6fjEbKetQiUmjlajO7dhhTijzo3Olnp1lI0hIHbObiqbeeuutQDnrq99dxy5GqZWZi5aFymjrfdEiU+JMCVajpaKySBoj8ZxSFkuZ4Jg5EjFyrAxj7hqrc6WR4+hIsDHGGGOMaTt8E2yMMcYYY9oOl0N0AUrJb7vttkA5vSUhh/xmoxhJAqWY7ld6QKm5GOqXaEHehdFj78QTT2xp3+UPuMEGGwDl1I48gWOqe2ailFJMH8mvsZFSjXh8lPqU8DBuU8X90V9U6VptI3aMiyk4KHv4SgwiUVAUZQ0ePLjDfW0GdeKRT20UxlW7H0Xk3ynRn7xXIQnb4neZNm0akFL6+juAU045BSiP50ZQKUyjIjSl4jojkFM3RKgVDsXSAnnQxnR8tatYFKxJ9KgSiXjsJfJQmjGmZtUdSp+teQPSOI3nodKoErJEX2p5jefKp7qaZlKMEZXHxHmu6lkax5HKJsaMGVMs0zFWGUScY+UPrvMgdmWrek9rPoY0rlWCNquQSCuKjW+77TagLBJV6ZAEzrGr2OTJk4E0lnKiNv1+8ZIvgaLm1SisVLpfvx+kMf7MM88A5VIblY40KrhqFHVWBVhhhRVKy/TvHLnup/pOOf9elWzkypuGDh1avB49enRpncriIJ0b2pau8ZDGuDq9RhG8jrtEYbl5phlUhhkFiZpDc8dOInhdt+J1QB0nYxljtRQk7q/OW5VG6XoFSeQngXSc17XNRktnVDahOTPuc/R1nh6OBBtjjDHGmLaj48px0zCKkEmcsM466xTrJJKTUCF2+9ETaIxURXEGpAgypKd72ZTFDin1yHXJEdVe8eoOBukp7bnnniuW6Sk21+mtq5HwKNpqqUtPPGYdETv6SAwgC6rpoQhwTkAlEZ+ivBtttFGxrhrhitHfqVOnAsmqrFWqUX9FOSAJ+BTxjN35FGXQe3JWe3H/qyIYReYhRTUUTc5ZpOWoRoBz3cdyEepqtqUZZEkGKQqisR0zLYpURjslWVWJaK0kkYfEk7ErnCJlir7H31ydybTtnNWPtgkpQi0rpx/96EfFumrnvhmBjrl+gxhp1XxVtUODJMyLkWuhcaDzKloW6ryNYpvYhRPK3SHV9VICzhj9rdpm5UTLsxr9rvH7xu51QkK4PffcE0hdPiEvrquieVTzACTRnISJUaik60XO1lCRthgBjMLZrkCCqs0337xYpmizrlsTJkwo1inKqPk5zotCUcYY8dYynWebbrppsU5ZGx2nHDHiXI1ixjlU563E4JH4m0DnrTVzlny6XlWv+VDb+TFaFua+uwR3uoeJdn1VQVvMaunvNO/qmgjpN1GGRjaokDJB0fa02hWuaoPXKI4EG2OMMcaYtsM3wcYYY4wxpu2wMK4LUFpq/PjxABxzzDHFOhWmK7UTU7NKPUUvSokj5D0b09MqRajXFUhitui1qrSCUiRRrKX0glIROU9RiScgCQpy6c9GiYXyOUGf0pQSKMW0iIQiSqOoaB9S6lMp65jqjmkUKKdXo4BOKJ2tFHkUA6lMI4dSmiodiD6fraTyRRTIqLRDfqixQ5bEffJojEhYKa/U2IlIYzgKKdRNSsKh2L1Q6VeNB4kLIY1rpcOVZoSUdlXKLIpKVIoTRT0qxZFPZ1f5jd5xxx1A6rAHtT6nEfm7ym8aUopd+6YOeZB8aZVmzHVflNdyFIDJgzd6XAsdu5hejOOrSr1uZc2gMS0xaUznSiR0wgkndPj3VXFhjniu6tyOPuhaJtFSPAf12RIaxv3TeMnNV/fffz+QzqNYeiaRXfxesQSjUTQPbLzxxjXrqmKoOM/Jb7weUcirY6X5VN8Nkm+rUtaa26Bcugfl76jjGNPMOkfk1xpFwjpX1QXwrbfeKtapw1gz6DPiOa+xr2NXr0yhHlHkpdIcdc2LJRYqcYrfRXOBbp2iH7LOtZz4Tdd5lRDI1xrS2FV5Uc4jtxl0vYiCXJVdaZzFa5q8oPXbxZIJlc9F8Wp17EZ0f6IS0OhLrWOtOTOW2lQFier6BqlEIpZmal8bERrWw5FgY4wxxhjTdjgS3CKxi466Jh144IEtbev2228vXtfrliURieyQJMyBFNWT2CZGoyVkicIdce211wJ5gY2eXNVpCFJ0tqssmUaNGgWUxQ8SJtTrxCWiGKMawZOVEMChhx7aqf2M4oc777wTSFHEKCKoRmBjtxyJJKL4sDPISmqLLbaoWaeISe6zZIsXbWRyXXckUFMESZESgJtuuqnD/VKkQRG2vn37drh/UWSljk+KXEGKXndV5z1Fl2TjFO2fFD2JXZ4aQVEbiWggdTOqR078pL+L58PEiRNLn1OPaMkk0ZKioPUyGPXQnKLxHuc+RV/GjRsHJHEwpHNGgqAYaRX1bNeiCFEWYMqQxYyVoubK2sToviLIscudqEb+Y3ZIUaxHHnmkWFYvkj099LtE0V49qzllWBQ1jPuUi35KOJT7jWV1qOMZo+KKaioyF7+vspBR9FpvXhG6pYjHs5XzVxnLnLVk1e4x9/lRZK5ItNbFroKadyQijlF0jfV4myThnSKruYh31XoU0tyj6GdV2AUpA5GbM5tBma443hTdrZfxUtZH8278u5jFVqZCnXB1HwEpWqvPrnetiFT3K2aglcmN12GJnHUc4++g/Wsko+JIsDHGGGOMaTt8E2yMMcYYY9oOl0M0idIMSiNB/Y5DSi+pe8pll11W855YWK80j0ofYmG70tJalvNTlQ9fztux+veQUmQS3cSUpcotYlmChEw5ocf0kPgqeiAq/Zv7LkpFRa9FpQlVChD9HpWi0zH/zne+U7MPjQoPdFqoJCSmYZQOU6pFYwKSv2ss4Bcq5G+FKIyTx62EWFHE0EjXHb0n+s0qdaVxAEnIN2zYMADOPvvsYl29aUMlDkrNxhSrUrFaFj2WRSwrkHBH3YCqHcCapeprHFPu8mvNobKWWFKklLzSoldddVWxTr+RhBn1ypxyRH/iww47rLQPsVRC6X6lI2MZRhSkdIbrrruu9O999tmn5j0qK4mewPLv/fa3v13zfo0RneOxNEfncRQMN5IeVno0pmbrdbtUuYjmI5VcxO8RvdXjvDmzyJ338kiPAumRI0cCMGLEiJptSKCaEz3XQ3Nezm9XxHNb53JXpfRVKhOFy/rNNEbida7e3C5hatUPF9J5K4FxLGnT9qO/tOYMCYDj5+VKDMSkSZNKnzMjqZZsQLo30DjOlRuqlC36P+ueJQpyJR7Ud4+lJ1qm8qEowFZHVV07c+UNKsPReyH1V1A5VPzMagfXZnEk2BhjjDHGtB2OBHchEkjFJ8nq07SeBiE9EcaCbkWacl2k9L6vf/3rQNnmpUrsJhRtr6pIIKBITs5aq6tQ9EeRCUgiuPjkqe8uoUsskNeTtorhYyTi8ccfB1JkPoqudFwVUYzWc7ICi33MFV3NRQ4UUVfEMgrT9LSt7xMjUbK/6yyKhkvwEseK+sHXi3zUI1qAye6n2uUtctBBBwFlAYg6d+XQ+aBuh1FsISFNzrKunoBqesTugvXsBRvZRvx7jUuNqdihUJZViljlugTKjipayDVCtMZSZF1Rmhi1lZCxq2yXRIyei1wUvSpCjHOL5kpFOmPUR+dqFJVq3zVeYrdMCWAVBY0WYLludc2Qy1A1QzX7FrNG+i4S/ci6EPJR/GpENx4DdbbMCYA1LmXJFc/VRjJHOfFxLishJByO50NnyEWblQmM0XldS3L7VrUL/NWvflWs0/yvDEK0ApPAPH6XapY1Zq5it7kqsstTtibO27p2KQMcO7DJmrCryNmHPf/880ASSMZ1ivpGO8uxY8cCKXOtYwjpWKnDYav7FTNBirprroXa7qyt4kiwMcYYY4xpO3wTbIwxxhhj2g6XQzSJBC633XZbsUxpDXUvk/dtRIKzKL4QseheaRFtI/r1yYNQqZ3oM6i0q1JB0Zcxpr+qyF9S6Xv5AENKjW+99dbFMnUGUxq8GeSLqtQLJAFZ9KDVZ6gkQakQSKILpYRjelrpE5VRxDTSXnvtBaTfL5YJKC0d0zfVFFQs15CQRr6S0Zuzmh7Mdb1phSieVOpy6NChQLnjmIRt6vIUS0L0d0o3xd9B4oLpddITVVFRbltaFseiyjX0O0Zxhs6j6I2qVKxEmlEk2SjRT1vjW0KLOLZzQsMqOvegsU5s6pYWz9Uo7qiiUoNYuqBSDIlJYpcvHR+lLOPcI39VfddWSkkgpY4b8U9W5yuATTbZBEjnR5z7TjrppNK+5bopqtwH6qeEdU7r+0ZxUr3SHKFzKwr45I2ujnhQvyve9NC8HMeM5kOV+qh8BPJlOyqH07GIHfVa7Uap8aw5M/r/arzE8rVmxXWdQanvKEZVeUq1A2hE5QmxNEHntv4fz0fNcxpvubKhKMDW/Kk5MI5NbV9zWM4fWedoHJvVjnoSiUGaMztLIx0kc+J53SLGuVqlYPV+h1wX0nolbyrN1FiM6NoeSxaXWGKJ0nvUSRVgl1126XC/qjgSbIwxxhhj2g5Hgo0xxhhjTNvhSLAxxhhjjGk7fBNsjDHGGGPaDt8EG2OMMcaYtsM3wcYYY4wxpu3wTbAxxhhjjGk7fBNsjDHGGGPaDt8EG2OMMcaYtsM3wcYYY4wxpu3wTbAxxhhjjGk7/j9ZN150mvuDbAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x200 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "rng = check_random_state(42)\n",
    "cols = 8\n",
    "w = 1.0\n",
    "fig, axs = plt.subplots(figsize=(cols*w, 2*w), ncols=cols, nrows=2)\n",
    "for col, (upper, lower) in enumerate(zip(axs[0], axs[1])):\n",
    "    if col == 0:\n",
    "        upper.text(-28, 14, 'ground\\ntruth')\n",
    "        lower.text(-28, 14, 'input')\n",
    "    i = rng.choice(len(X))\n",
    "    noisy = X[i].reshape(28, 28)\n",
    "    clean = y[i].reshape(28, 28)\n",
    "    kwargs = {'cbar': False, 'xticklabels': False, 'yticklabels': False, 'cmap': 'gray_r'}\n",
    "    sns.heatmap(noisy, ax=lower, **kwargs)\n",
    "    sns.heatmap(clean, ax=upper, **kwargs)\n",
    "plt.savefig(f\"{absolutepath_to_results}/input-output.svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The autoencoder model uses PyTorch through the scikit-learn interface [skorch].\n",
    "\n",
    "[skorch]:https://github.com/dnouri/skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import Autoencoder, NegLossScore\n",
    "import torch\n",
    "\n",
    "\n",
    "def trim_params(**kwargs):\n",
    "    if kwargs['optimizer'] != 'Adam':\n",
    "        kwargs.pop('optimizer__amsgrad', None)\n",
    "    if kwargs['optimizer'] == 'Adam':\n",
    "        kwargs.pop('optimizer__lr', None)\n",
    "    if kwargs['optimizer'] != 'SGD':\n",
    "        kwargs.pop('optimizer__nesterov', None)\n",
    "        kwargs.pop('optimizer__momentum', None)\n",
    "    kwargs['optimizer'] = getattr(torch.optim, kwargs['optimizer'])\n",
    "    return kwargs\n",
    "\n",
    "class TrimParams(NegLossScore):\n",
    "    def set_params(self, **kwargs):\n",
    "        kwargs = trim_params(**kwargs)\n",
    "        return super().set_params(**kwargs)\n",
    "\n",
    "model = TrimParams(\n",
    "    module=Autoencoder,\n",
    "    criterion=torch.nn.BCELoss,\n",
    "    warm_start=True,\n",
    "    train_split=None,\n",
    "    max_epochs=1,\n",
    "    callbacks=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't show it here; I'd rather concentrate on tuning hyperparameters. But briefly, it's a simple fully connected 3 hidden layer autoencoder with a latent dimension of 49."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Setup\n",
    "\n",
    "* Uses Dask for distributed computing.\n",
    "* Defines parameter search space (`params`).\n",
    "* Sets random state for reproducibility.\n",
    "\n",
    "##### Parameters\n",
    "\n",
    "The parameters interesting in tuning are\n",
    "\n",
    "* model\n",
    "    * initialization\n",
    "    * activation function\n",
    "    * weight decay (which is similar to $\\ell_2$ regularization)\n",
    "* optimizer\n",
    "    * which optimizer to use (e.g., Adam, SGD)\n",
    "    * batch size used to approximate gradient\n",
    "    * learning rate (but not for Adam)\n",
    "    * momentum for SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "params = {\n",
    "    'module__init': ['xavier_uniform_',\n",
    "                     'xavier_normal_',\n",
    "                     'kaiming_uniform_',\n",
    "                     'kaiming_normal_',\n",
    "                    ],\n",
    "    'module__activation': ['ReLU', 'LeakyReLU', 'ELU', 'PReLU'],\n",
    "    'optimizer': [\"SGD\"] * 5 + [\"Adam\"] * 2,\n",
    "    'batch_size': [32, 64, 128, 256, 512],\n",
    "    'optimizer__lr': np.logspace(1, -1.5, num=1000),\n",
    "    'optimizer__weight_decay': [0]*200 + np.logspace(-5, -3, num=1000).tolist(),\n",
    "    'optimizer__nesterov': [True],\n",
    "    'optimizer__momentum': np.linspace(0, 1, num=1000),\n",
    "    'train_split': [None],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing `optimizer` to be `SGD` or `Adam` comes from here \"[The Marginal Value of Adaptive Gradient Methods in Machine Learning][marginal]\". From their abstract,\n",
    "\n",
    "> We observe that the solutions found by adaptive methods generalize worse (often sig- nificantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.\n",
    "\n",
    "Their experiments in Figure 1b show that non-adaptive methods (SGD and heavy ball) perform much better than adaptive methods.\n",
    "\n",
    "They have to do some tuning for this. **Can we replicate their result?**\n",
    "\n",
    "[marginal]:https://arxiv.org/pdf/1705.08292.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for debugging; ignore this cell\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import ParameterSampler\n",
    "# import dask.array as da\n",
    "# import numpy as np\n",
    "# model = SGDClassifier()\n",
    "# params = {'alpha': np.logspace(-7, 0, num=int(1e6))}\n",
    "\n",
    "# n, d = int(10e3), 700\n",
    "# _X, _y = make_classification(n_samples=n, n_features=d,\n",
    "#                              random_state=1)\n",
    "# X = da.from_array(_X, chunks=(n // 10, d))\n",
    "# y = da.from_array(_y, chunks=n // 10)\n",
    "# X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def fmt(obj):\n",
    "    if isinstance(obj, list):\n",
    "        return [fmt(v) for v in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: fmt(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import msgpack\n",
    "\n",
    "def save_search(search, today, prefix, X, y):\n",
    "    # create the folder into results and direct the results over there\n",
    "    path = \"/\".join(os.getcwd().split(\"/\")[:-1])\n",
    "    results_folder = f\"results/skorch_run/{today}\"\n",
    "    absolutepath_to_results = os.path.join(path,results_folder)\n",
    "    os.makedirs(absolutepath_to_results,exist_ok=True)\n",
    "    pre = f\"{absolutepath_to_results}/{today}-{prefix}-\"\n",
    "\n",
    "\n",
    "    with open(pre + \"test.npz\", \"wb\") as f:\n",
    "        y_hat = search.predict(X)\n",
    "        y_hat = y_hat.compute()\n",
    "        np.savez(f, X=X, y=y, y_hat=y_hat)\n",
    "    # skorch models aren't pickable\n",
    "    with open(pre + \"params.json\", \"w\") as f:\n",
    "        params = {k: fmt(v) for k, v in search.get_params().items() if \"estimator\" not in k and \"param_distribution\" not in k}\n",
    "        json.dump(params, f)\n",
    "    # with open(pre + \"best-model.joblib\", \"wb\") as f:\n",
    "    #     joblib.dump(search.best_estimator_, f)\n",
    "    with open(pre + \"best-params-and-score.json\", \"w\") as f:\n",
    "        json.dump({\"params\": search.best_params_, \"score\": search.best_score_}, f)\n",
    "\n",
    "    with open(pre + \"history.json\", 'w') as f:\n",
    "        json.dump(search.history_, f)\n",
    "\n",
    "    with open(pre + \"cv_results.json\", 'w') as f:\n",
    "        json.dump(fmt(search.cv_results_), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Algorithms\n",
    "\n",
    "Compares different optimization algorithms:\n",
    "\n",
    "* **Hyperband:** Efficiently explores and exploits the parameter space.\n",
    "* **Hyperband with Successive Overfitting Prevention (SOP):** Early stops models that don't improve, potentially reducing wasted evaluations.\n",
    "* **Incremental Search with Patience:** Adaptively adds and removes models based on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration\n",
    "\n",
    "Visualizes noisy and clean images to understand the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 209.35 MiB </td>\n",
       "                        <td> 69.78 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (69999, 784) </td>\n",
       "                        <td> (23333, 784) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 3 chunks in 1 graph layer </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float32 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"40\" x2=\"25\" y2=\"40\" />\n",
       "  <line x1=\"0\" y1=\"80\" x2=\"25\" y2=\"80\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.96349288680034,0.0 25.96349288680034,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.981746\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >784</text>\n",
       "  <text x=\"45.963493\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.963493,60.000000)\">69999</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<array, shape=(69999, 784), dtype=float32, chunksize=(23333, 784), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dask.array<concatenate, shape=(62997, 784), dtype=float32, chunksize=(20999, 784), chunktype=numpy.ndarray>,\n",
       " dask.array<concatenate, shape=(7002, 784), dtype=float32, chunksize=(2334, 784), chunktype=numpy.ndarray>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 188.41 MiB </td>\n",
       "                        <td> 62.80 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (62997, 784) </td>\n",
       "                        <td> (20999, 784) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 3 chunks in 12 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float32 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"76\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"26\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"40\" x2=\"26\" y2=\"40\" />\n",
       "  <line x1=\"0\" y1=\"80\" x2=\"26\" y2=\"80\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"26\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"26\" y1=\"0\" x2=\"26\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 26.490396879174003,0.0 26.490396879174003,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"13.245198\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >784</text>\n",
       "  <text x=\"46.490397\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,46.490397,60.000000)\">62997</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<concatenate, shape=(62997, 784), dtype=float32, chunksize=(20999, 784), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "max_iter = 250 # originally this number was 250 to make a decent denoising \n",
    "history = {}\n",
    "cv_results = {}\n",
    "searches = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import HyperbandSearchCV\n",
    "\n",
    "fit_params = {}\n",
    "if isinstance(model, SGDClassifier):\n",
    "    fit_params = {'classes': da.unique(y).compute()}\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with Hyperband\n",
    "\n",
    "Trains the model with Hyperband and analyzes the results:\n",
    "\n",
    "* Best model parameters\n",
    "* Best model score\n",
    "* Visualizations of the best model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperbandSearchCV(model, params, max_iter, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62997, 784)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 1, loss = 2.202204704284668\n",
      "steps = 1, loss = 1.0604509115219116\n",
      "steps = 1, loss = 2.0787689685821533\n",
      "steps = 1, loss = 1.9983878135681152\n",
      "steps = 1, loss = 2.9078874588012695\n",
      "steps = 1, loss = 2.1339221000671387\n",
      "steps = 1, loss = 49.96958923339844\n",
      "steps = 1, loss = 1.6027408838272095\n",
      "steps = 1, loss = 1.6714401245117188\n",
      "steps = 83, loss = 2.4591171741485596\n",
      "steps = 83, loss = 2.6524462699890137\n",
      "steps = 83, loss = 2.4868273735046387\n",
      "steps = 83, loss = 3.1829371452331543\n",
      "steps = 83, loss = 2.051856279373169\n",
      "steps = 83, loss = 4.839390277862549\n",
      "steps = 1, loss = 2.002286911010742\n",
      "steps = 1, loss = 1.0298068523406982\n",
      "steps = 1, loss = 0.7160890102386475\n",
      "steps = 1, loss = 1.9761333465576172\n",
      "steps = 1, loss = 2.0289371013641357\n",
      "steps = 1, loss = 1.9753522872924805\n",
      "steps = 83, loss = 3.903987169265747\n",
      "steps = 83, loss = 2.9585912227630615\n",
      "steps = 83, loss = 50.01108932495117\n",
      "steps = 250, loss = 2.5863118171691895\n",
      "steps = 250, loss = 5.067054271697998\n",
      "steps = 250, loss = 3.46138858795166\n",
      "steps = 250, loss = 3.228792667388916\n",
      "steps = 250, loss = 3.7207000255584717\n",
      "steps = 250, loss = 2.7746689319610596\n",
      "steps = 1, loss = 1.9999332427978516\n",
      "steps = 1, loss = 1.7857023477554321\n",
      "steps = 1, loss = 1.8042144775390625\n",
      "steps = 1, loss = 0.970885157585144\n",
      "steps = 1, loss = 4.930239677429199\n",
      "steps = 1, loss = 1.2963420152664185\n",
      "steps = 1, loss = 2.734524726867676\n",
      "steps = 1, loss = 2.3401176929473877\n",
      "steps = 1, loss = 49.84938049316406\n",
      "steps = 1, loss = 4.534388542175293\n",
      "steps = 1, loss = 41.324462890625\n",
      "steps = 249, loss = 2.065657138824463\n",
      "steps = 1, loss = 2.1478776931762695\n",
      "steps = 1, loss = 1.9174538850784302\n",
      "steps = 1, loss = 1.5926084518432617\n",
      "steps = 1, loss = 1.8130954504013062\n",
      "steps = 1, loss = 1.9271926879882812\n",
      "steps = 1, loss = 2.45357084274292\n",
      "steps = 1, loss = 2.5355639457702637\n",
      "steps = 1, loss = 2.143592119216919\n",
      "steps = 1, loss = 2.098397970199585\n",
      "steps = 1, loss = 2.0105671882629395\n",
      "steps = 1, loss = 2.563915491104126\n",
      "steps = 1, loss = 2.2251832485198975\n",
      "steps = 1, loss = 2.529712200164795\n",
      "steps = 1, loss = 2.350214719772339\n",
      "steps = 1, loss = 0.9975816011428833\n",
      "steps = 1, loss = 2.2631888389587402\n",
      "steps = 1, loss = 3.120868444442749\n",
      "steps = 1, loss = 0.7099869251251221\n",
      "steps = 1, loss = 2.0806734561920166\n",
      "steps = 1, loss = 49.98344039916992\n",
      "steps = 1, loss = 2.4708755016326904\n",
      "steps = 1, loss = 1.7782317399978638\n",
      "steps = 1, loss = 1.8757960796356201\n",
      "steps = 1, loss = 1.949628233909607\n",
      "steps = 1, loss = 2.087263822555542\n",
      "steps = 1, loss = 0.8405274152755737\n",
      "steps = 1, loss = 1.7991031408309937\n",
      "steps = 1, loss = 2.2714898586273193\n",
      "steps = 1, loss = 2.034625291824341\n",
      "steps = 1, loss = 1.2400376796722412\n",
      "steps = 249, loss = 2.473175525665283\n",
      "steps = 9, loss = 3.046734094619751\n",
      "steps = 9, loss = 2.4863338470458984\n",
      "steps = 9, loss = 3.6132848262786865\n",
      "steps = 9, loss = 8.022050857543945\n",
      "steps = 9, loss = 1.9232337474822998\n",
      "steps = 9, loss = 2.603837490081787\n",
      "steps = 9, loss = 3.3170547485351562\n",
      "steps = 9, loss = 3.4651875495910645\n",
      "steps = 9, loss = 2.7739338874816895\n",
      "steps = 9, loss = 2.6896982192993164\n",
      "steps = 9, loss = 49.94734191894531\n",
      "steps = 9, loss = 2.9884090423583984\n",
      "steps = 9, loss = 2.912804126739502\n",
      "steps = 9, loss = 2.6405251026153564\n",
      "steps = 9, loss = 2.610262393951416\n",
      "steps = 9, loss = 2.366626262664795\n",
      "steps = 9, loss = 3.5408451557159424\n",
      "steps = 9, loss = 2.761626958847046\n",
      "steps = 9, loss = 2.204603433609009\n",
      "steps = 9, loss = 2.4619081020355225\n",
      "steps = 9, loss = 1.966184377670288\n",
      "steps = 9, loss = 2.283750057220459\n",
      "steps = 9, loss = 2.4842588901519775\n",
      "steps = 9, loss = 3.049499034881592\n",
      "steps = 9, loss = 1.8599188327789307\n",
      "steps = 9, loss = 2.2713217735290527\n",
      "steps = 9, loss = 2.11029052734375\n",
      "steps = 9, loss = 2.4884209632873535\n",
      "steps = 9, loss = 2.6915760040283203\n",
      "steps = 9, loss = 50.00514602661133\n",
      "steps = 9, loss = 2.1197168827056885\n",
      "steps = 9, loss = 3.3964104652404785\n",
      "steps = 9, loss = 3.387035608291626\n",
      "steps = 9, loss = 2.735473155975342\n",
      "steps = 9, loss = 2.7994236946105957\n",
      "steps = 9, loss = 1.9990870952606201\n",
      "steps = 9, loss = 3.4109694957733154\n",
      "steps = 9, loss = 2.3063104152679443\n",
      "steps = 9, loss = 2.1797211170196533\n",
      "steps = 9, loss = 2.0038392543792725\n",
      "steps = 9, loss = 1.9318982362747192\n",
      "steps = 27, loss = 2.2164974212646484\n",
      "steps = 27, loss = 2.064678430557251\n",
      "steps = 27, loss = 2.0301594734191895\n",
      "steps = 27, loss = 2.1418800354003906\n",
      "steps = 27, loss = 2.377723217010498\n",
      "steps = 27, loss = 2.1374242305755615\n",
      "steps = 249, loss = 2.605091094970703\n",
      "steps = 27, loss = 2.448249101638794\n",
      "steps = 27, loss = 2.295426368713379\n",
      "steps = 27, loss = 2.577955484390259\n",
      "steps = 27, loss = 2.1817567348480225\n",
      "steps = 27, loss = 2.195711612701416\n",
      "steps = 27, loss = 2.465963840484619\n",
      "steps = 27, loss = 2.685737371444702\n",
      "steps = 1, loss = 50.032718658447266\n",
      "steps = 1, loss = 4.679951190948486\n",
      "steps = 1, loss = 2.371753215789795\n",
      "steps = 1, loss = 2.6846392154693604\n",
      "steps = 1, loss = 1.9993358850479126\n",
      "steps = 1, loss = 0.8417843580245972\n",
      "steps = 1, loss = 6.803238391876221\n",
      "steps = 1, loss = 49.93523406982422\n",
      "steps = 1, loss = 1.7455096244812012\n",
      "steps = 1, loss = 50.002685546875\n",
      "steps = 1, loss = 1.9234718084335327\n",
      "steps = 1, loss = 2.177426338195801\n",
      "steps = 1, loss = 2.333813428878784\n",
      "steps = 1, loss = 2.0608274936676025\n",
      "steps = 1, loss = 2.187516689300537\n",
      "steps = 1, loss = 2.0072691440582275\n",
      "steps = 1, loss = 2.83785080909729\n",
      "steps = 1, loss = 2.7498059272766113\n",
      "steps = 1, loss = 2.0378077030181885\n",
      "steps = 1, loss = 1.6024774312973022\n",
      "steps = 1, loss = 1.5122641324996948\n",
      "steps = 1, loss = 50.01776123046875\n",
      "steps = 1, loss = 50.011173248291016\n",
      "steps = 1, loss = 2.351951837539673\n",
      "steps = 1, loss = 50.03340530395508\n",
      "steps = 1, loss = 0.7019572257995605\n",
      "steps = 1, loss = 2.1238062381744385\n",
      "steps = 1, loss = 2.3906102180480957\n",
      "steps = 1, loss = 2.399613618850708\n",
      "steps = 1, loss = 1.7182542085647583\n",
      "steps = 1, loss = 1.9222230911254883\n",
      "steps = 1, loss = 1.8773568868637085\n",
      "steps = 1, loss = 1.349562406539917\n",
      "steps = 1, loss = 2.0149919986724854\n",
      "steps = 1, loss = 0.6951484084129333\n",
      "steps = 1, loss = 50.007179260253906\n",
      "steps = 1, loss = 1.9817843437194824\n",
      "steps = 1, loss = 3.028855323791504\n",
      "steps = 1, loss = 2.018341302871704\n",
      "steps = 1, loss = 2.851391077041626\n",
      "steps = 1, loss = 1.2481611967086792\n",
      "steps = 1, loss = 1.9083788394927979\n",
      "steps = 1, loss = 2.2493174076080322\n",
      "steps = 1, loss = 1.6219161748886108\n",
      "steps = 1, loss = 4.589573860168457\n",
      "steps = 1, loss = 2.643268346786499\n",
      "steps = 1, loss = 1.9534186124801636\n",
      "steps = 1, loss = 1.8166946172714233\n",
      "steps = 1, loss = 1.8177889585494995\n",
      "steps = 1, loss = 2.0946853160858154\n",
      "steps = 1, loss = 2.4243786334991455\n",
      "steps = 1, loss = 1.975293517112732\n",
      "steps = 1, loss = 4.203957557678223\n",
      "steps = 1, loss = 1.938413143157959\n",
      "steps = 1, loss = 49.779136657714844\n",
      "steps = 1, loss = 2.4975955486297607\n",
      "steps = 1, loss = 2.4511969089508057\n",
      "steps = 1, loss = 0.7130215764045715\n",
      "steps = 1, loss = 0.7310213446617126\n",
      "steps = 81, loss = 2.083894729614258\n",
      "steps = 1, loss = 3.1736738681793213\n",
      "steps = 1, loss = 1.4476425647735596\n",
      "steps = 1, loss = 1.4477850198745728\n",
      "steps = 1, loss = 3.602219820022583\n",
      "steps = 1, loss = 2.537437677383423\n",
      "steps = 1, loss = 2.053941488265991\n",
      "steps = 1, loss = 2.157195806503296\n",
      "steps = 1, loss = 1.8389759063720703\n",
      "steps = 1, loss = 1.780927300453186\n",
      "steps = 1, loss = 2.0095810890197754\n",
      "steps = 1, loss = 49.94819641113281\n",
      "steps = 1, loss = 49.85722732543945\n",
      "steps = 1, loss = 0.9066066145896912\n",
      "steps = 1, loss = 1.9939522743225098\n",
      "steps = 1, loss = 2.0564301013946533\n",
      "steps = 1, loss = 50.005184173583984\n",
      "steps = 1, loss = 2.9199306964874268\n",
      "steps = 1, loss = 1.298521876335144\n",
      "steps = 1, loss = 1.8664628267288208\n",
      "steps = 1, loss = 50.017005920410156\n",
      "steps = 1, loss = 49.9835205078125\n",
      "steps = 1, loss = 2.5795366764068604\n",
      "steps = 1, loss = 2.143399477005005\n",
      "steps = 1, loss = 50.015769958496094\n",
      "steps = 1, loss = 2.3679003715515137\n",
      "steps = 1, loss = 1.6881660223007202\n",
      "steps = 1, loss = 1.1935145854949951\n",
      "steps = 1, loss = 49.98673629760742\n",
      "steps = 1, loss = 1.831294298171997\n",
      "steps = 1, loss = 50.00932693481445\n",
      "steps = 1, loss = 0.9960564374923706\n",
      "steps = 1, loss = 1.9433393478393555\n",
      "steps = 1, loss = 1.928364872932434\n",
      "steps = 1, loss = 2.4101498126983643\n",
      "steps = 1, loss = 2.2825045585632324\n",
      "steps = 1, loss = 2.1272799968719482\n",
      "steps = 1, loss = 49.99524688720703\n",
      "steps = 1, loss = 0.7660915851593018\n",
      "steps = 1, loss = 1.6854805946350098\n",
      "steps = 1, loss = 2.0361595153808594\n",
      "steps = 1, loss = 2.573859214782715\n",
      "steps = 1, loss = 0.755517840385437\n",
      "steps = 1, loss = 2.683659315109253\n",
      "steps = 1, loss = 0.7916966080665588\n",
      "steps = 1, loss = 2.337462902069092\n",
      "steps = 1, loss = 2.000476598739624\n",
      "steps = 1, loss = 3.623856782913208\n",
      "steps = 1, loss = 0.9026542901992798\n",
      "steps = 1, loss = 47.03567123413086\n",
      "steps = 1, loss = 1.2482517957687378\n",
      "steps = 1, loss = 2.701814651489258\n",
      "steps = 1, loss = 2.462686777114868\n",
      "steps = 1, loss = 2.5515925884246826\n",
      "steps = 1, loss = 50.008033752441406\n",
      "steps = 1, loss = 2.4103238582611084\n",
      "steps = 1, loss = 2.158217430114746\n",
      "steps = 1, loss = 2.3994967937469482\n",
      "steps = 1, loss = 1.736550211906433\n",
      "steps = 1, loss = 0.7259045243263245\n",
      "steps = 1, loss = 2.0171821117401123\n",
      "steps = 1, loss = 50.01250457763672\n",
      "steps = 1, loss = 50.02962112426758\n",
      "steps = 1, loss = 3.010918140411377\n",
      "steps = 1, loss = 4.5069661140441895\n",
      "steps = 1, loss = 2.3052752017974854\n",
      "steps = 1, loss = 2.283346176147461\n",
      "steps = 1, loss = 3.183513641357422\n",
      "steps = 1, loss = 2.1472208499908447\n",
      "steps = 1, loss = 2.670454740524292\n",
      "steps = 1, loss = 2.328421115875244\n",
      "steps = 1, loss = 2.5880024433135986\n",
      "steps = 1, loss = 2.07446026802063\n",
      "steps = 1, loss = 2.0590479373931885\n",
      "steps = 1, loss = 2.057513475418091\n",
      "steps = 1, loss = 50.00571823120117\n",
      "steps = 1, loss = 50.01814270019531\n",
      "steps = 1, loss = 1.8505221605300903\n",
      "steps = 1, loss = 1.661538004875183\n",
      "steps = 1, loss = 2.5104684829711914\n",
      "steps = 1, loss = 0.6967596411705017\n",
      "steps = 1, loss = 49.9971809387207\n",
      "steps = 1, loss = 2.0733237266540527\n",
      "steps = 81, loss = 2.7337915897369385\n",
      "steps = 1, loss = 1.8673076629638672\n",
      "steps = 1, loss = 2.461028814315796\n",
      "steps = 1, loss = 2.4548168182373047\n",
      "steps = 1, loss = 0.6952893733978271\n",
      "steps = 1, loss = 2.335280179977417\n",
      "steps = 1, loss = 2.665414333343506\n",
      "steps = 81, loss = 2.113412618637085\n",
      "steps = 1, loss = 0.7112677097320557\n",
      "steps = 81, loss = 2.164539098739624\n",
      "steps = 1, loss = 1.369569182395935\n",
      "steps = 1, loss = 1.708787202835083\n",
      "steps = 1, loss = 1.8556537628173828\n",
      "steps = 1, loss = 2.9766201972961426\n",
      "steps = 1, loss = 1.8417705297470093\n",
      "steps = 1, loss = 1.8450491428375244\n",
      "steps = 1, loss = 2.7239503860473633\n",
      "steps = 1, loss = 1.4235416650772095\n",
      "steps = 1, loss = 50.01491928100586\n",
      "steps = 1, loss = 50.02812576293945\n",
      "steps = 1, loss = 1.6192883253097534\n",
      "steps = 1, loss = 2.242403268814087\n",
      "steps = 1, loss = 1.7629363536834717\n",
      "steps = 1, loss = 1.934279203414917\n",
      "steps = 1, loss = 2.241748094558716\n",
      "steps = 1, loss = 1.4127460718154907\n",
      "steps = 1, loss = 2.6958422660827637\n",
      "steps = 1, loss = 2.064692735671997\n",
      "steps = 1, loss = 0.7383877038955688\n",
      "steps = 1, loss = 1.9448133707046509\n",
      "steps = 1, loss = 2.176271438598633\n",
      "steps = 1, loss = 1.8676103353500366\n",
      "steps = 1, loss = 2.086650848388672\n",
      "steps = 1, loss = 0.7096153497695923\n",
      "steps = 1, loss = 2.766810655593872\n",
      "steps = 1, loss = 2.2200021743774414\n",
      "steps = 1, loss = 0.7304058074951172\n",
      "steps = 1, loss = 2.1341066360473633\n",
      "steps = 1, loss = 1.9358752965927124\n",
      "steps = 1, loss = 1.9757051467895508\n",
      "steps = 1, loss = 2.6487958431243896\n",
      "steps = 1, loss = 2.4897818565368652\n",
      "steps = 1, loss = 2.4055421352386475\n",
      "steps = 1, loss = 49.99837875366211\n",
      "steps = 1, loss = 2.806938886642456\n",
      "steps = 1, loss = 1.5914547443389893\n",
      "steps = 1, loss = 50.02394485473633\n",
      "steps = 1, loss = 50.00922775268555\n",
      "steps = 1, loss = 2.8377413749694824\n",
      "steps = 1, loss = 2.586256504058838\n",
      "steps = 1, loss = 2.5059974193573\n",
      "steps = 1, loss = 2.669041633605957\n",
      "steps = 1, loss = 2.5779237747192383\n",
      "steps = 1, loss = 49.99397277832031\n",
      "steps = 1, loss = 1.6641597747802734\n",
      "steps = 1, loss = 2.3250558376312256\n",
      "steps = 1, loss = 0.7033761143684387\n",
      "steps = 1, loss = 1.8845086097717285\n",
      "steps = 1, loss = 49.991722106933594\n",
      "steps = 1, loss = 2.6523284912109375\n",
      "steps = 1, loss = 2.0764777660369873\n",
      "steps = 1, loss = 2.2022101879119873\n",
      "steps = 1, loss = 2.3414504528045654\n",
      "steps = 1, loss = 2.52833890914917\n",
      "steps = 1, loss = 2.2169203758239746\n",
      "steps = 1, loss = 0.7007049918174744\n",
      "steps = 1, loss = 2.0439469814300537\n",
      "steps = 1, loss = 2.2491633892059326\n",
      "steps = 1, loss = 2.9611589908599854\n",
      "steps = 1, loss = 2.9285905361175537\n",
      "steps = 1, loss = 48.82188034057617\n",
      "steps = 1, loss = 2.1010689735412598\n",
      "steps = 1, loss = 49.987693786621094\n",
      "steps = 1, loss = 2.1108415126800537\n",
      "steps = 1, loss = 2.2771811485290527\n",
      "steps = 1, loss = 5.8920392990112305\n",
      "steps = 1, loss = 2.8315727710723877\n",
      "steps = 1, loss = 2.3254363536834717\n",
      "steps = 1, loss = 2.351330280303955\n",
      "steps = 27, loss = 1.9543825387954712\n",
      "steps = 1, loss = 1.5809504985809326\n",
      "steps = 1, loss = 0.6957688927650452\n",
      "steps = 27, loss = 1.9616429805755615\n",
      "steps = 1, loss = 2.2617785930633545\n",
      "steps = 27, loss = 3.567753553390503\n",
      "steps = 1, loss = 2.0650789737701416\n",
      "steps = 1, loss = 2.821685791015625\n",
      "steps = 27, loss = 3.261350154876709\n",
      "steps = 1, loss = 50.02217483520508\n",
      "steps = 27, loss = 4.1289496421813965\n",
      "steps = 1, loss = 3.806715965270996\n",
      "steps = 243, loss = 2.147695779800415\n",
      "steps = 1, loss = 2.4797587394714355\n",
      "steps = 27, loss = 1.9735368490219116\n",
      "steps = 27, loss = 13.213842391967773\n",
      "steps = 1, loss = 2.08221697807312\n",
      "steps = 1, loss = 0.7122755646705627\n",
      "steps = 1, loss = 2.763974189758301\n",
      "steps = 27, loss = 2.909888744354248\n",
      "steps = 27, loss = 2.2022624015808105\n",
      "steps = 1, loss = 48.63536834716797\n",
      "steps = 1, loss = 2.721494197845459\n",
      "steps = 1, loss = 2.9249207973480225\n",
      "steps = 27, loss = 50.00017547607422\n",
      "steps = 27, loss = 3.2620136737823486\n",
      "steps = 1, loss = 1.8854249715805054\n",
      "steps = 1, loss = 2.7605111598968506\n",
      "steps = 1, loss = 49.84431076049805\n",
      "steps = 1, loss = 3.0921695232391357\n",
      "steps = 1, loss = 1.682149887084961\n",
      "steps = 1, loss = 1.9631861448287964\n",
      "steps = 1, loss = 2.0245823860168457\n",
      "steps = 1, loss = 2.8585917949676514\n",
      "steps = 1, loss = 2.479102849960327\n",
      "steps = 1, loss = 1.1743870973587036\n",
      "steps = 1, loss = 2.693814754486084\n",
      "steps = 1, loss = 1.8361246585845947\n",
      "steps = 1, loss = 3.2615468502044678\n",
      "steps = 1, loss = 1.9490408897399902\n",
      "steps = 1, loss = 49.99388885498047\n",
      "steps = 1, loss = 2.032689094543457\n",
      "steps = 1, loss = 1.788748025894165\n",
      "steps = 1, loss = 2.767390012741089\n",
      "steps = 1, loss = 1.7479889392852783\n",
      "steps = 1, loss = 2.196789503097534\n",
      "steps = 1, loss = 1.9676495790481567\n",
      "steps = 1, loss = 1.6745113134384155\n",
      "steps = 1, loss = 3.1350226402282715\n",
      "steps = 1, loss = 50.0130729675293\n",
      "steps = 1, loss = 2.598280906677246\n",
      "steps = 1, loss = 0.780847430229187\n",
      "steps = 1, loss = 3.5017411708831787\n",
      "steps = 1, loss = 2.02815580368042\n",
      "steps = 1, loss = 1.8911844491958618\n",
      "steps = 1, loss = 2.754382371902466\n",
      "steps = 3, loss = 1.6917210817337036\n",
      "steps = 3, loss = 3.5514140129089355\n",
      "steps = 3, loss = 1.9639184474945068\n",
      "steps = 3, loss = 1.8505604267120361\n",
      "steps = 3, loss = 1.8375787734985352\n",
      "steps = 3, loss = 1.9967200756072998\n",
      "steps = 3, loss = 2.0443854331970215\n",
      "steps = 3, loss = 2.099405527114868\n",
      "steps = 3, loss = 2.137349843978882\n",
      "steps = 3, loss = 1.8542428016662598\n",
      "steps = 3, loss = 1.916664481163025\n",
      "steps = 3, loss = 1.9990984201431274\n",
      "steps = 3, loss = 2.053607702255249\n",
      "steps = 3, loss = 1.6322160959243774\n",
      "steps = 3, loss = 2.047433614730835\n",
      "steps = 3, loss = 1.8303972482681274\n",
      "steps = 3, loss = 2.2528042793273926\n",
      "steps = 3, loss = 2.182697296142578\n",
      "steps = 3, loss = 0.8745545744895935\n",
      "steps = 3, loss = 3.992319107055664\n",
      "steps = 3, loss = 1.632623314857483\n",
      "steps = 3, loss = 2.120809555053711\n",
      "steps = 3, loss = 1.914734959602356\n",
      "steps = 27, loss = 49.989131927490234\n",
      "steps = 3, loss = 2.266648530960083\n",
      "steps = 3, loss = 1.8691051006317139\n",
      "steps = 3, loss = 1.279175043106079\n",
      "steps = 3, loss = 1.726184368133545\n",
      "steps = 3, loss = 2.56681752204895\n",
      "steps = 3, loss = 2.2105562686920166\n",
      "steps = 3, loss = 2.014338254928589\n",
      "steps = 27, loss = 2.6886889934539795\n",
      "steps = 3, loss = 0.8145648837089539\n",
      "steps = 3, loss = 2.1167337894439697\n",
      "steps = 3, loss = 2.2593181133270264\n",
      "steps = 3, loss = 2.5913989543914795\n",
      "steps = 3, loss = 2.0845744609832764\n",
      "steps = 3, loss = 1.6139880418777466\n",
      "steps = 3, loss = 1.6066166162490845\n",
      "steps = 3, loss = 1.1751691102981567\n",
      "steps = 3, loss = 0.8565540909767151\n",
      "steps = 3, loss = 0.8174631595611572\n",
      "steps = 3, loss = 2.175570249557495\n",
      "steps = 3, loss = 0.7541115283966064\n",
      "steps = 3, loss = 1.9867124557495117\n",
      "steps = 3, loss = 2.0774238109588623\n",
      "steps = 3, loss = 2.0266976356506348\n",
      "steps = 3, loss = 0.7017974853515625\n",
      "steps = 3, loss = 2.1302568912506104\n",
      "steps = 3, loss = 2.0982284545898438\n",
      "steps = 3, loss = 2.3737053871154785\n",
      "steps = 3, loss = 1.9574030637741089\n",
      "steps = 3, loss = 2.1273245811462402\n",
      "steps = 3, loss = 1.5338908433914185\n",
      "steps = 3, loss = 1.9446614980697632\n",
      "steps = 3, loss = 1.7213034629821777\n",
      "steps = 3, loss = 2.3107826709747314\n",
      "steps = 3, loss = 2.4434316158294678\n",
      "steps = 3, loss = 1.7615303993225098\n",
      "steps = 3, loss = 1.8977525234222412\n",
      "steps = 3, loss = 1.9457521438598633\n",
      "steps = 3, loss = 1.81900954246521\n",
      "steps = 3, loss = 2.108482837677002\n",
      "steps = 3, loss = 2.3145909309387207\n",
      "steps = 3, loss = 1.975504755973816\n",
      "steps = 27, loss = 3.150441884994507\n",
      "steps = 27, loss = 2.277653217315674\n",
      "steps = 27, loss = 3.5635459423065186\n",
      "steps = 3, loss = 2.5264015197753906\n",
      "steps = 3, loss = 2.020634889602661\n",
      "steps = 3, loss = 1.5007058382034302\n",
      "steps = 3, loss = 2.221799850463867\n",
      "steps = 3, loss = 2.136094093322754\n",
      "steps = 3, loss = 0.8219884037971497\n",
      "steps = 27, loss = 2.790250539779663\n",
      "steps = 3, loss = 2.401764154434204\n",
      "steps = 3, loss = 2.941251277923584\n",
      "steps = 3, loss = 1.9531548023223877\n",
      "steps = 3, loss = 1.8765037059783936\n",
      "steps = 3, loss = 1.970449447631836\n",
      "steps = 3, loss = 1.7807493209838867\n",
      "steps = 3, loss = 2.191854953765869\n",
      "steps = 3, loss = 2.0426976680755615\n",
      "steps = 3, loss = 2.0298943519592285\n",
      "steps = 3, loss = 2.1882474422454834\n",
      "steps = 3, loss = 0.9475615620613098\n",
      "steps = 3, loss = 2.086829423904419\n",
      "steps = 27, loss = 3.617429494857788\n",
      "steps = 9, loss = 1.6479161977767944\n",
      "steps = 9, loss = 2.1398658752441406\n",
      "steps = 9, loss = 1.6500917673110962\n",
      "steps = 9, loss = 1.940901756286621\n",
      "steps = 9, loss = 2.1139087677001953\n",
      "steps = 9, loss = 1.5651419162750244\n",
      "steps = 9, loss = 1.6419389247894287\n",
      "steps = 9, loss = 2.1011672019958496\n",
      "steps = 81, loss = 1.5750352144241333\n",
      "steps = 81, loss = 2.777022361755371\n",
      "steps = 9, loss = 1.9688149690628052\n",
      "steps = 81, loss = 4.401637554168701\n",
      "steps = 81, loss = 2.290764093399048\n",
      "steps = 9, loss = 2.108937978744507\n",
      "steps = 9, loss = 1.8205446004867554\n",
      "steps = 81, loss = 2.5024101734161377\n",
      "steps = 81, loss = 2.301663398742676\n",
      "steps = 9, loss = 1.9400818347930908\n",
      "steps = 9, loss = 1.8524816036224365\n",
      "steps = 9, loss = 1.958072543144226\n",
      "steps = 9, loss = 1.816413164138794\n",
      "steps = 9, loss = 1.9768017530441284\n",
      "steps = 9, loss = 2.005716323852539\n",
      "steps = 9, loss = 2.100964307785034\n",
      "steps = 9, loss = 2.0131993293762207\n",
      "steps = 9, loss = 1.7022590637207031\n",
      "steps = 9, loss = 1.6080759763717651\n",
      "steps = 9, loss = 1.9013375043869019\n",
      "steps = 9, loss = 2.040311098098755\n",
      "steps = 9, loss = 1.5553107261657715\n",
      "steps = 9, loss = 2.3723278045654297\n",
      "steps = 1, loss = 1.988043189048767\n",
      "steps = 1, loss = 1.4598603248596191\n",
      "steps = 1, loss = 2.864659070968628\n",
      "steps = 1, loss = 1.882656455039978\n",
      "steps = 1, loss = 1.1321369409561157\n",
      "steps = 1, loss = 2.3613624572753906\n",
      "steps = 1, loss = 2.0466296672821045\n",
      "steps = 1, loss = 0.748729944229126\n",
      "steps = 1, loss = 49.91206359863281\n",
      "steps = 1, loss = 1.8162975311279297\n",
      "steps = 1, loss = 3.0930423736572266\n",
      "steps = 1, loss = 1.6710162162780762\n",
      "steps = 1, loss = 5.314520359039307\n",
      "steps = 1, loss = 3.4586241245269775\n",
      "steps = 1, loss = 0.718695342540741\n",
      "steps = 1, loss = 1.96505868434906\n",
      "steps = 1, loss = 2.538666009902954\n",
      "steps = 1, loss = 1.8495349884033203\n",
      "steps = 1, loss = 2.456063747406006\n",
      "steps = 1, loss = 3.384243965148926\n",
      "steps = 1, loss = 49.55873489379883\n",
      "steps = 1, loss = 2.084094285964966\n",
      "steps = 1, loss = 1.5956233739852905\n",
      "steps = 1, loss = 2.0206804275512695\n",
      "steps = 1, loss = 49.998802185058594\n",
      "steps = 1, loss = 2.390878438949585\n",
      "steps = 1, loss = 0.7082163095474243\n",
      "steps = 1, loss = 1.4829858541488647\n",
      "steps = 1, loss = 2.7847604751586914\n",
      "steps = 1, loss = 1.946048378944397\n",
      "steps = 1, loss = 1.9663532972335815\n",
      "steps = 1, loss = 1.676689863204956\n",
      "steps = 1, loss = 2.829538106918335\n",
      "steps = 1, loss = 2.0278213024139404\n",
      "steps = 1, loss = 0.7155572772026062\n",
      "steps = 1, loss = 1.1391602754592896\n",
      "steps = 1, loss = 1.7052273750305176\n",
      "steps = 1, loss = 2.334047555923462\n",
      "steps = 1, loss = 2.6980667114257812\n",
      "steps = 1, loss = 4.827888488769531\n",
      "steps = 1, loss = 50.02153778076172\n",
      "steps = 1, loss = 1.9534801244735718\n",
      "steps = 1, loss = 1.7400643825531006\n",
      "steps = 1, loss = 2.843251943588257\n",
      "steps = 1, loss = 1.7459667921066284\n",
      "steps = 1, loss = 2.0712480545043945\n",
      "steps = 1, loss = 3.107914686203003\n",
      "steps = 1, loss = 1.827426552772522\n",
      "steps = 1, loss = 1.775277018547058\n",
      "steps = 9, loss = 2.1103503704071045\n",
      "steps = 1, loss = 2.511686325073242\n",
      "steps = 1, loss = 1.2025749683380127\n",
      "steps = 9, loss = 2.0397396087646484\n",
      "steps = 1, loss = 49.99958801269531\n",
      "steps = 1, loss = 2.414820671081543\n",
      "steps = 1, loss = 2.7962520122528076\n",
      "steps = 1, loss = 3.9332029819488525\n",
      "steps = 1, loss = 2.4446113109588623\n",
      "steps = 1, loss = 2.1588196754455566\n",
      "steps = 1, loss = 2.022237539291382\n",
      "steps = 1, loss = 1.9274396896362305\n",
      "steps = 1, loss = 2.7266290187835693\n",
      "steps = 1, loss = 3.6347999572753906\n",
      "steps = 1, loss = 1.7811264991760254\n",
      "steps = 1, loss = 1.9192476272583008\n",
      "steps = 1, loss = 0.9130846858024597\n",
      "steps = 1, loss = 2.1966991424560547\n",
      "steps = 1, loss = 1.7206494808197021\n",
      "steps = 1, loss = 1.9540541172027588\n",
      "steps = 1, loss = 2.577918529510498\n",
      "steps = 1, loss = 50.01491928100586\n",
      "steps = 1, loss = 1.9960544109344482\n",
      "steps = 1, loss = 2.6618423461914062\n",
      "steps = 1, loss = 2.1823067665100098\n",
      "steps = 1, loss = 5.159860610961914\n",
      "steps = 1, loss = 2.1380157470703125\n",
      "steps = 1, loss = 1.7392140626907349\n",
      "steps = 1, loss = 1.7864032983779907\n",
      "steps = 1, loss = 2.2831456661224365\n",
      "steps = 1, loss = 49.98958969116211\n",
      "steps = 1, loss = 1.4303823709487915\n",
      "steps = 1, loss = 2.1813876628875732\n",
      "steps = 1, loss = 1.9446218013763428\n",
      "steps = 1, loss = 2.55891489982605\n",
      "steps = 1, loss = 2.3727331161499023\n",
      "steps = 1, loss = 2.417633533477783\n",
      "steps = 1, loss = 4.508089542388916\n",
      "steps = 1, loss = 2.312091588973999\n",
      "steps = 1, loss = 2.1902389526367188\n",
      "steps = 1, loss = 49.998756408691406\n",
      "steps = 1, loss = 2.074096202850342\n",
      "steps = 1, loss = 1.7678146362304688\n",
      "steps = 1, loss = 3.0235111713409424\n",
      "steps = 1, loss = 2.6503663063049316\n",
      "steps = 1, loss = 1.99490487575531\n",
      "steps = 1, loss = 49.98341751098633\n",
      "steps = 1, loss = 2.0490405559539795\n",
      "steps = 1, loss = 1.588681697845459\n",
      "steps = 27, loss = 1.9530421495437622\n",
      "steps = 27, loss = 1.902204155921936\n",
      "steps = 27, loss = 1.9828373193740845\n",
      "steps = 1, loss = 50.015445709228516\n",
      "steps = 1, loss = 2.257749080657959\n",
      "steps = 3, loss = 49.552921295166016\n",
      "steps = 3, loss = 2.6728978157043457\n",
      "steps = 3, loss = 1.926630973815918\n",
      "steps = 3, loss = 1.8545657396316528\n",
      "steps = 3, loss = 1.5050550699234009\n",
      "steps = 3, loss = 49.99422073364258\n",
      "steps = 3, loss = 2.0101373195648193\n",
      "steps = 3, loss = 2.47819185256958\n",
      "steps = 3, loss = 3.0087037086486816\n",
      "steps = 3, loss = 8.058249473571777\n",
      "steps = 3, loss = 2.711496591567993\n",
      "steps = 3, loss = 2.643068790435791\n",
      "steps = 3, loss = 2.533818006515503\n",
      "steps = 3, loss = 2.18186354637146\n",
      "steps = 3, loss = 5.296260356903076\n",
      "steps = 3, loss = 3.5580012798309326\n",
      "steps = 3, loss = 2.089505672454834\n",
      "steps = 3, loss = 2.2816920280456543\n",
      "steps = 3, loss = 1.7493727207183838\n",
      "steps = 3, loss = 2.7419967651367188\n",
      "steps = 3, loss = 2.0205020904541016\n",
      "steps = 3, loss = 2.4196906089782715\n",
      "steps = 3, loss = 2.6094398498535156\n",
      "steps = 3, loss = 49.99393844604492\n",
      "steps = 3, loss = 2.4720845222473145\n",
      "steps = 3, loss = 3.157740831375122\n",
      "steps = 3, loss = 2.2218692302703857\n",
      "steps = 3, loss = 9.135293960571289\n",
      "steps = 3, loss = 2.1960935592651367\n",
      "steps = 3, loss = 2.0453128814697266\n",
      "steps = 3, loss = 2.090693712234497\n",
      "steps = 3, loss = 2.5889527797698975\n",
      "steps = 3, loss = 49.98958969116211\n",
      "steps = 3, loss = 1.8821475505828857\n",
      "steps = 3, loss = 2.2565972805023193\n",
      "steps = 3, loss = 2.2815539836883545\n",
      "steps = 3, loss = 2.744636058807373\n",
      "steps = 3, loss = 2.3036484718322754\n",
      "steps = 3, loss = 2.7280781269073486\n",
      "steps = 3, loss = 6.3437042236328125\n",
      "steps = 3, loss = 2.4101102352142334\n",
      "steps = 3, loss = 2.2122151851654053\n",
      "steps = 3, loss = 49.31107711791992\n",
      "steps = 3, loss = 2.5970654487609863\n",
      "steps = 3, loss = 2.06896710395813\n",
      "steps = 3, loss = 3.3772706985473633\n",
      "steps = 3, loss = 2.763056755065918\n",
      "steps = 3, loss = 2.440821886062622\n",
      "steps = 3, loss = 49.990760803222656\n",
      "steps = 3, loss = 2.64194393157959\n",
      "steps = 243, loss = 2.9970028400421143\n",
      "steps = 3, loss = 1.9779887199401855\n",
      "steps = 27, loss = 2.0856871604919434\n",
      "steps = 27, loss = 1.9381581544876099\n",
      "steps = 27, loss = 1.94623601436615\n",
      "steps = 27, loss = 1.9385390281677246\n",
      "steps = 3, loss = 2.286997079849243\n",
      "steps = 3, loss = 3.2281620502471924\n",
      "steps = 3, loss = 3.3155646324157715\n",
      "steps = 3, loss = 1.7414875030517578\n",
      "steps = 3, loss = 2.4223239421844482\n",
      "steps = 3, loss = 2.4171290397644043\n",
      "steps = 3, loss = 49.99544143676758\n",
      "steps = 3, loss = 3.529979705810547\n",
      "steps = 3, loss = 2.0710606575012207\n",
      "steps = 3, loss = 6.202219486236572\n",
      "steps = 3, loss = 0.9250831007957458\n",
      "steps = 3, loss = 2.3244833946228027\n",
      "steps = 3, loss = 2.6848907470703125\n",
      "steps = 3, loss = 2.1510684490203857\n",
      "steps = 3, loss = 3.0188961029052734\n",
      "steps = 3, loss = 3.466869831085205\n",
      "steps = 3, loss = 50.005775451660156\n",
      "steps = 3, loss = 2.1997628211975098\n",
      "steps = 3, loss = 1.9370272159576416\n",
      "steps = 3, loss = 2.3828299045562744\n",
      "steps = 3, loss = 49.99879455566406\n",
      "steps = 3, loss = 2.431885004043579\n",
      "steps = 3, loss = 1.7777316570281982\n",
      "steps = 3, loss = 1.770989179611206\n",
      "steps = 3, loss = 2.853121042251587\n",
      "steps = 3, loss = 2.132418632507324\n",
      "steps = 3, loss = 2.3347856998443604\n",
      "steps = 3, loss = 2.4079298973083496\n",
      "steps = 3, loss = 3.172027111053467\n",
      "steps = 3, loss = 2.2928285598754883\n",
      "steps = 3, loss = 0.7375800609588623\n",
      "steps = 3, loss = 1.8691169023513794\n",
      "steps = 3, loss = 1.9781814813613892\n",
      "steps = 3, loss = 2.6028101444244385\n",
      "steps = 3, loss = 49.99436569213867\n",
      "steps = 3, loss = 4.656055450439453\n",
      "steps = 3, loss = 50.020790100097656\n",
      "steps = 3, loss = 2.3491809368133545\n",
      "steps = 3, loss = 1.9913694858551025\n",
      "steps = 3, loss = 3.3406429290771484\n",
      "steps = 3, loss = 1.940172553062439\n",
      "steps = 3, loss = 2.6768643856048584\n",
      "steps = 3, loss = 3.6461634635925293\n",
      "steps = 3, loss = 2.016941547393799\n",
      "steps = 3, loss = 2.0781447887420654\n",
      "steps = 3, loss = 2.3339521884918213\n",
      "steps = 3, loss = 1.8425755500793457\n",
      "steps = 9, loss = 2.0260090827941895\n",
      "steps = 9, loss = 2.4877641201019287\n",
      "steps = 9, loss = 2.260096549987793\n",
      "steps = 9, loss = 2.1165499687194824\n",
      "steps = 9, loss = 1.9113595485687256\n",
      "steps = 9, loss = 2.269951105117798\n",
      "steps = 9, loss = 2.1714670658111572\n",
      "steps = 9, loss = 2.930767774581909\n",
      "steps = 9, loss = 1.9066249132156372\n",
      "steps = 9, loss = 2.502833366394043\n",
      "steps = 9, loss = 1.716842770576477\n",
      "steps = 9, loss = 2.0640578269958496\n",
      "steps = 9, loss = 2.2915515899658203\n",
      "steps = 9, loss = 2.28495717048645\n",
      "steps = 9, loss = 2.117328405380249\n",
      "steps = 9, loss = 2.0883705615997314\n",
      "steps = 9, loss = 2.280385732650757\n",
      "steps = 9, loss = 1.398707628250122\n",
      "steps = 9, loss = 2.5525670051574707\n",
      "steps = 9, loss = 2.2128031253814697\n",
      "steps = 9, loss = 2.692267894744873\n",
      "steps = 9, loss = 1.922197937965393\n",
      "steps = 9, loss = 2.3192384243011475\n",
      "steps = 9, loss = 2.4779279232025146\n",
      "steps = 9, loss = 2.405194044113159\n",
      "steps = 9, loss = 2.6589369773864746\n",
      "steps = 9, loss = 2.7105565071105957\n",
      "steps = 9, loss = 1.9156721830368042\n",
      "steps = 9, loss = 2.2701592445373535\n",
      "steps = 9, loss = 2.3251867294311523\n",
      "steps = 9, loss = 2.0768959522247314\n",
      "steps = 9, loss = 2.20330548286438\n",
      "steps = 27, loss = 2.017820358276367\n",
      "steps = 27, loss = 2.0844035148620605\n",
      "steps = 27, loss = 2.6684670448303223\n",
      "steps = 27, loss = 2.0784270763397217\n",
      "steps = 81, loss = 2.2164738178253174\n",
      "steps = 81, loss = 2.190943717956543\n",
      "steps = 27, loss = 1.8969391584396362\n",
      "steps = 27, loss = 2.8054726123809814\n",
      "steps = 27, loss = 2.495670795440674\n",
      "steps = 27, loss = 2.165088415145874\n",
      "steps = 27, loss = 2.250701904296875\n",
      "steps = 27, loss = 2.123720407485962\n",
      "steps = 27, loss = 2.6180331707000732\n",
      "steps = 27, loss = 2.0308263301849365\n",
      "steps = 81, loss = 2.202436923980713\n",
      "steps = 81, loss = 2.2496259212493896\n",
      "steps = 81, loss = 2.0358080863952637\n",
      "steps = 81, loss = 2.4858267307281494\n",
      "steps = 243, loss = 2.2632761001586914\n",
      "steps = 243, loss = 2.8700709342956543\n",
      "steps = 243, loss = 1.9401624202728271\n",
      "steps = 747, loss = 2.319584608078003\n",
      "CPU times: user 1h 23min 54s, sys: 11min 3s, total: 1h 34min 57s\n",
      "Wall time: 9h 38min 49s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HyperbandSearchCV(estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                  max_iter=250,\n",
       "                  parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                              &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;, &#x27;ELU&#x27;,\n",
       "                                                     &#x27;PReLU&#x27;],\n",
       "                              &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                               &#x27;xavier_normal_&#x27;,\n",
       "                                               &#x27;kaiming_uniform_&#x27;,\n",
       "                                               &#x27;kaiming_normal_&#x27;],\n",
       "                              &#x27;optimizer&#x27;: [&#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;S...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                              &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                              &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, ...],\n",
       "                              &#x27;train_split&#x27;: [None]},\n",
       "                  random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;HyperbandSearchCV<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>HyperbandSearchCV(estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                  max_iter=250,\n",
       "                  parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                              &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;, &#x27;ELU&#x27;,\n",
       "                                                     &#x27;PReLU&#x27;],\n",
       "                              &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                               &#x27;xavier_normal_&#x27;,\n",
       "                                               &#x27;kaiming_uniform_&#x27;,\n",
       "                                               &#x27;kaiming_normal_&#x27;],\n",
       "                              &#x27;optimizer&#x27;: [&#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;S...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                              &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                              &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, ...],\n",
       "                              &#x27;train_split&#x27;: [None]},\n",
       "                  random_state=42)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HyperbandSearchCV(estimator=<class '__main__.TrimParams'>[uninitialized](\n",
       "  module=<class 'autoencoder.Autoencoder'>,\n",
       "),\n",
       "                  max_iter=250,\n",
       "                  parameters={'batch_size': [32, 64, 128, 256, 512],\n",
       "                              'module__activation': ['ReLU', 'LeakyReLU', 'ELU',\n",
       "                                                     'PReLU'],\n",
       "                              'module__init': ['xavier_uniform_',\n",
       "                                               'xavier_normal_',\n",
       "                                               'kaiming_uniform_',\n",
       "                                               'kaiming_normal_'],\n",
       "                              'optimizer': ['SGD', 'SGD', 'SGD', 'S...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                              'optimizer__nesterov': [True],\n",
       "                              'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, ...],\n",
       "                              'train_split': [None]},\n",
       "                  random_state=42)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "search.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing_stats = client.profile(filename=\"hyperband.html\")\n",
    "# with open(f\"{absolutepath_to_results}/hyperband-timing.json\", \"w\") as f:\n",
    "#     json.dump(timing_stats[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class '__main__.TrimParams'>[initialized](\n",
       "  module_=Autoencoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=784, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (2): Linear(in_features=784, out_features=196, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=196, out_features=784, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "      (3): Sigmoid()\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9401624202728271"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_split': None,\n",
       " 'optimizer__weight_decay': 0.0009119267598459299,\n",
       " 'optimizer__nesterov': True,\n",
       " 'optimizer__momentum': 0.25725725725725723,\n",
       " 'optimizer__lr': 6.915758828738525,\n",
       " 'optimizer': 'SGD',\n",
       " 'module__init': 'xavier_normal_',\n",
       " 'module__activation': 'LeakyReLU',\n",
       " 'batch_size': 256}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_search(search, today, \"hyperband\", X_test.compute(), y_test.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperband with SOP\n",
    "\n",
    "Trains the model with Hyperband with SOP and analyzes the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_patience = HyperbandSearchCV(model, params, max_iter, random_state=42, patience=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 1, loss = 1.976102590560913\n",
      "steps = 1, loss = 2.0065276622772217\n",
      "steps = 1, loss = 2.022500514984131\n",
      "steps = 1, loss = 1.1510610580444336\n",
      "steps = 1, loss = 1.9715073108673096\n",
      "steps = 1, loss = 0.7229595184326172\n",
      "steps = 84, loss = 2.7753853797912598\n",
      "steps = 84, loss = 2.7443578243255615\n",
      "steps = 84, loss = 3.6286063194274902\n",
      "steps = 84, loss = 3.589615821838379\n",
      "steps = 1, loss = 2.0105245113372803\n",
      "steps = 1, loss = 2.005730390548706\n",
      "steps = 1, loss = 1.9459617137908936\n",
      "steps = 1, loss = 1.0126945972442627\n",
      "steps = 1, loss = 1.6624188423156738\n",
      "steps = 1, loss = 49.70864486694336\n",
      "steps = 1, loss = 1.7215481996536255\n",
      "steps = 1, loss = 1.5606186389923096\n",
      "steps = 1, loss = 3.1231038570404053\n",
      "steps = 84, loss = 2.5648086071014404\n",
      "steps = 84, loss = 3.58955454826355\n",
      "steps = 83, loss = 4.845966339111328\n",
      "steps = 83, loss = 2.49174165725708\n",
      "steps = 83, loss = 2.6515438556671143\n",
      "steps = 83, loss = 3.897847890853882\n",
      "steps = 83, loss = 3.4700748920440674\n",
      "steps = 83, loss = 2.441875696182251\n",
      "steps = 1, loss = 1.7992393970489502\n",
      "steps = 1, loss = 1.7342123985290527\n",
      "steps = 1, loss = 2.506152868270874\n",
      "steps = 1, loss = 0.8883376121520996\n",
      "steps = 1, loss = 2.082169771194458\n",
      "steps = 1, loss = 3.865260362625122\n",
      "steps = 1, loss = 2.399545192718506\n",
      "steps = 1, loss = 2.0154409408569336\n",
      "steps = 1, loss = 0.7719258666038513\n",
      "steps = 1, loss = 1.7636090517044067\n",
      "steps = 1, loss = 1.9718873500823975\n",
      "steps = 1, loss = 1.9374303817749023\n",
      "steps = 1, loss = 1.9311585426330566\n",
      "steps = 1, loss = 1.8002336025238037\n",
      "steps = 1, loss = 2.5854711532592773\n",
      "steps = 1, loss = 0.9661195874214172\n",
      "steps = 1, loss = 50.03294372558594\n",
      "steps = 1, loss = 1.7750695943832397\n",
      "steps = 1, loss = 2.5110323429107666\n",
      "steps = 1, loss = 2.032252311706543\n",
      "steps = 1, loss = 2.273576021194458\n",
      "steps = 1, loss = 1.9864051342010498\n",
      "steps = 1, loss = 2.0215888023376465\n",
      "steps = 1, loss = 2.4246129989624023\n",
      "steps = 1, loss = 1.8213036060333252\n",
      "steps = 83, loss = 2.6944053173065186\n",
      "steps = 1, loss = 2.022845506668091\n",
      "steps = 1, loss = 1.5835665464401245\n",
      "steps = 1, loss = 1.8579168319702148\n",
      "steps = 1, loss = 2.503469467163086\n",
      "steps = 1, loss = 2.9389548301696777\n",
      "steps = 1, loss = 2.431325674057007\n",
      "steps = 1, loss = 1.2279740571975708\n",
      "steps = 1, loss = 1.793084740638733\n",
      "steps = 1, loss = 2.597764015197754\n",
      "steps = 1, loss = 49.999183654785156\n",
      "steps = 1, loss = 2.750908374786377\n",
      "steps = 1, loss = 2.1595799922943115\n",
      "steps = 1, loss = 2.102537155151367\n",
      "steps = 1, loss = 0.7211442589759827\n",
      "steps = 1, loss = 2.0993521213531494\n",
      "steps = 1, loss = 2.36973237991333\n",
      "steps = 9, loss = 2.0741641521453857\n",
      "steps = 9, loss = 2.4624252319335938\n",
      "steps = 9, loss = 3.027205228805542\n",
      "steps = 9, loss = 6.602343559265137\n",
      "steps = 9, loss = 3.3072092533111572\n",
      "steps = 9, loss = 2.00130295753479\n",
      "steps = 9, loss = 2.1756234169006348\n",
      "steps = 9, loss = 2.8150124549865723\n",
      "steps = 9, loss = 2.486617088317871\n",
      "steps = 9, loss = 2.538470506668091\n",
      "steps = 83, loss = 2.5678772926330566\n",
      "steps = 9, loss = 3.5834124088287354\n",
      "steps = 9, loss = 1.93047297000885\n",
      "steps = 9, loss = 2.587425947189331\n",
      "steps = 9, loss = 2.0498063564300537\n",
      "steps = 9, loss = 50.03324890136719\n",
      "steps = 9, loss = 1.9639228582382202\n",
      "steps = 9, loss = 2.2143383026123047\n",
      "steps = 9, loss = 2.7151761054992676\n",
      "steps = 83, loss = 3.2396674156188965\n",
      "steps = 9, loss = 2.8679144382476807\n",
      "steps = 9, loss = 2.382416009902954\n",
      "steps = 9, loss = 3.6186742782592773\n",
      "steps = 9, loss = 2.592621088027954\n",
      "steps = 9, loss = 3.376551389694214\n",
      "steps = 9, loss = 3.638550043106079\n",
      "steps = 9, loss = 2.2905359268188477\n",
      "steps = 9, loss = 3.2603542804718018\n",
      "steps = 9, loss = 3.3332860469818115\n",
      "steps = 9, loss = 2.8528757095336914\n",
      "steps = 9, loss = 2.1080071926116943\n",
      "steps = 9, loss = 2.309544086456299\n",
      "steps = 9, loss = 2.6895992755889893\n",
      "steps = 9, loss = 3.4739747047424316\n",
      "steps = 9, loss = 49.99017333984375\n",
      "steps = 9, loss = 2.959926128387451\n",
      "steps = 9, loss = 2.7450175285339355\n",
      "steps = 9, loss = 2.98881459236145\n",
      "steps = 9, loss = 2.430205821990967\n",
      "steps = 9, loss = 2.331500291824341\n",
      "steps = 9, loss = 1.8820430040359497\n",
      "steps = 9, loss = 2.6268491744995117\n",
      "steps = 9, loss = 2.2821807861328125\n",
      "steps = 27, loss = 2.429812431335449\n",
      "steps = 27, loss = 2.6939525604248047\n",
      "steps = 27, loss = 2.177184820175171\n",
      "steps = 27, loss = 2.36126446723938\n",
      "steps = 27, loss = 2.4908735752105713\n",
      "steps = 27, loss = 2.040205240249634\n",
      "steps = 27, loss = 2.148772954940796\n",
      "steps = 27, loss = 2.192610740661621\n",
      "steps = 27, loss = 2.292963743209839\n",
      "steps = 27, loss = 2.419462203979492\n",
      "steps = 27, loss = 2.2230629920959473\n",
      "steps = 1, loss = 0.7132876515388489\n",
      "steps = 1, loss = 0.6947764754295349\n",
      "steps = 27, loss = 2.1621830463409424\n",
      "steps = 1, loss = 1.3921881914138794\n",
      "steps = 1, loss = 49.97151565551758\n",
      "steps = 1, loss = 1.9595893621444702\n",
      "steps = 1, loss = 1.93499755859375\n",
      "steps = 27, loss = 2.545840263366699\n",
      "steps = 1, loss = 2.0536389350891113\n",
      "steps = 1, loss = 2.0565898418426514\n",
      "steps = 1, loss = 1.8445072174072266\n",
      "steps = 1, loss = 2.3870468139648438\n",
      "steps = 1, loss = 1.5400933027267456\n",
      "steps = 1, loss = 2.081611394882202\n",
      "steps = 1, loss = 50.006195068359375\n",
      "steps = 1, loss = 2.466464042663574\n",
      "steps = 1, loss = 2.7470710277557373\n",
      "steps = 1, loss = 7.467029571533203\n",
      "steps = 1, loss = 0.6960509419441223\n",
      "steps = 1, loss = 1.4138689041137695\n",
      "steps = 27, loss = 1.9595837593078613\n",
      "steps = 81, loss = 2.062436819076538\n",
      "steps = 81, loss = 2.1939871311187744\n",
      "steps = 27, loss = 1.964181661605835\n",
      "steps = 81, loss = 2.0864391326904297\n",
      "steps = 27, loss = 2.7682650089263916\n",
      "steps = 81, loss = 2.76813006401062\n",
      "steps = 27, loss = 3.1566121578216553\n",
      "steps = 27, loss = 3.2393152713775635\n",
      "steps = 27, loss = 3.1296322345733643\n",
      "steps = 27, loss = 2.9063143730163574\n",
      "steps = 27, loss = 3.691540002822876\n",
      "steps = 27, loss = 3.562567710876465\n",
      "steps = 27, loss = 2.7324419021606445\n",
      "steps = 27, loss = 3.5421502590179443\n",
      "steps = 27, loss = 49.769107818603516\n",
      "steps = 27, loss = 2.201664447784424\n",
      "steps = 27, loss = 3.1907079219818115\n",
      "steps = 27, loss = 2.202320098876953\n",
      "steps = 27, loss = 49.576107025146484\n",
      "steps = 27, loss = 4.145674705505371\n",
      "steps = 27, loss = 12.067633628845215\n",
      "steps = 1, loss = 1.9640547037124634\n",
      "steps = 1, loss = 1.4937371015548706\n",
      "steps = 1, loss = 1.8534736633300781\n",
      "steps = 1, loss = 1.9374562501907349\n",
      "steps = 1, loss = 0.7024468779563904\n",
      "steps = 1, loss = 2.965411424636841\n",
      "steps = 1, loss = 1.0770316123962402\n",
      "steps = 1, loss = 49.99958038330078\n",
      "steps = 1, loss = 2.196505546569824\n",
      "steps = 1, loss = 2.7478020191192627\n",
      "steps = 1, loss = 50.02484130859375\n",
      "steps = 1, loss = 1.9063485860824585\n",
      "steps = 1, loss = 1.8170002698898315\n",
      "steps = 1, loss = 1.729100227355957\n",
      "steps = 1, loss = 2.595726728439331\n",
      "steps = 1, loss = 1.56155264377594\n",
      "steps = 1, loss = 2.2690179347991943\n",
      "steps = 1, loss = 2.781534433364868\n",
      "steps = 1, loss = 2.4304428100585938\n",
      "steps = 1, loss = 2.302105665206909\n",
      "steps = 1, loss = 1.7136836051940918\n",
      "steps = 1, loss = 2.020298480987549\n",
      "steps = 1, loss = 2.021958827972412\n",
      "steps = 1, loss = 2.692499876022339\n",
      "steps = 1, loss = 2.5109686851501465\n",
      "steps = 1, loss = 50.01271438598633\n",
      "steps = 1, loss = 1.7070250511169434\n",
      "steps = 1, loss = 2.2566511631011963\n",
      "steps = 1, loss = 50.01339340209961\n",
      "steps = 1, loss = 2.7648661136627197\n",
      "steps = 164, loss = 2.119516372680664\n",
      "steps = 1, loss = 1.4949305057525635\n",
      "steps = 1, loss = 2.4018468856811523\n",
      "steps = 1, loss = 1.6415451765060425\n",
      "steps = 1, loss = 1.932300329208374\n",
      "steps = 1, loss = 1.904356837272644\n",
      "steps = 1, loss = 50.032047271728516\n",
      "steps = 1, loss = 3.740142822265625\n",
      "steps = 81, loss = 2.288606882095337\n",
      "steps = 1, loss = 2.0409111976623535\n",
      "steps = 1, loss = 1.9937933683395386\n",
      "steps = 1, loss = 2.415637254714966\n",
      "steps = 1, loss = 1.9492605924606323\n",
      "steps = 1, loss = 1.9835436344146729\n",
      "steps = 1, loss = 3.5833349227905273\n",
      "steps = 1, loss = 49.996463775634766\n",
      "steps = 1, loss = 2.1819510459899902\n",
      "steps = 1, loss = 2.1049506664276123\n",
      "steps = 1, loss = 2.0917744636535645\n",
      "steps = 1, loss = 2.8436737060546875\n",
      "steps = 1, loss = 1.6908988952636719\n",
      "steps = 1, loss = 2.320211887359619\n",
      "steps = 1, loss = 2.180023193359375\n",
      "steps = 1, loss = 4.232182502746582\n",
      "steps = 1, loss = 1.7135908603668213\n",
      "steps = 1, loss = 49.992408752441406\n",
      "steps = 1, loss = 4.103856563568115\n",
      "steps = 1, loss = 1.957754135131836\n",
      "steps = 1, loss = 2.9487197399139404\n",
      "steps = 1, loss = 1.996193766593933\n",
      "steps = 1, loss = 49.99165725708008\n",
      "steps = 1, loss = 2.249173641204834\n",
      "steps = 1, loss = 2.436624526977539\n",
      "steps = 1, loss = 2.86205792427063\n",
      "steps = 81, loss = 2.300462007522583\n",
      "steps = 1, loss = 3.752155065536499\n",
      "steps = 1, loss = 3.680802583694458\n",
      "steps = 1, loss = 2.0377774238586426\n",
      "steps = 1, loss = 2.0586116313934326\n",
      "steps = 1, loss = 1.8457367420196533\n",
      "steps = 1, loss = 2.2953426837921143\n",
      "steps = 1, loss = 50.0015983581543\n",
      "steps = 1, loss = 1.5599546432495117\n",
      "steps = 1, loss = 1.8391436338424683\n",
      "steps = 1, loss = 3.007870674133301\n",
      "steps = 1, loss = 2.3317830562591553\n",
      "steps = 1, loss = 2.539541244506836\n",
      "steps = 1, loss = 1.3261024951934814\n",
      "steps = 1, loss = 2.2253730297088623\n",
      "steps = 1, loss = 2.107239246368408\n",
      "steps = 1, loss = 0.6953843832015991\n",
      "steps = 1, loss = 0.7304484844207764\n",
      "steps = 1, loss = 3.0402910709381104\n",
      "steps = 1, loss = 0.7138105630874634\n",
      "steps = 81, loss = 1.8000520467758179\n",
      "steps = 1, loss = 3.02960467338562\n",
      "steps = 1, loss = 50.01515197753906\n",
      "steps = 1, loss = 2.2845160961151123\n",
      "steps = 1, loss = 28.899627685546875\n",
      "steps = 1, loss = 1.910409927368164\n",
      "steps = 1, loss = 2.691826105117798\n",
      "steps = 1, loss = 50.01597595214844\n",
      "steps = 1, loss = 1.7868666648864746\n",
      "steps = 1, loss = 2.7539455890655518\n",
      "steps = 1, loss = 2.0828702449798584\n",
      "steps = 1, loss = 2.156998634338379\n",
      "steps = 1, loss = 0.9955209493637085\n",
      "steps = 81, loss = 4.2192888259887695\n",
      "steps = 1, loss = 2.1009914875030518\n",
      "steps = 1, loss = 2.9833433628082275\n",
      "steps = 1, loss = 0.7053812146186829\n",
      "steps = 1, loss = 1.8699783086776733\n",
      "steps = 1, loss = 50.00740432739258\n",
      "steps = 1, loss = 2.1582610607147217\n",
      "steps = 1, loss = 2.110292434692383\n",
      "steps = 1, loss = 2.1345510482788086\n",
      "steps = 1, loss = 2.348071813583374\n",
      "steps = 1, loss = 0.7261946201324463\n",
      "steps = 1, loss = 2.4299185276031494\n",
      "steps = 1, loss = 2.0131545066833496\n",
      "steps = 1, loss = 2.1797330379486084\n",
      "steps = 1, loss = 2.479950428009033\n",
      "steps = 1, loss = 3.055056571960449\n",
      "steps = 1, loss = 1.9773783683776855\n",
      "steps = 1, loss = 2.4471547603607178\n",
      "steps = 1, loss = 2.6044278144836426\n",
      "steps = 1, loss = 50.00352478027344\n",
      "steps = 81, loss = 3.344578981399536\n",
      "steps = 1, loss = 3.086059331893921\n",
      "steps = 1, loss = 49.95124435424805\n",
      "steps = 1, loss = 1.936660647392273\n",
      "steps = 1, loss = 2.306732416152954\n",
      "steps = 1, loss = 1.980758786201477\n",
      "steps = 1, loss = 50.01052474975586\n",
      "steps = 1, loss = 0.7021657824516296\n",
      "steps = 1, loss = 2.219506025314331\n",
      "steps = 1, loss = 3.30248761177063\n",
      "steps = 1, loss = 1.6418321132659912\n",
      "steps = 1, loss = 49.99880599975586\n",
      "steps = 1, loss = 1.7480353116989136\n",
      "steps = 1, loss = 1.7537955045700073\n",
      "steps = 1, loss = 2.7315175533294678\n",
      "steps = 1, loss = 2.7604799270629883\n",
      "steps = 1, loss = 1.935832142829895\n",
      "steps = 1, loss = 49.979820251464844\n",
      "steps = 1, loss = 0.7108551263809204\n",
      "steps = 1, loss = 2.685119867324829\n",
      "steps = 1, loss = 49.98785400390625\n",
      "steps = 1, loss = 49.81474685668945\n",
      "steps = 1, loss = 2.552600860595703\n",
      "steps = 1, loss = 1.8847416639328003\n",
      "steps = 1, loss = 2.211468458175659\n",
      "steps = 1, loss = 50.00260543823242\n",
      "steps = 1, loss = 2.4355108737945557\n",
      "steps = 1, loss = 2.359182834625244\n",
      "steps = 1, loss = 2.904634714126587\n",
      "steps = 1, loss = 1.8730379343032837\n",
      "steps = 1, loss = 2.7279160022735596\n",
      "steps = 1, loss = 2.289216995239258\n",
      "steps = 1, loss = 2.057981014251709\n",
      "steps = 1, loss = 2.5657103061676025\n",
      "steps = 1, loss = 49.98379135131836\n",
      "steps = 1, loss = 2.4205379486083984\n",
      "steps = 1, loss = 1.8661773204803467\n",
      "steps = 1, loss = 1.5764427185058594\n",
      "steps = 1, loss = 50.00696563720703\n",
      "steps = 1, loss = 3.1020309925079346\n",
      "steps = 1, loss = 1.835217833518982\n",
      "steps = 1, loss = 49.96995544433594\n",
      "steps = 1, loss = 2.6811013221740723\n",
      "steps = 1, loss = 2.195013999938965\n",
      "steps = 1, loss = 3.123790979385376\n",
      "steps = 1, loss = 2.1571567058563232\n",
      "steps = 1, loss = 1.0047521591186523\n",
      "steps = 1, loss = 50.00632095336914\n",
      "steps = 1, loss = 5.9599151611328125\n",
      "steps = 1, loss = 2.025648593902588\n",
      "steps = 1, loss = 2.0817062854766846\n",
      "steps = 1, loss = 2.5837910175323486\n",
      "steps = 1, loss = 1.1213078498840332\n",
      "steps = 1, loss = 2.458371877670288\n",
      "steps = 1, loss = 0.9341937899589539\n",
      "steps = 1, loss = 2.903982639312744\n",
      "steps = 1, loss = 2.004361391067505\n",
      "steps = 1, loss = 0.7118794322013855\n",
      "steps = 1, loss = 2.4500463008880615\n",
      "steps = 1, loss = 2.000138998031616\n",
      "steps = 1, loss = 0.7548988461494446\n",
      "steps = 1, loss = 3.040233850479126\n",
      "steps = 1, loss = 0.7893089056015015\n",
      "steps = 1, loss = 2.5804455280303955\n",
      "steps = 1, loss = 1.462814211845398\n",
      "steps = 1, loss = 3.0782651901245117\n",
      "steps = 1, loss = 2.083484172821045\n",
      "steps = 1, loss = 2.4949681758880615\n",
      "steps = 1, loss = 2.5063560009002686\n",
      "steps = 1, loss = 2.004786491394043\n",
      "steps = 1, loss = 2.7360849380493164\n",
      "steps = 1, loss = 2.4008426666259766\n",
      "steps = 1, loss = 3.2977101802825928\n",
      "steps = 1, loss = 50.03006362915039\n",
      "steps = 81, loss = 2.8245906829833984\n",
      "steps = 1, loss = 2.0515758991241455\n",
      "steps = 1, loss = 1.8366618156433105\n",
      "steps = 1, loss = 2.8082947731018066\n",
      "steps = 1, loss = 50.010684967041016\n",
      "steps = 1, loss = 1.7888903617858887\n",
      "steps = 1, loss = 1.6812955141067505\n",
      "steps = 1, loss = 1.7457085847854614\n",
      "steps = 1, loss = 2.835407257080078\n",
      "steps = 1, loss = 1.8007515668869019\n",
      "steps = 1, loss = 2.24324631690979\n",
      "steps = 1, loss = 2.146970510482788\n",
      "steps = 1, loss = 1.949595332145691\n",
      "steps = 1, loss = 2.5386223793029785\n",
      "steps = 1, loss = 2.689239740371704\n",
      "steps = 1, loss = 0.8053096532821655\n",
      "steps = 1, loss = 1.8670347929000854\n",
      "steps = 1, loss = 2.236208200454712\n",
      "steps = 1, loss = 2.68196439743042\n",
      "steps = 1, loss = 1.9637664556503296\n",
      "steps = 1, loss = 5.564571857452393\n",
      "steps = 1, loss = 1.6344884634017944\n",
      "steps = 1, loss = 2.567379951477051\n",
      "steps = 1, loss = 2.3932995796203613\n",
      "steps = 1, loss = 1.2144558429718018\n",
      "steps = 1, loss = 49.961692810058594\n",
      "steps = 1, loss = 1.7180464267730713\n",
      "steps = 1, loss = 2.8724544048309326\n",
      "steps = 1, loss = 50.00546646118164\n",
      "steps = 1, loss = 49.933860778808594\n",
      "steps = 1, loss = 2.3920938968658447\n",
      "steps = 1, loss = 2.154179096221924\n",
      "steps = 1, loss = 2.113872766494751\n",
      "steps = 1, loss = 2.0259921550750732\n",
      "steps = 1, loss = 2.603501081466675\n",
      "steps = 1, loss = 2.086256980895996\n",
      "steps = 1, loss = 4.052460670471191\n",
      "steps = 1, loss = 2.846071243286133\n",
      "steps = 1, loss = 2.0662474632263184\n",
      "steps = 1, loss = 0.7034077644348145\n",
      "steps = 1, loss = 1.177841305732727\n",
      "steps = 1, loss = 2.678959608078003\n",
      "steps = 1, loss = 0.7826295495033264\n",
      "steps = 1, loss = 0.6958963871002197\n",
      "steps = 1, loss = 2.7747793197631836\n",
      "steps = 1, loss = 2.537787675857544\n",
      "steps = 1, loss = 2.012955904006958\n",
      "steps = 1, loss = 0.8227781057357788\n",
      "steps = 1, loss = 49.98824691772461\n",
      "steps = 1, loss = 2.3321025371551514\n",
      "steps = 1, loss = 1.8884458541870117\n",
      "steps = 1, loss = 50.01002502441406\n",
      "steps = 1, loss = 1.8061846494674683\n",
      "steps = 1, loss = 1.9322619438171387\n",
      "steps = 1, loss = 1.6438653469085693\n",
      "steps = 1, loss = 2.763143301010132\n",
      "steps = 1, loss = 2.785893201828003\n",
      "steps = 1, loss = 1.472103476524353\n",
      "steps = 1, loss = 0.7571992874145508\n",
      "steps = 3, loss = 1.855315089225769\n",
      "steps = 3, loss = 2.2547543048858643\n",
      "steps = 3, loss = 2.085477828979492\n",
      "steps = 3, loss = 2.4827325344085693\n",
      "steps = 3, loss = 3.012885093688965\n",
      "steps = 3, loss = 2.091968059539795\n",
      "steps = 3, loss = 1.9492478370666504\n",
      "steps = 3, loss = 3.173290729522705\n",
      "steps = 3, loss = 1.8359686136245728\n",
      "steps = 3, loss = 2.0092945098876953\n",
      "steps = 3, loss = 2.1317827701568604\n",
      "steps = 3, loss = 2.085400104522705\n",
      "steps = 3, loss = 2.400439739227295\n",
      "steps = 3, loss = 0.8477190136909485\n",
      "steps = 3, loss = 1.862398624420166\n",
      "steps = 3, loss = 1.1943060159683228\n",
      "steps = 3, loss = 2.0647828578948975\n",
      "steps = 3, loss = 0.7676287889480591\n",
      "steps = 3, loss = 1.7244269847869873\n",
      "steps = 3, loss = 0.7210447788238525\n",
      "steps = 3, loss = 1.2954738140106201\n",
      "steps = 3, loss = 1.9802824258804321\n",
      "steps = 3, loss = 1.9367320537567139\n",
      "steps = 3, loss = 2.0760107040405273\n",
      "steps = 3, loss = 2.362640619277954\n",
      "steps = 3, loss = 1.8456590175628662\n",
      "steps = 3, loss = 2.3816022872924805\n",
      "steps = 3, loss = 3.0321216583251953\n",
      "steps = 3, loss = 2.126497983932495\n",
      "steps = 3, loss = 2.083153009414673\n",
      "steps = 3, loss = 2.4066925048828125\n",
      "steps = 3, loss = 2.359405517578125\n",
      "steps = 3, loss = 1.6079941987991333\n",
      "steps = 3, loss = 1.9611597061157227\n",
      "steps = 3, loss = 2.024406909942627\n",
      "steps = 3, loss = 2.005197048187256\n",
      "steps = 3, loss = 1.7224575281143188\n",
      "steps = 3, loss = 2.152313709259033\n",
      "steps = 3, loss = 2.1922245025634766\n",
      "steps = 3, loss = 2.098339319229126\n",
      "steps = 3, loss = 1.883231282234192\n",
      "steps = 3, loss = 1.9455828666687012\n",
      "steps = 3, loss = 1.6191914081573486\n",
      "steps = 3, loss = 1.7660757303237915\n",
      "steps = 3, loss = 0.8094541430473328\n",
      "steps = 3, loss = 1.524651050567627\n",
      "steps = 3, loss = 1.6460269689559937\n",
      "steps = 3, loss = 1.8905798196792603\n",
      "steps = 3, loss = 2.2412705421447754\n",
      "steps = 3, loss = 1.9770148992538452\n",
      "steps = 164, loss = 2.716118812561035\n",
      "steps = 3, loss = 2.1688249111175537\n",
      "steps = 3, loss = 1.6521267890930176\n",
      "steps = 3, loss = 2.017876386642456\n",
      "steps = 3, loss = 1.8900762796401978\n",
      "steps = 3, loss = 2.1431329250335693\n",
      "steps = 3, loss = 1.9306631088256836\n",
      "steps = 3, loss = 2.0830466747283936\n",
      "steps = 3, loss = 0.7037591338157654\n",
      "steps = 3, loss = 2.2497189044952393\n",
      "steps = 3, loss = 2.040952682495117\n",
      "steps = 3, loss = 2.112875461578369\n",
      "steps = 3, loss = 2.212273597717285\n",
      "steps = 3, loss = 1.831473708152771\n",
      "steps = 3, loss = 3.336906671524048\n",
      "steps = 3, loss = 2.006594181060791\n",
      "steps = 3, loss = 2.610340118408203\n",
      "steps = 3, loss = 2.0473783016204834\n",
      "steps = 3, loss = 2.217535972595215\n",
      "steps = 3, loss = 1.9574196338653564\n",
      "steps = 3, loss = 1.9342753887176514\n",
      "steps = 3, loss = 2.0015599727630615\n",
      "steps = 3, loss = 0.8698888421058655\n",
      "steps = 3, loss = 1.8197076320648193\n",
      "steps = 3, loss = 0.9935616254806519\n",
      "steps = 3, loss = 0.8688770532608032\n",
      "steps = 3, loss = 1.6892887353897095\n",
      "steps = 3, loss = 2.071824312210083\n",
      "steps = 3, loss = 2.0368661880493164\n",
      "steps = 3, loss = 2.22698712348938\n",
      "steps = 3, loss = 1.9211931228637695\n",
      "steps = 3, loss = 1.511291742324829\n",
      "steps = 9, loss = 1.6607881784439087\n",
      "steps = 9, loss = 2.120249032974243\n",
      "steps = 9, loss = 1.7270313501358032\n",
      "steps = 9, loss = 2.1209592819213867\n",
      "steps = 9, loss = 1.5881067514419556\n",
      "steps = 9, loss = 2.0426547527313232\n",
      "steps = 9, loss = 2.009592056274414\n",
      "steps = 9, loss = 1.7027232646942139\n",
      "steps = 9, loss = 2.086422920227051\n",
      "steps = 9, loss = 2.0096709728240967\n",
      "steps = 9, loss = 1.9636528491973877\n",
      "steps = 9, loss = 2.049146890640259\n",
      "steps = 9, loss = 1.8633979558944702\n",
      "steps = 9, loss = 1.952848196029663\n",
      "steps = 9, loss = 1.9769742488861084\n",
      "steps = 9, loss = 1.6348720788955688\n",
      "steps = 9, loss = 1.651846170425415\n",
      "steps = 9, loss = 2.11975359916687\n",
      "steps = 9, loss = 1.8377265930175781\n",
      "steps = 9, loss = 1.9322645664215088\n",
      "steps = 9, loss = 1.8323373794555664\n",
      "steps = 9, loss = 1.4832422733306885\n",
      "steps = 9, loss = 1.5780925750732422\n",
      "steps = 9, loss = 1.9759665727615356\n",
      "steps = 9, loss = 2.1082050800323486\n",
      "steps = 9, loss = 1.8985896110534668\n",
      "steps = 9, loss = 2.02573299407959\n",
      "steps = 27, loss = 2.1040382385253906\n",
      "steps = 27, loss = 1.9051071405410767\n",
      "steps = 27, loss = 1.933126449584961\n",
      "steps = 27, loss = 1.9565768241882324\n",
      "steps = 27, loss = 1.9646590948104858\n",
      "steps = 27, loss = 2.108924627304077\n",
      "steps = 27, loss = 1.932251214981079\n",
      "steps = 1, loss = 1.895315408706665\n",
      "steps = 1, loss = 2.393123149871826\n",
      "steps = 1, loss = 3.5921008586883545\n",
      "steps = 1, loss = 1.8387248516082764\n",
      "steps = 1, loss = 1.7891409397125244\n",
      "steps = 1, loss = 1.7044051885604858\n",
      "steps = 27, loss = 2.024177312850952\n",
      "steps = 27, loss = 1.9838441610336304\n",
      "steps = 1, loss = 2.0396249294281006\n",
      "steps = 1, loss = 5.046157360076904\n",
      "steps = 1, loss = 1.6774009466171265\n",
      "steps = 1, loss = 0.7628449201583862\n",
      "steps = 1, loss = 2.5139739513397217\n",
      "steps = 1, loss = 1.7412034273147583\n",
      "steps = 1, loss = 0.7005345821380615\n",
      "steps = 1, loss = 4.369958400726318\n",
      "steps = 1, loss = 2.0256917476654053\n",
      "steps = 1, loss = 2.871305465698242\n",
      "steps = 1, loss = 1.9939134120941162\n",
      "steps = 1, loss = 2.476270914077759\n",
      "steps = 1, loss = 1.9548014402389526\n",
      "steps = 1, loss = 3.0630900859832764\n",
      "steps = 1, loss = 2.106520414352417\n",
      "steps = 1, loss = 0.8362001776695251\n",
      "steps = 1, loss = 1.844232439994812\n",
      "steps = 1, loss = 2.083726406097412\n",
      "steps = 1, loss = 49.52632141113281\n",
      "steps = 1, loss = 50.00410461425781\n",
      "steps = 1, loss = 4.463320732116699\n",
      "steps = 1, loss = 2.4617886543273926\n",
      "steps = 1, loss = 2.676065444946289\n",
      "steps = 1, loss = 2.562161445617676\n",
      "steps = 1, loss = 3.7054591178894043\n",
      "steps = 1, loss = 1.7264755964279175\n",
      "steps = 1, loss = 2.1423280239105225\n",
      "steps = 1, loss = 3.276416063308716\n",
      "steps = 1, loss = 2.0452189445495605\n",
      "steps = 1, loss = 1.8593363761901855\n",
      "steps = 1, loss = 1.9370205402374268\n",
      "steps = 1, loss = 2.37599778175354\n",
      "steps = 1, loss = 3.033787727355957\n",
      "steps = 1, loss = 1.0351457595825195\n",
      "steps = 1, loss = 1.9453344345092773\n",
      "steps = 1, loss = 3.028352737426758\n",
      "steps = 1, loss = 2.071754217147827\n",
      "steps = 81, loss = 2.2148025035858154\n",
      "steps = 1, loss = 1.0793598890304565\n",
      "steps = 1, loss = 1.7056211233139038\n",
      "steps = 1, loss = 2.1204028129577637\n",
      "steps = 1, loss = 3.019948959350586\n",
      "steps = 1, loss = 2.191282033920288\n",
      "steps = 1, loss = 3.033582925796509\n",
      "steps = 1, loss = 1.9430780410766602\n",
      "steps = 1, loss = 2.496718168258667\n",
      "steps = 1, loss = 1.9281487464904785\n",
      "steps = 1, loss = 49.993125915527344\n",
      "steps = 1, loss = 1.5872251987457275\n",
      "steps = 1, loss = 2.5271575450897217\n",
      "steps = 1, loss = 1.7236967086791992\n",
      "steps = 1, loss = 2.3550612926483154\n",
      "steps = 1, loss = 2.349792718887329\n",
      "steps = 1, loss = 2.72906494140625\n",
      "steps = 1, loss = 50.01737594604492\n",
      "steps = 1, loss = 2.552330255508423\n",
      "steps = 1, loss = 1.7441860437393188\n",
      "steps = 1, loss = 0.7178204655647278\n",
      "steps = 1, loss = 49.98950958251953\n",
      "steps = 1, loss = 1.4244718551635742\n",
      "steps = 1, loss = 1.7995884418487549\n",
      "steps = 1, loss = 1.924985647201538\n",
      "steps = 1, loss = 2.5777511596679688\n",
      "steps = 1, loss = 2.9048030376434326\n",
      "steps = 1, loss = 2.1250522136688232\n",
      "steps = 1, loss = 1.772114872932434\n",
      "steps = 1, loss = 4.099021911621094\n",
      "steps = 1, loss = 2.548985481262207\n",
      "steps = 1, loss = 1.8010423183441162\n",
      "steps = 1, loss = 1.0331693887710571\n",
      "steps = 1, loss = 50.030277252197266\n",
      "steps = 1, loss = 2.918372869491577\n",
      "steps = 1, loss = 1.8175452947616577\n",
      "steps = 1, loss = 1.454298496246338\n",
      "steps = 1, loss = 2.0928523540496826\n",
      "steps = 1, loss = 49.9900016784668\n",
      "steps = 1, loss = 1.7071654796600342\n",
      "steps = 1, loss = 1.9996201992034912\n",
      "steps = 1, loss = 2.467400074005127\n",
      "steps = 1, loss = 0.7289989590644836\n",
      "steps = 1, loss = 1.999109148979187\n",
      "steps = 1, loss = 2.0680341720581055\n",
      "steps = 1, loss = 49.99685287475586\n",
      "steps = 1, loss = 50.00291061401367\n",
      "steps = 81, loss = 2.202017307281494\n",
      "steps = 1, loss = 2.208463430404663\n",
      "steps = 1, loss = 1.6046885251998901\n",
      "steps = 1, loss = 1.9615029096603394\n",
      "steps = 1, loss = 1.99561607837677\n",
      "steps = 1, loss = 1.4283978939056396\n",
      "steps = 1, loss = 2.8841774463653564\n",
      "steps = 1, loss = 2.3737595081329346\n",
      "steps = 1, loss = 2.3560969829559326\n",
      "steps = 1, loss = 50.011226654052734\n",
      "steps = 3, loss = 3.192354679107666\n",
      "steps = 3, loss = 2.0808093547821045\n",
      "steps = 3, loss = 2.182337522506714\n",
      "steps = 3, loss = 2.6706807613372803\n",
      "steps = 3, loss = 2.5108275413513184\n",
      "steps = 3, loss = 2.751431941986084\n",
      "steps = 3, loss = 2.6858725547790527\n",
      "steps = 3, loss = 49.99565124511719\n",
      "steps = 3, loss = 2.685760021209717\n",
      "steps = 3, loss = 50.00397491455078\n",
      "steps = 3, loss = 2.0177133083343506\n",
      "steps = 3, loss = 50.020172119140625\n",
      "steps = 3, loss = 3.231910228729248\n",
      "steps = 3, loss = 2.02769136428833\n",
      "steps = 3, loss = 2.2751128673553467\n",
      "steps = 3, loss = 1.744831919670105\n",
      "steps = 3, loss = 1.7370233535766602\n",
      "steps = 3, loss = 2.128261089324951\n",
      "steps = 3, loss = 2.1947312355041504\n",
      "steps = 3, loss = 2.1020779609680176\n",
      "steps = 3, loss = 2.303501605987549\n",
      "steps = 3, loss = 2.4379518032073975\n",
      "steps = 3, loss = 1.9313313961029053\n",
      "steps = 3, loss = 2.792066812515259\n",
      "steps = 3, loss = 50.01904296875\n",
      "steps = 3, loss = 2.0061841011047363\n",
      "steps = 3, loss = 2.320230722427368\n",
      "steps = 3, loss = 2.4446866512298584\n",
      "steps = 3, loss = 1.9082404375076294\n",
      "steps = 3, loss = 4.088883876800537\n",
      "steps = 3, loss = 4.8705949783325195\n",
      "steps = 3, loss = 8.433829307556152\n",
      "steps = 3, loss = 2.0264554023742676\n",
      "steps = 3, loss = 1.5250083208084106\n",
      "steps = 3, loss = 2.1829025745391846\n",
      "steps = 3, loss = 2.059101104736328\n",
      "steps = 3, loss = 6.445107936859131\n",
      "steps = 3, loss = 6.032118320465088\n",
      "steps = 3, loss = 2.1029958724975586\n",
      "steps = 3, loss = 1.8396025896072388\n",
      "steps = 3, loss = 50.02533721923828\n",
      "steps = 3, loss = 3.3168938159942627\n",
      "steps = 3, loss = 50.0108642578125\n",
      "steps = 3, loss = 2.308790922164917\n",
      "steps = 3, loss = 0.7523163557052612\n",
      "steps = 3, loss = 4.172092437744141\n",
      "steps = 3, loss = 3.0280823707580566\n",
      "steps = 3, loss = 3.1509923934936523\n",
      "steps = 3, loss = 2.7160305976867676\n",
      "steps = 3, loss = 7.003838539123535\n",
      "steps = 3, loss = 2.24890398979187\n",
      "steps = 3, loss = 2.6372578144073486\n",
      "steps = 3, loss = 1.9144850969314575\n",
      "steps = 3, loss = 3.395068407058716\n",
      "steps = 3, loss = 2.6224653720855713\n",
      "steps = 81, loss = 2.036036491394043\n",
      "steps = 3, loss = 2.156270742416382\n",
      "steps = 3, loss = 2.1467459201812744\n",
      "steps = 3, loss = 2.028358221054077\n",
      "steps = 3, loss = 2.5080618858337402\n",
      "steps = 3, loss = 3.3055343627929688\n",
      "steps = 3, loss = 1.86795973777771\n",
      "steps = 3, loss = 2.4270966053009033\n",
      "steps = 3, loss = 3.107546806335449\n",
      "steps = 3, loss = 2.580911159515381\n",
      "steps = 3, loss = 1.7390267848968506\n",
      "steps = 3, loss = 2.3826911449432373\n",
      "steps = 3, loss = 2.5610923767089844\n",
      "steps = 3, loss = 3.3957626819610596\n",
      "steps = 3, loss = 2.249329090118408\n",
      "steps = 3, loss = 3.3558335304260254\n",
      "steps = 3, loss = 2.3448314666748047\n",
      "steps = 3, loss = 2.3414411544799805\n",
      "steps = 3, loss = 2.3409276008605957\n",
      "steps = 3, loss = 2.1981630325317383\n",
      "steps = 3, loss = 49.993125915527344\n",
      "steps = 3, loss = 1.929463267326355\n",
      "steps = 3, loss = 2.843702554702759\n",
      "steps = 3, loss = 1.9720687866210938\n",
      "steps = 3, loss = 2.9017293453216553\n",
      "steps = 3, loss = 2.212996006011963\n",
      "steps = 3, loss = 2.1328182220458984\n",
      "steps = 3, loss = 49.798336029052734\n",
      "steps = 3, loss = 2.6117916107177734\n",
      "steps = 164, loss = 2.2924604415893555\n",
      "steps = 3, loss = 49.85498046875\n",
      "steps = 164, loss = 2.190166711807251\n",
      "steps = 3, loss = 1.9933873414993286\n",
      "steps = 3, loss = 0.9839263558387756\n",
      "steps = 3, loss = 2.279383659362793\n",
      "steps = 3, loss = 2.6321487426757812\n",
      "steps = 3, loss = 2.489528179168701\n",
      "steps = 3, loss = 49.99576950073242\n",
      "steps = 3, loss = 2.749919891357422\n",
      "steps = 3, loss = 49.99607849121094\n",
      "steps = 3, loss = 2.468491554260254\n",
      "steps = 3, loss = 1.9832215309143066\n",
      "steps = 3, loss = 2.3394975662231445\n",
      "steps = 3, loss = 2.4848289489746094\n",
      "steps = 3, loss = 3.2615303993225098\n",
      "steps = 3, loss = 1.8845576047897339\n",
      "steps = 9, loss = 2.243142604827881\n",
      "steps = 9, loss = 2.017806053161621\n",
      "steps = 9, loss = 2.058734178543091\n",
      "steps = 9, loss = 2.3040060997009277\n",
      "steps = 9, loss = 2.504309892654419\n",
      "steps = 9, loss = 2.0755722522735596\n",
      "steps = 9, loss = 2.180574655532837\n",
      "steps = 9, loss = 2.2753186225891113\n",
      "steps = 9, loss = 2.796196699142456\n",
      "steps = 9, loss = 2.46132230758667\n",
      "steps = 9, loss = 2.30694580078125\n",
      "steps = 9, loss = 2.6695594787597656\n",
      "steps = 9, loss = 2.4428586959838867\n",
      "steps = 9, loss = 2.1143031120300293\n",
      "steps = 9, loss = 2.2868926525115967\n",
      "steps = 9, loss = 2.3466382026672363\n",
      "steps = 9, loss = 1.745879888534546\n",
      "steps = 9, loss = 2.222287178039551\n",
      "steps = 9, loss = 2.481391429901123\n",
      "steps = 9, loss = 1.9006377458572388\n",
      "steps = 9, loss = 2.3799445629119873\n",
      "steps = 9, loss = 2.0893688201904297\n",
      "steps = 9, loss = 2.1625735759735107\n",
      "steps = 9, loss = 2.5292351245880127\n",
      "steps = 9, loss = 2.9985969066619873\n",
      "steps = 9, loss = 2.520145893096924\n",
      "steps = 9, loss = 2.1878502368927\n",
      "steps = 9, loss = 1.4346052408218384\n",
      "steps = 9, loss = 1.9777668714523315\n",
      "steps = 9, loss = 1.92705500125885\n",
      "steps = 9, loss = 1.9138002395629883\n",
      "steps = 9, loss = 2.4519124031066895\n",
      "steps = 27, loss = 2.160358190536499\n",
      "steps = 27, loss = 2.063892364501953\n",
      "steps = 27, loss = 2.6203665733337402\n",
      "steps = 27, loss = 2.9051148891448975\n",
      "steps = 27, loss = 2.6540467739105225\n",
      "steps = 27, loss = 2.2422797679901123\n",
      "steps = 27, loss = 1.8925540447235107\n",
      "steps = 27, loss = 2.1033377647399902\n",
      "steps = 27, loss = 2.0571844577789307\n",
      "steps = 27, loss = 2.443009853363037\n",
      "steps = 81, loss = 2.246981382369995\n",
      "steps = 81, loss = 2.166259765625\n",
      "steps = 81, loss = 2.5439112186431885\n",
      "steps = 164, loss = 2.5308125019073486\n",
      "CPU times: user 31min 33s, sys: 3min 53s, total: 35min 26s\n",
      "Wall time: 3h 8min 24s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HyperbandSearchCV(estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                  max_iter=250,\n",
       "                  parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                              &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;, &#x27;ELU&#x27;,\n",
       "                                                     &#x27;PReLU&#x27;],\n",
       "                              &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                               &#x27;xavier_normal_&#x27;,\n",
       "                                               &#x27;kaiming_uniform_&#x27;,\n",
       "                                               &#x27;kaiming_normal_&#x27;],\n",
       "                              &#x27;optimizer&#x27;: [&#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;S...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                              &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                              &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, ...],\n",
       "                              &#x27;train_split&#x27;: [None]},\n",
       "                  patience=True, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;HyperbandSearchCV<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>HyperbandSearchCV(estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                  max_iter=250,\n",
       "                  parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                              &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;, &#x27;ELU&#x27;,\n",
       "                                                     &#x27;PReLU&#x27;],\n",
       "                              &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                               &#x27;xavier_normal_&#x27;,\n",
       "                                               &#x27;kaiming_uniform_&#x27;,\n",
       "                                               &#x27;kaiming_normal_&#x27;],\n",
       "                              &#x27;optimizer&#x27;: [&#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;SGD&#x27;, &#x27;S...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                              &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                              &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, ...],\n",
       "                              &#x27;train_split&#x27;: [None]},\n",
       "                  patience=True, random_state=42)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HyperbandSearchCV(estimator=<class '__main__.TrimParams'>[uninitialized](\n",
       "  module=<class 'autoencoder.Autoencoder'>,\n",
       "),\n",
       "                  max_iter=250,\n",
       "                  parameters={'batch_size': [32, 64, 128, 256, 512],\n",
       "                              'module__activation': ['ReLU', 'LeakyReLU', 'ELU',\n",
       "                                                     'PReLU'],\n",
       "                              'module__init': ['xavier_uniform_',\n",
       "                                               'xavier_normal_',\n",
       "                                               'kaiming_uniform_',\n",
       "                                               'kaiming_normal_'],\n",
       "                              'optimizer': ['SGD', 'SGD', 'SGD', 'S...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                              'optimizer__nesterov': [True],\n",
       "                              'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, 0, 0, 0, 0, 0,\n",
       "                                                          0, 0, ...],\n",
       "                              'train_split': [None]},\n",
       "                  patience=True, random_state=42)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "search_patience.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_search(search_patience, today, \"hyperband-w-patience\", X_test.compute(), y_test.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing_stats = timing_stats = client.profile(filename=\"hyperband.html\")\n",
    "# with open(f\"{absolutepath_to_results}/hyperband+sop-timing.json\", \"w\") as f:\n",
    "#     json.dump(timing_stats[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class '__main__.TrimParams'>[initialized](\n",
       "  module_=Autoencoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=784, bias=True)\n",
       "      (1): ELU(alpha=1.0, inplace=True)\n",
       "      (2): Linear(in_features=784, out_features=196, bias=True)\n",
       "      (3): ELU(alpha=1.0, inplace=True)\n",
       "    )\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=196, out_features=784, bias=True)\n",
       "      (1): ELU(alpha=1.0, inplace=True)\n",
       "      (2): Linear(in_features=784, out_features=784, bias=True)\n",
       "      (3): Sigmoid()\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_patience.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.119516372680664"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_patience.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_split': None,\n",
       " 'optimizer__weight_decay': 0.0007479522515621821,\n",
       " 'optimizer__nesterov': True,\n",
       " 'optimizer__momentum': 0.4544544544544544,\n",
       " 'optimizer__lr': 3.6271002523306466,\n",
       " 'optimizer': 'SGD',\n",
       " 'module__init': 'xavier_uniform_',\n",
       " 'module__activation': 'ELU',\n",
       " 'batch_size': 128}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_patience.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing output of best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_test = X_test.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7002, 2, 784)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_hat = search.best_estimator_.predict(noisy_test)\n",
    "clean_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 24 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import check_random_state\n",
    "import seaborn as sns\n",
    "\n",
    "cols = 8\n",
    "w = 1.0\n",
    "fig, axs = plt.subplots(figsize=(cols*w, 3*w), ncols=cols, nrows=3)\n",
    "\n",
    "rng = check_random_state(42)\n",
    "for col, (upper, middle, lower) in enumerate(zip(axs[0], axs[1], axs[2])):\n",
    "    if col == 0:\n",
    "        upper.text(-28, 14, 'ground\\ntruth')\n",
    "        middle.text(-28, 14, 'input')\n",
    "        lower.text(-28, 14, 'output')\n",
    "    i = rng.choice(len(X_test))\n",
    "    noisy = X_test[i].reshape(28, 28)\n",
    "    clean = y_test[i].reshape(28, 28)\n",
    "    clean_hat_i = clean_hat[i][1].reshape(28,28)\n",
    "    kwargs = {'cbar': False, 'xticklabels': False, 'yticklabels': False, 'cmap': 'gray_r'}\n",
    "    sns.heatmap(noisy, ax=middle, **kwargs)\n",
    "    sns.heatmap(clean, ax=upper, **kwargs)\n",
    "    sns.heatmap(clean_hat_i, ax=lower, **kwargs)\n",
    "plt.savefig(f\"{absolutepath_to_results}/best-out.svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Parameters for Hyperband\n",
    "\n",
    "Tuning Hyperband requires setting:\n",
    "\n",
    "* `max_iter`: Number of epochs or data passes for training.\n",
    "* Chunk size for the data array.\n",
    "\n",
    "These determine the number of models evaluated and the overall search complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Early Stopping Techniques\n",
    "\n",
    "Compares Hyperband with:\n",
    "\n",
    "* **Passive Search:** No early stopping, evaluates all models.\n",
    "* **Incremental Search with Patience:** Early stops models that don't improve on a validation set.\n",
    "\n",
    "Analyzes:\n",
    "\n",
    "* Best model scores\n",
    "* Visualizations of the best model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 29, 7401)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_calls = search.metadata_['partial_fit_calls']\n",
    "num_calls = max_iter\n",
    "\n",
    "# n_workers = 32 or len(client.cluster.workers)\n",
    "n_workers = len(client.cluster.workers)\n",
    "num_models = max(n_workers, total_calls // num_calls)\n",
    "num_calls, num_models, total_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import IncrementalSearchCV\n",
    "\n",
    "passive_search = IncrementalSearchCV(\n",
    "    model,\n",
    "    params,\n",
    "    decay_rate=0,\n",
    "    patience=False,\n",
    "    n_initial_parameters=num_models,\n",
    "    max_iter=num_calls,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 1, loss = 0.7092676758766174\n",
      "steps = 1, loss = 0.7851949334144592\n",
      "steps = 1, loss = 2.9281046390533447\n",
      "steps = 1, loss = 2.0382602214813232\n",
      "steps = 1, loss = 1.7686915397644043\n",
      "steps = 1, loss = 2.0014758110046387\n",
      "steps = 1, loss = 2.4612090587615967\n",
      "steps = 1, loss = 2.0172791481018066\n",
      "steps = 1, loss = 2.428661346435547\n",
      "steps = 1, loss = 1.8866915702819824\n",
      "steps = 1, loss = 2.4018678665161133\n",
      "steps = 1, loss = 1.0489938259124756\n",
      "steps = 1, loss = 49.450504302978516\n",
      "steps = 1, loss = 2.036716938018799\n",
      "steps = 1, loss = 2.4377639293670654\n",
      "steps = 1, loss = 49.915061950683594\n",
      "steps = 1, loss = 2.382110834121704\n",
      "steps = 1, loss = 1.9739691019058228\n",
      "steps = 1, loss = 1.9683659076690674\n",
      "steps = 1, loss = 1.9700000286102295\n",
      "steps = 1, loss = 0.7204102277755737\n",
      "steps = 1, loss = 2.392211437225342\n",
      "steps = 1, loss = 2.286267042160034\n",
      "steps = 1, loss = 1.928123950958252\n",
      "steps = 1, loss = 1.1285806894302368\n",
      "steps = 1, loss = 1.7917665243148804\n",
      "steps = 1, loss = 1.9771867990493774\n",
      "steps = 1, loss = 2.61761736869812\n",
      "steps = 1, loss = 49.983497619628906\n",
      "steps = 2, loss = 2.283470630645752\n",
      "steps = 2, loss = 1.2986968755722046\n",
      "steps = 2, loss = 3.688019037246704\n",
      "steps = 2, loss = 2.2805848121643066\n",
      "steps = 2, loss = 1.4390194416046143\n",
      "steps = 2, loss = 1.9770604372024536\n",
      "steps = 2, loss = 2.324592351913452\n",
      "steps = 2, loss = 2.7795321941375732\n",
      "steps = 2, loss = 2.771923303604126\n",
      "steps = 2, loss = 2.9924490451812744\n",
      "steps = 2, loss = 2.0872504711151123\n",
      "steps = 2, loss = 1.4930354356765747\n",
      "steps = 2, loss = 50.0167350769043\n",
      "steps = 2, loss = 2.463853120803833\n",
      "steps = 2, loss = 2.665360689163208\n",
      "steps = 2, loss = 2.597959041595459\n",
      "steps = 2, loss = 49.92780303955078\n",
      "steps = 2, loss = 2.1742429733276367\n",
      "steps = 2, loss = 2.7490172386169434\n",
      "steps = 2, loss = 2.4847848415374756\n",
      "steps = 2, loss = 2.234210729598999\n",
      "steps = 2, loss = 2.164482593536377\n",
      "steps = 2, loss = 0.8171289563179016\n",
      "steps = 2, loss = 2.1029469966888428\n",
      "steps = 2, loss = 1.6517900228500366\n",
      "steps = 2, loss = 49.98497772216797\n",
      "steps = 2, loss = 1.953392505645752\n",
      "steps = 2, loss = 3.001072645187378\n",
      "steps = 2, loss = 2.110507011413574\n",
      "steps = 3, loss = 2.498430013656616\n",
      "steps = 3, loss = 2.272951126098633\n",
      "steps = 3, loss = 2.855734348297119\n",
      "steps = 3, loss = 49.97137451171875\n",
      "steps = 3, loss = 3.0477211475372314\n",
      "steps = 3, loss = 2.644404172897339\n",
      "steps = 3, loss = 1.9225727319717407\n",
      "steps = 3, loss = 2.444079637527466\n",
      "steps = 3, loss = 2.939403772354126\n",
      "steps = 3, loss = 2.6448261737823486\n",
      "steps = 3, loss = 4.150546073913574\n",
      "steps = 3, loss = 2.4208104610443115\n",
      "steps = 3, loss = 2.687669515609741\n",
      "steps = 3, loss = 1.7920019626617432\n",
      "steps = 3, loss = 2.42437744140625\n",
      "steps = 3, loss = 2.0425071716308594\n",
      "steps = 3, loss = 2.6016268730163574\n",
      "steps = 3, loss = 2.5177087783813477\n",
      "steps = 3, loss = 50.02452087402344\n",
      "steps = 3, loss = 1.5882035493850708\n",
      "steps = 3, loss = 2.229506015777588\n",
      "steps = 3, loss = 2.0686490535736084\n",
      "steps = 3, loss = 2.63291597366333\n",
      "steps = 3, loss = 2.0986995697021484\n",
      "steps = 3, loss = 1.7211902141571045\n",
      "steps = 3, loss = 49.9853630065918\n",
      "steps = 3, loss = 1.1351513862609863\n",
      "steps = 3, loss = 2.1509923934936523\n",
      "steps = 3, loss = 1.6638615131378174\n",
      "steps = 4, loss = 2.176088571548462\n",
      "steps = 4, loss = 1.7656171321868896\n",
      "steps = 4, loss = 1.847576379776001\n",
      "steps = 4, loss = 1.8835585117340088\n",
      "steps = 4, loss = 2.6987571716308594\n",
      "steps = 4, loss = 1.7220323085784912\n",
      "steps = 4, loss = 3.158294916152954\n",
      "steps = 4, loss = 2.2945172786712646\n",
      "steps = 4, loss = 49.9853630065918\n",
      "steps = 4, loss = 1.4219131469726562\n",
      "steps = 4, loss = 2.627537965774536\n",
      "steps = 4, loss = 2.342996835708618\n",
      "steps = 4, loss = 2.8625943660736084\n",
      "steps = 4, loss = 2.033984899520874\n",
      "steps = 4, loss = 2.9437384605407715\n",
      "steps = 4, loss = 2.71840500831604\n",
      "steps = 4, loss = 2.3450934886932373\n",
      "steps = 4, loss = 2.905958414077759\n",
      "steps = 4, loss = 50.018341064453125\n",
      "steps = 4, loss = 49.93511199951172\n",
      "steps = 4, loss = 2.7668113708496094\n",
      "steps = 4, loss = 2.5238282680511475\n",
      "steps = 4, loss = 2.604020118713379\n",
      "steps = 4, loss = 2.518935203552246\n",
      "steps = 4, loss = 2.4513299465179443\n",
      "steps = 4, loss = 3.2771856784820557\n",
      "steps = 4, loss = 2.7267062664031982\n",
      "steps = 4, loss = 2.5456979274749756\n",
      "steps = 4, loss = 2.1125948429107666\n",
      "steps = 5, loss = 2.591975688934326\n",
      "steps = 5, loss = 2.9962072372436523\n",
      "steps = 5, loss = 2.7171456813812256\n",
      "steps = 5, loss = 2.9979095458984375\n",
      "steps = 5, loss = 2.2575876712799072\n",
      "steps = 5, loss = 1.5709632635116577\n",
      "steps = 5, loss = 2.7865188121795654\n",
      "steps = 5, loss = 1.7926945686340332\n",
      "steps = 5, loss = 2.624917984008789\n",
      "steps = 5, loss = 2.7797257900238037\n",
      "steps = 5, loss = 2.08762788772583\n",
      "steps = 5, loss = 3.184272050857544\n",
      "steps = 5, loss = 2.538855791091919\n",
      "steps = 5, loss = 1.9321497678756714\n",
      "steps = 5, loss = 2.9009666442871094\n",
      "steps = 5, loss = 49.9853630065918\n",
      "steps = 5, loss = 2.466801881790161\n",
      "steps = 5, loss = 2.5644659996032715\n",
      "steps = 5, loss = 2.5636532306671143\n",
      "steps = 5, loss = 2.9108669757843018\n",
      "steps = 5, loss = 49.99802017211914\n",
      "steps = 5, loss = 1.9802407026290894\n",
      "steps = 5, loss = 49.90399169921875\n",
      "steps = 5, loss = 2.0888888835906982\n",
      "steps = 5, loss = 1.9140475988388062\n",
      "steps = 5, loss = 2.8158926963806152\n",
      "steps = 5, loss = 1.8236297369003296\n",
      "steps = 5, loss = 3.255171775817871\n",
      "steps = 5, loss = 2.845951557159424\n",
      "steps = 6, loss = 2.843693733215332\n",
      "steps = 6, loss = 3.0205156803131104\n",
      "steps = 6, loss = 3.113156795501709\n",
      "steps = 6, loss = 2.7823352813720703\n",
      "steps = 6, loss = 2.080512285232544\n",
      "steps = 6, loss = 2.8774542808532715\n",
      "steps = 6, loss = 49.9853630065918\n",
      "steps = 6, loss = 1.6527352333068848\n",
      "steps = 6, loss = 2.817758321762085\n",
      "steps = 6, loss = 2.7850255966186523\n",
      "steps = 6, loss = 2.739043951034546\n",
      "steps = 6, loss = 2.604259490966797\n",
      "steps = 6, loss = 50.0073356628418\n",
      "steps = 6, loss = 2.830275774002075\n",
      "steps = 6, loss = 1.8583101034164429\n",
      "steps = 6, loss = 2.1268110275268555\n",
      "steps = 6, loss = 1.9572749137878418\n",
      "steps = 6, loss = 2.5525870323181152\n",
      "steps = 6, loss = 2.072033643722534\n",
      "steps = 6, loss = 2.3177404403686523\n",
      "steps = 6, loss = 2.434527635574341\n",
      "steps = 6, loss = 2.763185739517212\n",
      "steps = 6, loss = 50.02309036254883\n",
      "steps = 6, loss = 1.8316380977630615\n",
      "steps = 6, loss = 2.613645553588867\n",
      "steps = 6, loss = 4.232259273529053\n",
      "steps = 6, loss = 1.9454008340835571\n",
      "steps = 6, loss = 2.693899154663086\n",
      "steps = 6, loss = 2.575777053833008\n",
      "steps = 7, loss = 2.766954183578491\n",
      "steps = 7, loss = 2.9180660247802734\n",
      "steps = 7, loss = 1.982602834701538\n",
      "steps = 7, loss = 2.881363868713379\n",
      "steps = 7, loss = 2.437645435333252\n",
      "steps = 7, loss = 2.5377602577209473\n",
      "steps = 7, loss = 2.3888442516326904\n",
      "steps = 7, loss = 2.134072780609131\n",
      "steps = 7, loss = 3.140765428543091\n",
      "steps = 7, loss = 1.713534951210022\n",
      "steps = 7, loss = 2.6024136543273926\n",
      "steps = 7, loss = 50.01506042480469\n",
      "steps = 7, loss = 1.9929887056350708\n",
      "steps = 7, loss = 3.0322647094726562\n",
      "steps = 7, loss = 2.979257583618164\n",
      "steps = 7, loss = 2.603130340576172\n",
      "steps = 7, loss = 2.6066625118255615\n",
      "steps = 7, loss = 2.0705363750457764\n",
      "steps = 7, loss = 1.8936560153961182\n",
      "steps = 7, loss = 3.334826707839966\n",
      "steps = 7, loss = 2.8801498413085938\n",
      "steps = 7, loss = 2.3018651008605957\n",
      "steps = 7, loss = 2.6584277153015137\n",
      "steps = 7, loss = 2.9496219158172607\n",
      "steps = 7, loss = 49.9853630065918\n",
      "steps = 7, loss = 1.8714599609375\n",
      "steps = 7, loss = 3.0587573051452637\n",
      "steps = 7, loss = 2.106884002685547\n",
      "steps = 7, loss = 50.0030403137207\n",
      "steps = 8, loss = 2.5007314682006836\n",
      "steps = 8, loss = 2.806699752807617\n",
      "steps = 8, loss = 2.4854001998901367\n",
      "steps = 8, loss = 49.960594177246094\n",
      "steps = 8, loss = 2.292832612991333\n",
      "steps = 8, loss = 3.2753729820251465\n",
      "steps = 8, loss = 3.20439076423645\n",
      "steps = 8, loss = 2.4783575534820557\n",
      "steps = 8, loss = 2.7478487491607666\n",
      "steps = 8, loss = 1.776298999786377\n",
      "steps = 8, loss = 3.5627968311309814\n",
      "steps = 8, loss = 3.0353875160217285\n",
      "steps = 8, loss = 49.95314407348633\n",
      "steps = 8, loss = 2.0399186611175537\n",
      "steps = 8, loss = 2.9788830280303955\n",
      "steps = 8, loss = 2.204698085784912\n",
      "steps = 8, loss = 2.927579164505005\n",
      "steps = 8, loss = 3.0640900135040283\n",
      "steps = 8, loss = 3.0189855098724365\n",
      "steps = 8, loss = 1.9266670942306519\n",
      "steps = 8, loss = 2.7846274375915527\n",
      "steps = 8, loss = 2.9159653186798096\n",
      "steps = 8, loss = 1.9413307905197144\n",
      "steps = 8, loss = 2.1412928104400635\n",
      "steps = 8, loss = 2.052168369293213\n",
      "steps = 8, loss = 2.6599910259246826\n",
      "steps = 8, loss = 3.052593469619751\n",
      "steps = 8, loss = 2.588672637939453\n",
      "steps = 8, loss = 49.9853630065918\n",
      "steps = 9, loss = 2.204284191131592\n",
      "steps = 9, loss = 2.377063751220703\n",
      "steps = 9, loss = 2.6483991146087646\n",
      "steps = 9, loss = 1.9667856693267822\n",
      "steps = 9, loss = 2.9818665981292725\n",
      "steps = 9, loss = 2.7982590198516846\n",
      "steps = 9, loss = 2.1702675819396973\n",
      "steps = 9, loss = 1.9536094665527344\n",
      "steps = 9, loss = 49.9853630065918\n",
      "steps = 9, loss = 2.0669426918029785\n",
      "steps = 9, loss = 3.011249542236328\n",
      "steps = 9, loss = 1.8122304677963257\n",
      "steps = 9, loss = 2.7557694911956787\n",
      "steps = 9, loss = 2.0640196800231934\n",
      "steps = 9, loss = 2.661135673522949\n",
      "steps = 9, loss = 2.94327974319458\n",
      "steps = 9, loss = 50.00458526611328\n",
      "steps = 9, loss = 3.4573073387145996\n",
      "steps = 9, loss = 2.557910203933716\n",
      "steps = 9, loss = 3.1820521354675293\n",
      "steps = 9, loss = 2.658606767654419\n",
      "steps = 9, loss = 3.2971720695495605\n",
      "steps = 9, loss = 2.5273728370666504\n",
      "steps = 9, loss = 2.787742853164673\n",
      "steps = 9, loss = 2.0807101726531982\n",
      "steps = 9, loss = 3.0637612342834473\n",
      "steps = 9, loss = 2.8993730545043945\n",
      "steps = 9, loss = 2.9104559421539307\n",
      "steps = 9, loss = 49.807987213134766\n",
      "steps = 10, loss = 2.1780946254730225\n",
      "steps = 10, loss = 49.9853630065918\n",
      "steps = 10, loss = 1.9862208366394043\n",
      "steps = 10, loss = 2.5848631858825684\n",
      "steps = 10, loss = 3.49436092376709\n",
      "steps = 10, loss = 2.999671697616577\n",
      "steps = 10, loss = 3.074965000152588\n",
      "steps = 10, loss = 2.997954845428467\n",
      "steps = 10, loss = 3.390896797180176\n",
      "steps = 10, loss = 2.8113229274749756\n",
      "steps = 10, loss = 1.9985440969467163\n",
      "steps = 10, loss = 2.680097818374634\n",
      "steps = 10, loss = 2.5925276279449463\n",
      "steps = 10, loss = 2.8026058673858643\n",
      "steps = 10, loss = 2.9399893283843994\n",
      "steps = 10, loss = 2.911876916885376\n",
      "steps = 10, loss = 36.0801887512207\n",
      "steps = 10, loss = 2.8856160640716553\n",
      "steps = 10, loss = 2.0952250957489014\n",
      "steps = 10, loss = 1.8494077920913696\n",
      "steps = 10, loss = 3.198085069656372\n",
      "steps = 10, loss = 3.0966250896453857\n",
      "steps = 10, loss = 2.917884349822998\n",
      "steps = 10, loss = 2.1612792015075684\n",
      "steps = 10, loss = 49.661773681640625\n",
      "steps = 10, loss = 2.659083843231201\n",
      "steps = 10, loss = 2.296572208404541\n",
      "steps = 10, loss = 2.842698097229004\n",
      "steps = 10, loss = 2.1202855110168457\n",
      "steps = 11, loss = 2.0052847862243652\n",
      "steps = 11, loss = 2.1490187644958496\n",
      "steps = 11, loss = 2.346592664718628\n",
      "steps = 11, loss = 50.0150260925293\n",
      "steps = 11, loss = 3.3929293155670166\n",
      "steps = 11, loss = 2.7793667316436768\n",
      "steps = 11, loss = 3.6797287464141846\n",
      "steps = 11, loss = 2.0122408866882324\n",
      "steps = 11, loss = 3.5735881328582764\n",
      "steps = 11, loss = 2.6631577014923096\n",
      "steps = 11, loss = 2.513608694076538\n",
      "steps = 11, loss = 2.020322561264038\n",
      "steps = 11, loss = 3.0461089611053467\n",
      "steps = 11, loss = 2.7978644371032715\n",
      "steps = 11, loss = 2.621613025665283\n",
      "steps = 11, loss = 3.1397578716278076\n",
      "steps = 11, loss = 3.2546637058258057\n",
      "steps = 11, loss = 3.0887210369110107\n",
      "steps = 11, loss = 2.684490203857422\n",
      "steps = 11, loss = 2.894408941268921\n",
      "steps = 11, loss = 2.2161006927490234\n",
      "steps = 11, loss = 2.9578893184661865\n",
      "steps = 11, loss = 3.0977671146392822\n",
      "steps = 11, loss = 1.872293472290039\n",
      "steps = 11, loss = 2.667140245437622\n",
      "steps = 11, loss = 2.1136417388916016\n",
      "steps = 11, loss = 49.9853630065918\n",
      "steps = 11, loss = 20.934431076049805\n",
      "steps = 11, loss = 2.709932804107666\n",
      "steps = 12, loss = 2.548823118209839\n",
      "steps = 12, loss = 2.0584654808044434\n",
      "steps = 12, loss = 2.7356576919555664\n",
      "steps = 12, loss = 3.1637802124023438\n",
      "steps = 12, loss = 2.6446692943573\n",
      "steps = 12, loss = 2.7755355834960938\n",
      "steps = 12, loss = 1.8840566873550415\n",
      "steps = 12, loss = 3.3767926692962646\n",
      "steps = 12, loss = 2.7088255882263184\n",
      "steps = 12, loss = 2.625074625015259\n",
      "steps = 12, loss = 2.115837574005127\n",
      "steps = 12, loss = 2.0110437870025635\n",
      "steps = 12, loss = 2.1653144359588623\n",
      "steps = 12, loss = 2.7520570755004883\n",
      "steps = 12, loss = 2.980480194091797\n",
      "steps = 12, loss = 3.0847010612487793\n",
      "steps = 12, loss = 2.034013509750366\n",
      "steps = 12, loss = 9.64026165008545\n",
      "steps = 12, loss = 2.8663125038146973\n",
      "steps = 12, loss = 2.663911819458008\n",
      "steps = 12, loss = 3.0310800075531006\n",
      "steps = 12, loss = 50.01127624511719\n",
      "steps = 12, loss = 3.4007151126861572\n",
      "steps = 12, loss = 2.760413885116577\n",
      "steps = 12, loss = 2.624009847640991\n",
      "steps = 12, loss = 3.0541162490844727\n",
      "steps = 12, loss = 2.1537790298461914\n",
      "steps = 12, loss = 2.132988452911377\n",
      "steps = 12, loss = 49.9853630065918\n",
      "steps = 13, loss = 49.92717742919922\n",
      "steps = 13, loss = 2.4862701892852783\n",
      "steps = 13, loss = 3.510712146759033\n",
      "steps = 13, loss = 2.565135955810547\n",
      "steps = 13, loss = 2.156400203704834\n",
      "steps = 13, loss = 3.210444450378418\n",
      "steps = 13, loss = 3.3814759254455566\n",
      "steps = 13, loss = 2.6668701171875\n",
      "steps = 13, loss = 2.605642080307007\n",
      "steps = 13, loss = 2.672523021697998\n",
      "steps = 13, loss = 2.137491226196289\n",
      "steps = 13, loss = 1.9016013145446777\n",
      "steps = 13, loss = 2.1993868350982666\n",
      "steps = 13, loss = 3.1398844718933105\n",
      "steps = 13, loss = 3.092039108276367\n",
      "steps = 13, loss = 6.199883460998535\n",
      "steps = 13, loss = 2.028501033782959\n",
      "steps = 13, loss = 3.0680625438690186\n",
      "steps = 13, loss = 2.7861485481262207\n",
      "steps = 13, loss = 2.05822491645813\n",
      "steps = 13, loss = 2.6811954975128174\n",
      "steps = 13, loss = 49.9853630065918\n",
      "steps = 13, loss = 3.833543062210083\n",
      "steps = 13, loss = 2.079693555831909\n",
      "steps = 13, loss = 2.1322925090789795\n",
      "steps = 13, loss = 2.866452932357788\n",
      "steps = 13, loss = 2.7620978355407715\n",
      "steps = 13, loss = 2.9633679389953613\n",
      "steps = 13, loss = 2.5244157314300537\n",
      "steps = 14, loss = 2.7832694053649902\n",
      "steps = 14, loss = 2.809636354446411\n",
      "steps = 14, loss = 3.396346092224121\n",
      "steps = 14, loss = 49.940189361572266\n",
      "steps = 14, loss = 3.832576036453247\n",
      "steps = 14, loss = 1.934689998626709\n",
      "steps = 14, loss = 2.739224910736084\n",
      "steps = 14, loss = 2.664121627807617\n",
      "steps = 14, loss = 3.1840808391571045\n",
      "steps = 14, loss = 2.9772205352783203\n",
      "steps = 14, loss = 2.907240152359009\n",
      "steps = 14, loss = 2.7091281414031982\n",
      "steps = 14, loss = 2.0995211601257324\n",
      "steps = 14, loss = 2.884490728378296\n",
      "steps = 14, loss = 2.067467212677002\n",
      "steps = 14, loss = 3.113041877746582\n",
      "steps = 14, loss = 49.9853630065918\n",
      "steps = 14, loss = 2.1836917400360107\n",
      "steps = 14, loss = 2.630082607269287\n",
      "steps = 14, loss = 49.99591827392578\n",
      "steps = 14, loss = 3.555476665496826\n",
      "steps = 14, loss = 3.1740214824676514\n",
      "steps = 14, loss = 5.482105731964111\n",
      "steps = 14, loss = 2.2567784786224365\n",
      "steps = 14, loss = 2.2872557640075684\n",
      "steps = 14, loss = 2.1887094974517822\n",
      "steps = 14, loss = 2.812370777130127\n",
      "steps = 14, loss = 2.952502727508545\n",
      "steps = 14, loss = 2.900676965713501\n",
      "steps = 15, loss = 3.6440391540527344\n",
      "steps = 15, loss = 2.4930920600891113\n",
      "steps = 15, loss = 3.1637792587280273\n",
      "steps = 15, loss = 3.3191323280334473\n",
      "steps = 15, loss = 2.7623391151428223\n",
      "steps = 15, loss = 2.084801197052002\n",
      "steps = 15, loss = 3.315541982650757\n",
      "steps = 15, loss = 2.124105215072632\n",
      "steps = 15, loss = 2.2068512439727783\n",
      "steps = 15, loss = 2.2429561614990234\n",
      "steps = 15, loss = 49.9853630065918\n",
      "steps = 15, loss = 3.5268428325653076\n",
      "steps = 15, loss = 1.9496510028839111\n",
      "steps = 15, loss = 3.0065600872039795\n",
      "steps = 15, loss = 2.8832855224609375\n",
      "steps = 15, loss = 2.290470600128174\n",
      "steps = 15, loss = 2.7609829902648926\n",
      "steps = 15, loss = 2.058344602584839\n",
      "steps = 15, loss = 3.0274147987365723\n",
      "steps = 15, loss = 2.994286060333252\n",
      "steps = 15, loss = 2.7441937923431396\n",
      "steps = 15, loss = 3.194983959197998\n",
      "steps = 15, loss = 2.2166073322296143\n",
      "steps = 15, loss = 2.6687910556793213\n",
      "steps = 15, loss = 2.6876022815704346\n",
      "steps = 15, loss = 4.289825916290283\n",
      "steps = 15, loss = 48.762962341308594\n",
      "steps = 15, loss = 3.345649480819702\n",
      "steps = 15, loss = 2.755363941192627\n",
      "steps = 16, loss = 2.256756544113159\n",
      "steps = 16, loss = 2.798131227493286\n",
      "steps = 16, loss = 2.1706960201263428\n",
      "steps = 16, loss = 8.052592277526855\n",
      "steps = 16, loss = 4.845584869384766\n",
      "steps = 16, loss = 2.7163496017456055\n",
      "steps = 16, loss = 2.8521029949188232\n",
      "steps = 16, loss = 2.9874114990234375\n",
      "steps = 16, loss = 50.0142822265625\n",
      "steps = 16, loss = 49.9853630065918\n",
      "steps = 16, loss = 3.473845958709717\n",
      "steps = 16, loss = 2.751112937927246\n",
      "steps = 16, loss = 3.5429744720458984\n",
      "steps = 16, loss = 2.896865129470825\n",
      "steps = 16, loss = 2.109086751937866\n",
      "steps = 16, loss = 2.329084634780884\n",
      "steps = 16, loss = 3.1314148902893066\n",
      "steps = 16, loss = 3.57318115234375\n",
      "steps = 16, loss = 1.970076084136963\n",
      "steps = 16, loss = 2.9740233421325684\n",
      "steps = 16, loss = 3.1630935668945312\n",
      "steps = 16, loss = 2.5829885005950928\n",
      "steps = 16, loss = 3.2229270935058594\n",
      "steps = 16, loss = 2.713580846786499\n",
      "steps = 16, loss = 2.2816097736358643\n",
      "steps = 16, loss = 2.154445171356201\n",
      "steps = 16, loss = 2.0673916339874268\n",
      "steps = 16, loss = 3.282722234725952\n",
      "steps = 16, loss = 2.776073694229126\n",
      "steps = 17, loss = 3.079655885696411\n",
      "steps = 17, loss = 2.272899627685547\n",
      "steps = 17, loss = 2.799485683441162\n",
      "steps = 17, loss = 2.726808786392212\n",
      "steps = 17, loss = 2.5822248458862305\n",
      "steps = 17, loss = 3.2572884559631348\n",
      "steps = 17, loss = 2.7949023246765137\n",
      "steps = 17, loss = 2.1086487770080566\n",
      "steps = 17, loss = 2.1662697792053223\n",
      "steps = 17, loss = 1.9405739307403564\n",
      "steps = 17, loss = 1.9694023132324219\n",
      "steps = 17, loss = 2.982370615005493\n",
      "steps = 17, loss = 3.654588222503662\n",
      "steps = 17, loss = 2.3423280715942383\n",
      "steps = 17, loss = 2.9701333045959473\n",
      "steps = 17, loss = 2.747371196746826\n",
      "steps = 17, loss = 3.270237684249878\n",
      "steps = 17, loss = 3.5361382961273193\n",
      "steps = 17, loss = 3.3348164558410645\n",
      "steps = 17, loss = 2.135662317276001\n",
      "steps = 17, loss = 4.713506698608398\n",
      "steps = 17, loss = 50.00657272338867\n",
      "steps = 17, loss = 1.967599630355835\n",
      "steps = 17, loss = 49.9853630065918\n",
      "steps = 17, loss = 2.4318647384643555\n",
      "steps = 17, loss = 2.8527817726135254\n",
      "steps = 17, loss = 2.8581204414367676\n",
      "steps = 17, loss = 2.687671184539795\n",
      "steps = 17, loss = 2.8806228637695312\n",
      "steps = 18, loss = 2.202176570892334\n",
      "steps = 18, loss = 2.8639347553253174\n",
      "steps = 18, loss = 2.8354122638702393\n",
      "steps = 18, loss = 5.132136821746826\n",
      "steps = 18, loss = 3.2351057529449463\n",
      "steps = 18, loss = 2.2635762691497803\n",
      "steps = 18, loss = 2.7552454471588135\n",
      "steps = 18, loss = 3.435883045196533\n",
      "steps = 18, loss = 2.7544639110565186\n",
      "steps = 18, loss = 2.0763025283813477\n",
      "steps = 18, loss = 2.3353002071380615\n",
      "steps = 18, loss = 3.552027463912964\n",
      "steps = 18, loss = 2.3906242847442627\n",
      "steps = 18, loss = 2.780226469039917\n",
      "steps = 18, loss = 2.666543483734131\n",
      "steps = 18, loss = 2.8898332118988037\n",
      "steps = 18, loss = 49.9853630065918\n",
      "steps = 18, loss = 3.496500253677368\n",
      "steps = 18, loss = 3.531524181365967\n",
      "steps = 18, loss = 1.9899858236312866\n",
      "steps = 18, loss = 3.5810389518737793\n",
      "steps = 18, loss = 2.877086877822876\n",
      "steps = 18, loss = 2.681769609451294\n",
      "steps = 18, loss = 3.210097551345825\n",
      "steps = 18, loss = 2.754988193511963\n",
      "steps = 18, loss = 50.010807037353516\n",
      "steps = 18, loss = 2.6931231021881104\n",
      "steps = 18, loss = 2.0889313220977783\n",
      "steps = 18, loss = 2.140493154525757\n",
      "steps = 19, loss = 2.744797945022583\n",
      "steps = 19, loss = 3.287344455718994\n",
      "steps = 19, loss = 2.4264893531799316\n",
      "steps = 19, loss = 2.11490797996521\n",
      "steps = 19, loss = 2.755882740020752\n",
      "steps = 19, loss = 2.969963550567627\n",
      "steps = 19, loss = 3.528085231781006\n",
      "steps = 19, loss = 2.7228760719299316\n",
      "steps = 19, loss = 3.491140604019165\n",
      "steps = 19, loss = 2.790149211883545\n",
      "steps = 19, loss = 50.01514434814453\n",
      "steps = 19, loss = 3.693793296813965\n",
      "steps = 19, loss = 2.8919382095336914\n",
      "steps = 19, loss = 3.187192678451538\n",
      "steps = 19, loss = 2.867262840270996\n",
      "steps = 19, loss = 2.0081863403320312\n",
      "steps = 19, loss = 2.695984125137329\n",
      "steps = 19, loss = 2.272940158843994\n",
      "steps = 19, loss = 3.5058131217956543\n",
      "steps = 19, loss = 5.361784934997559\n",
      "steps = 19, loss = 2.165588617324829\n",
      "steps = 19, loss = 2.9906346797943115\n",
      "steps = 19, loss = 2.232107639312744\n",
      "steps = 19, loss = 2.2390220165252686\n",
      "steps = 19, loss = 49.9853630065918\n",
      "steps = 19, loss = 2.864725112915039\n",
      "steps = 19, loss = 3.019928216934204\n",
      "steps = 19, loss = 3.0589895248413086\n",
      "steps = 19, loss = 2.3858344554901123\n",
      "steps = 20, loss = 3.2922253608703613\n",
      "steps = 20, loss = 3.5508475303649902\n",
      "steps = 20, loss = 2.2435944080352783\n",
      "steps = 20, loss = 2.4371373653411865\n",
      "steps = 20, loss = 50.00845718383789\n",
      "steps = 20, loss = 3.4985830783843994\n",
      "steps = 20, loss = 3.2875242233276367\n",
      "steps = 20, loss = 3.0645384788513184\n",
      "steps = 20, loss = 2.839770793914795\n",
      "steps = 20, loss = 2.8606154918670654\n",
      "steps = 20, loss = 2.71914005279541\n",
      "steps = 20, loss = 2.0051300525665283\n",
      "steps = 20, loss = 3.095344305038452\n",
      "steps = 20, loss = 2.689887285232544\n",
      "steps = 20, loss = 2.136664628982544\n",
      "steps = 20, loss = 2.166429042816162\n",
      "steps = 20, loss = 2.7280702590942383\n",
      "steps = 20, loss = 49.9853630065918\n",
      "steps = 20, loss = 2.7524170875549316\n",
      "steps = 20, loss = 3.302528142929077\n",
      "steps = 20, loss = 1.99472177028656\n",
      "steps = 20, loss = 5.052082061767578\n",
      "steps = 20, loss = 2.7162654399871826\n",
      "steps = 20, loss = 2.8542966842651367\n",
      "steps = 20, loss = 2.979457378387451\n",
      "steps = 20, loss = 2.409364700317383\n",
      "steps = 20, loss = 2.6024017333984375\n",
      "steps = 20, loss = 1.9452698230743408\n",
      "steps = 20, loss = 2.3516833782196045\n",
      "steps = 21, loss = 3.0636510848999023\n",
      "steps = 21, loss = 3.0017740726470947\n",
      "steps = 21, loss = 2.6942546367645264\n",
      "steps = 21, loss = 2.2713122367858887\n",
      "steps = 21, loss = 1.9349939823150635\n",
      "steps = 21, loss = 3.3052384853363037\n",
      "steps = 21, loss = 2.491947889328003\n",
      "steps = 21, loss = 2.665658950805664\n",
      "steps = 21, loss = 2.7430732250213623\n",
      "steps = 21, loss = 2.450277805328369\n",
      "steps = 21, loss = 3.467967987060547\n",
      "steps = 21, loss = 2.873286724090576\n",
      "steps = 21, loss = 50.00518798828125\n",
      "steps = 21, loss = 2.2785260677337646\n",
      "steps = 21, loss = 49.9853630065918\n",
      "steps = 21, loss = 2.474058151245117\n",
      "steps = 21, loss = 2.8903725147247314\n",
      "steps = 21, loss = 2.199394941329956\n",
      "steps = 21, loss = 3.4746477603912354\n",
      "steps = 21, loss = 3.524322986602783\n",
      "steps = 21, loss = 2.4815165996551514\n",
      "steps = 21, loss = 5.1713762283325195\n",
      "steps = 21, loss = 2.745520830154419\n",
      "steps = 21, loss = 2.991788148880005\n",
      "steps = 21, loss = 3.0891315937042236\n",
      "steps = 21, loss = 2.0791218280792236\n",
      "steps = 21, loss = 3.4792306423187256\n",
      "steps = 21, loss = 2.568133592605591\n",
      "steps = 21, loss = 2.0238468647003174\n",
      "steps = 22, loss = 30.73686408996582\n",
      "steps = 22, loss = 3.0219621658325195\n",
      "steps = 22, loss = 2.6354386806488037\n",
      "steps = 22, loss = 2.421400785446167\n",
      "steps = 22, loss = 2.905761957168579\n",
      "steps = 22, loss = 2.8730556964874268\n",
      "steps = 22, loss = 2.6946892738342285\n",
      "steps = 22, loss = 2.586369752883911\n",
      "steps = 22, loss = 2.6842567920684814\n",
      "steps = 22, loss = 2.721607208251953\n",
      "steps = 22, loss = 49.9853630065918\n",
      "steps = 22, loss = 3.139204502105713\n",
      "steps = 22, loss = 2.2591142654418945\n",
      "steps = 22, loss = 3.6428232192993164\n",
      "steps = 22, loss = 2.1312525272369385\n",
      "steps = 22, loss = 2.2213692665100098\n",
      "steps = 22, loss = 3.314098834991455\n",
      "steps = 22, loss = 2.304117202758789\n",
      "steps = 22, loss = 2.0353941917419434\n",
      "steps = 22, loss = 2.5224344730377197\n",
      "steps = 22, loss = 3.3329153060913086\n",
      "steps = 22, loss = 3.0139987468719482\n",
      "steps = 22, loss = 5.6166582107543945\n",
      "steps = 22, loss = 2.04528546333313\n",
      "steps = 22, loss = 2.510462760925293\n",
      "steps = 22, loss = 3.2603390216827393\n",
      "steps = 22, loss = 3.5020480155944824\n",
      "steps = 22, loss = 2.9204745292663574\n",
      "steps = 22, loss = 2.593338966369629\n",
      "steps = 23, loss = 2.562880516052246\n",
      "steps = 23, loss = 12.198476791381836\n",
      "steps = 23, loss = 3.520151376724243\n",
      "steps = 23, loss = 2.5727427005767822\n",
      "steps = 23, loss = 2.6926262378692627\n",
      "steps = 23, loss = 3.084901809692383\n",
      "steps = 23, loss = 3.1582400798797607\n",
      "steps = 23, loss = 2.6400270462036133\n",
      "steps = 23, loss = 2.2668023109436035\n",
      "steps = 23, loss = 2.928860902786255\n",
      "steps = 23, loss = 3.3432328701019287\n",
      "steps = 23, loss = 5.6998114585876465\n",
      "steps = 23, loss = 2.2455132007598877\n",
      "steps = 23, loss = 3.4217076301574707\n",
      "steps = 23, loss = 2.0742688179016113\n",
      "steps = 23, loss = 2.779165029525757\n",
      "steps = 23, loss = 2.0500760078430176\n",
      "steps = 23, loss = 3.5308146476745605\n",
      "steps = 23, loss = 49.9853630065918\n",
      "steps = 23, loss = 2.3314638137817383\n",
      "steps = 23, loss = 2.7967350482940674\n",
      "steps = 23, loss = 2.724459409713745\n",
      "steps = 23, loss = 3.0632708072662354\n",
      "steps = 23, loss = 2.54111385345459\n",
      "steps = 23, loss = 2.887606620788574\n",
      "steps = 23, loss = 2.9551608562469482\n",
      "steps = 23, loss = 3.2993037700653076\n",
      "steps = 23, loss = 2.024075746536255\n",
      "steps = 23, loss = 2.928408622741699\n",
      "steps = 24, loss = 6.268621921539307\n",
      "steps = 24, loss = 2.5235865116119385\n",
      "steps = 24, loss = 2.3413994312286377\n",
      "steps = 24, loss = 49.9853630065918\n",
      "steps = 24, loss = 2.850346565246582\n",
      "steps = 24, loss = 2.6887192726135254\n",
      "steps = 24, loss = 3.0938799381256104\n",
      "steps = 24, loss = 2.0447750091552734\n",
      "steps = 24, loss = 3.135399580001831\n",
      "steps = 24, loss = 2.0445990562438965\n",
      "steps = 24, loss = 3.3621068000793457\n",
      "steps = 24, loss = 3.626572608947754\n",
      "steps = 24, loss = 2.734747886657715\n",
      "steps = 24, loss = 3.2384073734283447\n",
      "steps = 24, loss = 6.175465106964111\n",
      "steps = 24, loss = 2.608046293258667\n",
      "steps = 24, loss = 3.4633090496063232\n",
      "steps = 24, loss = 2.951234817504883\n",
      "steps = 24, loss = 2.1865086555480957\n",
      "steps = 24, loss = 2.1372506618499756\n",
      "steps = 24, loss = 2.597151279449463\n",
      "steps = 24, loss = 2.6740827560424805\n",
      "steps = 24, loss = 2.547854423522949\n",
      "steps = 24, loss = 3.3717007637023926\n",
      "steps = 24, loss = 2.919173002243042\n",
      "steps = 24, loss = 2.2477662563323975\n",
      "steps = 24, loss = 2.3671376705169678\n",
      "steps = 24, loss = 2.7171010971069336\n",
      "steps = 24, loss = 2.5723378658294678\n",
      "steps = 25, loss = 2.8508458137512207\n",
      "steps = 25, loss = 2.0505003929138184\n",
      "steps = 25, loss = 2.571761131286621\n",
      "steps = 25, loss = 3.0440521240234375\n",
      "steps = 25, loss = 2.8532052040100098\n",
      "steps = 25, loss = 2.3589704036712646\n",
      "steps = 25, loss = 3.450328826904297\n",
      "steps = 25, loss = 2.930246591567993\n",
      "steps = 25, loss = 2.362982749938965\n",
      "steps = 25, loss = 3.364469528198242\n",
      "steps = 25, loss = 6.198672294616699\n",
      "steps = 25, loss = 2.973558187484741\n",
      "steps = 25, loss = 2.25463604927063\n",
      "steps = 25, loss = 2.47285795211792\n",
      "steps = 25, loss = 3.314027786254883\n",
      "steps = 25, loss = 2.6444711685180664\n",
      "steps = 25, loss = 1.8200819492340088\n",
      "steps = 25, loss = 2.7319369316101074\n",
      "steps = 25, loss = 2.716601610183716\n",
      "steps = 25, loss = 2.110900402069092\n",
      "steps = 25, loss = 3.222003698348999\n",
      "steps = 25, loss = 2.690582036972046\n",
      "steps = 25, loss = 3.2345550060272217\n",
      "steps = 25, loss = 2.9367470741271973\n",
      "steps = 25, loss = 4.57144021987915\n",
      "steps = 25, loss = 2.622652530670166\n",
      "steps = 25, loss = 2.8516149520874023\n",
      "steps = 25, loss = 49.9853630065918\n",
      "steps = 25, loss = 2.265486478805542\n",
      "steps = 26, loss = 2.809131145477295\n",
      "steps = 26, loss = 2.780412435531616\n",
      "steps = 26, loss = 2.6918413639068604\n",
      "steps = 26, loss = 2.887284278869629\n",
      "steps = 26, loss = 2.7200891971588135\n",
      "steps = 26, loss = 50.01760482788086\n",
      "steps = 26, loss = 2.302950382232666\n",
      "steps = 26, loss = 6.15773344039917\n",
      "steps = 26, loss = 2.962829828262329\n",
      "steps = 26, loss = 3.352513074874878\n",
      "steps = 26, loss = 2.8796629905700684\n",
      "steps = 26, loss = 3.352456569671631\n",
      "steps = 26, loss = 3.5260119438171387\n",
      "steps = 26, loss = 49.9853630065918\n",
      "steps = 26, loss = 3.375430107116699\n",
      "steps = 26, loss = 2.2598557472229004\n",
      "steps = 26, loss = 3.4951012134552\n",
      "steps = 26, loss = 2.1580393314361572\n",
      "steps = 26, loss = 2.615330934524536\n",
      "steps = 26, loss = 2.7251808643341064\n",
      "steps = 26, loss = 2.6285927295684814\n",
      "steps = 26, loss = 3.376467704772949\n",
      "steps = 26, loss = 3.8105356693267822\n",
      "steps = 26, loss = 2.6988325119018555\n",
      "steps = 26, loss = 3.213319778442383\n",
      "steps = 26, loss = 2.3994576930999756\n",
      "steps = 26, loss = 2.745974540710449\n",
      "steps = 26, loss = 2.073674440383911\n",
      "steps = 26, loss = 2.152914524078369\n",
      "steps = 27, loss = 2.8470230102539062\n",
      "steps = 27, loss = 2.4596004486083984\n",
      "steps = 27, loss = 3.2409844398498535\n",
      "steps = 27, loss = 3.8462438583374023\n",
      "steps = 27, loss = 2.739964723587036\n",
      "steps = 27, loss = 2.2692527770996094\n",
      "steps = 27, loss = 3.3690903186798096\n",
      "steps = 27, loss = 2.081484079360962\n",
      "steps = 27, loss = 2.323910713195801\n",
      "steps = 27, loss = 2.636021375656128\n",
      "steps = 27, loss = 3.5089499950408936\n",
      "steps = 27, loss = 50.01777267456055\n",
      "steps = 27, loss = 49.9853630065918\n",
      "steps = 27, loss = 2.131256580352783\n",
      "steps = 27, loss = 2.4206576347351074\n",
      "steps = 27, loss = 2.8789498805999756\n",
      "steps = 27, loss = 2.6537978649139404\n",
      "steps = 27, loss = 3.3967580795288086\n",
      "steps = 27, loss = 2.576448917388916\n",
      "steps = 27, loss = 2.751314401626587\n",
      "steps = 27, loss = 3.5430965423583984\n",
      "steps = 27, loss = 2.7717795372009277\n",
      "steps = 27, loss = 2.9647204875946045\n",
      "steps = 27, loss = 2.6942532062530518\n",
      "steps = 27, loss = 6.0276689529418945\n",
      "steps = 27, loss = 3.3793323040008545\n",
      "steps = 27, loss = 3.586841583251953\n",
      "steps = 27, loss = 2.245635747909546\n",
      "steps = 27, loss = 2.8697240352630615\n",
      "steps = 28, loss = 50.00189208984375\n",
      "steps = 28, loss = 2.658277988433838\n",
      "steps = 28, loss = 2.257542133331299\n",
      "steps = 28, loss = 2.7978265285491943\n",
      "steps = 28, loss = 2.1591928005218506\n",
      "steps = 28, loss = 2.7447946071624756\n",
      "steps = 28, loss = 2.345167875289917\n",
      "steps = 28, loss = 5.8826141357421875\n",
      "steps = 28, loss = 2.956193208694458\n",
      "steps = 28, loss = 3.3318593502044678\n",
      "steps = 28, loss = 2.6952433586120605\n",
      "steps = 28, loss = 2.11702823638916\n",
      "steps = 28, loss = 3.5094943046569824\n",
      "steps = 28, loss = 2.8164737224578857\n",
      "steps = 28, loss = 3.451106548309326\n",
      "steps = 28, loss = 2.828234910964966\n",
      "steps = 28, loss = 3.1077518463134766\n",
      "steps = 28, loss = 2.869534492492676\n",
      "steps = 28, loss = 3.9270877838134766\n",
      "steps = 28, loss = 3.3991189002990723\n",
      "steps = 28, loss = 2.7422990798950195\n",
      "steps = 28, loss = 2.442580461502075\n",
      "steps = 28, loss = 3.6108388900756836\n",
      "steps = 28, loss = 2.5304431915283203\n",
      "steps = 28, loss = 2.0904877185821533\n",
      "steps = 28, loss = 3.3982980251312256\n",
      "steps = 28, loss = 2.9728481769561768\n",
      "steps = 28, loss = 3.484339475631714\n",
      "steps = 28, loss = 49.9853630065918\n",
      "steps = 29, loss = 3.557711362838745\n",
      "steps = 29, loss = 2.466825246810913\n",
      "steps = 29, loss = 2.8521440029144287\n",
      "steps = 29, loss = 6.834939956665039\n",
      "steps = 29, loss = 2.7222676277160645\n",
      "steps = 29, loss = 50.02125930786133\n",
      "steps = 29, loss = 2.2568209171295166\n",
      "steps = 29, loss = 2.8169031143188477\n",
      "steps = 29, loss = 2.684234142303467\n",
      "steps = 29, loss = 2.8850622177124023\n",
      "steps = 29, loss = 2.9165985584259033\n",
      "steps = 29, loss = 2.892958879470825\n",
      "steps = 29, loss = 2.988936424255371\n",
      "steps = 29, loss = 2.4335312843322754\n",
      "steps = 29, loss = 3.4940905570983887\n",
      "steps = 29, loss = 2.6115059852600098\n",
      "steps = 29, loss = 2.8452963829040527\n",
      "steps = 29, loss = 3.375694513320923\n",
      "steps = 29, loss = 2.7630200386047363\n",
      "steps = 29, loss = 2.1031417846679688\n",
      "steps = 29, loss = 3.313710927963257\n",
      "steps = 29, loss = 2.889894485473633\n",
      "steps = 29, loss = 2.367736577987671\n",
      "steps = 29, loss = 3.263538122177124\n",
      "steps = 29, loss = 49.9853630065918\n",
      "steps = 29, loss = 3.4107718467712402\n",
      "steps = 29, loss = 3.7752842903137207\n",
      "steps = 29, loss = 1.8774572610855103\n",
      "steps = 29, loss = 3.1212985515594482\n",
      "steps = 30, loss = 2.136718988418579\n",
      "steps = 30, loss = 2.687750816345215\n",
      "steps = 30, loss = 2.9749672412872314\n",
      "steps = 30, loss = 3.3910090923309326\n",
      "steps = 30, loss = 2.370776891708374\n",
      "steps = 30, loss = 2.6864495277404785\n",
      "steps = 30, loss = 6.326103210449219\n",
      "steps = 30, loss = 2.1803982257843018\n",
      "steps = 30, loss = 43.30643844604492\n",
      "steps = 30, loss = 2.4741790294647217\n",
      "steps = 30, loss = 3.1294078826904297\n",
      "steps = 30, loss = 2.7433884143829346\n",
      "steps = 30, loss = 2.847834348678589\n",
      "steps = 30, loss = 3.3109259605407715\n",
      "steps = 30, loss = 2.921050548553467\n",
      "steps = 30, loss = 2.6872005462646484\n",
      "steps = 30, loss = 3.256636381149292\n",
      "steps = 30, loss = 2.6121437549591064\n",
      "steps = 30, loss = 49.9853630065918\n",
      "steps = 30, loss = 1.9951808452606201\n",
      "steps = 30, loss = 3.3702995777130127\n",
      "steps = 30, loss = 2.7907307147979736\n",
      "steps = 30, loss = 3.0332119464874268\n",
      "steps = 30, loss = 3.4340920448303223\n",
      "steps = 30, loss = 2.9572627544403076\n",
      "steps = 30, loss = 2.388214349746704\n",
      "steps = 30, loss = 2.096853494644165\n",
      "steps = 30, loss = 3.4197299480438232\n",
      "steps = 30, loss = 3.247227668762207\n",
      "steps = 31, loss = 2.117426872253418\n",
      "steps = 31, loss = 2.4928524494171143\n",
      "steps = 31, loss = 49.9853630065918\n",
      "steps = 31, loss = 2.597661018371582\n",
      "steps = 31, loss = 3.315598726272583\n",
      "steps = 31, loss = 3.3777668476104736\n",
      "steps = 31, loss = 3.38503098487854\n",
      "steps = 31, loss = 2.7128984928131104\n",
      "steps = 31, loss = 2.1020395755767822\n",
      "steps = 31, loss = 2.7514536380767822\n",
      "steps = 31, loss = 6.163338661193848\n",
      "steps = 31, loss = 3.6683685779571533\n",
      "steps = 31, loss = 2.706007957458496\n",
      "steps = 31, loss = 2.741142511367798\n",
      "steps = 31, loss = 2.9810054302215576\n",
      "steps = 31, loss = 2.3880221843719482\n",
      "steps = 31, loss = 2.7021331787109375\n",
      "steps = 31, loss = 2.719808578491211\n",
      "steps = 31, loss = 3.4341986179351807\n",
      "steps = 31, loss = 2.1245620250701904\n",
      "steps = 31, loss = 2.849252462387085\n",
      "steps = 31, loss = 3.1144745349884033\n",
      "steps = 31, loss = 2.8764867782592773\n",
      "steps = 31, loss = 2.981009006500244\n",
      "steps = 31, loss = 3.3779795169830322\n",
      "steps = 31, loss = 2.6893889904022217\n",
      "steps = 31, loss = 3.4167301654815674\n",
      "steps = 31, loss = 2.4601151943206787\n",
      "steps = 31, loss = 2.0419516563415527\n",
      "steps = 32, loss = 2.5227036476135254\n",
      "steps = 32, loss = 2.134589672088623\n",
      "steps = 32, loss = 3.3638148307800293\n",
      "steps = 32, loss = 2.9422199726104736\n",
      "steps = 32, loss = 2.999122381210327\n",
      "steps = 32, loss = 3.071233034133911\n",
      "steps = 32, loss = 2.8672068119049072\n",
      "steps = 32, loss = 2.4961562156677246\n",
      "steps = 32, loss = 2.742130994796753\n",
      "steps = 32, loss = 2.6952738761901855\n",
      "steps = 32, loss = 49.9853630065918\n",
      "steps = 32, loss = 2.814347505569458\n",
      "steps = 32, loss = 3.2864229679107666\n",
      "steps = 32, loss = 2.470658302307129\n",
      "steps = 32, loss = 6.198876857757568\n",
      "steps = 32, loss = 2.2871992588043213\n",
      "steps = 32, loss = 3.428187608718872\n",
      "steps = 32, loss = 3.703688859939575\n",
      "steps = 32, loss = 3.522609233856201\n",
      "steps = 32, loss = 3.511898994445801\n",
      "steps = 32, loss = 2.8146777153015137\n",
      "steps = 32, loss = 2.7395970821380615\n",
      "steps = 32, loss = 2.7006583213806152\n",
      "steps = 32, loss = 2.1211447715759277\n",
      "steps = 32, loss = 3.5788257122039795\n",
      "steps = 32, loss = 2.4620559215545654\n",
      "steps = 32, loss = 2.422755479812622\n",
      "steps = 32, loss = 2.107783555984497\n",
      "steps = 32, loss = 3.5153329372406006\n",
      "steps = 33, loss = 2.6350178718566895\n",
      "steps = 33, loss = 2.759248971939087\n",
      "steps = 33, loss = 3.013150930404663\n",
      "steps = 33, loss = 2.5464179515838623\n",
      "steps = 33, loss = 2.315610647201538\n",
      "steps = 33, loss = 2.7209160327911377\n",
      "steps = 33, loss = 2.764561176300049\n",
      "steps = 33, loss = 2.8053154945373535\n",
      "steps = 33, loss = 2.1354074478149414\n",
      "steps = 33, loss = 3.5287885665893555\n",
      "steps = 33, loss = 2.883612632751465\n",
      "steps = 33, loss = 6.652508735656738\n",
      "steps = 33, loss = 2.8655171394348145\n",
      "steps = 33, loss = 3.1465306282043457\n",
      "steps = 33, loss = 2.704442262649536\n",
      "steps = 33, loss = 3.9132883548736572\n",
      "steps = 33, loss = 2.1648237705230713\n",
      "steps = 33, loss = 2.935203790664673\n",
      "steps = 33, loss = 3.4693048000335693\n",
      "steps = 33, loss = 3.5210628509521484\n",
      "steps = 33, loss = 2.4459421634674072\n",
      "steps = 33, loss = 2.7528011798858643\n",
      "steps = 33, loss = 3.34826397895813\n",
      "steps = 33, loss = 3.435396909713745\n",
      "steps = 33, loss = 3.204270601272583\n",
      "steps = 33, loss = 3.4200551509857178\n",
      "steps = 33, loss = 49.9853630065918\n",
      "steps = 33, loss = 2.2519519329071045\n",
      "steps = 33, loss = 2.938056707382202\n",
      "steps = 34, loss = 3.3566906452178955\n",
      "steps = 34, loss = 2.765195608139038\n",
      "steps = 34, loss = 2.794724941253662\n",
      "steps = 34, loss = 2.847720146179199\n",
      "steps = 34, loss = 2.1301798820495605\n",
      "steps = 34, loss = 2.064246416091919\n",
      "steps = 34, loss = 3.4490435123443604\n",
      "steps = 34, loss = 2.9975438117980957\n",
      "steps = 34, loss = 3.2944703102111816\n",
      "steps = 34, loss = 2.827251672744751\n",
      "steps = 34, loss = 3.333684206008911\n",
      "steps = 34, loss = 2.410543918609619\n",
      "steps = 34, loss = 3.185250759124756\n",
      "steps = 34, loss = 2.6634418964385986\n",
      "steps = 34, loss = 2.688507556915283\n",
      "steps = 34, loss = 2.584524631500244\n",
      "steps = 34, loss = 3.0274851322174072\n",
      "steps = 34, loss = 2.7585256099700928\n",
      "steps = 34, loss = 49.9853630065918\n",
      "steps = 34, loss = 6.464747905731201\n",
      "steps = 34, loss = 3.3590176105499268\n",
      "steps = 34, loss = 3.437539577484131\n",
      "steps = 34, loss = 3.7650227546691895\n",
      "steps = 34, loss = 2.01261305809021\n",
      "steps = 34, loss = 2.77744460105896\n",
      "steps = 34, loss = 2.449756622314453\n",
      "steps = 34, loss = 2.552631378173828\n",
      "steps = 34, loss = 2.1363847255706787\n",
      "steps = 34, loss = 3.133913278579712\n",
      "steps = 35, loss = 3.590428352355957\n",
      "steps = 35, loss = 2.483168601989746\n",
      "steps = 35, loss = 2.5798263549804688\n",
      "steps = 35, loss = 2.7428834438323975\n",
      "steps = 35, loss = 2.695619821548462\n",
      "steps = 35, loss = 3.648322820663452\n",
      "steps = 35, loss = 2.117929697036743\n",
      "steps = 35, loss = 3.464416980743408\n",
      "steps = 35, loss = 49.9853630065918\n",
      "steps = 35, loss = 2.6797664165496826\n",
      "steps = 35, loss = 3.1658666133880615\n",
      "steps = 35, loss = 3.0133514404296875\n",
      "steps = 35, loss = 2.7959189414978027\n",
      "steps = 35, loss = 3.325578451156616\n",
      "steps = 35, loss = 2.1484756469726562\n",
      "steps = 35, loss = 3.3393421173095703\n",
      "steps = 35, loss = 3.2976274490356445\n",
      "steps = 35, loss = 2.280505657196045\n",
      "steps = 35, loss = 2.112985134124756\n",
      "steps = 35, loss = 2.866457223892212\n",
      "steps = 35, loss = 2.8776915073394775\n",
      "steps = 35, loss = 3.4328134059906006\n",
      "steps = 35, loss = 3.021390199661255\n",
      "steps = 35, loss = 6.727156162261963\n",
      "steps = 35, loss = 2.768460988998413\n",
      "steps = 35, loss = 3.2893075942993164\n",
      "steps = 35, loss = 2.8869166374206543\n",
      "steps = 35, loss = 3.3876748085021973\n",
      "steps = 35, loss = 2.464195728302002\n",
      "steps = 36, loss = 2.720010757446289\n",
      "steps = 36, loss = 2.577784538269043\n",
      "steps = 36, loss = 2.6021735668182373\n",
      "steps = 36, loss = 3.4379730224609375\n",
      "steps = 36, loss = 3.0422253608703613\n",
      "steps = 36, loss = 3.243778705596924\n",
      "steps = 36, loss = 2.740701913833618\n",
      "steps = 36, loss = 2.162332057952881\n",
      "steps = 36, loss = 3.322685956954956\n",
      "steps = 36, loss = 2.9042141437530518\n",
      "steps = 36, loss = 49.9853630065918\n",
      "steps = 36, loss = 3.025531053543091\n",
      "steps = 36, loss = 6.685896396636963\n",
      "steps = 36, loss = 2.953917980194092\n",
      "steps = 36, loss = 2.7681884765625\n",
      "steps = 36, loss = 3.376312732696533\n",
      "steps = 36, loss = 3.1801891326904297\n",
      "steps = 36, loss = 2.376993417739868\n",
      "steps = 36, loss = 2.883056879043579\n",
      "steps = 36, loss = 3.525413751602173\n",
      "steps = 36, loss = 3.6580307483673096\n",
      "steps = 36, loss = 3.480240821838379\n",
      "steps = 36, loss = 2.2502009868621826\n",
      "steps = 36, loss = 3.526503801345825\n",
      "steps = 36, loss = 2.819497585296631\n",
      "steps = 36, loss = 2.505352020263672\n",
      "steps = 36, loss = 2.9676194190979004\n",
      "steps = 36, loss = 2.977086067199707\n",
      "steps = 36, loss = 2.1714353561401367\n",
      "steps = 37, loss = 3.3930907249450684\n",
      "steps = 37, loss = 3.5074210166931152\n",
      "steps = 37, loss = 3.3229424953460693\n",
      "steps = 37, loss = 3.1364970207214355\n",
      "steps = 37, loss = 2.926473617553711\n",
      "steps = 37, loss = 2.262998342514038\n",
      "steps = 37, loss = 2.72114634513855\n",
      "steps = 37, loss = 2.9355719089508057\n",
      "steps = 37, loss = 2.8856265544891357\n",
      "steps = 37, loss = 3.0301594734191895\n",
      "steps = 37, loss = 2.6310973167419434\n",
      "steps = 37, loss = 3.4906575679779053\n",
      "steps = 37, loss = 2.838329792022705\n",
      "steps = 37, loss = 2.4399044513702393\n",
      "steps = 37, loss = 2.526052713394165\n",
      "steps = 37, loss = 3.200737476348877\n",
      "steps = 37, loss = 3.2085516452789307\n",
      "steps = 37, loss = 2.6209869384765625\n",
      "steps = 37, loss = 6.7613911628723145\n",
      "steps = 37, loss = 3.517266273498535\n",
      "steps = 37, loss = 2.961794853210449\n",
      "steps = 37, loss = 3.4547829627990723\n",
      "steps = 37, loss = 2.9613778591156006\n",
      "steps = 37, loss = 2.173022985458374\n",
      "steps = 37, loss = 2.1344852447509766\n",
      "steps = 37, loss = 3.461371421813965\n",
      "steps = 37, loss = 2.747445821762085\n",
      "steps = 37, loss = 49.9853630065918\n",
      "steps = 37, loss = 2.7623403072357178\n",
      "steps = 38, loss = 3.4287664890289307\n",
      "steps = 38, loss = 6.727615833282471\n",
      "steps = 38, loss = 2.6349737644195557\n",
      "steps = 38, loss = 2.7304372787475586\n",
      "steps = 38, loss = 2.2604901790618896\n",
      "steps = 38, loss = 3.4328489303588867\n",
      "steps = 38, loss = 3.074415683746338\n",
      "steps = 38, loss = 3.367500066757202\n",
      "steps = 38, loss = 3.833310604095459\n",
      "steps = 38, loss = 2.5027501583099365\n",
      "steps = 38, loss = 3.281595230102539\n",
      "steps = 38, loss = 2.714016914367676\n",
      "steps = 38, loss = 2.921203136444092\n",
      "steps = 38, loss = 3.024793863296509\n",
      "steps = 38, loss = 3.437394380569458\n",
      "steps = 38, loss = 2.866792678833008\n",
      "steps = 38, loss = 3.8810296058654785\n",
      "steps = 38, loss = 2.975790500640869\n",
      "steps = 38, loss = 2.1801273822784424\n",
      "steps = 38, loss = 2.696150779724121\n",
      "steps = 38, loss = 3.3751413822174072\n",
      "steps = 38, loss = 49.9853630065918\n",
      "steps = 38, loss = 3.5412964820861816\n",
      "steps = 38, loss = 2.849616765975952\n",
      "steps = 38, loss = 2.947911500930786\n",
      "steps = 38, loss = 1.3221474885940552\n",
      "steps = 38, loss = 2.5452075004577637\n",
      "steps = 38, loss = 2.1706762313842773\n",
      "steps = 38, loss = 2.8211522102355957\n",
      "steps = 39, loss = 3.832429885864258\n",
      "steps = 39, loss = 2.0099990367889404\n",
      "steps = 39, loss = 6.717414855957031\n",
      "steps = 39, loss = 3.3415346145629883\n",
      "steps = 39, loss = 3.2959017753601074\n",
      "steps = 39, loss = 49.9853630065918\n",
      "steps = 39, loss = 2.7569801807403564\n",
      "steps = 39, loss = 2.6849989891052246\n",
      "steps = 39, loss = 2.6414783000946045\n",
      "steps = 39, loss = 3.592095136642456\n",
      "steps = 39, loss = 2.851635694503784\n",
      "steps = 39, loss = 8.980977058410645\n",
      "steps = 39, loss = 3.1285274028778076\n",
      "steps = 39, loss = 3.3048057556152344\n",
      "steps = 39, loss = 3.4864540100097656\n",
      "steps = 39, loss = 1.9334720373153687\n",
      "steps = 39, loss = 2.573399066925049\n",
      "steps = 39, loss = 2.176396608352661\n",
      "steps = 39, loss = 2.625192642211914\n",
      "steps = 39, loss = 2.1341733932495117\n",
      "steps = 39, loss = 2.5483107566833496\n",
      "steps = 39, loss = 2.8446662425994873\n",
      "steps = 39, loss = 2.723642349243164\n",
      "steps = 39, loss = 3.0134410858154297\n",
      "steps = 39, loss = 3.497213363647461\n",
      "steps = 39, loss = 2.5811917781829834\n",
      "steps = 39, loss = 2.401782989501953\n",
      "steps = 39, loss = 2.899092435836792\n",
      "steps = 39, loss = 3.412428617477417\n",
      "steps = 40, loss = 3.614600658416748\n",
      "steps = 40, loss = 3.379469633102417\n",
      "steps = 40, loss = 2.656702995300293\n",
      "steps = 40, loss = 2.1826813220977783\n",
      "steps = 40, loss = 2.75244402885437\n",
      "steps = 40, loss = 2.0215399265289307\n",
      "steps = 40, loss = 3.511763095855713\n",
      "steps = 40, loss = 2.5803115367889404\n",
      "steps = 40, loss = 3.2004330158233643\n",
      "steps = 40, loss = 2.687457799911499\n",
      "steps = 40, loss = 2.5931506156921387\n",
      "steps = 40, loss = 2.7343204021453857\n",
      "steps = 40, loss = 3.7050857543945312\n",
      "steps = 40, loss = 2.5648961067199707\n",
      "steps = 40, loss = 2.825516700744629\n",
      "steps = 40, loss = 2.8465144634246826\n",
      "steps = 40, loss = 2.8673689365386963\n",
      "steps = 40, loss = 3.6590816974639893\n",
      "steps = 40, loss = 2.9476444721221924\n",
      "steps = 40, loss = 1.9444012641906738\n",
      "steps = 40, loss = 3.681673049926758\n",
      "steps = 40, loss = 2.802649736404419\n",
      "steps = 40, loss = 2.4242684841156006\n",
      "steps = 40, loss = 3.01509952545166\n",
      "steps = 40, loss = 6.404320240020752\n",
      "steps = 40, loss = 49.9853630065918\n",
      "steps = 40, loss = 3.3364481925964355\n",
      "steps = 40, loss = 2.135406255722046\n",
      "steps = 40, loss = 3.170041084289551\n",
      "steps = 41, loss = 3.5160398483276367\n",
      "steps = 41, loss = 2.892086982727051\n",
      "steps = 41, loss = 3.494718074798584\n",
      "steps = 41, loss = 2.8646719455718994\n",
      "steps = 41, loss = 3.166595220565796\n",
      "steps = 41, loss = 2.203357696533203\n",
      "steps = 41, loss = 2.739985942840576\n",
      "steps = 41, loss = 3.0286478996276855\n",
      "steps = 41, loss = 2.907764196395874\n",
      "steps = 41, loss = 2.6970582008361816\n",
      "steps = 41, loss = 2.90234112739563\n",
      "steps = 41, loss = 2.090878963470459\n",
      "steps = 41, loss = 2.7302610874176025\n",
      "steps = 41, loss = 49.9853630065918\n",
      "steps = 41, loss = 7.013145446777344\n",
      "steps = 41, loss = 2.768963098526001\n",
      "steps = 41, loss = 3.813304901123047\n",
      "steps = 41, loss = 3.255995035171509\n",
      "steps = 41, loss = 3.455822467803955\n",
      "steps = 41, loss = 2.223411798477173\n",
      "steps = 41, loss = 3.327044725418091\n",
      "steps = 41, loss = 3.2634940147399902\n",
      "steps = 41, loss = 2.681889057159424\n",
      "steps = 41, loss = 2.2877211570739746\n",
      "steps = 41, loss = 2.6055963039398193\n",
      "steps = 41, loss = 2.5983901023864746\n",
      "steps = 41, loss = 3.793006658554077\n",
      "steps = 41, loss = 2.89577054977417\n",
      "steps = 41, loss = 3.3100154399871826\n",
      "steps = 42, loss = 3.038332223892212\n",
      "steps = 42, loss = 3.1999497413635254\n",
      "steps = 42, loss = 3.2679214477539062\n",
      "steps = 42, loss = 2.790497064590454\n",
      "steps = 42, loss = 3.4789156913757324\n",
      "steps = 42, loss = 2.624537229537964\n",
      "steps = 42, loss = 2.9803524017333984\n",
      "steps = 42, loss = 3.261383533477783\n",
      "steps = 42, loss = 3.530323028564453\n",
      "steps = 42, loss = 3.535562038421631\n",
      "steps = 42, loss = 2.9175260066986084\n",
      "steps = 42, loss = 49.9853630065918\n",
      "steps = 42, loss = 2.2494394779205322\n",
      "steps = 42, loss = 7.1302080154418945\n",
      "steps = 42, loss = 3.0038325786590576\n",
      "steps = 42, loss = 2.7192864418029785\n",
      "steps = 42, loss = 2.095468282699585\n",
      "steps = 42, loss = 2.636063575744629\n",
      "steps = 42, loss = 3.920764446258545\n",
      "steps = 42, loss = 3.5088541507720947\n",
      "steps = 42, loss = 2.2176854610443115\n",
      "steps = 42, loss = 2.8813271522521973\n",
      "steps = 42, loss = 3.157679557800293\n",
      "steps = 42, loss = 2.6198127269744873\n",
      "steps = 42, loss = 2.7545135021209717\n",
      "steps = 42, loss = 2.115586042404175\n",
      "steps = 42, loss = 3.4074947834014893\n",
      "steps = 42, loss = 2.7029309272766113\n",
      "steps = 42, loss = 2.816561698913574\n",
      "steps = 43, loss = 2.6974880695343018\n",
      "steps = 43, loss = 3.540621519088745\n",
      "steps = 43, loss = 2.805415630340576\n",
      "steps = 43, loss = 2.7832870483398438\n",
      "steps = 43, loss = 2.8651177883148193\n",
      "steps = 43, loss = 6.9725775718688965\n",
      "steps = 43, loss = 3.5390572547912598\n",
      "steps = 43, loss = 2.042722225189209\n",
      "steps = 43, loss = 2.927743434906006\n",
      "steps = 43, loss = 3.3570799827575684\n",
      "steps = 43, loss = 4.017431735992432\n",
      "steps = 43, loss = 2.4786837100982666\n",
      "steps = 43, loss = 3.0321619510650635\n",
      "steps = 43, loss = 2.9715726375579834\n",
      "steps = 43, loss = 3.5124990940093994\n",
      "steps = 43, loss = 2.6209325790405273\n",
      "steps = 43, loss = 2.226588487625122\n",
      "steps = 43, loss = 2.928701639175415\n",
      "steps = 43, loss = 2.7158000469207764\n",
      "steps = 43, loss = 2.096007823944092\n",
      "steps = 43, loss = 49.9853630065918\n",
      "steps = 43, loss = 2.255121946334839\n",
      "steps = 43, loss = 2.7465035915374756\n",
      "steps = 43, loss = 3.569248914718628\n",
      "steps = 43, loss = 2.701352834701538\n",
      "steps = 43, loss = 3.255394697189331\n",
      "steps = 43, loss = 3.0473079681396484\n",
      "steps = 43, loss = 2.6389830112457275\n",
      "steps = 43, loss = 3.6579384803771973\n",
      "steps = 44, loss = 3.5244898796081543\n",
      "steps = 44, loss = 2.7187414169311523\n",
      "steps = 44, loss = 6.752771377563477\n",
      "steps = 44, loss = 3.4875035285949707\n",
      "steps = 44, loss = 3.0404272079467773\n",
      "steps = 44, loss = 3.102154493331909\n",
      "steps = 44, loss = 3.3531384468078613\n",
      "steps = 44, loss = 3.4718282222747803\n",
      "steps = 44, loss = 3.4300198554992676\n",
      "steps = 44, loss = 2.07726788520813\n",
      "steps = 44, loss = 4.122926235198975\n",
      "steps = 44, loss = 3.561397075653076\n",
      "steps = 44, loss = 39.718048095703125\n",
      "steps = 44, loss = 2.9043354988098145\n",
      "steps = 44, loss = 2.88165283203125\n",
      "steps = 44, loss = 2.7701900005340576\n",
      "steps = 44, loss = 49.9853630065918\n",
      "steps = 44, loss = 2.2513391971588135\n",
      "steps = 44, loss = 2.629528760910034\n",
      "steps = 44, loss = 3.1694767475128174\n",
      "steps = 44, loss = 2.2388274669647217\n",
      "steps = 44, loss = 2.7073309421539307\n",
      "steps = 44, loss = 2.91314959526062\n",
      "steps = 44, loss = 2.658212661743164\n",
      "steps = 44, loss = 2.747938871383667\n",
      "steps = 44, loss = 2.7351198196411133\n",
      "steps = 44, loss = 2.8201205730438232\n",
      "steps = 44, loss = 2.9481546878814697\n",
      "steps = 44, loss = 3.433202028274536\n",
      "steps = 45, loss = 2.948967218399048\n",
      "steps = 45, loss = 3.5529627799987793\n",
      "steps = 45, loss = 3.359260082244873\n",
      "steps = 45, loss = 3.189173936843872\n",
      "steps = 45, loss = 3.303403615951538\n",
      "steps = 45, loss = 3.195482015609741\n",
      "steps = 45, loss = 2.2340221405029297\n",
      "steps = 45, loss = 2.6602368354797363\n",
      "steps = 45, loss = 4.168706893920898\n",
      "steps = 45, loss = 2.7352263927459717\n",
      "steps = 45, loss = 2.722900629043579\n",
      "steps = 45, loss = 2.137434959411621\n",
      "steps = 45, loss = 2.6859729290008545\n",
      "steps = 45, loss = 2.403122663497925\n",
      "steps = 45, loss = 2.944589614868164\n",
      "steps = 45, loss = 2.9368278980255127\n",
      "steps = 45, loss = 3.021574020385742\n",
      "steps = 45, loss = 2.6011860370635986\n",
      "steps = 45, loss = 2.9525153636932373\n",
      "steps = 45, loss = 2.8203353881835938\n",
      "steps = 45, loss = 2.0583927631378174\n",
      "steps = 45, loss = 3.374434232711792\n",
      "steps = 45, loss = 3.0887832641601562\n",
      "steps = 45, loss = 1.921647071838379\n",
      "steps = 45, loss = 3.402151346206665\n",
      "steps = 45, loss = 2.84421706199646\n",
      "steps = 45, loss = 2.737760305404663\n",
      "steps = 45, loss = 49.9853630065918\n",
      "steps = 45, loss = 7.142106533050537\n",
      "steps = 46, loss = 2.255622148513794\n",
      "steps = 46, loss = 49.9853630065918\n",
      "steps = 46, loss = 2.7542154788970947\n",
      "steps = 46, loss = 2.6924405097961426\n",
      "steps = 46, loss = 2.7193408012390137\n",
      "steps = 46, loss = 2.8822221755981445\n",
      "steps = 46, loss = 4.051302433013916\n",
      "steps = 46, loss = 4.321166038513184\n",
      "steps = 46, loss = 2.7481307983398438\n",
      "steps = 46, loss = 2.611318349838257\n",
      "steps = 46, loss = 2.9005813598632812\n",
      "steps = 46, loss = 3.6073687076568604\n",
      "steps = 46, loss = 3.2846484184265137\n",
      "steps = 46, loss = 3.513550281524658\n",
      "steps = 46, loss = 2.9797544479370117\n",
      "steps = 46, loss = 2.352050542831421\n",
      "steps = 46, loss = 2.2568235397338867\n",
      "steps = 46, loss = 3.213366746902466\n",
      "steps = 46, loss = 3.5005223751068115\n",
      "steps = 46, loss = 2.7643115520477295\n",
      "steps = 46, loss = 2.9605183601379395\n",
      "steps = 46, loss = 2.195266008377075\n",
      "steps = 46, loss = 3.3418986797332764\n",
      "steps = 46, loss = 3.041316032409668\n",
      "steps = 46, loss = 3.339186191558838\n",
      "steps = 46, loss = 3.271003484725952\n",
      "steps = 46, loss = 7.189951419830322\n",
      "steps = 46, loss = 2.9771997928619385\n",
      "steps = 46, loss = 2.7375035285949707\n",
      "steps = 47, loss = 3.1605124473571777\n",
      "steps = 47, loss = 3.034832239151001\n",
      "steps = 47, loss = 3.5022125244140625\n",
      "steps = 47, loss = 3.429755449295044\n",
      "steps = 47, loss = 2.069384813308716\n",
      "steps = 47, loss = 2.720989942550659\n",
      "steps = 47, loss = 2.2640233039855957\n",
      "steps = 47, loss = 2.6982288360595703\n",
      "steps = 47, loss = 6.974052429199219\n",
      "steps = 47, loss = 2.26546049118042\n",
      "steps = 47, loss = 3.2972521781921387\n",
      "steps = 47, loss = 3.8615100383758545\n",
      "steps = 47, loss = 3.5323667526245117\n",
      "steps = 47, loss = 2.739952325820923\n",
      "steps = 47, loss = 2.586808681488037\n",
      "steps = 47, loss = 2.7116758823394775\n",
      "steps = 47, loss = 2.7954394817352295\n",
      "steps = 47, loss = 2.864091396331787\n",
      "steps = 47, loss = 3.245487689971924\n",
      "steps = 47, loss = 3.056903839111328\n",
      "steps = 47, loss = 2.8514225482940674\n",
      "steps = 47, loss = 49.9853630065918\n",
      "steps = 47, loss = 4.419848442077637\n",
      "steps = 47, loss = 2.776310443878174\n",
      "steps = 47, loss = 2.985783815383911\n",
      "steps = 47, loss = 3.6027944087982178\n",
      "steps = 47, loss = 2.500187635421753\n",
      "steps = 47, loss = 2.240417003631592\n",
      "steps = 47, loss = 3.8959579467773438\n",
      "steps = 48, loss = 3.3123810291290283\n",
      "steps = 48, loss = 2.8604044914245605\n",
      "steps = 48, loss = 2.108332395553589\n",
      "steps = 48, loss = 3.1403462886810303\n",
      "steps = 48, loss = 3.0219171047210693\n",
      "steps = 48, loss = 2.9864799976348877\n",
      "steps = 48, loss = 3.6389074325561523\n",
      "steps = 48, loss = 2.7508580684661865\n",
      "steps = 48, loss = 2.781714677810669\n",
      "steps = 48, loss = 2.42501163482666\n",
      "steps = 48, loss = 6.851104736328125\n",
      "steps = 48, loss = 3.2013745307922363\n",
      "steps = 48, loss = 3.342012405395508\n",
      "steps = 48, loss = 2.9467434883117676\n",
      "steps = 48, loss = 2.754905939102173\n",
      "steps = 48, loss = 2.6403491497039795\n",
      "steps = 48, loss = 2.129603624343872\n",
      "steps = 48, loss = 3.556281089782715\n",
      "steps = 48, loss = 2.781604290008545\n",
      "steps = 48, loss = 2.7137296199798584\n",
      "steps = 48, loss = 2.8428423404693604\n",
      "steps = 48, loss = 49.9853630065918\n",
      "steps = 48, loss = 2.1289210319519043\n",
      "steps = 48, loss = 2.6851847171783447\n",
      "steps = 48, loss = 3.391972064971924\n",
      "steps = 48, loss = 2.26226806640625\n",
      "steps = 48, loss = 3.197726249694824\n",
      "steps = 48, loss = 3.415334463119507\n",
      "steps = 48, loss = 4.4647536277771\n",
      "steps = 49, loss = 2.7212114334106445\n",
      "steps = 49, loss = 3.4805994033813477\n",
      "steps = 49, loss = 2.222194194793701\n",
      "steps = 49, loss = 3.4881858825683594\n",
      "steps = 49, loss = 2.8808979988098145\n",
      "steps = 49, loss = 2.25521183013916\n",
      "steps = 49, loss = 3.0409719944000244\n",
      "steps = 49, loss = 3.018460988998413\n",
      "steps = 49, loss = 3.7083842754364014\n",
      "steps = 49, loss = 2.8844358921051025\n",
      "steps = 49, loss = 3.3688859939575195\n",
      "steps = 49, loss = 2.2840123176574707\n",
      "steps = 49, loss = 2.7506279945373535\n",
      "steps = 49, loss = 2.576643466949463\n",
      "steps = 49, loss = 4.615753173828125\n",
      "steps = 49, loss = 2.735459089279175\n",
      "steps = 49, loss = 2.7189977169036865\n",
      "steps = 49, loss = 3.0980777740478516\n",
      "steps = 49, loss = 49.9853630065918\n",
      "steps = 49, loss = 2.235830068588257\n",
      "steps = 49, loss = 7.168598175048828\n",
      "steps = 49, loss = 12.663097381591797\n",
      "steps = 49, loss = 2.8083953857421875\n",
      "steps = 49, loss = 3.4958183765411377\n",
      "steps = 49, loss = 3.160597324371338\n",
      "steps = 49, loss = 2.7806262969970703\n",
      "steps = 49, loss = 3.4254956245422363\n",
      "steps = 49, loss = 2.900224208831787\n",
      "steps = 49, loss = 2.7458651065826416\n",
      "steps = 50, loss = 3.1110379695892334\n",
      "steps = 50, loss = 2.8444597721099854\n",
      "steps = 50, loss = 3.541482925415039\n",
      "steps = 50, loss = 6.572935104370117\n",
      "steps = 50, loss = 2.9175570011138916\n",
      "steps = 50, loss = 3.0230753421783447\n",
      "steps = 50, loss = 3.0149354934692383\n",
      "steps = 50, loss = 2.5184085369110107\n",
      "steps = 50, loss = 2.723212957382202\n",
      "steps = 50, loss = 2.280863046646118\n",
      "steps = 50, loss = 2.469850540161133\n",
      "steps = 50, loss = 2.0925440788269043\n",
      "steps = 50, loss = 2.7247328758239746\n",
      "steps = 50, loss = 2.7602221965789795\n",
      "steps = 50, loss = 2.2667183876037598\n",
      "steps = 50, loss = 3.551792621612549\n",
      "steps = 50, loss = 3.471325397491455\n",
      "steps = 50, loss = 2.6878933906555176\n",
      "steps = 50, loss = 49.9853630065918\n",
      "steps = 50, loss = 2.7482941150665283\n",
      "steps = 50, loss = 4.65548849105835\n",
      "steps = 50, loss = 3.3387324810028076\n",
      "steps = 50, loss = 3.264765739440918\n",
      "steps = 50, loss = 2.138340711593628\n",
      "steps = 50, loss = 3.6381876468658447\n",
      "steps = 50, loss = 3.214202642440796\n",
      "steps = 50, loss = 3.4322707653045654\n",
      "steps = 50, loss = 2.9637820720672607\n",
      "steps = 50, loss = 2.811316728591919\n",
      "steps = 51, loss = 3.3981082439422607\n",
      "steps = 51, loss = 2.779721260070801\n",
      "steps = 51, loss = 2.7624425888061523\n",
      "steps = 51, loss = 3.039073944091797\n",
      "steps = 51, loss = 3.4475481510162354\n",
      "steps = 51, loss = 2.833101987838745\n",
      "steps = 51, loss = 2.816741943359375\n",
      "steps = 51, loss = 2.2020208835601807\n",
      "steps = 51, loss = 4.798367977142334\n",
      "steps = 51, loss = 2.6082804203033447\n",
      "steps = 51, loss = 2.3008534908294678\n",
      "steps = 51, loss = 3.0332322120666504\n",
      "steps = 51, loss = 2.8622043132781982\n",
      "steps = 51, loss = 3.3707048892974854\n",
      "steps = 51, loss = 3.489279270172119\n",
      "steps = 51, loss = 6.479232311248779\n",
      "steps = 51, loss = 3.455322742462158\n",
      "steps = 51, loss = 2.941873073577881\n",
      "steps = 51, loss = 3.1344499588012695\n",
      "steps = 51, loss = 3.4763026237487793\n",
      "steps = 51, loss = 3.1632447242736816\n",
      "steps = 51, loss = 2.075695276260376\n",
      "steps = 51, loss = 3.6528239250183105\n",
      "steps = 51, loss = 49.9853630065918\n",
      "steps = 51, loss = 3.207803249359131\n",
      "steps = 51, loss = 2.4629645347595215\n",
      "steps = 51, loss = 2.292792320251465\n",
      "steps = 51, loss = 2.7455475330352783\n",
      "steps = 51, loss = 2.698934555053711\n",
      "steps = 52, loss = 3.60737681388855\n",
      "steps = 52, loss = 3.3889646530151367\n",
      "steps = 52, loss = 6.843835353851318\n",
      "steps = 52, loss = 2.0976040363311768\n",
      "steps = 52, loss = 2.5040998458862305\n",
      "steps = 52, loss = 49.9853630065918\n",
      "steps = 52, loss = 3.1763200759887695\n",
      "steps = 52, loss = 2.8554916381835938\n",
      "steps = 52, loss = 4.834690570831299\n",
      "steps = 52, loss = 2.842808246612549\n",
      "steps = 52, loss = 2.6038904190063477\n",
      "steps = 52, loss = 2.599452495574951\n",
      "steps = 52, loss = 2.687140941619873\n",
      "steps = 52, loss = 3.021000862121582\n",
      "steps = 52, loss = 2.740063190460205\n",
      "steps = 52, loss = 2.129260540008545\n",
      "steps = 52, loss = 3.040478229522705\n",
      "steps = 52, loss = 2.9120473861694336\n",
      "steps = 52, loss = 2.8830411434173584\n",
      "steps = 52, loss = 2.83984375\n",
      "steps = 52, loss = 3.017565965652466\n",
      "steps = 52, loss = 2.782404899597168\n",
      "steps = 52, loss = 2.2988739013671875\n",
      "steps = 52, loss = 3.31955885887146\n",
      "steps = 52, loss = 3.923872232437134\n",
      "steps = 52, loss = 2.0120368003845215\n",
      "steps = 52, loss = 3.376640558242798\n",
      "steps = 52, loss = 3.191624402999878\n",
      "steps = 52, loss = 3.316180467605591\n",
      "steps = 53, loss = 3.1707863807678223\n",
      "steps = 53, loss = 3.303269386291504\n",
      "steps = 53, loss = 2.742964506149292\n",
      "steps = 53, loss = 2.255444288253784\n",
      "steps = 53, loss = 6.941827774047852\n",
      "steps = 53, loss = 2.857051372528076\n",
      "steps = 53, loss = 2.4507763385772705\n",
      "steps = 53, loss = 3.071122407913208\n",
      "steps = 53, loss = 3.600832223892212\n",
      "steps = 53, loss = 2.703077793121338\n",
      "steps = 53, loss = 2.865974187850952\n",
      "steps = 53, loss = 4.976302623748779\n",
      "steps = 53, loss = 3.2759478092193604\n",
      "steps = 53, loss = 3.5087897777557373\n",
      "steps = 53, loss = 3.5602214336395264\n",
      "steps = 53, loss = 49.9853630065918\n",
      "steps = 53, loss = 2.9330382347106934\n",
      "steps = 53, loss = 3.2857768535614014\n",
      "steps = 53, loss = 2.814908027648926\n",
      "steps = 53, loss = 2.7617437839508057\n",
      "steps = 53, loss = 3.3844103813171387\n",
      "steps = 53, loss = 3.22676157951355\n",
      "steps = 53, loss = 2.220163106918335\n",
      "steps = 53, loss = 2.6503326892852783\n",
      "steps = 53, loss = 2.3208229541778564\n",
      "steps = 53, loss = 2.8796634674072266\n",
      "steps = 53, loss = 3.0394396781921387\n",
      "steps = 53, loss = 3.519845724105835\n",
      "steps = 53, loss = 2.7195756435394287\n",
      "steps = 54, loss = 5.062932968139648\n",
      "steps = 54, loss = 6.9697489738464355\n",
      "steps = 54, loss = 3.4628069400787354\n",
      "steps = 54, loss = 3.444951057434082\n",
      "steps = 54, loss = 49.9853630065918\n",
      "steps = 54, loss = 2.876941680908203\n",
      "steps = 54, loss = 2.8621490001678467\n",
      "steps = 54, loss = 2.924617052078247\n",
      "steps = 54, loss = 3.353167772293091\n",
      "steps = 54, loss = 2.699981212615967\n",
      "steps = 54, loss = 3.0778627395629883\n",
      "steps = 54, loss = 2.273499011993408\n",
      "steps = 54, loss = 3.0325448513031006\n",
      "steps = 54, loss = 2.8817496299743652\n",
      "steps = 54, loss = 3.253387689590454\n",
      "steps = 54, loss = 3.370739698410034\n",
      "steps = 54, loss = 3.428431272506714\n",
      "steps = 54, loss = 3.4957141876220703\n",
      "steps = 54, loss = 2.232088327407837\n",
      "steps = 54, loss = 3.0786752700805664\n",
      "steps = 54, loss = 2.5077707767486572\n",
      "steps = 54, loss = 49.32400894165039\n",
      "steps = 54, loss = 3.2607421875\n",
      "steps = 54, loss = 2.8127388954162598\n",
      "steps = 54, loss = 2.8332595825195312\n",
      "steps = 54, loss = 2.711291551589966\n",
      "steps = 54, loss = 2.744542121887207\n",
      "steps = 54, loss = 3.650782823562622\n",
      "steps = 54, loss = 2.3312087059020996\n",
      "steps = 55, loss = 1.9790282249450684\n",
      "steps = 55, loss = 2.7171640396118164\n",
      "steps = 55, loss = 2.5186612606048584\n",
      "steps = 55, loss = 3.0195491313934326\n",
      "steps = 55, loss = 3.0206196308135986\n",
      "steps = 55, loss = 49.96149444580078\n",
      "steps = 55, loss = 2.8418047428131104\n",
      "steps = 55, loss = 2.1292309761047363\n",
      "steps = 55, loss = 2.8345439434051514\n",
      "steps = 55, loss = 3.078775405883789\n",
      "steps = 55, loss = 2.6247873306274414\n",
      "steps = 55, loss = 6.896695613861084\n",
      "steps = 55, loss = 3.1438493728637695\n",
      "steps = 55, loss = 3.788673162460327\n",
      "steps = 55, loss = 2.452603816986084\n",
      "steps = 55, loss = 3.099984884262085\n",
      "steps = 55, loss = 49.9853630065918\n",
      "steps = 55, loss = 2.6872642040252686\n",
      "steps = 55, loss = 3.3213796615600586\n",
      "steps = 55, loss = 3.235826253890991\n",
      "steps = 55, loss = 2.8823049068450928\n",
      "steps = 55, loss = 2.989844799041748\n",
      "steps = 55, loss = 2.675723075866699\n",
      "steps = 55, loss = 3.5807807445526123\n",
      "steps = 55, loss = 3.0086965560913086\n",
      "steps = 55, loss = 2.327791929244995\n",
      "steps = 55, loss = 3.1954007148742676\n",
      "steps = 55, loss = 5.087667465209961\n",
      "steps = 55, loss = 3.584169864654541\n",
      "steps = 56, loss = 3.494974374771118\n",
      "steps = 56, loss = 2.1223950386047363\n",
      "steps = 56, loss = 2.922961711883545\n",
      "steps = 56, loss = 2.259368658065796\n",
      "steps = 56, loss = 3.3200292587280273\n",
      "steps = 56, loss = 3.0534846782684326\n",
      "steps = 56, loss = 2.76335072517395\n",
      "steps = 56, loss = 2.6348578929901123\n",
      "steps = 56, loss = 3.046236991882324\n",
      "steps = 56, loss = 2.9079315662384033\n",
      "steps = 56, loss = 3.108595609664917\n",
      "steps = 56, loss = 6.791036128997803\n",
      "steps = 56, loss = 3.4925003051757812\n",
      "steps = 56, loss = 2.719773292541504\n",
      "steps = 56, loss = 3.533276319503784\n",
      "steps = 56, loss = 2.970320463180542\n",
      "steps = 56, loss = 3.0373311042785645\n",
      "steps = 56, loss = 5.221935749053955\n",
      "steps = 56, loss = 3.035515546798706\n",
      "steps = 56, loss = 2.6044812202453613\n",
      "steps = 56, loss = 2.8791158199310303\n",
      "steps = 56, loss = 2.8668980598449707\n",
      "steps = 56, loss = 3.5433058738708496\n",
      "steps = 56, loss = 3.16365909576416\n",
      "steps = 56, loss = 3.212688684463501\n",
      "steps = 56, loss = 2.3860628604888916\n",
      "steps = 56, loss = 49.9853630065918\n",
      "steps = 56, loss = 2.349048137664795\n",
      "steps = 56, loss = 3.3152637481689453\n",
      "steps = 57, loss = 3.5646703243255615\n",
      "steps = 57, loss = 2.113227605819702\n",
      "steps = 57, loss = 3.335846424102783\n",
      "steps = 57, loss = 2.797593116760254\n",
      "steps = 57, loss = 3.0303101539611816\n",
      "steps = 57, loss = 2.6873583793640137\n",
      "steps = 57, loss = 2.842890501022339\n",
      "steps = 57, loss = 6.054017543792725\n",
      "steps = 57, loss = 5.305266857147217\n",
      "steps = 57, loss = 3.5490431785583496\n",
      "steps = 57, loss = 2.8615505695343018\n",
      "steps = 57, loss = 2.274052381515503\n",
      "steps = 57, loss = 2.700892210006714\n",
      "steps = 57, loss = 3.663017988204956\n",
      "steps = 57, loss = 3.115412712097168\n",
      "steps = 57, loss = 3.015986442565918\n",
      "steps = 57, loss = 3.421828269958496\n",
      "steps = 57, loss = 2.516219139099121\n",
      "steps = 57, loss = 2.8847498893737793\n",
      "steps = 57, loss = 2.91831636428833\n",
      "steps = 57, loss = 2.7039272785186768\n",
      "steps = 57, loss = 2.753258466720581\n",
      "steps = 57, loss = 3.2499258518218994\n",
      "steps = 57, loss = 3.570559501647949\n",
      "steps = 57, loss = 3.0494327545166016\n",
      "steps = 57, loss = 49.9853630065918\n",
      "steps = 57, loss = 3.532031536102295\n",
      "steps = 57, loss = 3.6918411254882812\n",
      "steps = 57, loss = 2.35917329788208\n",
      "steps = 58, loss = 3.2881813049316406\n",
      "steps = 58, loss = 3.127070426940918\n",
      "steps = 58, loss = 2.165172815322876\n",
      "steps = 58, loss = 2.8020968437194824\n",
      "steps = 58, loss = 2.9322121143341064\n",
      "steps = 58, loss = 2.368875026702881\n",
      "steps = 58, loss = 3.4656333923339844\n",
      "steps = 58, loss = 3.4086215496063232\n",
      "steps = 58, loss = 3.629897117614746\n",
      "steps = 58, loss = 3.053711414337158\n",
      "steps = 58, loss = 2.7591776847839355\n",
      "steps = 58, loss = 2.7145674228668213\n",
      "steps = 58, loss = 49.9853630065918\n",
      "steps = 58, loss = 3.246837615966797\n",
      "steps = 58, loss = 6.31166934967041\n",
      "steps = 58, loss = 3.5571842193603516\n",
      "steps = 58, loss = 5.382134914398193\n",
      "steps = 58, loss = 2.9023656845092773\n",
      "steps = 58, loss = 3.403850793838501\n",
      "steps = 58, loss = 2.6511802673339844\n",
      "steps = 58, loss = 2.674980640411377\n",
      "steps = 58, loss = 3.028639554977417\n",
      "steps = 58, loss = 2.701436996459961\n",
      "steps = 58, loss = 2.269270658493042\n",
      "steps = 58, loss = 3.452819585800171\n",
      "steps = 58, loss = 2.9683871269226074\n",
      "steps = 58, loss = 3.2926480770111084\n",
      "steps = 58, loss = 2.8615026473999023\n",
      "steps = 58, loss = 2.574575901031494\n",
      "steps = 59, loss = 3.3654699325561523\n",
      "steps = 59, loss = 2.9378933906555176\n",
      "steps = 59, loss = 3.197803497314453\n",
      "steps = 59, loss = 2.9027817249298096\n",
      "steps = 59, loss = 3.3803627490997314\n",
      "steps = 59, loss = 3.0521326065063477\n",
      "steps = 59, loss = 3.1271581649780273\n",
      "steps = 59, loss = 2.165482997894287\n",
      "steps = 59, loss = 3.388077735900879\n",
      "steps = 59, loss = 2.1312127113342285\n",
      "steps = 59, loss = 3.015958786010742\n",
      "steps = 59, loss = 3.097674608230591\n",
      "steps = 59, loss = 2.5068306922912598\n",
      "steps = 59, loss = 5.396451473236084\n",
      "steps = 59, loss = 2.6865313053131104\n",
      "steps = 59, loss = 2.917712688446045\n",
      "steps = 59, loss = 7.0228447914123535\n",
      "steps = 59, loss = 3.4325079917907715\n",
      "steps = 59, loss = 2.7907016277313232\n",
      "steps = 59, loss = 2.364732027053833\n",
      "steps = 59, loss = 49.9853630065918\n",
      "steps = 59, loss = 2.750410795211792\n",
      "steps = 59, loss = 2.671298027038574\n",
      "steps = 59, loss = 2.8397412300109863\n",
      "steps = 59, loss = 3.2556684017181396\n",
      "steps = 59, loss = 2.631204605102539\n",
      "steps = 59, loss = 3.4643149375915527\n",
      "steps = 59, loss = 3.7320101261138916\n",
      "steps = 59, loss = 2.7257368564605713\n",
      "steps = 60, loss = 2.4290802478790283\n",
      "steps = 60, loss = 2.886669158935547\n",
      "steps = 60, loss = 3.6164135932922363\n",
      "steps = 60, loss = 2.963627338409424\n",
      "steps = 60, loss = 2.761392831802368\n",
      "steps = 60, loss = 7.288426399230957\n",
      "steps = 60, loss = 5.518108367919922\n",
      "steps = 60, loss = 49.9853630065918\n",
      "steps = 60, loss = 2.5472049713134766\n",
      "steps = 60, loss = 3.3128323554992676\n",
      "steps = 60, loss = 3.559359312057495\n",
      "steps = 60, loss = 2.262600898742676\n",
      "steps = 60, loss = 3.58308744430542\n",
      "steps = 60, loss = 3.0333056449890137\n",
      "steps = 60, loss = 3.2650387287139893\n",
      "steps = 60, loss = 3.134093999862671\n",
      "steps = 60, loss = 2.5985281467437744\n",
      "steps = 60, loss = 2.3854241371154785\n",
      "steps = 60, loss = 3.1563405990600586\n",
      "steps = 60, loss = 2.9481258392333984\n",
      "steps = 60, loss = 3.394969940185547\n",
      "steps = 60, loss = 2.8775675296783447\n",
      "steps = 60, loss = 2.681431293487549\n",
      "steps = 60, loss = 2.9352221488952637\n",
      "steps = 60, loss = 2.720287799835205\n",
      "steps = 60, loss = 2.091329336166382\n",
      "steps = 60, loss = 3.531602621078491\n",
      "steps = 60, loss = 2.586667776107788\n",
      "steps = 60, loss = 3.4633588790893555\n",
      "steps = 61, loss = 11.03502368927002\n",
      "steps = 61, loss = 3.6266090869903564\n",
      "steps = 61, loss = 2.8922317028045654\n",
      "steps = 61, loss = 6.871798515319824\n",
      "steps = 61, loss = 2.3650312423706055\n",
      "steps = 61, loss = 3.0266382694244385\n",
      "steps = 61, loss = 2.9530935287475586\n",
      "steps = 61, loss = 2.8608508110046387\n",
      "steps = 61, loss = 3.4987568855285645\n",
      "steps = 61, loss = 3.5814383029937744\n",
      "steps = 61, loss = 3.3493237495422363\n",
      "steps = 61, loss = 5.59712028503418\n",
      "steps = 61, loss = 7.714179039001465\n",
      "steps = 61, loss = 2.9739272594451904\n",
      "steps = 61, loss = 3.3796334266662598\n",
      "steps = 61, loss = 2.590987205505371\n",
      "steps = 61, loss = 2.6955158710479736\n",
      "steps = 61, loss = 2.7024993896484375\n",
      "steps = 61, loss = 2.7429111003875732\n",
      "steps = 61, loss = 3.1236209869384766\n",
      "steps = 61, loss = 3.1627771854400635\n",
      "steps = 61, loss = 2.395261526107788\n",
      "steps = 61, loss = 3.50413179397583\n",
      "steps = 61, loss = 3.005510091781616\n",
      "steps = 61, loss = 2.6895337104797363\n",
      "steps = 61, loss = 2.1595606803894043\n",
      "steps = 61, loss = 2.266233444213867\n",
      "steps = 61, loss = 3.3299880027770996\n",
      "steps = 61, loss = 49.9853630065918\n",
      "steps = 62, loss = 5.609557151794434\n",
      "steps = 62, loss = 7.184773921966553\n",
      "steps = 62, loss = 2.521824359893799\n",
      "steps = 62, loss = 3.00602650642395\n",
      "steps = 62, loss = 3.3720133304595947\n",
      "steps = 62, loss = 2.5637900829315186\n",
      "steps = 62, loss = 2.0711917877197266\n",
      "steps = 62, loss = 2.9531078338623047\n",
      "steps = 62, loss = 2.6788127422332764\n",
      "steps = 62, loss = 2.900467872619629\n",
      "steps = 62, loss = 3.2751357555389404\n",
      "steps = 62, loss = 2.8397908210754395\n",
      "steps = 62, loss = 3.014131784439087\n",
      "steps = 62, loss = 3.77703595161438\n",
      "steps = 62, loss = 3.2669708728790283\n",
      "steps = 62, loss = 2.1716623306274414\n",
      "steps = 62, loss = 2.391108989715576\n",
      "steps = 62, loss = 49.9853630065918\n",
      "steps = 62, loss = 4.07401180267334\n",
      "steps = 62, loss = 2.687805652618408\n",
      "steps = 62, loss = 2.123188018798828\n",
      "steps = 62, loss = 3.1625816822052\n",
      "steps = 62, loss = 2.545654535293579\n",
      "steps = 62, loss = 2.9798476696014404\n",
      "steps = 62, loss = 3.361293077468872\n",
      "steps = 62, loss = 3.262648582458496\n",
      "steps = 62, loss = 2.7275989055633545\n",
      "steps = 62, loss = 4.782081127166748\n",
      "steps = 62, loss = 3.4964590072631836\n",
      "steps = 63, loss = 2.6275646686553955\n",
      "steps = 63, loss = 2.9863998889923096\n",
      "steps = 63, loss = 4.058592796325684\n",
      "steps = 63, loss = 2.13970947265625\n",
      "steps = 63, loss = 2.720839262008667\n",
      "steps = 63, loss = 2.877516031265259\n",
      "steps = 63, loss = 5.722207069396973\n",
      "steps = 63, loss = 2.6330795288085938\n",
      "steps = 63, loss = 3.5644822120666504\n",
      "steps = 63, loss = 2.3257408142089844\n",
      "steps = 63, loss = 2.700051784515381\n",
      "steps = 63, loss = 2.7737839221954346\n",
      "steps = 63, loss = 3.554337501525879\n",
      "steps = 63, loss = 2.963491678237915\n",
      "steps = 63, loss = 3.1913492679595947\n",
      "steps = 63, loss = 3.293178081512451\n",
      "steps = 63, loss = 3.3262741565704346\n",
      "steps = 63, loss = 3.031083583831787\n",
      "steps = 63, loss = 3.393853187561035\n",
      "steps = 63, loss = 2.9138028621673584\n",
      "steps = 63, loss = 49.9853630065918\n",
      "steps = 63, loss = 3.0056564807891846\n",
      "steps = 63, loss = 2.265742540359497\n",
      "steps = 63, loss = 2.411569595336914\n",
      "steps = 63, loss = 3.5504770278930664\n",
      "steps = 63, loss = 2.8724350929260254\n",
      "steps = 63, loss = 3.355229377746582\n",
      "steps = 63, loss = 2.0411083698272705\n",
      "steps = 63, loss = 6.94705867767334\n",
      "steps = 64, loss = 49.9853630065918\n",
      "steps = 64, loss = 2.140258550643921\n",
      "steps = 64, loss = 2.835597276687622\n",
      "steps = 64, loss = 3.5841407775878906\n",
      "steps = 64, loss = 3.007892370223999\n",
      "steps = 64, loss = 3.434061288833618\n",
      "steps = 64, loss = 2.7590548992156982\n",
      "steps = 64, loss = 2.5777957439422607\n",
      "steps = 64, loss = 6.799905300140381\n",
      "steps = 64, loss = 2.046672821044922\n",
      "steps = 64, loss = 3.339650869369507\n",
      "steps = 64, loss = 2.5946638584136963\n",
      "steps = 64, loss = 2.580723762512207\n",
      "steps = 64, loss = 3.0139787197113037\n",
      "steps = 64, loss = 5.748314380645752\n",
      "steps = 64, loss = 3.3719301223754883\n",
      "steps = 64, loss = 2.724881649017334\n",
      "steps = 64, loss = 3.853996515274048\n",
      "steps = 64, loss = 2.4079225063323975\n",
      "steps = 64, loss = 2.4150848388671875\n",
      "steps = 64, loss = 2.0718507766723633\n",
      "steps = 64, loss = 2.8416035175323486\n",
      "steps = 64, loss = 2.69065523147583\n",
      "steps = 64, loss = 2.9868223667144775\n",
      "steps = 64, loss = 3.689309597015381\n",
      "steps = 64, loss = 3.1867597103118896\n",
      "steps = 64, loss = 3.3419556617736816\n",
      "steps = 64, loss = 2.27116060256958\n",
      "steps = 64, loss = 3.365722417831421\n",
      "steps = 65, loss = 7.000542640686035\n",
      "steps = 65, loss = 4.407445430755615\n",
      "steps = 65, loss = 2.428330898284912\n",
      "steps = 65, loss = 3.2151713371276855\n",
      "steps = 65, loss = 2.757269859313965\n",
      "steps = 65, loss = 2.27298903465271\n",
      "steps = 65, loss = 2.908935070037842\n",
      "steps = 65, loss = 3.303469181060791\n",
      "steps = 65, loss = 3.7269983291625977\n",
      "steps = 65, loss = 2.7140989303588867\n",
      "steps = 65, loss = 2.8785126209259033\n",
      "steps = 65, loss = 3.288501024246216\n",
      "steps = 65, loss = 3.3547568321228027\n",
      "steps = 65, loss = 3.0198798179626465\n",
      "steps = 65, loss = 2.5324389934539795\n",
      "steps = 65, loss = 3.030269145965576\n",
      "steps = 65, loss = 2.9987332820892334\n",
      "steps = 65, loss = 2.1286001205444336\n",
      "steps = 65, loss = 49.9853630065918\n",
      "steps = 65, loss = 2.236774206161499\n",
      "steps = 65, loss = 2.6689255237579346\n",
      "steps = 65, loss = 3.4601709842681885\n",
      "steps = 65, loss = 2.7217113971710205\n",
      "steps = 65, loss = 5.852781295776367\n",
      "steps = 65, loss = 3.5540027618408203\n",
      "steps = 65, loss = 3.033921480178833\n",
      "steps = 65, loss = 2.5071582794189453\n",
      "steps = 65, loss = 3.0192947387695312\n",
      "steps = 65, loss = 2.747375249862671\n",
      "steps = 66, loss = 6.729287147521973\n",
      "steps = 66, loss = 49.9853630065918\n",
      "steps = 66, loss = 3.3135900497436523\n",
      "steps = 66, loss = 5.923345565795898\n",
      "steps = 66, loss = 3.2206878662109375\n",
      "steps = 66, loss = 2.929884433746338\n",
      "steps = 66, loss = 4.226733684539795\n",
      "steps = 66, loss = 3.3747870922088623\n",
      "steps = 66, loss = 3.0377001762390137\n",
      "steps = 66, loss = 3.274646282196045\n",
      "steps = 66, loss = 3.0439162254333496\n",
      "steps = 66, loss = 3.2586171627044678\n",
      "steps = 66, loss = 2.437926769256592\n",
      "steps = 66, loss = 2.3214480876922607\n",
      "steps = 66, loss = 3.3464455604553223\n",
      "steps = 66, loss = 2.5806069374084473\n",
      "steps = 66, loss = 2.703277587890625\n",
      "steps = 66, loss = 1.9684624671936035\n",
      "steps = 66, loss = 2.982614517211914\n",
      "steps = 66, loss = 2.756131172180176\n",
      "steps = 66, loss = 3.5952632427215576\n",
      "steps = 66, loss = 2.283550262451172\n",
      "steps = 66, loss = 2.860079050064087\n",
      "steps = 66, loss = 3.2375521659851074\n",
      "steps = 66, loss = 2.6648974418640137\n",
      "steps = 66, loss = 2.7573721408843994\n",
      "steps = 66, loss = 2.937140941619873\n",
      "steps = 66, loss = 3.0233237743377686\n",
      "steps = 66, loss = 3.5615792274475098\n",
      "steps = 67, loss = 3.026873826980591\n",
      "steps = 67, loss = 3.0289506912231445\n",
      "steps = 67, loss = 3.641813278198242\n",
      "steps = 67, loss = 3.329756736755371\n",
      "steps = 67, loss = 2.271865129470825\n",
      "steps = 67, loss = 3.6461379528045654\n",
      "steps = 67, loss = 2.7044270038604736\n",
      "steps = 67, loss = 2.220238208770752\n",
      "steps = 67, loss = 2.989414930343628\n",
      "steps = 67, loss = 2.928990364074707\n",
      "steps = 67, loss = 5.974895477294922\n",
      "steps = 67, loss = 3.582509756088257\n",
      "steps = 67, loss = 2.720811128616333\n",
      "steps = 67, loss = 2.447874069213867\n",
      "steps = 67, loss = 2.87780499458313\n",
      "steps = 67, loss = 2.6294100284576416\n",
      "steps = 67, loss = 2.4930381774902344\n",
      "steps = 67, loss = 3.8291537761688232\n",
      "steps = 67, loss = 3.542675018310547\n",
      "steps = 67, loss = 3.346737861633301\n",
      "steps = 67, loss = 3.05692458152771\n",
      "steps = 67, loss = 2.708850383758545\n",
      "steps = 67, loss = 6.439640998840332\n",
      "steps = 67, loss = 3.0630548000335693\n",
      "steps = 67, loss = 3.266092300415039\n",
      "steps = 67, loss = 49.9853630065918\n",
      "steps = 67, loss = 2.7638347148895264\n",
      "steps = 67, loss = 50.005916595458984\n",
      "steps = 67, loss = 3.238440752029419\n",
      "steps = 68, loss = 6.866631984710693\n",
      "steps = 68, loss = 2.4910900592803955\n",
      "steps = 68, loss = 2.788583993911743\n",
      "steps = 68, loss = 2.7371301651000977\n",
      "steps = 68, loss = 3.21596360206604\n",
      "steps = 68, loss = 3.4642233848571777\n",
      "steps = 68, loss = 2.455915689468384\n",
      "steps = 68, loss = 2.7035961151123047\n",
      "steps = 68, loss = 2.919163942337036\n",
      "steps = 68, loss = 3.348731756210327\n",
      "steps = 68, loss = 3.073049545288086\n",
      "steps = 68, loss = 3.3972959518432617\n",
      "steps = 68, loss = 3.947065830230713\n",
      "steps = 68, loss = 6.168979167938232\n",
      "steps = 68, loss = 2.7561943531036377\n",
      "steps = 68, loss = 3.591789722442627\n",
      "steps = 68, loss = 3.475951671600342\n",
      "steps = 68, loss = 3.6724822521209717\n",
      "steps = 68, loss = 3.071977138519287\n",
      "steps = 68, loss = 2.861926555633545\n",
      "steps = 68, loss = 49.9853630065918\n",
      "steps = 68, loss = 2.8600637912750244\n",
      "steps = 68, loss = 2.9923503398895264\n",
      "steps = 68, loss = 3.2430615425109863\n",
      "steps = 68, loss = 3.553649663925171\n",
      "steps = 68, loss = 6.03695821762085\n",
      "steps = 68, loss = 2.257432222366333\n",
      "steps = 68, loss = 3.0216619968414307\n",
      "steps = 68, loss = 2.2104382514953613\n",
      "steps = 69, loss = 3.113717794418335\n",
      "steps = 69, loss = 7.181641578674316\n",
      "steps = 69, loss = 3.5772006511688232\n",
      "steps = 69, loss = 2.4290173053741455\n",
      "steps = 69, loss = 1.8627917766571045\n",
      "steps = 69, loss = 3.2428371906280518\n",
      "steps = 69, loss = 2.2175002098083496\n",
      "steps = 69, loss = 2.1296274662017822\n",
      "steps = 69, loss = 3.613287925720215\n",
      "steps = 69, loss = 2.7352821826934814\n",
      "steps = 69, loss = 3.6542177200317383\n",
      "steps = 69, loss = 2.757966995239258\n",
      "steps = 69, loss = 3.0722270011901855\n",
      "steps = 69, loss = 2.6554718017578125\n",
      "steps = 69, loss = 3.3413071632385254\n",
      "steps = 69, loss = 2.6879348754882812\n",
      "steps = 69, loss = 2.7040112018585205\n",
      "steps = 69, loss = 3.2287752628326416\n",
      "steps = 69, loss = 2.80037260055542\n",
      "steps = 69, loss = 2.8384625911712646\n",
      "steps = 69, loss = 3.0778145790100098\n",
      "steps = 69, loss = 6.059062480926514\n",
      "steps = 69, loss = 2.615102529525757\n",
      "steps = 69, loss = 3.009136199951172\n",
      "steps = 69, loss = 49.9853630065918\n",
      "steps = 69, loss = 2.450584650039673\n",
      "steps = 69, loss = 3.5144314765930176\n",
      "steps = 69, loss = 2.8487167358398438\n",
      "steps = 69, loss = 3.3642735481262207\n",
      "steps = 70, loss = 2.6217257976531982\n",
      "steps = 70, loss = 3.4984419345855713\n",
      "steps = 70, loss = 2.721773624420166\n",
      "steps = 70, loss = 3.3604278564453125\n",
      "steps = 70, loss = 6.140327453613281\n",
      "steps = 70, loss = 2.752781629562378\n",
      "steps = 70, loss = 3.495584726333618\n",
      "steps = 70, loss = 2.2758829593658447\n",
      "steps = 70, loss = 3.2709059715270996\n",
      "steps = 70, loss = 2.630734443664551\n",
      "steps = 70, loss = 2.7633912563323975\n",
      "steps = 70, loss = 3.3027138710021973\n",
      "steps = 70, loss = 2.936441421508789\n",
      "steps = 70, loss = 3.01304030418396\n",
      "steps = 70, loss = 3.543677568435669\n",
      "steps = 70, loss = 2.8769969940185547\n",
      "steps = 70, loss = 3.480177402496338\n",
      "steps = 70, loss = 6.83951473236084\n",
      "steps = 70, loss = 3.0251317024230957\n",
      "steps = 70, loss = 3.103701114654541\n",
      "steps = 70, loss = 2.4701263904571533\n",
      "steps = 70, loss = 3.105651617050171\n",
      "steps = 70, loss = 3.256089448928833\n",
      "steps = 70, loss = 3.232081413269043\n",
      "steps = 70, loss = 2.1937296390533447\n",
      "steps = 70, loss = 2.069199562072754\n",
      "steps = 70, loss = 2.8613715171813965\n",
      "steps = 70, loss = 49.9853630065918\n",
      "steps = 70, loss = 3.0407159328460693\n",
      "steps = 71, loss = 3.791841983795166\n",
      "steps = 71, loss = 2.566765069961548\n",
      "steps = 71, loss = 2.859337091445923\n",
      "steps = 71, loss = 3.022244930267334\n",
      "steps = 71, loss = 3.602905750274658\n",
      "steps = 71, loss = 3.1136035919189453\n",
      "steps = 71, loss = 3.2760322093963623\n",
      "steps = 71, loss = 2.9684693813323975\n",
      "steps = 71, loss = 2.568798542022705\n",
      "steps = 71, loss = 3.0187063217163086\n",
      "steps = 71, loss = 6.18455171585083\n",
      "steps = 71, loss = 3.0708885192871094\n",
      "steps = 71, loss = 2.704468250274658\n",
      "steps = 71, loss = 2.060905694961548\n",
      "steps = 71, loss = 3.612961530685425\n",
      "steps = 71, loss = 3.2054622173309326\n",
      "steps = 71, loss = 2.8427364826202393\n",
      "steps = 71, loss = 3.3776025772094727\n",
      "steps = 71, loss = 2.4791760444641113\n",
      "steps = 71, loss = 3.3058981895446777\n",
      "steps = 71, loss = 3.458320379257202\n",
      "steps = 71, loss = 49.9853630065918\n",
      "steps = 71, loss = 2.1399409770965576\n",
      "steps = 71, loss = 6.715888977050781\n",
      "steps = 71, loss = 2.652982234954834\n",
      "steps = 71, loss = 3.1229617595672607\n",
      "steps = 71, loss = 2.7592570781707764\n",
      "steps = 71, loss = 2.7540602684020996\n",
      "steps = 71, loss = 2.2696990966796875\n",
      "steps = 72, loss = 2.8354597091674805\n",
      "steps = 72, loss = 2.3960068225860596\n",
      "steps = 72, loss = 1.967673897743225\n",
      "steps = 72, loss = 3.276134967803955\n",
      "steps = 72, loss = 3.398895263671875\n",
      "steps = 72, loss = 2.7405011653900146\n",
      "steps = 72, loss = 3.343193531036377\n",
      "steps = 72, loss = 2.6828935146331787\n",
      "steps = 72, loss = 3.510477304458618\n",
      "steps = 72, loss = 6.87058162689209\n",
      "steps = 72, loss = 2.126131057739258\n",
      "steps = 72, loss = 6.10913610458374\n",
      "steps = 72, loss = 3.2005794048309326\n",
      "steps = 72, loss = 2.744229793548584\n",
      "steps = 72, loss = 3.1223526000976562\n",
      "steps = 72, loss = 3.360665798187256\n",
      "steps = 72, loss = 2.4740004539489746\n",
      "steps = 72, loss = 3.2018415927886963\n",
      "steps = 72, loss = 2.947204113006592\n",
      "steps = 72, loss = 3.119983673095703\n",
      "steps = 72, loss = 2.171173572540283\n",
      "steps = 72, loss = 2.6884219646453857\n",
      "steps = 72, loss = 2.8383076190948486\n",
      "steps = 72, loss = 2.4253029823303223\n",
      "steps = 72, loss = 3.006657123565674\n",
      "steps = 72, loss = 49.9853630065918\n",
      "steps = 72, loss = 3.0282979011535645\n",
      "steps = 72, loss = 3.322540283203125\n",
      "steps = 72, loss = 2.7012875080108643\n",
      "steps = 73, loss = 3.3037118911743164\n",
      "steps = 73, loss = 3.6175477504730225\n",
      "steps = 73, loss = 2.282777786254883\n",
      "steps = 73, loss = 49.9853630065918\n",
      "steps = 73, loss = 2.585256814956665\n",
      "steps = 73, loss = 2.64082407951355\n",
      "steps = 73, loss = 2.382657766342163\n",
      "steps = 73, loss = 2.740088701248169\n",
      "steps = 73, loss = 2.866384267807007\n",
      "steps = 73, loss = 6.425825595855713\n",
      "steps = 73, loss = 3.146085500717163\n",
      "steps = 73, loss = 3.0223641395568848\n",
      "steps = 73, loss = 2.8765852451324463\n",
      "steps = 73, loss = 6.153599739074707\n",
      "steps = 73, loss = 3.3218603134155273\n",
      "steps = 73, loss = 3.156615734100342\n",
      "steps = 73, loss = 3.5647265911102295\n",
      "steps = 73, loss = 2.4935038089752197\n",
      "steps = 73, loss = 2.161936044692993\n",
      "steps = 73, loss = 3.100449800491333\n",
      "steps = 73, loss = 2.9394397735595703\n",
      "steps = 73, loss = 3.301964044570923\n",
      "steps = 73, loss = 3.3167874813079834\n",
      "steps = 73, loss = 3.1561458110809326\n",
      "steps = 73, loss = 2.7223355770111084\n",
      "steps = 73, loss = 3.433181047439575\n",
      "steps = 73, loss = 3.4047043323516846\n",
      "steps = 73, loss = 2.2768123149871826\n",
      "steps = 73, loss = 2.7585504055023193\n",
      "steps = 74, loss = 12.592546463012695\n",
      "steps = 74, loss = 3.299142360687256\n",
      "steps = 74, loss = 3.1484756469726562\n",
      "steps = 74, loss = 2.7998037338256836\n",
      "steps = 74, loss = 2.156956911087036\n",
      "steps = 74, loss = 5.9738054275512695\n",
      "steps = 74, loss = 2.648790121078491\n",
      "steps = 74, loss = 3.279120445251465\n",
      "steps = 74, loss = 3.1557321548461914\n",
      "steps = 74, loss = 3.006197452545166\n",
      "steps = 74, loss = 2.134819507598877\n",
      "steps = 74, loss = 2.8401780128479004\n",
      "steps = 74, loss = 49.9853630065918\n",
      "steps = 74, loss = 2.709906578063965\n",
      "steps = 74, loss = 3.521825075149536\n",
      "steps = 74, loss = 6.643322944641113\n",
      "steps = 74, loss = 3.299391031265259\n",
      "steps = 74, loss = 3.2545652389526367\n",
      "steps = 74, loss = 2.953561782836914\n",
      "steps = 74, loss = 2.0050134658813477\n",
      "steps = 74, loss = 2.6910488605499268\n",
      "steps = 74, loss = 3.119875192642212\n",
      "steps = 74, loss = 3.6455912590026855\n",
      "steps = 74, loss = 3.3263275623321533\n",
      "steps = 74, loss = 2.010442018508911\n",
      "steps = 74, loss = 3.4158644676208496\n",
      "steps = 74, loss = 2.4889957904815674\n",
      "steps = 74, loss = 2.4244801998138428\n",
      "steps = 74, loss = 2.563199281692505\n",
      "steps = 75, loss = 2.5615153312683105\n",
      "steps = 75, loss = 3.188453435897827\n",
      "steps = 75, loss = 3.359896659851074\n",
      "steps = 75, loss = 3.5763163566589355\n",
      "steps = 75, loss = 2.122546672821045\n",
      "steps = 75, loss = 5.992232322692871\n",
      "steps = 75, loss = 2.2819199562072754\n",
      "steps = 75, loss = 2.5074191093444824\n",
      "steps = 75, loss = 2.8580989837646484\n",
      "steps = 75, loss = 3.3182976245880127\n",
      "steps = 75, loss = 3.344681739807129\n",
      "steps = 75, loss = 2.107391595840454\n",
      "steps = 75, loss = 3.0142040252685547\n",
      "steps = 75, loss = 2.7049577236175537\n",
      "steps = 75, loss = 2.266746997833252\n",
      "steps = 75, loss = 2.74735951423645\n",
      "steps = 75, loss = 3.6053264141082764\n",
      "steps = 75, loss = 3.410165309906006\n",
      "steps = 75, loss = 2.864790201187134\n",
      "steps = 75, loss = 3.367563247680664\n",
      "steps = 75, loss = 49.9853630065918\n",
      "steps = 75, loss = 50.02783203125\n",
      "steps = 75, loss = 2.5987536907196045\n",
      "steps = 75, loss = 3.1800320148468018\n",
      "steps = 75, loss = 2.534904956817627\n",
      "steps = 75, loss = 2.7259089946746826\n",
      "steps = 75, loss = 3.168346881866455\n",
      "steps = 75, loss = 2.5427870750427246\n",
      "steps = 75, loss = 3.292759895324707\n",
      "steps = 76, loss = 3.442270517349243\n",
      "steps = 76, loss = 2.762040138244629\n",
      "steps = 76, loss = 2.875883102416992\n",
      "steps = 76, loss = 3.560661554336548\n",
      "steps = 76, loss = 3.3365869522094727\n",
      "steps = 76, loss = 3.6046841144561768\n",
      "steps = 76, loss = 2.6314680576324463\n",
      "steps = 76, loss = 49.9853630065918\n",
      "steps = 76, loss = 2.6526222229003906\n",
      "steps = 76, loss = 2.5175437927246094\n",
      "steps = 76, loss = 3.016415596008301\n",
      "steps = 76, loss = 2.2736244201660156\n",
      "steps = 76, loss = 3.1891586780548096\n",
      "steps = 76, loss = 6.063289165496826\n",
      "steps = 76, loss = 3.367666721343994\n",
      "steps = 76, loss = 2.2409186363220215\n",
      "steps = 76, loss = 2.767615556716919\n",
      "steps = 76, loss = 3.4979190826416016\n",
      "steps = 76, loss = 3.1136889457702637\n",
      "steps = 76, loss = 2.0537376403808594\n",
      "steps = 76, loss = 3.2087996006011963\n",
      "steps = 76, loss = 2.724219560623169\n",
      "steps = 76, loss = 2.7221884727478027\n",
      "steps = 76, loss = 2.923841714859009\n",
      "steps = 76, loss = 3.020165205001831\n",
      "steps = 76, loss = 3.4629786014556885\n",
      "steps = 76, loss = 50.0261344909668\n",
      "steps = 76, loss = 2.1583073139190674\n",
      "steps = 76, loss = 3.3998348712921143\n",
      "steps = 77, loss = 2.525243043899536\n",
      "steps = 77, loss = 2.793386459350586\n",
      "steps = 77, loss = 3.1116979122161865\n",
      "steps = 77, loss = 2.7289319038391113\n",
      "steps = 77, loss = 3.59563946723938\n",
      "steps = 77, loss = 6.15315580368042\n",
      "steps = 77, loss = 3.656153678894043\n",
      "steps = 77, loss = 3.3613150119781494\n",
      "steps = 77, loss = 49.960880279541016\n",
      "steps = 77, loss = 3.340653896331787\n",
      "steps = 77, loss = 2.858757257461548\n",
      "steps = 77, loss = 2.0491132736206055\n",
      "steps = 77, loss = 3.224987745285034\n",
      "steps = 77, loss = 3.198305368423462\n",
      "steps = 77, loss = 2.6549417972564697\n",
      "steps = 77, loss = 3.256938934326172\n",
      "steps = 77, loss = 2.7485408782958984\n",
      "steps = 77, loss = 2.5327560901641846\n",
      "steps = 77, loss = 2.167649745941162\n",
      "steps = 77, loss = 49.9853630065918\n",
      "steps = 77, loss = 2.8514797687530518\n",
      "steps = 77, loss = 2.7513904571533203\n",
      "steps = 77, loss = 3.0132782459259033\n",
      "steps = 77, loss = 3.3934473991394043\n",
      "steps = 77, loss = 3.6255176067352295\n",
      "steps = 77, loss = 2.2456095218658447\n",
      "steps = 77, loss = 3.536101818084717\n",
      "steps = 77, loss = 2.705369234085083\n",
      "steps = 77, loss = 2.107304334640503\n",
      "steps = 78, loss = 49.9853630065918\n",
      "steps = 78, loss = 2.8816325664520264\n",
      "steps = 78, loss = 2.4415156841278076\n",
      "steps = 78, loss = 2.519320249557495\n",
      "steps = 78, loss = 3.075922727584839\n",
      "steps = 78, loss = 3.524508476257324\n",
      "steps = 78, loss = 3.223811626434326\n",
      "steps = 78, loss = 3.204434633255005\n",
      "steps = 78, loss = 49.9529914855957\n",
      "steps = 78, loss = 2.8375298976898193\n",
      "steps = 78, loss = 2.031622886657715\n",
      "steps = 78, loss = 3.0014753341674805\n",
      "steps = 78, loss = 2.91770076751709\n",
      "steps = 78, loss = 3.0974788665771484\n",
      "steps = 78, loss = 3.3390064239501953\n",
      "steps = 78, loss = 2.575686454772949\n",
      "steps = 78, loss = 2.731078624725342\n",
      "steps = 78, loss = 3.670262336730957\n",
      "steps = 78, loss = 3.2599971294403076\n",
      "steps = 78, loss = 3.572998046875\n",
      "steps = 78, loss = 3.4021646976470947\n",
      "steps = 78, loss = 2.075124740600586\n",
      "steps = 78, loss = 2.7417216300964355\n",
      "steps = 78, loss = 5.508689880371094\n",
      "steps = 78, loss = 2.121593713760376\n",
      "steps = 78, loss = 2.577502489089966\n",
      "steps = 78, loss = 3.341294050216675\n",
      "steps = 78, loss = 2.6882591247558594\n",
      "steps = 78, loss = 3.2073843479156494\n",
      "steps = 79, loss = 3.3518102169036865\n",
      "steps = 79, loss = 2.7226061820983887\n",
      "steps = 79, loss = 2.8395090103149414\n",
      "steps = 79, loss = 3.1192984580993652\n",
      "steps = 79, loss = 5.782959938049316\n",
      "steps = 79, loss = 2.0253353118896484\n",
      "steps = 79, loss = 3.0647776126861572\n",
      "steps = 79, loss = 2.422452688217163\n",
      "steps = 79, loss = 3.2383363246917725\n",
      "steps = 79, loss = 2.6193766593933105\n",
      "steps = 79, loss = 3.369293451309204\n",
      "steps = 79, loss = 2.9414384365081787\n",
      "steps = 79, loss = 2.7426977157592773\n",
      "steps = 79, loss = 3.0003578662872314\n",
      "steps = 79, loss = 3.232769250869751\n",
      "steps = 79, loss = 49.9853630065918\n",
      "steps = 79, loss = 2.6906473636627197\n",
      "steps = 79, loss = 2.137421131134033\n",
      "steps = 79, loss = 3.525418996810913\n",
      "steps = 79, loss = 3.384028673171997\n",
      "steps = 79, loss = 2.5239412784576416\n",
      "steps = 79, loss = 3.2716832160949707\n",
      "steps = 79, loss = 50.014244079589844\n",
      "steps = 79, loss = 2.418353319168091\n",
      "steps = 79, loss = 3.649366617202759\n",
      "steps = 79, loss = 3.216989278793335\n",
      "steps = 79, loss = 2.6975646018981934\n",
      "steps = 79, loss = 2.0848238468170166\n",
      "steps = 79, loss = 3.3524298667907715\n",
      "steps = 80, loss = 3.2721056938171387\n",
      "steps = 80, loss = 49.9853630065918\n",
      "steps = 80, loss = 2.2850747108459473\n",
      "steps = 80, loss = 2.8575379848480225\n",
      "steps = 80, loss = 2.5771546363830566\n",
      "steps = 80, loss = 2.0739266872406006\n",
      "steps = 80, loss = 2.743878126144409\n",
      "steps = 80, loss = 3.2382004261016846\n",
      "steps = 80, loss = 3.520768642425537\n",
      "steps = 80, loss = 2.8396966457366943\n",
      "steps = 80, loss = 2.5027284622192383\n",
      "steps = 80, loss = 2.6628262996673584\n",
      "steps = 80, loss = 3.418407440185547\n",
      "steps = 80, loss = 2.790266275405884\n",
      "steps = 80, loss = 3.3106002807617188\n",
      "steps = 80, loss = 2.755523204803467\n",
      "steps = 80, loss = 5.905546188354492\n",
      "steps = 80, loss = 2.93109393119812\n",
      "steps = 80, loss = 3.008873701095581\n",
      "steps = 80, loss = 2.061647653579712\n",
      "steps = 80, loss = 3.425842761993408\n",
      "steps = 80, loss = 2.5429773330688477\n",
      "steps = 80, loss = 3.2430167198181152\n",
      "steps = 80, loss = 3.455225944519043\n",
      "steps = 80, loss = 3.42464280128479\n",
      "steps = 80, loss = 3.3704140186309814\n",
      "steps = 80, loss = 2.706303358078003\n",
      "steps = 80, loss = 50.00918197631836\n",
      "steps = 80, loss = 3.596827507019043\n",
      "steps = 81, loss = 2.2309913635253906\n",
      "steps = 81, loss = 3.3464033603668213\n",
      "steps = 81, loss = 3.6128621101379395\n",
      "steps = 81, loss = 3.4215924739837646\n",
      "steps = 81, loss = 2.7228450775146484\n",
      "steps = 81, loss = 2.7652502059936523\n",
      "steps = 81, loss = 2.8745577335357666\n",
      "steps = 81, loss = 2.8532965183258057\n",
      "steps = 81, loss = 3.372450113296509\n",
      "steps = 81, loss = 2.5528738498687744\n",
      "steps = 81, loss = 3.0679430961608887\n",
      "steps = 81, loss = 2.6898858547210693\n",
      "steps = 81, loss = 50.01015090942383\n",
      "steps = 81, loss = 2.8891258239746094\n",
      "steps = 81, loss = 6.058984279632568\n",
      "steps = 81, loss = 3.539008855819702\n",
      "steps = 81, loss = 49.9853630065918\n",
      "steps = 81, loss = 2.2713282108306885\n",
      "steps = 81, loss = 2.6091597080230713\n",
      "steps = 81, loss = 3.3884122371673584\n",
      "steps = 81, loss = 2.6314682960510254\n",
      "steps = 81, loss = 3.259113073348999\n",
      "steps = 81, loss = 3.3801910877227783\n",
      "steps = 81, loss = 3.0147297382354736\n",
      "steps = 81, loss = 2.9262688159942627\n",
      "steps = 81, loss = 49.56186294555664\n",
      "steps = 81, loss = 3.292940855026245\n",
      "steps = 81, loss = 3.2849371433258057\n",
      "steps = 81, loss = 3.427968978881836\n",
      "steps = 82, loss = 3.008329391479492\n",
      "steps = 82, loss = 2.8582510948181152\n",
      "steps = 82, loss = 3.3924343585968018\n",
      "steps = 82, loss = 2.788172960281372\n",
      "steps = 82, loss = 3.5980122089385986\n",
      "steps = 82, loss = 2.2218613624572754\n",
      "steps = 82, loss = 3.1894102096557617\n",
      "steps = 82, loss = 2.736659049987793\n",
      "steps = 82, loss = 3.286381721496582\n",
      "steps = 82, loss = 3.1083061695098877\n",
      "steps = 82, loss = 2.2282872200012207\n",
      "steps = 82, loss = 3.545090675354004\n",
      "steps = 82, loss = 5.679436683654785\n",
      "steps = 82, loss = 3.267634153366089\n",
      "steps = 82, loss = 3.272024393081665\n",
      "steps = 82, loss = 49.9853630065918\n",
      "steps = 82, loss = 2.3949029445648193\n",
      "steps = 82, loss = 3.3093321323394775\n",
      "steps = 82, loss = 20.255117416381836\n",
      "steps = 82, loss = 2.706444263458252\n",
      "steps = 82, loss = 49.98916244506836\n",
      "steps = 82, loss = 2.9279074668884277\n",
      "steps = 82, loss = 3.4826974868774414\n",
      "steps = 82, loss = 2.560370922088623\n",
      "steps = 82, loss = 3.619866371154785\n",
      "steps = 82, loss = 3.3486440181732178\n",
      "steps = 82, loss = 2.553652286529541\n",
      "steps = 82, loss = 2.7523131370544434\n",
      "steps = 82, loss = 3.6107685565948486\n",
      "steps = 83, loss = 3.3280746936798096\n",
      "steps = 83, loss = 2.76043438911438\n",
      "steps = 83, loss = 2.946180582046509\n",
      "steps = 83, loss = 3.288081169128418\n",
      "steps = 83, loss = 3.4100940227508545\n",
      "steps = 83, loss = 2.6532952785491943\n",
      "steps = 83, loss = 2.705418348312378\n",
      "steps = 83, loss = 3.484191417694092\n",
      "steps = 83, loss = 5.876994609832764\n",
      "steps = 83, loss = 3.350649118423462\n",
      "steps = 83, loss = 2.046046257019043\n",
      "steps = 83, loss = 3.2670695781707764\n",
      "steps = 83, loss = 3.013350009918213\n",
      "steps = 83, loss = 3.3625566959381104\n",
      "steps = 83, loss = 3.5937957763671875\n",
      "steps = 83, loss = 2.7226672172546387\n",
      "steps = 83, loss = 2.281741142272949\n",
      "steps = 83, loss = 3.535939931869507\n",
      "steps = 83, loss = 3.598349094390869\n",
      "steps = 83, loss = 2.7805190086364746\n",
      "steps = 83, loss = 2.8007664680480957\n",
      "steps = 83, loss = 3.003746271133423\n",
      "steps = 83, loss = 2.875208616256714\n",
      "steps = 83, loss = 3.476820230484009\n",
      "steps = 83, loss = 2.257080316543579\n",
      "steps = 83, loss = 49.97860336303711\n",
      "steps = 83, loss = 49.9853630065918\n",
      "steps = 83, loss = 3.3755173683166504\n",
      "steps = 83, loss = 2.5683634281158447\n",
      "steps = 84, loss = 2.837982177734375\n",
      "steps = 84, loss = 3.319478750228882\n",
      "steps = 84, loss = 2.6955246925354004\n",
      "steps = 84, loss = 2.7419445514678955\n",
      "steps = 84, loss = 3.2100515365600586\n",
      "steps = 84, loss = 3.5288681983947754\n",
      "steps = 84, loss = 1.9159818887710571\n",
      "steps = 84, loss = 2.9972305297851562\n",
      "steps = 84, loss = 3.5435287952423096\n",
      "steps = 84, loss = 49.9853630065918\n",
      "steps = 84, loss = 5.845399379730225\n",
      "steps = 84, loss = 1.9934353828430176\n",
      "steps = 84, loss = 2.6067237854003906\n",
      "steps = 84, loss = 50.015655517578125\n",
      "steps = 84, loss = 2.7765395641326904\n",
      "steps = 84, loss = 3.3085827827453613\n",
      "steps = 84, loss = 3.4052438735961914\n",
      "steps = 84, loss = 2.136603593826294\n",
      "steps = 84, loss = 2.689577341079712\n",
      "steps = 84, loss = 2.4171836376190186\n",
      "steps = 84, loss = 3.3249871730804443\n",
      "steps = 84, loss = 2.561558961868286\n",
      "steps = 84, loss = 2.894914388656616\n",
      "steps = 84, loss = 3.4931094646453857\n",
      "steps = 84, loss = 2.74191951751709\n",
      "steps = 84, loss = 3.497161388397217\n",
      "steps = 84, loss = 3.490203380584717\n",
      "steps = 84, loss = 3.289275646209717\n",
      "steps = 84, loss = 3.6089768409729004\n",
      "steps = 85, loss = 2.704674005508423\n",
      "steps = 85, loss = 2.995820999145508\n",
      "steps = 85, loss = 3.564465045928955\n",
      "steps = 85, loss = 2.54935884475708\n",
      "steps = 85, loss = 3.4157118797302246\n",
      "steps = 85, loss = 2.741765022277832\n",
      "steps = 85, loss = 2.0577468872070312\n",
      "steps = 85, loss = 3.1097052097320557\n",
      "steps = 85, loss = 1.9162517786026\n",
      "steps = 85, loss = 3.470027446746826\n",
      "steps = 85, loss = 2.521289587020874\n",
      "steps = 85, loss = 2.83939266204834\n",
      "steps = 85, loss = 2.1129868030548096\n",
      "steps = 85, loss = 2.5655875205993652\n",
      "steps = 85, loss = 5.874943256378174\n",
      "steps = 85, loss = 3.3643875122070312\n",
      "steps = 85, loss = 3.2847161293029785\n",
      "steps = 85, loss = 3.5324299335479736\n",
      "steps = 85, loss = 49.9853630065918\n",
      "steps = 85, loss = 2.690922737121582\n",
      "steps = 85, loss = 50.0066032409668\n",
      "steps = 85, loss = 3.338961601257324\n",
      "steps = 85, loss = 2.699479579925537\n",
      "steps = 85, loss = 3.302485227584839\n",
      "steps = 85, loss = 3.190021276473999\n",
      "steps = 85, loss = 2.7246901988983154\n",
      "steps = 85, loss = 3.6970064640045166\n",
      "steps = 85, loss = 3.615212917327881\n",
      "steps = 85, loss = 3.322387933731079\n",
      "steps = 86, loss = 3.4369096755981445\n",
      "steps = 86, loss = 3.3665764331817627\n",
      "steps = 86, loss = 2.1998820304870605\n",
      "steps = 86, loss = 3.480647563934326\n",
      "steps = 86, loss = 2.0538389682769775\n",
      "steps = 86, loss = 3.4329769611358643\n",
      "steps = 86, loss = 3.8155899047851562\n",
      "steps = 86, loss = 2.857009172439575\n",
      "steps = 86, loss = 3.415436029434204\n",
      "steps = 86, loss = 2.694035291671753\n",
      "steps = 86, loss = 3.5647828578948975\n",
      "steps = 86, loss = 3.3636322021484375\n",
      "steps = 86, loss = 3.373596429824829\n",
      "steps = 86, loss = 3.0623888969421387\n",
      "steps = 86, loss = 3.323331356048584\n",
      "steps = 86, loss = 5.843348503112793\n",
      "steps = 86, loss = 2.7483253479003906\n",
      "steps = 86, loss = 2.5595710277557373\n",
      "steps = 86, loss = 3.574176788330078\n",
      "steps = 86, loss = 2.707105875015259\n",
      "steps = 86, loss = 49.994510650634766\n",
      "steps = 86, loss = 49.9853630065918\n",
      "steps = 86, loss = 2.5838935375213623\n",
      "steps = 86, loss = 3.003812074661255\n",
      "steps = 86, loss = 2.6776373386383057\n",
      "steps = 86, loss = 3.364361047744751\n",
      "steps = 86, loss = 2.60709285736084\n",
      "steps = 86, loss = 2.1428189277648926\n",
      "steps = 86, loss = 2.7936220169067383\n",
      "steps = 87, loss = 3.4865541458129883\n",
      "steps = 87, loss = 2.2703959941864014\n",
      "steps = 87, loss = 3.395108461380005\n",
      "steps = 87, loss = 3.3434150218963623\n",
      "steps = 87, loss = 2.9579925537109375\n",
      "steps = 87, loss = 2.723351240158081\n",
      "steps = 87, loss = 3.451176643371582\n",
      "steps = 87, loss = 50.00809860229492\n",
      "steps = 87, loss = 2.1123063564300537\n",
      "steps = 87, loss = 2.6478748321533203\n",
      "steps = 87, loss = 3.3070030212402344\n",
      "steps = 87, loss = 2.0965511798858643\n",
      "steps = 87, loss = 3.344435214996338\n",
      "steps = 87, loss = 2.8906376361846924\n",
      "steps = 87, loss = 2.5812668800354004\n",
      "steps = 87, loss = 3.620410919189453\n",
      "steps = 87, loss = 2.714649200439453\n",
      "steps = 87, loss = 2.7658607959747314\n",
      "steps = 87, loss = 49.9853630065918\n",
      "steps = 87, loss = 3.051640272140503\n",
      "steps = 87, loss = 3.688459634780884\n",
      "steps = 87, loss = 3.562070369720459\n",
      "steps = 87, loss = 3.279430389404297\n",
      "steps = 87, loss = 3.975435733795166\n",
      "steps = 87, loss = 3.009488582611084\n",
      "steps = 87, loss = 2.8743715286254883\n",
      "steps = 87, loss = 6.025900363922119\n",
      "steps = 87, loss = 3.510462522506714\n",
      "steps = 87, loss = 2.593196392059326\n",
      "steps = 88, loss = 2.7071969509124756\n",
      "steps = 88, loss = 49.9853630065918\n",
      "steps = 88, loss = 3.454989433288574\n",
      "steps = 88, loss = 2.219398260116577\n",
      "steps = 88, loss = 2.026510715484619\n",
      "steps = 88, loss = 2.6391422748565674\n",
      "steps = 88, loss = 2.857767343521118\n",
      "steps = 88, loss = 2.1907613277435303\n",
      "steps = 88, loss = 49.96025466918945\n",
      "steps = 88, loss = 2.754626750946045\n",
      "steps = 88, loss = 2.5773518085479736\n",
      "steps = 88, loss = 3.3943257331848145\n",
      "steps = 88, loss = 3.3310184478759766\n",
      "steps = 88, loss = 3.3531620502471924\n",
      "steps = 88, loss = 3.0032613277435303\n",
      "steps = 88, loss = 3.2779648303985596\n",
      "steps = 88, loss = 2.589085817337036\n",
      "steps = 88, loss = 3.411494255065918\n",
      "steps = 88, loss = 2.775322437286377\n",
      "steps = 88, loss = 2.584590196609497\n",
      "steps = 88, loss = 5.6943745613098145\n",
      "steps = 88, loss = 3.439943313598633\n",
      "steps = 88, loss = 2.756324052810669\n",
      "steps = 88, loss = 3.8921902179718018\n",
      "steps = 88, loss = 3.3144888877868652\n",
      "steps = 88, loss = 3.3075692653656006\n",
      "steps = 88, loss = 3.551981210708618\n",
      "steps = 88, loss = 2.6000828742980957\n",
      "steps = 88, loss = 3.2935478687286377\n",
      "steps = 89, loss = 1.9685726165771484\n",
      "steps = 89, loss = 3.455623149871826\n",
      "steps = 89, loss = 2.7338223457336426\n",
      "steps = 89, loss = 3.458158493041992\n",
      "steps = 89, loss = 2.136199474334717\n",
      "steps = 89, loss = 3.385554075241089\n",
      "steps = 89, loss = 3.9520678520202637\n",
      "steps = 89, loss = 2.925676107406616\n",
      "steps = 89, loss = 3.8324873447418213\n",
      "steps = 89, loss = 2.5933234691619873\n",
      "steps = 89, loss = 2.686063528060913\n",
      "steps = 89, loss = 3.421506643295288\n",
      "steps = 89, loss = 5.853616714477539\n",
      "steps = 89, loss = 49.9853630065918\n",
      "steps = 89, loss = 2.7649238109588623\n",
      "steps = 89, loss = 2.5840792655944824\n",
      "steps = 89, loss = 3.292811155319214\n",
      "steps = 89, loss = 2.820495367050171\n",
      "steps = 89, loss = 50.017860412597656\n",
      "steps = 89, loss = 2.4977355003356934\n",
      "steps = 89, loss = 3.642725944519043\n",
      "steps = 89, loss = 1.9968940019607544\n",
      "steps = 89, loss = 2.836775302886963\n",
      "steps = 89, loss = 2.688570022583008\n",
      "steps = 89, loss = 3.3597824573516846\n",
      "steps = 89, loss = 3.4086949825286865\n",
      "steps = 89, loss = 3.464470863342285\n",
      "steps = 89, loss = 2.9921748638153076\n",
      "steps = 89, loss = 3.401456117630005\n",
      "steps = 90, loss = 3.0667054653167725\n",
      "steps = 90, loss = 50.02325439453125\n",
      "steps = 90, loss = 2.2588577270507812\n",
      "steps = 90, loss = 2.972207546234131\n",
      "steps = 90, loss = 2.6114070415496826\n",
      "steps = 90, loss = 2.617313861846924\n",
      "steps = 90, loss = 3.3894317150115967\n",
      "steps = 90, loss = 2.76361346244812\n",
      "steps = 90, loss = 2.1565258502960205\n",
      "steps = 90, loss = 3.321974515914917\n",
      "steps = 90, loss = 3.8628907203674316\n",
      "steps = 90, loss = 3.5557053089141846\n",
      "steps = 90, loss = 3.5424280166625977\n",
      "steps = 90, loss = 3.34303617477417\n",
      "steps = 90, loss = 3.483079433441162\n",
      "steps = 90, loss = 3.322418689727783\n",
      "steps = 90, loss = 2.7662460803985596\n",
      "steps = 90, loss = 3.0067458152770996\n",
      "steps = 90, loss = 3.3858251571655273\n",
      "steps = 90, loss = 3.443441390991211\n",
      "steps = 90, loss = 2.874689817428589\n",
      "steps = 90, loss = 2.724181890487671\n",
      "steps = 90, loss = 3.771738290786743\n",
      "steps = 90, loss = 2.712653160095215\n",
      "steps = 90, loss = 6.032445430755615\n",
      "steps = 90, loss = 2.2876780033111572\n",
      "steps = 90, loss = 49.9853630065918\n",
      "steps = 90, loss = 3.326249599456787\n",
      "steps = 90, loss = 2.9061713218688965\n",
      "steps = 91, loss = 2.5357275009155273\n",
      "steps = 91, loss = 2.0198450088500977\n",
      "steps = 91, loss = 3.4098331928253174\n",
      "steps = 91, loss = 3.1920528411865234\n",
      "steps = 91, loss = 2.618680000305176\n",
      "steps = 91, loss = 2.857461929321289\n",
      "steps = 91, loss = 2.707634925842285\n",
      "steps = 91, loss = 5.574397563934326\n",
      "steps = 91, loss = 3.6841092109680176\n",
      "steps = 91, loss = 49.9853630065918\n",
      "steps = 91, loss = 3.4623281955718994\n",
      "steps = 91, loss = 1.9598157405853271\n",
      "steps = 91, loss = 50.01921081542969\n",
      "steps = 91, loss = 3.3947689533233643\n",
      "steps = 91, loss = 3.4368066787719727\n",
      "steps = 91, loss = 3.116199493408203\n",
      "steps = 91, loss = 2.658517599105835\n",
      "steps = 91, loss = 3.4605746269226074\n",
      "steps = 91, loss = 3.0008180141448975\n",
      "steps = 91, loss = 3.4291930198669434\n",
      "steps = 91, loss = 4.183347225189209\n",
      "steps = 91, loss = 2.884976625442505\n",
      "steps = 91, loss = 2.7578725814819336\n",
      "steps = 91, loss = 3.3191378116607666\n",
      "steps = 91, loss = 3.560290813446045\n",
      "steps = 91, loss = 3.4869978427886963\n",
      "steps = 91, loss = 2.9272472858428955\n",
      "steps = 91, loss = 3.5869152545928955\n",
      "steps = 91, loss = 2.2389538288116455\n",
      "steps = 92, loss = 3.402043342590332\n",
      "steps = 92, loss = 3.1395790576934814\n",
      "steps = 92, loss = 2.4921371936798096\n",
      "steps = 92, loss = 3.457911729812622\n",
      "steps = 92, loss = 50.01643753051758\n",
      "steps = 92, loss = 49.9853630065918\n",
      "steps = 92, loss = 3.267174243927002\n",
      "steps = 92, loss = 2.9900052547454834\n",
      "steps = 92, loss = 1.9685763120651245\n",
      "steps = 92, loss = 2.6120142936706543\n",
      "steps = 92, loss = 3.583575963973999\n",
      "steps = 92, loss = 5.789478778839111\n",
      "steps = 92, loss = 3.2761621475219727\n",
      "steps = 92, loss = 2.8367621898651123\n",
      "steps = 92, loss = 3.4238176345825195\n",
      "steps = 92, loss = 2.0313358306884766\n",
      "steps = 92, loss = 3.3376452922821045\n",
      "steps = 92, loss = 2.741858959197998\n",
      "steps = 92, loss = 3.4259750843048096\n",
      "steps = 92, loss = 3.6156527996063232\n",
      "steps = 92, loss = 2.6886978149414062\n",
      "steps = 92, loss = 3.4913759231567383\n",
      "steps = 92, loss = 2.5428850650787354\n",
      "steps = 92, loss = 2.1339714527130127\n",
      "steps = 92, loss = 2.6849844455718994\n",
      "steps = 92, loss = 2.6153416633605957\n",
      "steps = 92, loss = 3.9879794120788574\n",
      "steps = 92, loss = 3.399840831756592\n",
      "steps = 92, loss = 2.7065484523773193\n",
      "steps = 93, loss = 3.943918466567993\n",
      "steps = 93, loss = 2.763073682785034\n",
      "steps = 93, loss = 3.0043792724609375\n",
      "steps = 93, loss = 2.3009073734283447\n",
      "steps = 93, loss = 3.112433910369873\n",
      "steps = 93, loss = 2.7268013954162598\n",
      "steps = 93, loss = 3.5962884426116943\n",
      "steps = 93, loss = 3.3811757564544678\n",
      "steps = 93, loss = 3.3213932514190674\n",
      "steps = 93, loss = 2.2892465591430664\n",
      "steps = 93, loss = 3.515831708908081\n",
      "steps = 93, loss = 3.4293465614318848\n",
      "steps = 93, loss = 4.126142501831055\n",
      "steps = 93, loss = 2.5970537662506104\n",
      "steps = 93, loss = 3.372379779815674\n",
      "steps = 93, loss = 2.7246346473693848\n",
      "steps = 93, loss = 3.493028402328491\n",
      "steps = 93, loss = 2.1523633003234863\n",
      "steps = 93, loss = 5.927679538726807\n",
      "steps = 93, loss = 3.6553361415863037\n",
      "steps = 93, loss = 49.9853630065918\n",
      "steps = 93, loss = 3.3782498836517334\n",
      "steps = 93, loss = 3.5428173542022705\n",
      "steps = 93, loss = 2.9128096103668213\n",
      "steps = 93, loss = 2.874654531478882\n",
      "steps = 93, loss = 2.6718392372131348\n",
      "steps = 93, loss = 2.629852294921875\n",
      "steps = 93, loss = 50.015907287597656\n",
      "steps = 93, loss = 3.0082356929779053\n",
      "steps = 94, loss = 2.5879619121551514\n",
      "steps = 94, loss = 3.3659017086029053\n",
      "steps = 94, loss = 3.089925765991211\n",
      "steps = 94, loss = 3.522810935974121\n",
      "steps = 94, loss = 3.2959742546081543\n",
      "steps = 94, loss = 2.449885845184326\n",
      "steps = 94, loss = 2.7241387367248535\n",
      "steps = 94, loss = 3.4895012378692627\n",
      "steps = 94, loss = 3.0111565589904785\n",
      "steps = 94, loss = 49.92577362060547\n",
      "steps = 94, loss = 3.113133668899536\n",
      "steps = 94, loss = 2.6886801719665527\n",
      "steps = 94, loss = 2.2116034030914307\n",
      "steps = 94, loss = 4.432950496673584\n",
      "steps = 94, loss = 49.9853630065918\n",
      "steps = 94, loss = 5.656606674194336\n",
      "steps = 94, loss = 3.1481266021728516\n",
      "steps = 94, loss = 2.623779058456421\n",
      "steps = 94, loss = 3.514740467071533\n",
      "steps = 94, loss = 2.8386757373809814\n",
      "steps = 94, loss = 2.989616632461548\n",
      "steps = 94, loss = 3.1837728023529053\n",
      "steps = 94, loss = 2.6915016174316406\n",
      "steps = 94, loss = 3.47822642326355\n",
      "steps = 94, loss = 3.632631301879883\n",
      "steps = 94, loss = 2.138291120529175\n",
      "steps = 94, loss = 3.429961919784546\n",
      "steps = 94, loss = 50.01498031616211\n",
      "steps = 94, loss = 3.4310877323150635\n",
      "steps = 95, loss = 2.7079250812530518\n",
      "steps = 95, loss = 3.529679775238037\n",
      "steps = 95, loss = 30.892349243164062\n",
      "steps = 95, loss = 3.524014711380005\n",
      "steps = 95, loss = 3.5739316940307617\n",
      "steps = 95, loss = 3.017090320587158\n",
      "steps = 95, loss = 2.7604968547821045\n",
      "steps = 95, loss = 2.960175037384033\n",
      "steps = 95, loss = 3.465226650238037\n",
      "steps = 95, loss = 3.54380464553833\n",
      "steps = 95, loss = 2.243783712387085\n",
      "steps = 95, loss = 49.9853630065918\n",
      "steps = 95, loss = 5.705196380615234\n",
      "steps = 95, loss = 2.4789953231811523\n",
      "steps = 95, loss = 3.3641817569732666\n",
      "steps = 95, loss = 3.6274611949920654\n",
      "steps = 95, loss = 2.856489419937134\n",
      "steps = 95, loss = 2.250335931777954\n",
      "steps = 95, loss = 2.8857951164245605\n",
      "steps = 95, loss = 3.4504499435424805\n",
      "steps = 95, loss = 3.3191049098968506\n",
      "steps = 95, loss = 49.96024703979492\n",
      "steps = 95, loss = 3.3870866298675537\n",
      "steps = 95, loss = 2.6405868530273438\n",
      "steps = 95, loss = 2.7495992183685303\n",
      "steps = 95, loss = 3.0203371047973633\n",
      "steps = 95, loss = 2.9966940879821777\n",
      "steps = 95, loss = 3.643984079360962\n",
      "steps = 95, loss = 50.00654602050781\n",
      "steps = 96, loss = 3.471848487854004\n",
      "steps = 96, loss = 2.9403493404388428\n",
      "steps = 96, loss = 2.5261292457580566\n",
      "steps = 96, loss = 2.7243900299072266\n",
      "steps = 96, loss = 3.54984450340271\n",
      "steps = 96, loss = 49.9853630065918\n",
      "steps = 96, loss = 2.8742003440856934\n",
      "steps = 96, loss = 3.5067715644836426\n",
      "steps = 96, loss = 5.8282999992370605\n",
      "steps = 96, loss = 2.9650092124938965\n",
      "steps = 96, loss = 3.002239227294922\n",
      "steps = 96, loss = 1.9494596719741821\n",
      "steps = 96, loss = 3.351149082183838\n",
      "steps = 96, loss = 3.116379976272583\n",
      "steps = 96, loss = 3.47222638130188\n",
      "steps = 96, loss = 3.358738422393799\n",
      "steps = 96, loss = 2.671931028366089\n",
      "steps = 96, loss = 3.091507911682129\n",
      "steps = 96, loss = 50.01520919799805\n",
      "steps = 96, loss = 3.5439438819885254\n",
      "steps = 96, loss = 2.6482036113739014\n",
      "steps = 96, loss = 3.383180618286133\n",
      "steps = 96, loss = 2.2632224559783936\n",
      "steps = 96, loss = 2.649066925048828\n",
      "steps = 96, loss = 3.286250352859497\n",
      "steps = 96, loss = 3.80197811126709\n",
      "steps = 96, loss = 2.282865285873413\n",
      "steps = 96, loss = 3.4282150268554688\n",
      "steps = 96, loss = 2.763343095779419\n",
      "steps = 97, loss = 2.987417459487915\n",
      "steps = 97, loss = 2.1375768184661865\n",
      "steps = 97, loss = 2.5055928230285645\n",
      "steps = 97, loss = 2.867342472076416\n",
      "steps = 97, loss = 2.1409730911254883\n",
      "steps = 97, loss = 3.4732964038848877\n",
      "steps = 97, loss = 3.549077272415161\n",
      "steps = 97, loss = 2.8380191326141357\n",
      "steps = 97, loss = 2.5624923706054688\n",
      "steps = 97, loss = 49.9853630065918\n",
      "steps = 97, loss = 3.303054094314575\n",
      "steps = 97, loss = 1.9353581666946411\n",
      "steps = 97, loss = 3.47110915184021\n",
      "steps = 97, loss = 3.5391054153442383\n",
      "steps = 97, loss = 3.727768898010254\n",
      "steps = 97, loss = 5.920867919921875\n",
      "steps = 97, loss = 3.3009164333343506\n",
      "steps = 97, loss = 2.707733631134033\n",
      "steps = 97, loss = 2.9905080795288086\n",
      "steps = 97, loss = 2.641875982284546\n",
      "steps = 97, loss = 20.084815979003906\n",
      "steps = 97, loss = 2.452110767364502\n",
      "steps = 97, loss = 3.3622539043426514\n",
      "steps = 97, loss = 2.6907808780670166\n",
      "steps = 97, loss = 3.115635633468628\n",
      "steps = 97, loss = 49.94755935668945\n",
      "steps = 97, loss = 2.739407777786255\n",
      "steps = 97, loss = 3.4960391521453857\n",
      "steps = 97, loss = 2.691117525100708\n",
      "steps = 98, loss = 2.76745867729187\n",
      "steps = 98, loss = 3.3886899948120117\n",
      "steps = 98, loss = 2.825809955596924\n",
      "steps = 98, loss = 2.7254996299743652\n",
      "steps = 98, loss = 2.677013635635376\n",
      "steps = 98, loss = 3.37619948387146\n",
      "steps = 98, loss = 2.659358024597168\n",
      "steps = 98, loss = 2.848280906677246\n",
      "steps = 98, loss = 2.7256264686584473\n",
      "steps = 98, loss = 2.563685655593872\n",
      "steps = 98, loss = 49.9853630065918\n",
      "steps = 98, loss = 3.5115864276885986\n",
      "steps = 98, loss = 50.00794219970703\n",
      "steps = 98, loss = 3.6142072677612305\n",
      "steps = 98, loss = 2.939936637878418\n",
      "steps = 98, loss = 3.0837903022766113\n",
      "steps = 98, loss = 2.292914628982544\n",
      "steps = 98, loss = 3.0011847019195557\n",
      "steps = 98, loss = 2.667301893234253\n",
      "steps = 98, loss = 2.048553705215454\n",
      "steps = 98, loss = 2.2077207565307617\n",
      "steps = 98, loss = 3.4104716777801514\n",
      "steps = 98, loss = 2.8755369186401367\n",
      "steps = 98, loss = 3.3937723636627197\n",
      "steps = 98, loss = 3.5731239318847656\n",
      "steps = 98, loss = 6.097996711730957\n",
      "steps = 98, loss = 3.499492645263672\n",
      "steps = 98, loss = 3.416517734527588\n",
      "steps = 98, loss = 3.5754377841949463\n",
      "steps = 99, loss = 3.62709379196167\n",
      "steps = 99, loss = 2.0500402450561523\n",
      "steps = 99, loss = 2.3949086666107178\n",
      "steps = 99, loss = 3.394680976867676\n",
      "steps = 99, loss = 2.986851930618286\n",
      "steps = 99, loss = 49.9853630065918\n",
      "steps = 99, loss = 2.9102113246917725\n",
      "steps = 99, loss = 3.592813014984131\n",
      "steps = 99, loss = 3.5079879760742188\n",
      "steps = 99, loss = 3.4497687816619873\n",
      "steps = 99, loss = 50.031097412109375\n",
      "steps = 99, loss = 2.647873640060425\n",
      "steps = 99, loss = 2.939101457595825\n",
      "steps = 99, loss = 3.3255109786987305\n",
      "steps = 99, loss = 2.2136292457580566\n",
      "steps = 99, loss = 25.08943748474121\n",
      "steps = 99, loss = 2.7081079483032227\n",
      "steps = 99, loss = 3.689133882522583\n",
      "steps = 99, loss = 3.5729336738586426\n",
      "steps = 99, loss = 3.3515374660491943\n",
      "steps = 99, loss = 2.995191812515259\n",
      "steps = 99, loss = 5.724013805389404\n",
      "steps = 99, loss = 2.223496198654175\n",
      "steps = 99, loss = 2.856884717941284\n",
      "steps = 99, loss = 2.6657238006591797\n",
      "steps = 99, loss = 2.510769844055176\n",
      "steps = 99, loss = 2.762535572052002\n",
      "steps = 99, loss = 3.5762484073638916\n",
      "steps = 99, loss = 2.81198787689209\n",
      "steps = 100, loss = 3.5543484687805176\n",
      "steps = 100, loss = 2.7314329147338867\n",
      "steps = 100, loss = 5.919785499572754\n",
      "steps = 100, loss = 3.5893683433532715\n",
      "steps = 100, loss = 3.2832281589508057\n",
      "steps = 100, loss = 3.5148732662200928\n",
      "steps = 100, loss = 2.6895339488983154\n",
      "steps = 100, loss = 2.658684492111206\n",
      "steps = 100, loss = 3.398547887802124\n",
      "steps = 100, loss = 3.40871524810791\n",
      "steps = 100, loss = 49.9853630065918\n",
      "steps = 100, loss = 2.6073176860809326\n",
      "steps = 100, loss = 2.9013383388519287\n",
      "steps = 100, loss = 3.5841269493103027\n",
      "steps = 100, loss = 2.4522435665130615\n",
      "steps = 100, loss = 2.1359047889709473\n",
      "steps = 100, loss = 3.648620367050171\n",
      "steps = 100, loss = 2.773690938949585\n",
      "steps = 100, loss = 2.6622700691223145\n",
      "steps = 100, loss = 2.9847614765167236\n",
      "steps = 100, loss = 2.0205142498016357\n",
      "steps = 100, loss = 50.02638244628906\n",
      "steps = 100, loss = 2.6522068977355957\n",
      "steps = 100, loss = 2.836799383163452\n",
      "steps = 100, loss = 2.6999247074127197\n",
      "steps = 100, loss = 2.822505235671997\n",
      "steps = 100, loss = 3.2515721321105957\n",
      "steps = 100, loss = 3.1854984760284424\n",
      "steps = 100, loss = 1.9796169996261597\n",
      "steps = 101, loss = 2.2915542125701904\n",
      "steps = 101, loss = 2.676116466522217\n",
      "steps = 101, loss = 2.725393295288086\n",
      "steps = 101, loss = 2.7048468589782715\n",
      "steps = 101, loss = 5.997576713562012\n",
      "steps = 101, loss = 2.0489819049835205\n",
      "steps = 101, loss = 2.597050428390503\n",
      "steps = 101, loss = 3.364626407623291\n",
      "steps = 101, loss = 3.5412673950195312\n",
      "steps = 101, loss = 2.9462037086486816\n",
      "steps = 101, loss = 3.3690686225891113\n",
      "steps = 101, loss = 3.625014543533325\n",
      "steps = 101, loss = 49.9853630065918\n",
      "steps = 101, loss = 3.3282766342163086\n",
      "steps = 101, loss = 3.5902764797210693\n",
      "steps = 101, loss = 2.8174641132354736\n",
      "steps = 101, loss = 2.129945755004883\n",
      "steps = 101, loss = 3.351114511489868\n",
      "steps = 101, loss = 3.570561170578003\n",
      "steps = 101, loss = 3.067596435546875\n",
      "steps = 101, loss = 3.6533899307250977\n",
      "steps = 101, loss = 2.9985764026641846\n",
      "steps = 101, loss = 2.8486924171447754\n",
      "steps = 101, loss = 2.8747177124023438\n",
      "steps = 101, loss = 2.76608943939209\n",
      "steps = 101, loss = 3.390950918197632\n",
      "steps = 101, loss = 2.975688934326172\n",
      "steps = 101, loss = 3.6079044342041016\n",
      "steps = 101, loss = 50.02745056152344\n",
      "steps = 102, loss = 3.028550863265991\n",
      "steps = 102, loss = 3.5575647354125977\n",
      "steps = 102, loss = 2.0797972679138184\n",
      "steps = 102, loss = 2.7567927837371826\n",
      "steps = 102, loss = 2.9927785396575928\n",
      "steps = 102, loss = 2.231999635696411\n",
      "steps = 102, loss = 3.408823251724243\n",
      "steps = 102, loss = 49.9853630065918\n",
      "steps = 102, loss = 2.044316291809082\n",
      "steps = 102, loss = 2.634680986404419\n",
      "steps = 102, loss = 3.549501657485962\n",
      "steps = 102, loss = 2.856626272201538\n",
      "steps = 102, loss = 3.0467896461486816\n",
      "steps = 102, loss = 3.6424527168273926\n",
      "steps = 102, loss = 2.572134017944336\n",
      "steps = 102, loss = 2.606670379638672\n",
      "steps = 102, loss = 3.3395020961761475\n",
      "steps = 102, loss = 2.7086398601531982\n",
      "steps = 102, loss = 2.561279535293579\n",
      "steps = 102, loss = 2.8694097995758057\n",
      "steps = 102, loss = 3.2982842922210693\n",
      "steps = 102, loss = 3.5161070823669434\n",
      "steps = 102, loss = 5.726139545440674\n",
      "steps = 102, loss = 3.1384549140930176\n",
      "steps = 102, loss = 49.929931640625\n",
      "steps = 102, loss = 2.6824300289154053\n",
      "steps = 102, loss = 3.470696449279785\n",
      "steps = 102, loss = 3.3962056636810303\n",
      "steps = 102, loss = 3.610466957092285\n",
      "steps = 103, loss = 2.764660358428955\n",
      "steps = 103, loss = 3.600691556930542\n",
      "steps = 103, loss = 2.6883585453033447\n",
      "steps = 103, loss = 2.8580291271209717\n",
      "steps = 103, loss = 3.641430616378784\n",
      "steps = 103, loss = 3.6220390796661377\n",
      "steps = 103, loss = 2.765390634536743\n",
      "steps = 103, loss = 3.4225802421569824\n",
      "steps = 103, loss = 2.9740211963653564\n",
      "steps = 103, loss = 2.2291762828826904\n",
      "steps = 103, loss = 3.615406036376953\n",
      "steps = 103, loss = 2.856649398803711\n",
      "steps = 103, loss = 2.037522792816162\n",
      "steps = 103, loss = 2.9866533279418945\n",
      "steps = 103, loss = 3.660710573196411\n",
      "steps = 103, loss = 3.3418400287628174\n",
      "steps = 103, loss = 3.5634782314300537\n",
      "steps = 103, loss = 3.3651673793792725\n",
      "steps = 103, loss = 2.9912025928497314\n",
      "steps = 103, loss = 5.857150554656982\n",
      "steps = 103, loss = 3.001605987548828\n",
      "steps = 103, loss = 2.5809810161590576\n",
      "steps = 103, loss = 2.5789310932159424\n",
      "steps = 103, loss = 49.9853630065918\n",
      "steps = 103, loss = 2.708526134490967\n",
      "steps = 103, loss = 2.7600600719451904\n",
      "steps = 103, loss = 3.357347249984741\n",
      "steps = 103, loss = 2.189791440963745\n",
      "steps = 103, loss = 50.027278900146484\n",
      "steps = 104, loss = 3.4592528343200684\n",
      "steps = 104, loss = 2.630821943283081\n",
      "steps = 104, loss = 2.0983378887176514\n",
      "steps = 104, loss = 3.679352045059204\n",
      "steps = 104, loss = 2.7052528858184814\n",
      "steps = 104, loss = 49.9853630065918\n",
      "steps = 104, loss = 3.3900434970855713\n",
      "steps = 104, loss = 2.6952009201049805\n",
      "steps = 104, loss = 3.3847012519836426\n",
      "steps = 104, loss = 3.0774953365325928\n",
      "steps = 104, loss = 2.7243282794952393\n",
      "steps = 104, loss = 2.9961159229278564\n",
      "steps = 104, loss = 3.375185966491699\n",
      "steps = 104, loss = 5.954137325286865\n",
      "steps = 104, loss = 3.584103584289551\n",
      "steps = 104, loss = 2.939948558807373\n",
      "steps = 104, loss = 2.8735737800598145\n",
      "steps = 104, loss = 2.0867831707000732\n",
      "steps = 104, loss = 3.6429712772369385\n",
      "steps = 104, loss = 3.349740505218506\n",
      "steps = 104, loss = 3.534120559692383\n",
      "steps = 104, loss = 2.2872369289398193\n",
      "steps = 104, loss = 2.7650229930877686\n",
      "steps = 104, loss = 2.953533411026001\n",
      "steps = 104, loss = 3.352776050567627\n",
      "steps = 104, loss = 2.8426098823547363\n",
      "steps = 104, loss = 2.926589012145996\n",
      "steps = 104, loss = 3.5910301208496094\n",
      "steps = 104, loss = 50.02821731567383\n",
      "steps = 105, loss = 2.981658458709717\n",
      "steps = 105, loss = 2.893355131149292\n",
      "steps = 105, loss = 3.359405279159546\n",
      "steps = 105, loss = 49.9853630065918\n",
      "steps = 105, loss = 50.026832580566406\n",
      "steps = 105, loss = 2.6899049282073975\n",
      "steps = 105, loss = 3.4289002418518066\n",
      "steps = 105, loss = 2.7515060901641846\n",
      "steps = 105, loss = 2.7402749061584473\n",
      "steps = 105, loss = 2.6148691177368164\n",
      "steps = 105, loss = 3.5839426517486572\n",
      "steps = 105, loss = 3.06671404838562\n",
      "steps = 105, loss = 2.1386358737945557\n",
      "steps = 105, loss = 2.4281022548675537\n",
      "steps = 105, loss = 1.9328609704971313\n",
      "steps = 105, loss = 3.2548916339874268\n",
      "steps = 105, loss = 3.367187976837158\n",
      "steps = 105, loss = 6.020326137542725\n",
      "steps = 105, loss = 1.951153039932251\n",
      "steps = 105, loss = 3.186645269393921\n",
      "steps = 105, loss = 2.6050057411193848\n",
      "steps = 105, loss = 3.7525267601013184\n",
      "steps = 105, loss = 3.4352896213531494\n",
      "steps = 105, loss = 2.8363072872161865\n",
      "steps = 105, loss = 3.643759250640869\n",
      "steps = 105, loss = 2.6548070907592773\n",
      "steps = 105, loss = 2.6869864463806152\n",
      "steps = 105, loss = 3.4578607082366943\n",
      "steps = 105, loss = 3.6726789474487305\n",
      "steps = 106, loss = 2.9949734210968018\n",
      "steps = 106, loss = 3.5257813930511475\n",
      "steps = 106, loss = 3.3668229579925537\n",
      "steps = 106, loss = 3.6102676391601562\n",
      "steps = 106, loss = 2.848269462585449\n",
      "steps = 106, loss = 3.391756057739258\n",
      "steps = 106, loss = 6.196220397949219\n",
      "steps = 106, loss = 2.7038986682891846\n",
      "steps = 106, loss = 2.8749308586120605\n",
      "steps = 106, loss = 24.18004608154297\n",
      "steps = 106, loss = 2.7616612911224365\n",
      "steps = 106, loss = 2.1318461894989014\n",
      "steps = 106, loss = 2.9125216007232666\n",
      "steps = 106, loss = 3.6658129692077637\n",
      "steps = 106, loss = 2.7139322757720947\n",
      "steps = 106, loss = 2.726191997528076\n",
      "steps = 106, loss = 50.0121955871582\n",
      "steps = 106, loss = 2.718395948410034\n",
      "steps = 106, loss = 2.2929587364196777\n",
      "steps = 106, loss = 3.466322422027588\n",
      "steps = 106, loss = 2.04532790184021\n",
      "steps = 106, loss = 3.5138394832611084\n",
      "steps = 106, loss = 3.381819248199463\n",
      "steps = 106, loss = 3.708754539489746\n",
      "steps = 106, loss = 49.9853630065918\n",
      "steps = 106, loss = 3.608423948287964\n",
      "steps = 106, loss = 2.963806390762329\n",
      "steps = 106, loss = 2.7667388916015625\n",
      "steps = 106, loss = 3.3492705821990967\n",
      "steps = 107, loss = 2.9810872077941895\n",
      "steps = 107, loss = 2.4321377277374268\n",
      "steps = 107, loss = 3.703681230545044\n",
      "steps = 107, loss = 3.484537124633789\n",
      "steps = 107, loss = 2.771867513656616\n",
      "steps = 107, loss = 2.0087976455688477\n",
      "steps = 107, loss = 2.837864875793457\n",
      "steps = 107, loss = 50.00666809082031\n",
      "steps = 107, loss = 3.414412021636963\n",
      "steps = 107, loss = 3.352004051208496\n",
      "steps = 107, loss = 2.6968319416046143\n",
      "steps = 107, loss = 3.1081759929656982\n",
      "steps = 107, loss = 5.845081806182861\n",
      "steps = 107, loss = 3.3299286365509033\n",
      "steps = 107, loss = 2.729004383087158\n",
      "steps = 107, loss = 2.607551336288452\n",
      "steps = 107, loss = 2.808234214782715\n",
      "steps = 107, loss = 3.6110787391662598\n",
      "steps = 107, loss = 49.9853630065918\n",
      "steps = 107, loss = 3.5985021591186523\n",
      "steps = 107, loss = 2.9395062923431396\n",
      "steps = 107, loss = 2.6918699741363525\n",
      "steps = 107, loss = 3.6684584617614746\n",
      "steps = 107, loss = 48.86896514892578\n",
      "steps = 107, loss = 2.5040273666381836\n",
      "steps = 107, loss = 2.1382687091827393\n",
      "steps = 107, loss = 3.4497623443603516\n",
      "steps = 107, loss = 3.7007062435150146\n",
      "steps = 107, loss = 3.19592022895813\n",
      "steps = 108, loss = 2.7125489711761475\n",
      "steps = 108, loss = 6.0137858390808105\n",
      "steps = 108, loss = 3.608093500137329\n",
      "steps = 108, loss = 3.678658962249756\n",
      "steps = 108, loss = 3.3612241744995117\n",
      "steps = 108, loss = 49.9853630065918\n",
      "steps = 108, loss = 2.2436635494232178\n",
      "steps = 108, loss = 2.699641466140747\n",
      "steps = 108, loss = 2.5426106452941895\n",
      "steps = 108, loss = 2.7089600563049316\n",
      "steps = 108, loss = 2.7512941360473633\n",
      "steps = 108, loss = 2.21647572517395\n",
      "steps = 108, loss = 2.6386969089508057\n",
      "steps = 108, loss = 3.348626136779785\n",
      "steps = 108, loss = 2.9876809120178223\n",
      "steps = 108, loss = 1.7977626323699951\n",
      "steps = 108, loss = 3.5653722286224365\n",
      "steps = 108, loss = 3.362506866455078\n",
      "steps = 108, loss = 3.3094868659973145\n",
      "steps = 108, loss = 50.006690979003906\n",
      "steps = 108, loss = 2.881596565246582\n",
      "steps = 108, loss = 3.3774304389953613\n",
      "steps = 108, loss = 2.4708549976348877\n",
      "steps = 108, loss = 3.362408399581909\n",
      "steps = 108, loss = 3.3431754112243652\n",
      "steps = 108, loss = 3.7386879920959473\n",
      "steps = 108, loss = 2.855593204498291\n",
      "steps = 108, loss = 3.630002021789551\n",
      "steps = 108, loss = 2.752418279647827\n",
      "steps = 109, loss = 3.6444809436798096\n",
      "steps = 109, loss = 2.7192530632019043\n",
      "steps = 109, loss = 3.5638606548309326\n",
      "steps = 109, loss = 2.072526454925537\n",
      "steps = 109, loss = 3.5461134910583496\n",
      "steps = 109, loss = 2.7557156085968018\n",
      "steps = 109, loss = 6.092577934265137\n",
      "steps = 109, loss = 2.4887890815734863\n",
      "steps = 109, loss = 3.690685987472534\n",
      "steps = 109, loss = 2.9966464042663574\n",
      "steps = 109, loss = 2.19754695892334\n",
      "steps = 109, loss = 3.3088412284851074\n",
      "steps = 109, loss = 2.31809663772583\n",
      "steps = 109, loss = 49.9853630065918\n",
      "steps = 109, loss = 2.9866979122161865\n",
      "steps = 109, loss = 2.884446144104004\n",
      "steps = 109, loss = 3.7579050064086914\n",
      "steps = 109, loss = 12.308351516723633\n",
      "steps = 109, loss = 2.836320400238037\n",
      "steps = 109, loss = 2.163412570953369\n",
      "steps = 109, loss = 3.25015926361084\n",
      "steps = 109, loss = 50.0053825378418\n",
      "steps = 109, loss = 2.856100082397461\n",
      "steps = 109, loss = 2.418386697769165\n",
      "steps = 109, loss = 2.7093546390533447\n",
      "steps = 109, loss = 2.7091493606567383\n",
      "steps = 109, loss = 3.3999016284942627\n",
      "steps = 109, loss = 3.555685043334961\n",
      "steps = 109, loss = 3.688291311264038\n",
      "steps = 110, loss = 2.7119433879852295\n",
      "steps = 110, loss = 2.446082830429077\n",
      "steps = 110, loss = 3.753195285797119\n",
      "steps = 110, loss = 2.83536434173584\n",
      "steps = 110, loss = 2.9207801818847656\n",
      "steps = 110, loss = 3.702220916748047\n",
      "steps = 110, loss = 3.253573179244995\n",
      "steps = 110, loss = 2.6888718605041504\n",
      "steps = 110, loss = 3.6445298194885254\n",
      "steps = 110, loss = 6.078636646270752\n",
      "steps = 110, loss = 2.140782594680786\n",
      "steps = 110, loss = 3.3630928993225098\n",
      "steps = 110, loss = 2.018018960952759\n",
      "steps = 110, loss = 2.732778787612915\n",
      "steps = 110, loss = 3.48016357421875\n",
      "steps = 110, loss = 2.718657970428467\n",
      "steps = 110, loss = 3.1247470378875732\n",
      "steps = 110, loss = 49.9853630065918\n",
      "steps = 110, loss = 50.02254867553711\n",
      "steps = 110, loss = 3.3071372509002686\n",
      "steps = 110, loss = 2.8409037590026855\n",
      "steps = 110, loss = 2.0870866775512695\n",
      "steps = 110, loss = 3.396695613861084\n",
      "steps = 110, loss = 3.2482213973999023\n",
      "steps = 110, loss = 3.382683515548706\n",
      "steps = 110, loss = 2.977184295654297\n",
      "steps = 110, loss = 2.572082996368408\n",
      "steps = 110, loss = 3.6515719890594482\n",
      "steps = 110, loss = 2.5961601734161377\n",
      "steps = 111, loss = 2.5978775024414062\n",
      "steps = 111, loss = 2.8732831478118896\n",
      "steps = 111, loss = 2.7262356281280518\n",
      "steps = 111, loss = 49.9853630065918\n",
      "steps = 111, loss = 2.5882413387298584\n",
      "steps = 111, loss = 3.475600004196167\n",
      "steps = 111, loss = 3.6774275302886963\n",
      "steps = 111, loss = 6.229401588439941\n",
      "steps = 111, loss = 3.4859907627105713\n",
      "steps = 111, loss = 3.7234690189361572\n",
      "steps = 111, loss = 3.273642063140869\n",
      "steps = 111, loss = 3.642547369003296\n",
      "steps = 111, loss = 3.790140151977539\n",
      "steps = 111, loss = 50.017005920410156\n",
      "steps = 111, loss = 2.9906656742095947\n",
      "steps = 111, loss = 2.768132448196411\n",
      "steps = 111, loss = 2.1626052856445312\n",
      "steps = 111, loss = 2.7338056564331055\n",
      "steps = 111, loss = 2.179105758666992\n",
      "steps = 111, loss = 2.936964988708496\n",
      "steps = 111, loss = 2.9548468589782715\n",
      "steps = 111, loss = 3.57304310798645\n",
      "steps = 111, loss = 2.2915823459625244\n",
      "steps = 111, loss = 2.647818088531494\n",
      "steps = 111, loss = 2.483659029006958\n",
      "steps = 111, loss = 3.320235252380371\n",
      "steps = 111, loss = 3.361067771911621\n",
      "steps = 111, loss = 2.729018449783325\n",
      "steps = 111, loss = 2.64741587638855\n",
      "steps = 112, loss = 3.7847578525543213\n",
      "steps = 112, loss = 3.100430727005005\n",
      "steps = 112, loss = 2.6560842990875244\n",
      "steps = 112, loss = 2.7220113277435303\n",
      "steps = 112, loss = 3.479093313217163\n",
      "steps = 112, loss = 49.9853630065918\n",
      "steps = 112, loss = 3.41634202003479\n",
      "steps = 112, loss = 2.005373954772949\n",
      "steps = 112, loss = 2.5518226623535156\n",
      "steps = 112, loss = 2.9718570709228516\n",
      "steps = 112, loss = 2.1389143466949463\n",
      "steps = 112, loss = 2.97735595703125\n",
      "steps = 112, loss = 3.6128315925598145\n",
      "steps = 112, loss = 3.6779544353485107\n",
      "steps = 112, loss = 2.837583065032959\n",
      "steps = 112, loss = 2.7560243606567383\n",
      "steps = 112, loss = 2.6922647953033447\n",
      "steps = 112, loss = 2.4287734031677246\n",
      "steps = 112, loss = 3.1573524475097656\n",
      "steps = 112, loss = 3.3704710006713867\n",
      "steps = 112, loss = 2.7410800457000732\n",
      "steps = 112, loss = 3.7269484996795654\n",
      "steps = 112, loss = 2.0459342002868652\n",
      "steps = 112, loss = 2.3943533897399902\n",
      "steps = 112, loss = 2.893613576889038\n",
      "steps = 112, loss = 6.035703182220459\n",
      "steps = 112, loss = 3.3588130474090576\n",
      "steps = 112, loss = 3.695178985595703\n",
      "steps = 112, loss = 50.02321243286133\n",
      "steps = 113, loss = 2.9840140342712402\n",
      "steps = 113, loss = 2.7096784114837646\n",
      "steps = 113, loss = 3.5654947757720947\n",
      "steps = 113, loss = 2.235846757888794\n",
      "steps = 113, loss = 2.737548589706421\n",
      "steps = 113, loss = 3.3521323204040527\n",
      "steps = 113, loss = 2.8555057048797607\n",
      "steps = 113, loss = 2.625121593475342\n",
      "steps = 113, loss = 6.151645660400391\n",
      "steps = 113, loss = 2.7568585872650146\n",
      "steps = 113, loss = 2.7660045623779297\n",
      "steps = 113, loss = 3.247755289077759\n",
      "steps = 113, loss = 2.984046697616577\n",
      "steps = 113, loss = 2.6292479038238525\n",
      "steps = 113, loss = 1.9700003862380981\n",
      "steps = 113, loss = 3.1660261154174805\n",
      "steps = 113, loss = 3.5198590755462646\n",
      "steps = 113, loss = 3.683015823364258\n",
      "steps = 113, loss = 3.6330502033233643\n",
      "steps = 113, loss = 3.6584818363189697\n",
      "steps = 113, loss = 3.7355499267578125\n",
      "steps = 113, loss = 3.820580244064331\n",
      "steps = 113, loss = 3.6424448490142822\n",
      "steps = 113, loss = 2.9404165744781494\n",
      "steps = 113, loss = 2.041217088699341\n",
      "steps = 113, loss = 3.697211980819702\n",
      "steps = 113, loss = 50.02900314331055\n",
      "steps = 113, loss = 49.9853630065918\n",
      "steps = 113, loss = 2.414128065109253\n",
      "steps = 114, loss = 2.28654146194458\n",
      "steps = 114, loss = 3.360677480697632\n",
      "steps = 114, loss = 2.9892849922180176\n",
      "steps = 114, loss = 3.7187118530273438\n",
      "steps = 114, loss = 3.4906651973724365\n",
      "steps = 114, loss = 3.4065849781036377\n",
      "steps = 114, loss = 2.8678295612335205\n",
      "steps = 114, loss = 2.889420747756958\n",
      "steps = 114, loss = 2.8732686042785645\n",
      "steps = 114, loss = 3.4667892456054688\n",
      "steps = 114, loss = 2.1650023460388184\n",
      "steps = 114, loss = 3.8402559757232666\n",
      "steps = 114, loss = 2.977800130844116\n",
      "steps = 114, loss = 2.7454209327697754\n",
      "steps = 114, loss = 2.6714067459106445\n",
      "steps = 114, loss = 2.7200379371643066\n",
      "steps = 114, loss = 2.998673677444458\n",
      "steps = 114, loss = 6.081485748291016\n",
      "steps = 114, loss = 2.7684731483459473\n",
      "steps = 114, loss = 3.4429595470428467\n",
      "steps = 114, loss = 3.5260567665100098\n",
      "steps = 114, loss = 3.9594550132751465\n",
      "steps = 114, loss = 2.725933074951172\n",
      "steps = 114, loss = 3.7592318058013916\n",
      "steps = 114, loss = 49.9853630065918\n",
      "steps = 114, loss = 2.119551658630371\n",
      "steps = 114, loss = 3.3981993198394775\n",
      "steps = 114, loss = 3.4813342094421387\n",
      "steps = 114, loss = 50.02900314331055\n",
      "steps = 115, loss = 3.3103349208831787\n",
      "steps = 115, loss = 3.8339946269989014\n",
      "steps = 115, loss = 2.7211811542510986\n",
      "steps = 115, loss = 3.058518409729004\n",
      "steps = 115, loss = 2.0483617782592773\n",
      "steps = 115, loss = 2.749342203140259\n",
      "steps = 115, loss = 3.516065835952759\n",
      "steps = 115, loss = 6.280248641967773\n",
      "steps = 115, loss = 49.9853630065918\n",
      "steps = 115, loss = 3.3126044273376465\n",
      "steps = 115, loss = 2.739717721939087\n",
      "steps = 115, loss = 2.3073360919952393\n",
      "steps = 115, loss = 3.76279878616333\n",
      "steps = 115, loss = 2.581505060195923\n",
      "steps = 115, loss = 3.032197952270508\n",
      "steps = 115, loss = 3.725346326828003\n",
      "steps = 115, loss = 3.46124267578125\n",
      "steps = 115, loss = 2.975717782974243\n",
      "steps = 115, loss = 3.5993096828460693\n",
      "steps = 115, loss = 2.138679027557373\n",
      "steps = 115, loss = 2.6918249130249023\n",
      "steps = 115, loss = 2.837183952331543\n",
      "steps = 115, loss = 3.316598415374756\n",
      "steps = 115, loss = 2.7173032760620117\n",
      "steps = 115, loss = 49.982826232910156\n",
      "steps = 115, loss = 2.4515838623046875\n",
      "steps = 115, loss = 2.7374186515808105\n",
      "steps = 115, loss = 3.7185540199279785\n",
      "steps = 115, loss = 3.439194917678833\n",
      "steps = 116, loss = 3.330758810043335\n",
      "steps = 116, loss = 3.869349718093872\n",
      "steps = 116, loss = 3.3518495559692383\n",
      "steps = 116, loss = 2.7624435424804688\n",
      "steps = 116, loss = 3.3636646270751953\n",
      "steps = 116, loss = 49.9853630065918\n",
      "steps = 116, loss = 6.059188365936279\n",
      "steps = 116, loss = 2.8554298877716064\n",
      "steps = 116, loss = 3.410645008087158\n",
      "steps = 116, loss = 3.7374391555786133\n",
      "steps = 116, loss = 50.016441345214844\n",
      "steps = 116, loss = 2.1608951091766357\n",
      "steps = 116, loss = 2.681623935699463\n",
      "steps = 116, loss = 2.752523183822632\n",
      "steps = 116, loss = 2.63982892036438\n",
      "steps = 116, loss = 3.305562734603882\n",
      "steps = 116, loss = 3.769667863845825\n",
      "steps = 116, loss = 3.5678012371063232\n",
      "steps = 116, loss = 3.413452386856079\n",
      "steps = 116, loss = 2.356919765472412\n",
      "steps = 116, loss = 2.069749355316162\n",
      "steps = 116, loss = 3.367659330368042\n",
      "steps = 116, loss = 2.6735188961029053\n",
      "steps = 116, loss = 2.2185182571411133\n",
      "steps = 116, loss = 3.3483386039733887\n",
      "steps = 116, loss = 2.9395155906677246\n",
      "steps = 116, loss = 2.9822683334350586\n",
      "steps = 116, loss = 2.7098231315612793\n",
      "steps = 116, loss = 3.7579312324523926\n",
      "steps = 117, loss = 2.7138779163360596\n",
      "steps = 117, loss = 3.7595736980438232\n",
      "steps = 117, loss = 3.2914657592773438\n",
      "steps = 117, loss = 3.497694253921509\n",
      "steps = 117, loss = 2.387869358062744\n",
      "steps = 117, loss = 6.185079097747803\n",
      "steps = 117, loss = 2.9874377250671387\n",
      "steps = 117, loss = 2.7656288146972656\n",
      "steps = 117, loss = 50.01902389526367\n",
      "steps = 117, loss = 2.760202407836914\n",
      "steps = 117, loss = 3.793093681335449\n",
      "steps = 117, loss = 2.529010534286499\n",
      "steps = 117, loss = 2.653006076812744\n",
      "steps = 117, loss = 2.6698713302612305\n",
      "steps = 117, loss = 3.4383785724639893\n",
      "steps = 117, loss = 49.9853630065918\n",
      "steps = 117, loss = 2.8732428550720215\n",
      "steps = 117, loss = 3.8890347480773926\n",
      "steps = 117, loss = 2.151667594909668\n",
      "steps = 117, loss = 3.363300085067749\n",
      "steps = 117, loss = 3.0149641036987305\n",
      "steps = 117, loss = 3.870023012161255\n",
      "steps = 117, loss = 3.633885622024536\n",
      "steps = 117, loss = 3.541576385498047\n",
      "steps = 117, loss = 3.6267685890197754\n",
      "steps = 117, loss = 2.9581665992736816\n",
      "steps = 117, loss = 2.938692808151245\n",
      "steps = 117, loss = 2.726165771484375\n",
      "steps = 117, loss = 2.287687301635742\n",
      "steps = 118, loss = 49.9853630065918\n",
      "steps = 118, loss = 3.3923025131225586\n",
      "steps = 118, loss = 3.7966339588165283\n",
      "steps = 118, loss = 49.92012405395508\n",
      "steps = 118, loss = 2.1387932300567627\n",
      "steps = 118, loss = 3.1585843563079834\n",
      "steps = 118, loss = 50.02305221557617\n",
      "steps = 118, loss = 2.1051881313323975\n",
      "steps = 118, loss = 3.882591485977173\n",
      "steps = 118, loss = 3.39380145072937\n",
      "steps = 118, loss = 2.6917848587036133\n",
      "steps = 118, loss = 6.285810470581055\n",
      "steps = 118, loss = 2.6880416870117188\n",
      "steps = 118, loss = 2.837019920349121\n",
      "steps = 118, loss = 3.3472893238067627\n",
      "steps = 118, loss = 2.4162020683288574\n",
      "steps = 118, loss = 3.387647867202759\n",
      "steps = 118, loss = 3.454352617263794\n",
      "steps = 118, loss = 2.7383482456207275\n",
      "steps = 118, loss = 3.4928855895996094\n",
      "steps = 118, loss = 2.906355381011963\n",
      "steps = 118, loss = 2.8527562618255615\n",
      "steps = 118, loss = 3.7588043212890625\n",
      "steps = 118, loss = 2.7519593238830566\n",
      "steps = 118, loss = 2.7805116176605225\n",
      "steps = 118, loss = 2.974085807800293\n",
      "steps = 118, loss = 3.5214836597442627\n",
      "steps = 118, loss = 2.5914313793182373\n",
      "steps = 118, loss = 3.241741418838501\n",
      "steps = 119, loss = 3.6349704265594482\n",
      "steps = 119, loss = 2.4149351119995117\n",
      "steps = 119, loss = 2.839024543762207\n",
      "steps = 119, loss = 49.56241226196289\n",
      "steps = 119, loss = 6.401491165161133\n",
      "steps = 119, loss = 3.3132810592651367\n",
      "steps = 119, loss = 2.3864076137542725\n",
      "steps = 119, loss = 2.756504774093628\n",
      "steps = 119, loss = 3.3819425106048584\n",
      "steps = 119, loss = 49.9853630065918\n",
      "steps = 119, loss = 3.7780425548553467\n",
      "steps = 119, loss = 2.8553051948547363\n",
      "steps = 119, loss = 2.2069449424743652\n",
      "steps = 119, loss = 3.560833215713501\n",
      "steps = 119, loss = 3.816138982772827\n",
      "steps = 119, loss = 3.437958002090454\n",
      "steps = 119, loss = 3.0726590156555176\n",
      "steps = 119, loss = 3.3887641429901123\n",
      "steps = 119, loss = 2.8678910732269287\n",
      "steps = 119, loss = 2.9806220531463623\n",
      "steps = 119, loss = 3.4997153282165527\n",
      "steps = 119, loss = 2.760152816772461\n",
      "steps = 119, loss = 3.918851137161255\n",
      "steps = 119, loss = 50.02180099487305\n",
      "steps = 119, loss = 2.7382476329803467\n",
      "steps = 119, loss = 2.7101004123687744\n",
      "steps = 119, loss = 2.7669975757598877\n",
      "steps = 119, loss = 3.2427048683166504\n",
      "steps = 119, loss = 3.8023271560668945\n",
      "steps = 120, loss = 3.4924561977386475\n",
      "steps = 120, loss = 3.8296096324920654\n",
      "steps = 120, loss = 2.9857571125030518\n",
      "steps = 120, loss = 2.6875979900360107\n",
      "steps = 120, loss = 3.3232436180114746\n",
      "steps = 120, loss = 3.799570083618164\n",
      "steps = 120, loss = 2.8731892108917236\n",
      "steps = 120, loss = 2.883948564529419\n",
      "steps = 120, loss = 49.9853630065918\n",
      "steps = 120, loss = 3.4627768993377686\n",
      "steps = 120, loss = 3.3912220001220703\n",
      "steps = 120, loss = 3.023981809616089\n",
      "steps = 120, loss = 2.2883825302124023\n",
      "steps = 120, loss = 3.5429701805114746\n",
      "steps = 120, loss = 3.538111925125122\n",
      "steps = 120, loss = 3.4822919368743896\n",
      "steps = 120, loss = 2.7674412727355957\n",
      "steps = 120, loss = 6.123870372772217\n",
      "steps = 120, loss = 2.6312901973724365\n",
      "steps = 120, loss = 3.938417673110962\n",
      "steps = 120, loss = 2.7264645099639893\n",
      "steps = 120, loss = 2.601292133331299\n",
      "steps = 120, loss = 2.7748048305511475\n",
      "steps = 120, loss = 2.2746407985687256\n",
      "steps = 120, loss = 3.826230525970459\n",
      "steps = 120, loss = 3.4477736949920654\n",
      "steps = 120, loss = 2.6385512351989746\n",
      "steps = 120, loss = 50.01633071899414\n",
      "steps = 120, loss = 2.1107914447784424\n",
      "steps = 121, loss = 3.5447514057159424\n",
      "steps = 121, loss = 49.9853630065918\n",
      "steps = 121, loss = 2.704702138900757\n",
      "steps = 121, loss = 2.766369342803955\n",
      "steps = 121, loss = 2.0946927070617676\n",
      "steps = 121, loss = 2.7395362854003906\n",
      "steps = 121, loss = 2.138852119445801\n",
      "steps = 121, loss = 3.310272216796875\n",
      "steps = 121, loss = 3.7985377311706543\n",
      "steps = 121, loss = 6.282905101776123\n",
      "steps = 121, loss = 3.0117080211639404\n",
      "steps = 121, loss = 3.8292601108551025\n",
      "steps = 121, loss = 16.348134994506836\n",
      "steps = 121, loss = 3.515458345413208\n",
      "steps = 121, loss = 3.6862587928771973\n",
      "steps = 121, loss = 3.370087146759033\n",
      "steps = 121, loss = 3.3329203128814697\n",
      "steps = 121, loss = 2.7324652671813965\n",
      "steps = 121, loss = 3.584695816040039\n",
      "steps = 121, loss = 2.8368947505950928\n",
      "steps = 121, loss = 2.6919422149658203\n",
      "steps = 121, loss = 1.98841392993927\n",
      "steps = 121, loss = 3.9312503337860107\n",
      "steps = 121, loss = 2.509129047393799\n",
      "steps = 121, loss = 50.02095413208008\n",
      "steps = 121, loss = 3.096953868865967\n",
      "steps = 121, loss = 2.5770225524902344\n",
      "steps = 121, loss = 2.9725825786590576\n",
      "steps = 121, loss = 3.43230938911438\n",
      "steps = 122, loss = 2.1975746154785156\n",
      "steps = 122, loss = 2.979074478149414\n",
      "steps = 122, loss = 3.4684159755706787\n",
      "steps = 122, loss = 49.9853630065918\n",
      "steps = 122, loss = 3.8175008296966553\n",
      "steps = 122, loss = 50.020164489746094\n",
      "steps = 122, loss = 2.7813379764556885\n",
      "steps = 122, loss = 3.2320430278778076\n",
      "steps = 122, loss = 2.772300958633423\n",
      "steps = 122, loss = 2.5769073963165283\n",
      "steps = 122, loss = 2.6825456619262695\n",
      "steps = 122, loss = 2.7103493213653564\n",
      "steps = 122, loss = 3.238762617111206\n",
      "steps = 122, loss = 3.508513927459717\n",
      "steps = 122, loss = 2.134263277053833\n",
      "steps = 122, loss = 3.6463282108306885\n",
      "steps = 122, loss = 2.7629356384277344\n",
      "steps = 122, loss = 3.329235076904297\n",
      "steps = 122, loss = 3.9677793979644775\n",
      "steps = 122, loss = 2.412874460220337\n",
      "steps = 122, loss = 3.423403739929199\n",
      "steps = 122, loss = 3.8338277339935303\n",
      "steps = 122, loss = 3.4272964000701904\n",
      "steps = 122, loss = 3.1339221000671387\n",
      "steps = 122, loss = 2.855182409286499\n",
      "steps = 122, loss = 2.8581631183624268\n",
      "steps = 122, loss = 2.185137987136841\n",
      "steps = 122, loss = 6.348460674285889\n",
      "steps = 122, loss = 3.459453821182251\n",
      "steps = 123, loss = 6.2257080078125\n",
      "steps = 123, loss = 2.915386915206909\n",
      "steps = 123, loss = 49.9853630065918\n",
      "steps = 123, loss = 3.597552537918091\n",
      "steps = 123, loss = 3.98628568649292\n",
      "steps = 123, loss = 2.7266664505004883\n",
      "steps = 123, loss = 3.27471661567688\n",
      "steps = 123, loss = 2.9780149459838867\n",
      "steps = 123, loss = 2.766986846923828\n",
      "steps = 123, loss = 3.4232871532440186\n",
      "steps = 123, loss = 2.6224474906921387\n",
      "steps = 123, loss = 2.984138250350952\n",
      "steps = 123, loss = 2.9716122150421143\n",
      "steps = 123, loss = 3.5406672954559326\n",
      "steps = 123, loss = 3.838268518447876\n",
      "steps = 123, loss = 3.3662490844726562\n",
      "steps = 123, loss = 2.3422980308532715\n",
      "steps = 123, loss = 50.02494430541992\n",
      "steps = 123, loss = 2.78879976272583\n",
      "steps = 123, loss = 4.018458366394043\n",
      "steps = 123, loss = 2.873131275177002\n",
      "steps = 123, loss = 3.857633590698242\n",
      "steps = 123, loss = 2.656968832015991\n",
      "steps = 123, loss = 3.500173807144165\n",
      "steps = 123, loss = 2.2888543605804443\n",
      "steps = 123, loss = 3.377551317214966\n",
      "steps = 123, loss = 2.176922559738159\n",
      "steps = 123, loss = 2.699105978012085\n",
      "steps = 123, loss = 3.6813414096832275\n",
      "steps = 124, loss = 2.1667988300323486\n",
      "steps = 124, loss = 3.89046573638916\n",
      "steps = 124, loss = 2.9790217876434326\n",
      "steps = 124, loss = 2.792724609375\n",
      "steps = 124, loss = 3.8544270992279053\n",
      "steps = 124, loss = 3.6500229835510254\n",
      "steps = 124, loss = 3.565986394882202\n",
      "steps = 124, loss = 2.1127521991729736\n",
      "steps = 124, loss = 4.002721309661865\n",
      "steps = 124, loss = 6.054965496063232\n",
      "steps = 124, loss = 2.7238473892211914\n",
      "steps = 124, loss = 50.02229690551758\n",
      "steps = 124, loss = 2.7632312774658203\n",
      "steps = 124, loss = 2.554439067840576\n",
      "steps = 124, loss = 3.4705255031585693\n",
      "steps = 124, loss = 49.9853630065918\n",
      "steps = 124, loss = 3.845187187194824\n",
      "steps = 124, loss = 3.3866748809814453\n",
      "steps = 124, loss = 3.6606526374816895\n",
      "steps = 124, loss = 3.013376235961914\n",
      "steps = 124, loss = 3.3824989795684814\n",
      "steps = 124, loss = 2.166367769241333\n",
      "steps = 124, loss = 2.7105329036712646\n",
      "steps = 124, loss = 2.6838183403015137\n",
      "steps = 124, loss = 2.4949302673339844\n",
      "steps = 124, loss = 2.765711784362793\n",
      "steps = 124, loss = 3.3720319271087646\n",
      "steps = 124, loss = 3.2259793281555176\n",
      "steps = 124, loss = 2.8559441566467285\n",
      "steps = 125, loss = 2.666217565536499\n",
      "steps = 125, loss = 3.6490375995635986\n",
      "steps = 125, loss = 2.429504871368408\n",
      "steps = 125, loss = 2.969478130340576\n",
      "steps = 125, loss = 6.175025463104248\n",
      "steps = 125, loss = 2.142749786376953\n",
      "steps = 125, loss = 2.033613681793213\n",
      "steps = 125, loss = 3.8507871627807617\n",
      "steps = 125, loss = 3.6403708457946777\n",
      "steps = 125, loss = 3.868973970413208\n",
      "steps = 125, loss = 3.996431350708008\n",
      "steps = 125, loss = 2.5998330116271973\n",
      "steps = 125, loss = 2.784735918045044\n",
      "steps = 125, loss = 3.683030128479004\n",
      "steps = 125, loss = 2.8350470066070557\n",
      "steps = 125, loss = 2.026606798171997\n",
      "steps = 125, loss = 3.512570858001709\n",
      "steps = 125, loss = 2.857271671295166\n",
      "steps = 125, loss = 3.3769969940185547\n",
      "steps = 125, loss = 2.6897614002227783\n",
      "steps = 125, loss = 49.9853630065918\n",
      "steps = 125, loss = 2.7346770763397217\n",
      "steps = 125, loss = 3.174461603164673\n",
      "steps = 125, loss = 3.610447883605957\n",
      "steps = 125, loss = 3.3867640495300293\n",
      "steps = 125, loss = 3.79429292678833\n",
      "steps = 125, loss = 2.973968267440796\n",
      "steps = 125, loss = 50.01782989501953\n",
      "steps = 125, loss = 2.7316806316375732\n",
      "steps = 126, loss = 3.8869004249572754\n",
      "steps = 126, loss = 3.538726806640625\n",
      "steps = 126, loss = 2.727743148803711\n",
      "steps = 126, loss = 2.766878604888916\n",
      "steps = 126, loss = 2.873701810836792\n",
      "steps = 126, loss = 4.032626628875732\n",
      "steps = 126, loss = 2.8010542392730713\n",
      "steps = 126, loss = 3.399122953414917\n",
      "steps = 126, loss = 3.8601999282836914\n",
      "steps = 126, loss = 2.982255220413208\n",
      "steps = 126, loss = 3.880549907684326\n",
      "steps = 126, loss = 3.1621615886688232\n",
      "steps = 126, loss = 49.9853630065918\n",
      "steps = 126, loss = 50.01403045654297\n",
      "steps = 126, loss = 3.511793851852417\n",
      "steps = 126, loss = 2.289335012435913\n",
      "steps = 126, loss = 3.4184937477111816\n",
      "steps = 126, loss = 3.175739288330078\n",
      "steps = 126, loss = 3.398552417755127\n",
      "steps = 126, loss = 2.6720783710479736\n",
      "steps = 126, loss = 3.8766136169433594\n",
      "steps = 126, loss = 2.304506540298462\n",
      "steps = 126, loss = 2.873410224914551\n",
      "steps = 126, loss = 3.554985523223877\n",
      "steps = 126, loss = 2.9915096759796143\n",
      "steps = 126, loss = 2.7169876098632812\n",
      "steps = 126, loss = 6.250164985656738\n",
      "steps = 126, loss = 3.0118398666381836\n",
      "steps = 126, loss = 2.1845202445983887\n",
      "steps = 127, loss = 3.7364695072174072\n",
      "steps = 127, loss = 4.0272626876831055\n",
      "steps = 127, loss = 3.5265371799468994\n",
      "steps = 127, loss = 2.39711856842041\n",
      "steps = 127, loss = 3.4363629817962646\n",
      "steps = 127, loss = 2.7933671474456787\n",
      "steps = 127, loss = 3.4004600048065186\n",
      "steps = 127, loss = 2.947497606277466\n",
      "steps = 127, loss = 3.2902517318725586\n",
      "steps = 127, loss = 2.731013059616089\n",
      "steps = 127, loss = 3.1387758255004883\n",
      "steps = 127, loss = 2.9696173667907715\n",
      "steps = 127, loss = 2.604919672012329\n",
      "steps = 127, loss = 2.1388120651245117\n",
      "steps = 127, loss = 1.9671677350997925\n",
      "steps = 127, loss = 3.88983154296875\n",
      "steps = 127, loss = 3.4008028507232666\n",
      "steps = 127, loss = 2.8372137546539307\n",
      "steps = 127, loss = 6.33450984954834\n",
      "steps = 127, loss = 3.228360414505005\n",
      "steps = 127, loss = 2.692960262298584\n",
      "steps = 127, loss = 3.4392337799072266\n",
      "steps = 127, loss = 2.6936452388763428\n",
      "steps = 127, loss = 49.9853630065918\n",
      "steps = 127, loss = 3.8763039112091064\n",
      "steps = 127, loss = 2.6722986698150635\n",
      "steps = 127, loss = 3.526024103164673\n",
      "steps = 127, loss = 2.117216110229492\n",
      "steps = 127, loss = 50.02338409423828\n",
      "steps = 128, loss = 2.630211114883423\n",
      "steps = 128, loss = 3.0543947219848633\n",
      "steps = 128, loss = 3.395303726196289\n",
      "steps = 128, loss = 6.413873195648193\n",
      "steps = 128, loss = 3.38916277885437\n",
      "steps = 128, loss = 49.9853630065918\n",
      "steps = 128, loss = 3.3565456867218018\n",
      "steps = 128, loss = 3.4116010665893555\n",
      "steps = 128, loss = 50.02254104614258\n",
      "steps = 128, loss = 3.9079957008361816\n",
      "steps = 128, loss = 2.103076457977295\n",
      "steps = 128, loss = 3.0043821334838867\n",
      "steps = 128, loss = 4.064101219177246\n",
      "steps = 128, loss = 3.398859739303589\n",
      "steps = 128, loss = 3.82954478263855\n",
      "steps = 128, loss = 2.869081735610962\n",
      "steps = 128, loss = 2.619414806365967\n",
      "steps = 128, loss = 2.2905147075653076\n",
      "steps = 128, loss = 3.559174060821533\n",
      "steps = 128, loss = 2.7695798873901367\n",
      "steps = 128, loss = 2.8099772930145264\n",
      "steps = 128, loss = 3.9020097255706787\n",
      "steps = 128, loss = 4.170846939086914\n",
      "steps = 128, loss = 2.2753729820251465\n",
      "steps = 128, loss = 3.0048367977142334\n",
      "steps = 128, loss = 2.7286674976348877\n",
      "steps = 128, loss = 3.4140548706054688\n",
      "steps = 128, loss = 2.8746674060821533\n",
      "steps = 128, loss = 2.982088804244995\n",
      "steps = 129, loss = 2.2775213718414307\n",
      "steps = 129, loss = 4.082603454589844\n",
      "steps = 129, loss = 2.2281908988952637\n",
      "steps = 129, loss = 3.9090051651000977\n",
      "steps = 129, loss = 2.1936874389648438\n",
      "steps = 129, loss = 2.8555777072906494\n",
      "steps = 129, loss = 2.603656053543091\n",
      "steps = 129, loss = 2.9770708084106445\n",
      "steps = 129, loss = 3.870447874069214\n",
      "steps = 129, loss = 3.0170700550079346\n",
      "steps = 129, loss = 3.198514223098755\n",
      "steps = 129, loss = 3.6654748916625977\n",
      "steps = 129, loss = 50.02110290527344\n",
      "steps = 129, loss = 3.555835723876953\n",
      "steps = 129, loss = 2.814643144607544\n",
      "steps = 129, loss = 3.4421958923339844\n",
      "steps = 129, loss = 9.94421100616455\n",
      "steps = 129, loss = 3.370291233062744\n",
      "steps = 129, loss = 2.7108466625213623\n",
      "steps = 129, loss = 3.5401766300201416\n",
      "steps = 129, loss = 2.7621097564697266\n",
      "steps = 129, loss = 3.903455972671509\n",
      "steps = 129, loss = 2.052826404571533\n",
      "steps = 129, loss = 6.498339653015137\n",
      "steps = 129, loss = 2.6700053215026855\n",
      "steps = 129, loss = 49.9853630065918\n",
      "steps = 129, loss = 2.883746385574341\n",
      "steps = 129, loss = 3.702787160873413\n",
      "steps = 129, loss = 3.436583995819092\n",
      "steps = 130, loss = 3.3008317947387695\n",
      "steps = 130, loss = 2.7010881900787354\n",
      "steps = 130, loss = 2.7403903007507324\n",
      "steps = 130, loss = 2.6717708110809326\n",
      "steps = 130, loss = 3.3357925415039062\n",
      "steps = 130, loss = 50.01164245605469\n",
      "steps = 130, loss = 2.143758773803711\n",
      "steps = 130, loss = 3.917823076248169\n",
      "steps = 130, loss = 3.5817549228668213\n",
      "steps = 130, loss = 2.201364278793335\n",
      "steps = 130, loss = 2.691213846206665\n",
      "steps = 130, loss = 2.4132611751556396\n",
      "steps = 130, loss = 4.076963901519775\n",
      "steps = 130, loss = 2.798316717147827\n",
      "steps = 130, loss = 3.4464290142059326\n",
      "steps = 130, loss = 6.258160591125488\n",
      "steps = 130, loss = 3.7457826137542725\n",
      "steps = 130, loss = 3.347194194793701\n",
      "steps = 130, loss = 2.9196994304656982\n",
      "steps = 130, loss = 2.7668097019195557\n",
      "steps = 130, loss = 1.962052583694458\n",
      "steps = 130, loss = 2.967742681503296\n",
      "steps = 130, loss = 3.142329216003418\n",
      "steps = 130, loss = 49.9853630065918\n",
      "steps = 130, loss = 2.835822105407715\n",
      "steps = 130, loss = 2.8068556785583496\n",
      "steps = 130, loss = 3.3335115909576416\n",
      "steps = 130, loss = 2.4927115440368652\n",
      "steps = 130, loss = 3.915102958679199\n",
      "steps = 131, loss = 3.900919198989868\n",
      "steps = 131, loss = 3.920295476913452\n",
      "steps = 131, loss = 3.326058864593506\n",
      "steps = 131, loss = 3.9337828159332275\n",
      "steps = 131, loss = 3.5696136951446533\n",
      "steps = 131, loss = 50.018978118896484\n",
      "steps = 131, loss = 2.2865400314331055\n",
      "steps = 131, loss = 2.1676459312438965\n",
      "steps = 131, loss = 2.821730136871338\n",
      "steps = 131, loss = 3.3505284786224365\n",
      "steps = 131, loss = 2.7108302116394043\n",
      "steps = 131, loss = 6.359527111053467\n",
      "steps = 131, loss = 3.2593469619750977\n",
      "steps = 131, loss = 2.9016530513763428\n",
      "steps = 131, loss = 2.758711338043213\n",
      "steps = 131, loss = 3.1644694805145264\n",
      "steps = 131, loss = 3.2833564281463623\n",
      "steps = 131, loss = 2.729477643966675\n",
      "steps = 131, loss = 2.854796886444092\n",
      "steps = 131, loss = 49.9853630065918\n",
      "steps = 131, loss = 2.501331329345703\n",
      "steps = 131, loss = 3.6722028255462646\n",
      "steps = 131, loss = 3.0489442348480225\n",
      "steps = 131, loss = 3.3660852909088135\n",
      "steps = 131, loss = 2.915327787399292\n",
      "steps = 131, loss = 1.92360258102417\n",
      "steps = 131, loss = 4.113954067230225\n",
      "steps = 131, loss = 2.8948638439178467\n",
      "steps = 131, loss = 2.9743831157684326\n",
      "steps = 132, loss = 2.5704455375671387\n",
      "steps = 132, loss = 2.7687089443206787\n",
      "steps = 132, loss = 3.455687999725342\n",
      "steps = 132, loss = 2.637921094894409\n",
      "steps = 132, loss = 2.727238178253174\n",
      "steps = 132, loss = 2.9793970584869385\n",
      "steps = 132, loss = 2.872699737548828\n",
      "steps = 132, loss = 3.0514233112335205\n",
      "steps = 132, loss = 2.240083932876587\n",
      "steps = 132, loss = 3.358778238296509\n",
      "steps = 132, loss = 3.5129048824310303\n",
      "steps = 132, loss = 4.459334850311279\n",
      "steps = 132, loss = 50.01770782470703\n",
      "steps = 132, loss = 2.8290464878082275\n",
      "steps = 132, loss = 2.727822780609131\n",
      "steps = 132, loss = 3.568303346633911\n",
      "steps = 132, loss = 2.9204068183898926\n",
      "steps = 132, loss = 3.2618422508239746\n",
      "steps = 132, loss = 3.954529047012329\n",
      "steps = 132, loss = 3.424027442932129\n",
      "steps = 132, loss = 16.048826217651367\n",
      "steps = 132, loss = 2.6977107524871826\n",
      "steps = 132, loss = 3.943079948425293\n",
      "steps = 132, loss = 6.416095733642578\n",
      "steps = 132, loss = 3.224191665649414\n",
      "steps = 132, loss = 3.340665578842163\n",
      "steps = 132, loss = 49.9853630065918\n",
      "steps = 132, loss = 2.2903482913970947\n",
      "steps = 132, loss = 4.133725643157959\n",
      "steps = 133, loss = 2.601177930831909\n",
      "steps = 133, loss = 2.708937168121338\n",
      "steps = 133, loss = 3.236524820327759\n",
      "steps = 133, loss = 3.17311429977417\n",
      "steps = 133, loss = 2.1387600898742676\n",
      "steps = 133, loss = 2.6926021575927734\n",
      "steps = 133, loss = 3.5319557189941406\n",
      "steps = 133, loss = 2.2385778427124023\n",
      "steps = 133, loss = 2.836498737335205\n",
      "steps = 133, loss = 3.241549015045166\n",
      "steps = 133, loss = 2.959139823913574\n",
      "steps = 133, loss = 49.9853630065918\n",
      "steps = 133, loss = 1.932712435722351\n",
      "steps = 133, loss = 2.7361531257629395\n",
      "steps = 133, loss = 4.692916393280029\n",
      "steps = 133, loss = 2.8203678131103516\n",
      "steps = 133, loss = 2.821068525314331\n",
      "steps = 133, loss = 3.953012704849243\n",
      "steps = 133, loss = 50.02657699584961\n",
      "steps = 133, loss = 3.5403733253479004\n",
      "steps = 133, loss = 2.966798782348633\n",
      "steps = 133, loss = 3.7011468410491943\n",
      "steps = 133, loss = 3.9440500736236572\n",
      "steps = 133, loss = 3.1207122802734375\n",
      "steps = 133, loss = 3.467085838317871\n",
      "steps = 133, loss = 3.3561770915985107\n",
      "steps = 133, loss = 6.560477256774902\n",
      "steps = 133, loss = 2.4693663120269775\n",
      "steps = 133, loss = 4.127110004425049\n",
      "steps = 134, loss = 3.9709434509277344\n",
      "steps = 134, loss = 3.644507646560669\n",
      "steps = 134, loss = 50.027740478515625\n",
      "steps = 134, loss = 3.5317509174346924\n",
      "steps = 134, loss = 3.9463577270507812\n",
      "steps = 134, loss = 3.4045603275299072\n",
      "steps = 134, loss = 3.142220973968506\n",
      "steps = 134, loss = 2.6925275325775146\n",
      "steps = 134, loss = 2.758265733718872\n",
      "steps = 134, loss = 2.7700984477996826\n",
      "steps = 134, loss = 4.163476467132568\n",
      "steps = 134, loss = 3.3223752975463867\n",
      "steps = 134, loss = 2.8547589778900146\n",
      "steps = 134, loss = 2.185633420944214\n",
      "steps = 134, loss = 2.1797730922698975\n",
      "steps = 134, loss = 3.633004903793335\n",
      "steps = 134, loss = 3.4846351146698\n",
      "steps = 134, loss = 2.037667989730835\n",
      "steps = 134, loss = 3.365806818008423\n",
      "steps = 134, loss = 2.505664825439453\n",
      "steps = 134, loss = 6.151942253112793\n",
      "steps = 134, loss = 2.834674596786499\n",
      "steps = 134, loss = 3.3795697689056396\n",
      "steps = 134, loss = 2.711151599884033\n",
      "steps = 134, loss = 2.8154728412628174\n",
      "steps = 134, loss = 2.9731955528259277\n",
      "steps = 134, loss = 2.7380807399749756\n",
      "steps = 134, loss = 49.9853630065918\n",
      "steps = 134, loss = 3.274059772491455\n",
      "steps = 135, loss = 2.7276294231414795\n",
      "steps = 135, loss = 2.2896385192871094\n",
      "steps = 135, loss = 3.3662359714508057\n",
      "steps = 135, loss = 2.435372829437256\n",
      "steps = 135, loss = 3.4379794597625732\n",
      "steps = 135, loss = 3.991931200027466\n",
      "steps = 135, loss = 3.470001697540283\n",
      "steps = 135, loss = 3.084484100341797\n",
      "steps = 135, loss = 2.1821796894073486\n",
      "steps = 135, loss = 49.9853630065918\n",
      "steps = 135, loss = 2.5168192386627197\n",
      "steps = 135, loss = 3.3830907344818115\n",
      "steps = 135, loss = 3.342876672744751\n",
      "steps = 135, loss = 2.6505484580993652\n",
      "steps = 135, loss = 3.6635067462921143\n",
      "steps = 135, loss = 2.978076696395874\n",
      "steps = 135, loss = 2.768340587615967\n",
      "steps = 135, loss = 2.8728504180908203\n",
      "steps = 135, loss = 50.00961685180664\n",
      "steps = 135, loss = 6.31261682510376\n",
      "steps = 135, loss = 2.841975450515747\n",
      "steps = 135, loss = 2.9475784301757812\n",
      "steps = 135, loss = 4.182572364807129\n",
      "steps = 135, loss = 2.6627609729766846\n",
      "steps = 135, loss = 2.1168212890625\n",
      "steps = 135, loss = 2.9037272930145264\n",
      "steps = 135, loss = 3.9686484336853027\n",
      "steps = 135, loss = 3.4803740978240967\n",
      "steps = 135, loss = 4.295069217681885\n",
      "steps = 136, loss = 3.3786754608154297\n",
      "steps = 136, loss = 3.9688167572021484\n",
      "steps = 136, loss = 3.408820152282715\n",
      "steps = 136, loss = 49.9853630065918\n",
      "steps = 136, loss = 2.0786585807800293\n",
      "steps = 136, loss = 50.02431869506836\n",
      "steps = 136, loss = 2.4225447177886963\n",
      "steps = 136, loss = 3.9899344444274902\n",
      "steps = 136, loss = 2.7389168739318848\n",
      "steps = 136, loss = 3.4370298385620117\n",
      "steps = 136, loss = 3.5239920616149902\n",
      "steps = 136, loss = 2.692856550216675\n",
      "steps = 136, loss = 2.9655725955963135\n",
      "steps = 136, loss = 2.836507558822632\n",
      "steps = 136, loss = 2.833219051361084\n",
      "steps = 136, loss = 2.759643316268921\n",
      "steps = 136, loss = 2.1390082836151123\n",
      "steps = 136, loss = 4.176403045654297\n",
      "steps = 136, loss = 2.317610025405884\n",
      "steps = 136, loss = 2.5675861835479736\n",
      "steps = 136, loss = 2.971712827682495\n",
      "steps = 136, loss = 2.0679242610931396\n",
      "steps = 136, loss = 2.7527577877044678\n",
      "steps = 136, loss = 6.417201995849609\n",
      "steps = 136, loss = 3.2914540767669678\n",
      "steps = 136, loss = 3.679185628890991\n",
      "steps = 136, loss = 3.4894566535949707\n",
      "steps = 136, loss = 3.499669075012207\n",
      "steps = 136, loss = 2.538560152053833\n",
      "steps = 137, loss = 2.959681749343872\n",
      "steps = 137, loss = 3.3183753490448\n",
      "steps = 137, loss = 2.7113070487976074\n",
      "steps = 137, loss = 4.213188648223877\n",
      "steps = 137, loss = 2.4562392234802246\n",
      "steps = 137, loss = 3.6670727729797363\n",
      "steps = 137, loss = 50.01585006713867\n",
      "steps = 137, loss = 4.008234024047852\n",
      "steps = 137, loss = 3.971620559692383\n",
      "steps = 137, loss = 2.627943277359009\n",
      "steps = 137, loss = 3.6705920696258545\n",
      "steps = 137, loss = 2.886305809020996\n",
      "steps = 137, loss = 3.042198657989502\n",
      "steps = 137, loss = 2.971952438354492\n",
      "steps = 137, loss = 2.854689121246338\n",
      "steps = 137, loss = 2.8522591590881348\n",
      "steps = 137, loss = 2.0389175415039062\n",
      "steps = 137, loss = 2.8471920490264893\n",
      "steps = 137, loss = 2.7641959190368652\n",
      "steps = 137, loss = 3.358167886734009\n",
      "steps = 137, loss = 49.9853630065918\n",
      "steps = 137, loss = 3.54472017288208\n",
      "steps = 137, loss = 2.1858668327331543\n",
      "steps = 137, loss = 2.1758933067321777\n",
      "steps = 137, loss = 2.909844160079956\n",
      "steps = 137, loss = 6.500740051269531\n",
      "steps = 137, loss = 3.4393444061279297\n",
      "steps = 137, loss = 3.9902853965759277\n",
      "steps = 137, loss = 3.308953285217285\n",
      "steps = 138, loss = 3.1679811477661133\n",
      "steps = 138, loss = 2.921107292175293\n",
      "steps = 138, loss = 3.993475914001465\n",
      "steps = 138, loss = 2.7278778553009033\n",
      "steps = 138, loss = 2.9767560958862305\n",
      "steps = 138, loss = 2.2897121906280518\n",
      "steps = 138, loss = 3.549614191055298\n",
      "steps = 138, loss = 2.8546364307403564\n",
      "steps = 138, loss = 3.5471065044403076\n",
      "steps = 138, loss = 3.382453441619873\n",
      "steps = 138, loss = 4.0287322998046875\n",
      "steps = 138, loss = 2.135988473892212\n",
      "steps = 138, loss = 4.232111930847168\n",
      "steps = 138, loss = 2.1656596660614014\n",
      "steps = 138, loss = 2.8728582859039307\n",
      "steps = 138, loss = 3.4820921421051025\n",
      "steps = 138, loss = 3.5123801231384277\n",
      "steps = 138, loss = 4.343315124511719\n",
      "steps = 138, loss = 2.6596219539642334\n",
      "steps = 138, loss = 3.3797848224639893\n",
      "steps = 138, loss = 3.0747556686401367\n",
      "steps = 138, loss = 2.980400323867798\n",
      "steps = 138, loss = 2.7685303688049316\n",
      "steps = 138, loss = 2.9770359992980957\n",
      "steps = 138, loss = 3.4209747314453125\n",
      "steps = 138, loss = 6.416923999786377\n",
      "steps = 138, loss = 2.6283528804779053\n",
      "steps = 138, loss = 50.017642974853516\n",
      "steps = 138, loss = 49.9853630065918\n",
      "steps = 139, loss = 4.034903049468994\n",
      "steps = 139, loss = 2.3313496112823486\n",
      "steps = 139, loss = 3.5749104022979736\n",
      "steps = 139, loss = 2.855477809906006\n",
      "steps = 139, loss = 2.6509087085723877\n",
      "steps = 139, loss = 4.249279975891113\n",
      "steps = 139, loss = 3.1392810344696045\n",
      "steps = 139, loss = 3.400664806365967\n",
      "steps = 139, loss = 2.7116005420684814\n",
      "steps = 139, loss = 3.5256993770599365\n",
      "steps = 139, loss = 2.1068224906921387\n",
      "steps = 139, loss = 2.766512393951416\n",
      "steps = 139, loss = 50.015541076660156\n",
      "steps = 139, loss = 6.365546703338623\n",
      "steps = 139, loss = 3.082003116607666\n",
      "steps = 139, loss = 2.1739141941070557\n",
      "steps = 139, loss = 2.7632553577423096\n",
      "steps = 139, loss = 4.353874206542969\n",
      "steps = 139, loss = 3.6800992488861084\n",
      "steps = 139, loss = 3.0811643600463867\n",
      "steps = 139, loss = 3.9887561798095703\n",
      "steps = 139, loss = 3.344480276107788\n",
      "steps = 139, loss = 2.1523473262786865\n",
      "steps = 139, loss = 2.857954740524292\n",
      "steps = 139, loss = 3.3828349113464355\n",
      "steps = 139, loss = 3.5047554969787598\n",
      "steps = 139, loss = 2.9721295833587646\n",
      "steps = 139, loss = 2.7630152702331543\n",
      "steps = 139, loss = 49.9853630065918\n",
      "steps = 140, loss = 3.9991650581359863\n",
      "steps = 140, loss = 3.5302717685699463\n",
      "steps = 140, loss = 2.4104485511779785\n",
      "steps = 140, loss = 2.72044038772583\n",
      "steps = 140, loss = 2.1513254642486572\n",
      "steps = 140, loss = 3.0532448291778564\n",
      "steps = 140, loss = 2.6419174671173096\n",
      "steps = 140, loss = 4.40363883972168\n",
      "steps = 140, loss = 2.9628725051879883\n",
      "steps = 140, loss = 3.1762940883636475\n",
      "steps = 140, loss = 4.242475509643555\n",
      "steps = 140, loss = 2.736368179321289\n",
      "steps = 140, loss = 3.3904616832733154\n",
      "steps = 140, loss = 6.407644271850586\n",
      "steps = 140, loss = 50.02706527709961\n",
      "steps = 140, loss = 3.603428602218628\n",
      "steps = 140, loss = 2.20540452003479\n",
      "steps = 140, loss = 3.3291659355163574\n",
      "steps = 140, loss = 2.849944591522217\n",
      "steps = 140, loss = 3.2644240856170654\n",
      "steps = 140, loss = 2.7131259441375732\n",
      "steps = 140, loss = 2.6906557083129883\n",
      "steps = 140, loss = 2.8985040187835693\n",
      "steps = 140, loss = 2.039970636367798\n",
      "steps = 140, loss = 3.394308090209961\n",
      "steps = 140, loss = 49.9853630065918\n",
      "steps = 140, loss = 2.8347225189208984\n",
      "steps = 140, loss = 4.0397047996521\n",
      "steps = 140, loss = 3.158724308013916\n",
      "steps = 141, loss = 3.377537965774536\n",
      "steps = 141, loss = 49.9853630065918\n",
      "steps = 141, loss = 50.027931213378906\n",
      "steps = 141, loss = 2.6043407917022705\n",
      "steps = 141, loss = 2.5827720165252686\n",
      "steps = 141, loss = 2.8657572269439697\n",
      "steps = 141, loss = 4.348130226135254\n",
      "steps = 141, loss = 4.064584732055664\n",
      "steps = 141, loss = 2.8731930255889893\n",
      "steps = 141, loss = 3.413516044616699\n",
      "steps = 141, loss = 2.2367844581604004\n",
      "steps = 141, loss = 3.0873851776123047\n",
      "steps = 141, loss = 2.9751904010772705\n",
      "steps = 141, loss = 2.292170524597168\n",
      "steps = 141, loss = 2.6160826683044434\n",
      "steps = 141, loss = 2.7288031578063965\n",
      "steps = 141, loss = 3.4753878116607666\n",
      "steps = 141, loss = 2.802675247192383\n",
      "steps = 141, loss = 3.446528196334839\n",
      "steps = 141, loss = 3.337862491607666\n",
      "steps = 141, loss = 3.6551995277404785\n",
      "steps = 141, loss = 2.1565444469451904\n",
      "steps = 141, loss = 4.279698848724365\n",
      "steps = 141, loss = 2.7689309120178223\n",
      "steps = 141, loss = 3.442768096923828\n",
      "steps = 141, loss = 6.4696364402771\n",
      "steps = 141, loss = 3.369175910949707\n",
      "steps = 141, loss = 3.0327725410461426\n",
      "steps = 141, loss = 4.016688346862793\n",
      "steps = 142, loss = 2.022357225418091\n",
      "steps = 142, loss = 3.460859537124634\n",
      "steps = 142, loss = 2.562394380569458\n",
      "steps = 142, loss = 3.365039348602295\n",
      "steps = 142, loss = 2.7348906993865967\n",
      "steps = 142, loss = 50.021705627441406\n",
      "steps = 142, loss = 3.527775287628174\n",
      "steps = 142, loss = 4.064021587371826\n",
      "steps = 142, loss = 2.6545662879943848\n",
      "steps = 142, loss = 2.6937081813812256\n",
      "steps = 142, loss = 4.0163984298706055\n",
      "steps = 142, loss = 3.314307928085327\n",
      "steps = 142, loss = 3.336094856262207\n",
      "steps = 142, loss = 2.749215602874756\n",
      "steps = 142, loss = 3.181974411010742\n",
      "steps = 142, loss = 2.9632110595703125\n",
      "steps = 142, loss = 3.4130232334136963\n",
      "steps = 142, loss = 3.024660110473633\n",
      "steps = 142, loss = 2.857698917388916\n",
      "steps = 142, loss = 2.8369271755218506\n",
      "steps = 142, loss = 2.4118781089782715\n",
      "steps = 142, loss = 6.55670690536499\n",
      "steps = 142, loss = 2.60489559173584\n",
      "steps = 142, loss = 2.134540557861328\n",
      "steps = 142, loss = 3.937837600708008\n",
      "steps = 142, loss = 49.9853630065918\n",
      "steps = 142, loss = 2.049652338027954\n",
      "steps = 142, loss = 3.4237236976623535\n",
      "steps = 142, loss = 4.273300647735596\n",
      "steps = 143, loss = 3.595244884490967\n",
      "steps = 143, loss = 3.3969056606292725\n",
      "steps = 143, loss = 2.604365348815918\n",
      "steps = 143, loss = 3.659435987472534\n",
      "steps = 143, loss = 3.4525256156921387\n",
      "steps = 143, loss = 2.884775161743164\n",
      "steps = 143, loss = 2.6498806476593018\n",
      "steps = 143, loss = 2.9752418994903564\n",
      "steps = 143, loss = 2.771101474761963\n",
      "steps = 143, loss = 4.309605598449707\n",
      "steps = 143, loss = 2.2862348556518555\n",
      "steps = 143, loss = 4.088339805603027\n",
      "steps = 143, loss = 50.01755142211914\n",
      "steps = 143, loss = 2.874484062194824\n",
      "steps = 143, loss = 2.7297885417938232\n",
      "steps = 143, loss = 2.8737881183624268\n",
      "steps = 143, loss = 4.313478469848633\n",
      "steps = 143, loss = 2.082923173904419\n",
      "steps = 143, loss = 3.6510889530181885\n",
      "steps = 143, loss = 2.648648262023926\n",
      "steps = 143, loss = 2.1583006381988525\n",
      "steps = 143, loss = 2.9751803874969482\n",
      "steps = 143, loss = 4.0347466468811035\n",
      "steps = 143, loss = 3.002074718475342\n",
      "steps = 143, loss = 3.486848831176758\n",
      "steps = 143, loss = 6.533361434936523\n",
      "steps = 143, loss = 2.932169198989868\n",
      "steps = 143, loss = 3.3512439727783203\n",
      "steps = 143, loss = 49.9853630065918\n",
      "steps = 144, loss = 3.2839407920837402\n",
      "steps = 144, loss = 2.216946840286255\n",
      "steps = 144, loss = 3.400646209716797\n",
      "steps = 144, loss = 2.0605523586273193\n",
      "steps = 144, loss = 2.0747289657592773\n",
      "steps = 144, loss = 2.7140719890594482\n",
      "steps = 144, loss = 3.37985897064209\n",
      "steps = 144, loss = 2.970655679702759\n",
      "steps = 144, loss = 49.9853630065918\n",
      "steps = 144, loss = 3.486967086791992\n",
      "steps = 144, loss = 3.5349392890930176\n",
      "steps = 144, loss = 2.631777048110962\n",
      "steps = 144, loss = 6.372716426849365\n",
      "steps = 144, loss = 4.328648567199707\n",
      "steps = 144, loss = 2.8892476558685303\n",
      "steps = 144, loss = 2.5431363582611084\n",
      "steps = 144, loss = 4.029951095581055\n",
      "steps = 144, loss = 3.137333869934082\n",
      "steps = 144, loss = 2.7119317054748535\n",
      "steps = 144, loss = 2.855163812637329\n",
      "steps = 144, loss = 3.3803422451019287\n",
      "steps = 144, loss = 3.451197862625122\n",
      "steps = 144, loss = 2.7216806411743164\n",
      "steps = 144, loss = 2.877659559249878\n",
      "steps = 144, loss = 2.76316499710083\n",
      "steps = 144, loss = 50.0178337097168\n",
      "steps = 144, loss = 3.6334409713745117\n",
      "steps = 144, loss = 4.094547748565674\n",
      "steps = 144, loss = 3.307344913482666\n",
      "steps = 145, loss = 2.855069398880005\n",
      "steps = 145, loss = 3.038360357284546\n",
      "steps = 145, loss = 2.4316422939300537\n",
      "steps = 145, loss = 3.648946762084961\n",
      "steps = 145, loss = 3.46748948097229\n",
      "steps = 145, loss = 3.5525081157684326\n",
      "steps = 145, loss = 2.7115888595581055\n",
      "steps = 145, loss = 2.9081757068634033\n",
      "steps = 145, loss = 3.6124110221862793\n",
      "steps = 145, loss = 2.7723541259765625\n",
      "steps = 145, loss = 50.01691818237305\n",
      "steps = 145, loss = 3.3466057777404785\n",
      "steps = 145, loss = 3.4847781658172607\n",
      "steps = 145, loss = 2.969309091567993\n",
      "steps = 145, loss = 4.106813430786133\n",
      "steps = 145, loss = 2.5870234966278076\n",
      "steps = 145, loss = 2.76119065284729\n",
      "steps = 145, loss = 50.021270751953125\n",
      "steps = 145, loss = 4.0364227294921875\n",
      "steps = 145, loss = 2.0787765979766846\n",
      "steps = 145, loss = 3.5287556648254395\n",
      "steps = 145, loss = 3.5141074657440186\n",
      "steps = 145, loss = 4.346724510192871\n",
      "steps = 145, loss = 2.8819124698638916\n",
      "steps = 145, loss = 2.920133113861084\n",
      "steps = 145, loss = 2.1715965270996094\n",
      "steps = 145, loss = 49.9853630065918\n",
      "steps = 145, loss = 6.395871162414551\n",
      "steps = 145, loss = 5.151381492614746\n",
      "steps = 146, loss = 2.6905999183654785\n",
      "steps = 146, loss = 2.834385633468628\n",
      "steps = 146, loss = 3.620744228363037\n",
      "steps = 146, loss = 3.306762456893921\n",
      "steps = 146, loss = 2.1425957679748535\n",
      "steps = 146, loss = 2.535322666168213\n",
      "steps = 146, loss = 3.604501485824585\n",
      "steps = 146, loss = 3.0503411293029785\n",
      "steps = 146, loss = 3.372102975845337\n",
      "steps = 146, loss = 50.01595687866211\n",
      "steps = 146, loss = 2.367985486984253\n",
      "steps = 146, loss = 3.165834426879883\n",
      "steps = 146, loss = 2.7396750450134277\n",
      "steps = 146, loss = 2.7675204277038574\n",
      "steps = 146, loss = 2.873981237411499\n",
      "steps = 146, loss = 3.41121506690979\n",
      "steps = 146, loss = 4.045825004577637\n",
      "steps = 146, loss = 4.339652061462402\n",
      "steps = 146, loss = 6.503807067871094\n",
      "steps = 146, loss = 2.578073740005493\n",
      "steps = 146, loss = 2.57112717628479\n",
      "steps = 146, loss = 3.419646978378296\n",
      "steps = 146, loss = 1.9659596681594849\n",
      "steps = 146, loss = 1.8722134828567505\n",
      "steps = 146, loss = 3.4712252616882324\n",
      "steps = 146, loss = 49.9853630065918\n",
      "steps = 146, loss = 4.111767292022705\n",
      "steps = 146, loss = 3.397745370864868\n",
      "steps = 146, loss = 2.9603259563446045\n",
      "steps = 147, loss = 3.5268731117248535\n",
      "steps = 147, loss = 4.376145839691162\n",
      "steps = 147, loss = 4.063994884490967\n",
      "steps = 147, loss = 2.889967441558838\n",
      "steps = 147, loss = 3.335667371749878\n",
      "steps = 147, loss = 3.065537691116333\n",
      "steps = 147, loss = 3.3399507999420166\n",
      "steps = 147, loss = 2.9782094955444336\n",
      "steps = 147, loss = 4.136073112487793\n",
      "steps = 147, loss = 2.629086971282959\n",
      "steps = 147, loss = 3.3629262447357178\n",
      "steps = 147, loss = 6.5435967445373535\n",
      "steps = 147, loss = 2.872847080230713\n",
      "steps = 147, loss = 2.1634151935577393\n",
      "steps = 147, loss = 2.729217290878296\n",
      "steps = 147, loss = 3.5728280544281006\n",
      "steps = 147, loss = 2.0453310012817383\n",
      "steps = 147, loss = 2.972576856613159\n",
      "steps = 147, loss = 2.6875131130218506\n",
      "steps = 147, loss = 2.9922585487365723\n",
      "steps = 147, loss = 2.7697203159332275\n",
      "steps = 147, loss = 3.39009428024292\n",
      "steps = 147, loss = 3.8494627475738525\n",
      "steps = 147, loss = 3.6597659587860107\n",
      "steps = 147, loss = 2.2860970497131348\n",
      "steps = 147, loss = 49.9853630065918\n",
      "steps = 147, loss = 50.014217376708984\n",
      "steps = 147, loss = 3.66475248336792\n",
      "steps = 147, loss = 2.914937734603882\n",
      "steps = 148, loss = 4.060214519500732\n",
      "steps = 148, loss = 6.620391845703125\n",
      "steps = 148, loss = 3.4232208728790283\n",
      "steps = 148, loss = 2.7123162746429443\n",
      "steps = 148, loss = 2.0413589477539062\n",
      "steps = 148, loss = 2.8550467491149902\n",
      "steps = 148, loss = 3.3733906745910645\n",
      "steps = 148, loss = 49.9853630065918\n",
      "steps = 148, loss = 3.3700830936431885\n",
      "steps = 148, loss = 2.173898696899414\n",
      "steps = 148, loss = 3.3992483615875244\n",
      "steps = 148, loss = 2.202627658843994\n",
      "steps = 148, loss = 3.2911276817321777\n",
      "steps = 148, loss = 2.74514102935791\n",
      "steps = 148, loss = 4.395331859588623\n",
      "steps = 148, loss = 3.2256877422332764\n",
      "steps = 148, loss = 4.1423420906066895\n",
      "steps = 148, loss = 9.47352123260498\n",
      "steps = 148, loss = 2.846259832382202\n",
      "steps = 148, loss = 2.8937761783599854\n",
      "steps = 148, loss = 5.5338640213012695\n",
      "steps = 148, loss = 3.4047744274139404\n",
      "steps = 148, loss = 50.0119514465332\n",
      "steps = 148, loss = 2.4445459842681885\n",
      "steps = 148, loss = 3.149312734603882\n",
      "steps = 148, loss = 2.759178876876831\n",
      "steps = 148, loss = 2.8138983249664307\n",
      "steps = 148, loss = 2.9684364795684814\n",
      "steps = 148, loss = 3.5052554607391357\n",
      "steps = 149, loss = 2.9543795585632324\n",
      "steps = 149, loss = 3.444209575653076\n",
      "steps = 149, loss = 1.992799997329712\n",
      "steps = 149, loss = 3.1144237518310547\n",
      "steps = 149, loss = 4.079776287078857\n",
      "steps = 149, loss = 2.841386079788208\n",
      "steps = 149, loss = 2.873286724090576\n",
      "steps = 149, loss = 4.411456108093262\n",
      "steps = 149, loss = 2.728534698486328\n",
      "steps = 149, loss = 3.1564295291900635\n",
      "steps = 149, loss = 3.5033891201019287\n",
      "steps = 149, loss = 3.656604290008545\n",
      "steps = 149, loss = 50.01468276977539\n",
      "steps = 149, loss = 4.160881519317627\n",
      "steps = 149, loss = 49.9853630065918\n",
      "steps = 149, loss = 3.4311280250549316\n",
      "steps = 149, loss = 6.57407283782959\n",
      "steps = 149, loss = 2.768617868423462\n",
      "steps = 149, loss = 3.030099630355835\n",
      "steps = 149, loss = 3.516695022583008\n",
      "steps = 149, loss = 2.1093716621398926\n",
      "steps = 149, loss = 2.8999335765838623\n",
      "steps = 149, loss = 2.9725584983825684\n",
      "steps = 149, loss = 3.0321946144104004\n",
      "steps = 149, loss = 2.613909959793091\n",
      "steps = 149, loss = 2.289794683456421\n",
      "steps = 149, loss = 2.660149097442627\n",
      "steps = 149, loss = 2.787163257598877\n",
      "steps = 149, loss = 3.3964035511016846\n",
      "steps = 150, loss = 2.2844672203063965\n",
      "steps = 150, loss = 2.4459052085876465\n",
      "steps = 150, loss = 49.9853630065918\n",
      "steps = 150, loss = 2.697739601135254\n",
      "steps = 150, loss = 3.4125514030456543\n",
      "steps = 150, loss = 4.158506870269775\n",
      "steps = 150, loss = 2.739497661590576\n",
      "steps = 150, loss = 6.755863666534424\n",
      "steps = 150, loss = 3.47180438041687\n",
      "steps = 150, loss = 2.9503798484802246\n",
      "steps = 150, loss = 2.9602575302124023\n",
      "steps = 150, loss = 3.08512544631958\n",
      "steps = 150, loss = 2.1340677738189697\n",
      "steps = 150, loss = 2.6928913593292236\n",
      "steps = 150, loss = 3.4500393867492676\n",
      "steps = 150, loss = 50.01150131225586\n",
      "steps = 150, loss = 2.0385470390319824\n",
      "steps = 150, loss = 3.372483730316162\n",
      "steps = 150, loss = 3.061121940612793\n",
      "steps = 150, loss = 2.829528570175171\n",
      "steps = 150, loss = 2.02184796333313\n",
      "steps = 150, loss = 4.077910423278809\n",
      "steps = 150, loss = 3.427370071411133\n",
      "steps = 150, loss = 2.5145132541656494\n",
      "steps = 150, loss = 3.2380104064941406\n",
      "steps = 150, loss = 4.404734134674072\n",
      "steps = 150, loss = 2.75787091255188\n",
      "steps = 150, loss = 2.8357303142547607\n",
      "steps = 150, loss = 2.8903586864471436\n",
      "steps = 151, loss = 2.4395158290863037\n",
      "steps = 151, loss = 2.287266254425049\n",
      "steps = 151, loss = 4.082622051239014\n",
      "steps = 151, loss = 1.9366062879562378\n",
      "steps = 151, loss = 3.1391565799713135\n",
      "steps = 151, loss = 3.5109763145446777\n",
      "steps = 151, loss = 3.6878230571746826\n",
      "steps = 151, loss = 3.101059675216675\n",
      "steps = 151, loss = 49.9853630065918\n",
      "steps = 151, loss = 3.5634539127349854\n",
      "steps = 151, loss = 49.9950065612793\n",
      "steps = 151, loss = 6.394695281982422\n",
      "steps = 151, loss = 2.9666054248809814\n",
      "steps = 151, loss = 2.6826233863830566\n",
      "steps = 151, loss = 4.176698684692383\n",
      "steps = 151, loss = 2.7593448162078857\n",
      "steps = 151, loss = 3.566934823989868\n",
      "steps = 151, loss = 2.996072769165039\n",
      "steps = 151, loss = 2.175751209259033\n",
      "steps = 151, loss = 2.7121877670288086\n",
      "steps = 151, loss = 3.3241264820098877\n",
      "steps = 151, loss = 2.131011486053467\n",
      "steps = 151, loss = 2.7074644565582275\n",
      "steps = 151, loss = 3.394515037536621\n",
      "steps = 151, loss = 2.8543241024017334\n",
      "steps = 151, loss = 2.8462975025177\n",
      "steps = 151, loss = 4.441274166107178\n",
      "steps = 151, loss = 2.9038686752319336\n",
      "steps = 151, loss = 3.486978769302368\n",
      "steps = 152, loss = 3.388538122177124\n",
      "steps = 152, loss = 2.6381592750549316\n",
      "steps = 152, loss = 2.872887372970581\n",
      "steps = 152, loss = 2.65617299079895\n",
      "steps = 152, loss = 4.195630073547363\n",
      "steps = 152, loss = 2.3756937980651855\n",
      "steps = 152, loss = 2.7693052291870117\n",
      "steps = 152, loss = 2.9454023838043213\n",
      "steps = 152, loss = 3.578176736831665\n",
      "steps = 152, loss = 3.4991958141326904\n",
      "steps = 152, loss = 2.893834352493286\n",
      "steps = 152, loss = 2.9711952209472656\n",
      "steps = 152, loss = 3.339176893234253\n",
      "steps = 152, loss = 2.796299457550049\n",
      "steps = 152, loss = 2.268043041229248\n",
      "steps = 152, loss = 4.459056377410889\n",
      "steps = 152, loss = 2.192397117614746\n",
      "steps = 152, loss = 2.681330919265747\n",
      "steps = 152, loss = 50.01362991333008\n",
      "steps = 152, loss = 2.290485382080078\n",
      "steps = 152, loss = 3.3086276054382324\n",
      "steps = 152, loss = 2.7287237644195557\n",
      "steps = 152, loss = 3.3904874324798584\n",
      "steps = 152, loss = 3.0095744132995605\n",
      "steps = 152, loss = 6.526520729064941\n",
      "steps = 152, loss = 4.102711200714111\n",
      "steps = 152, loss = 3.481424570083618\n",
      "steps = 152, loss = 49.9853630065918\n",
      "steps = 152, loss = 2.9110822677612305\n",
      "steps = 153, loss = 2.728808641433716\n",
      "steps = 153, loss = 2.5930495262145996\n",
      "steps = 153, loss = 3.0872645378112793\n",
      "steps = 153, loss = 3.6914803981781006\n",
      "steps = 153, loss = 2.3422508239746094\n",
      "steps = 153, loss = 2.901524305343628\n",
      "steps = 153, loss = 2.738211154937744\n",
      "steps = 153, loss = 3.7362804412841797\n",
      "steps = 153, loss = 2.1391117572784424\n",
      "steps = 153, loss = 2.4319088459014893\n",
      "steps = 153, loss = 4.4527082443237305\n",
      "steps = 153, loss = 4.193023204803467\n",
      "steps = 153, loss = 3.0507943630218506\n",
      "steps = 153, loss = 49.9853630065918\n",
      "steps = 153, loss = 2.959127187728882\n",
      "steps = 153, loss = 50.011810302734375\n",
      "steps = 153, loss = 2.8358936309814453\n",
      "steps = 153, loss = 3.571392059326172\n",
      "steps = 153, loss = 6.58173131942749\n",
      "steps = 153, loss = 3.2612268924713135\n",
      "steps = 153, loss = 2.7925209999084473\n",
      "steps = 153, loss = 3.561037302017212\n",
      "steps = 153, loss = 3.4157912731170654\n",
      "steps = 153, loss = 2.0851309299468994\n",
      "steps = 153, loss = 3.3990395069122314\n",
      "steps = 153, loss = 3.351879596710205\n",
      "steps = 153, loss = 2.0394692420959473\n",
      "steps = 153, loss = 2.693296194076538\n",
      "steps = 153, loss = 4.1003241539001465\n",
      "steps = 154, loss = 3.54640793800354\n",
      "steps = 154, loss = 49.9853630065918\n",
      "steps = 154, loss = 2.8542613983154297\n",
      "steps = 154, loss = 2.7075917720794678\n",
      "steps = 154, loss = 3.75797700881958\n",
      "steps = 154, loss = 2.8818109035491943\n",
      "steps = 154, loss = 2.350362777709961\n",
      "steps = 154, loss = 3.517915964126587\n",
      "steps = 154, loss = 2.9655210971832275\n",
      "steps = 154, loss = 2.496367931365967\n",
      "steps = 154, loss = 2.198620319366455\n",
      "steps = 154, loss = 4.489985942840576\n",
      "steps = 154, loss = 2.762716293334961\n",
      "steps = 154, loss = 2.7756216526031494\n",
      "steps = 154, loss = 2.712329387664795\n",
      "steps = 154, loss = 2.7377190589904785\n",
      "steps = 154, loss = 4.210357666015625\n",
      "steps = 154, loss = 4.105603218078613\n",
      "steps = 154, loss = 2.4992573261260986\n",
      "steps = 154, loss = 3.352167844772339\n",
      "steps = 154, loss = 6.65431022644043\n",
      "steps = 154, loss = 3.5442934036254883\n",
      "steps = 154, loss = 3.4143877029418945\n",
      "steps = 154, loss = 50.01051330566406\n",
      "steps = 154, loss = 2.2650444507598877\n",
      "steps = 154, loss = 2.088618040084839\n",
      "steps = 154, loss = 2.9152209758758545\n",
      "steps = 154, loss = 3.3695967197418213\n",
      "steps = 154, loss = 3.635864496231079\n",
      "steps = 155, loss = 2.7912580966949463\n",
      "steps = 155, loss = 3.5736284255981445\n",
      "steps = 155, loss = 2.872835874557495\n",
      "steps = 155, loss = 2.769834280014038\n",
      "steps = 155, loss = 4.5074143409729\n",
      "steps = 155, loss = 3.77023983001709\n",
      "steps = 155, loss = 2.9284048080444336\n",
      "steps = 155, loss = 50.00947189331055\n",
      "steps = 155, loss = 49.9853630065918\n",
      "steps = 155, loss = 2.290534019470215\n",
      "steps = 155, loss = 2.580540657043457\n",
      "steps = 155, loss = 2.7289233207702637\n",
      "steps = 155, loss = 6.606025695800781\n",
      "steps = 155, loss = 2.646833658218384\n",
      "steps = 155, loss = 3.6338207721710205\n",
      "steps = 155, loss = 4.229144096374512\n",
      "steps = 155, loss = 3.4023189544677734\n",
      "steps = 155, loss = 2.4687180519104004\n",
      "steps = 155, loss = 3.467848300933838\n",
      "steps = 155, loss = 4.12538480758667\n",
      "steps = 155, loss = 3.1724295616149902\n",
      "steps = 155, loss = 2.9223430156707764\n",
      "steps = 155, loss = 2.157151222229004\n",
      "steps = 155, loss = 3.3895652294158936\n",
      "steps = 155, loss = 2.2064480781555176\n",
      "steps = 155, loss = 2.9700968265533447\n",
      "steps = 155, loss = 3.5738701820373535\n",
      "steps = 155, loss = 2.937044143676758\n",
      "steps = 155, loss = 3.277758836746216\n",
      "steps = 156, loss = 2.752209186553955\n",
      "steps = 156, loss = 3.02937650680542\n",
      "steps = 156, loss = 2.912933826446533\n",
      "steps = 156, loss = 3.149738073348999\n",
      "steps = 156, loss = 2.442270517349243\n",
      "steps = 156, loss = 2.835996627807617\n",
      "steps = 156, loss = 49.9853630065918\n",
      "steps = 156, loss = 6.728143692016602\n",
      "steps = 156, loss = 2.7383062839508057\n",
      "steps = 156, loss = 2.9581265449523926\n",
      "steps = 156, loss = 2.6000583171844482\n",
      "steps = 156, loss = 2.6935999393463135\n",
      "steps = 156, loss = 4.123204231262207\n",
      "steps = 156, loss = 2.059882164001465\n",
      "steps = 156, loss = 3.3805148601531982\n",
      "steps = 156, loss = 3.4858386516571045\n",
      "steps = 156, loss = 1.96433424949646\n",
      "steps = 156, loss = 3.30855131149292\n",
      "steps = 156, loss = 3.62579083442688\n",
      "steps = 156, loss = 2.1390254497528076\n",
      "steps = 156, loss = 3.1957616806030273\n",
      "steps = 156, loss = 50.01176452636719\n",
      "steps = 156, loss = 3.497098684310913\n",
      "steps = 156, loss = 3.499889850616455\n",
      "steps = 156, loss = 2.556819438934326\n",
      "steps = 156, loss = 4.226792335510254\n",
      "steps = 156, loss = 4.501432418823242\n",
      "steps = 156, loss = 3.1940226554870605\n",
      "steps = 156, loss = 3.4004902839660645\n",
      "steps = 157, loss = 3.4860215187072754\n",
      "steps = 157, loss = 3.808196544647217\n",
      "steps = 157, loss = 3.3156251907348633\n",
      "steps = 157, loss = 2.9286351203918457\n",
      "steps = 157, loss = 3.0306599140167236\n",
      "steps = 157, loss = 3.3697073459625244\n",
      "steps = 157, loss = 3.455396890640259\n",
      "steps = 157, loss = 2.160567283630371\n",
      "steps = 157, loss = 6.6273627281188965\n",
      "steps = 157, loss = 2.5832884311676025\n",
      "steps = 157, loss = 4.250403881072998\n",
      "steps = 157, loss = 2.8743221759796143\n",
      "steps = 157, loss = 49.9853630065918\n",
      "steps = 157, loss = 2.981252431869507\n",
      "steps = 157, loss = 2.2813782691955566\n",
      "steps = 157, loss = 2.243826389312744\n",
      "steps = 157, loss = 2.969918727874756\n",
      "steps = 157, loss = 2.7304463386535645\n",
      "steps = 157, loss = 4.142920970916748\n",
      "steps = 157, loss = 2.7707769870758057\n",
      "steps = 157, loss = 50.008846282958984\n",
      "steps = 157, loss = 2.7776408195495605\n",
      "steps = 157, loss = 3.6006672382354736\n",
      "steps = 157, loss = 2.7892982959747314\n",
      "steps = 157, loss = 2.558077573776245\n",
      "steps = 157, loss = 4.537539482116699\n",
      "steps = 157, loss = 3.6246469020843506\n",
      "steps = 157, loss = 3.120692253112793\n",
      "steps = 157, loss = 2.9527482986450195\n",
      "steps = 158, loss = 6.7932610511779785\n",
      "steps = 158, loss = 2.1736578941345215\n",
      "steps = 158, loss = 2.9747722148895264\n",
      "steps = 158, loss = 2.5447890758514404\n",
      "steps = 158, loss = 50.008087158203125\n",
      "steps = 158, loss = 3.4101474285125732\n",
      "steps = 158, loss = 3.414160966873169\n",
      "steps = 158, loss = 3.416412353515625\n",
      "steps = 158, loss = 2.147801399230957\n",
      "steps = 158, loss = 2.7370359897613525\n",
      "steps = 158, loss = 2.8373262882232666\n",
      "steps = 158, loss = 49.9853630065918\n",
      "steps = 158, loss = 3.2216920852661133\n",
      "steps = 158, loss = 2.9198505878448486\n",
      "steps = 158, loss = 3.3706417083740234\n",
      "steps = 158, loss = 3.532684564590454\n",
      "steps = 158, loss = 2.695235252380371\n",
      "steps = 158, loss = 2.795604705810547\n",
      "steps = 158, loss = 2.9388034343719482\n",
      "steps = 158, loss = 2.9582395553588867\n",
      "steps = 158, loss = 2.1386728286743164\n",
      "steps = 158, loss = 3.1336588859558105\n",
      "steps = 158, loss = 4.533111572265625\n",
      "steps = 158, loss = 3.1513943672180176\n",
      "steps = 158, loss = 3.585172176361084\n",
      "steps = 158, loss = 4.14128303527832\n",
      "steps = 158, loss = 8.941234588623047\n",
      "steps = 158, loss = 2.572002649307251\n",
      "steps = 158, loss = 4.248317718505859\n",
      "steps = 159, loss = 3.088772773742676\n",
      "steps = 159, loss = 2.7623109817504883\n",
      "steps = 159, loss = 6.533670902252197\n",
      "steps = 159, loss = 3.435497283935547\n",
      "steps = 159, loss = 4.570931911468506\n",
      "steps = 159, loss = 3.5185747146606445\n",
      "steps = 159, loss = 2.775228500366211\n",
      "steps = 159, loss = 3.4824061393737793\n",
      "steps = 159, loss = 2.8540701866149902\n",
      "steps = 159, loss = 2.9643821716308594\n",
      "steps = 159, loss = 2.829054594039917\n",
      "steps = 159, loss = 2.381150245666504\n",
      "steps = 159, loss = 3.1955742835998535\n",
      "steps = 159, loss = 2.048067331314087\n",
      "steps = 159, loss = 3.4796712398529053\n",
      "steps = 159, loss = 49.9853630065918\n",
      "steps = 159, loss = 3.349058151245117\n",
      "steps = 159, loss = 1.919219732284546\n",
      "steps = 159, loss = 2.203402280807495\n",
      "steps = 159, loss = 4.266148567199707\n",
      "steps = 159, loss = 2.9335250854492188\n",
      "steps = 159, loss = 2.712388753890991\n",
      "steps = 159, loss = 2.6932549476623535\n",
      "steps = 159, loss = 3.4093739986419678\n",
      "steps = 159, loss = 3.293696641921997\n",
      "steps = 159, loss = 4.146468639373779\n",
      "steps = 159, loss = 3.6722447872161865\n",
      "steps = 159, loss = 2.7569990158081055\n",
      "steps = 159, loss = 50.006412506103516\n",
      "steps = 160, loss = 6.615705490112305\n",
      "steps = 160, loss = 3.6197075843811035\n",
      "steps = 160, loss = 3.596998691558838\n",
      "steps = 160, loss = 3.3959507942199707\n",
      "steps = 160, loss = 4.590738773345947\n",
      "steps = 160, loss = 2.0768048763275146\n",
      "steps = 160, loss = 2.9383718967437744\n",
      "steps = 160, loss = 4.153616428375244\n",
      "steps = 160, loss = 3.512934923171997\n",
      "steps = 160, loss = 3.021627187728882\n",
      "steps = 160, loss = 2.8544788360595703\n",
      "steps = 160, loss = 2.963698625564575\n",
      "steps = 160, loss = 2.735555410385132\n",
      "steps = 160, loss = 3.0683705806732178\n",
      "steps = 160, loss = 2.712332248687744\n",
      "steps = 160, loss = 2.6881089210510254\n",
      "steps = 160, loss = 2.760653257369995\n",
      "steps = 160, loss = 3.0246362686157227\n",
      "steps = 160, loss = 3.4880876541137695\n",
      "steps = 160, loss = 2.1238491535186768\n",
      "steps = 160, loss = 3.548319101333618\n",
      "steps = 160, loss = 4.278449535369873\n",
      "steps = 160, loss = 50.00716781616211\n",
      "steps = 160, loss = 3.4105162620544434\n",
      "steps = 160, loss = 3.6258187294006348\n",
      "steps = 160, loss = 2.4065017700195312\n",
      "steps = 160, loss = 2.1415047645568848\n",
      "steps = 160, loss = 2.910071849822998\n",
      "steps = 160, loss = 49.9853630065918\n",
      "steps = 161, loss = 2.711949348449707\n",
      "steps = 161, loss = 4.16070556640625\n",
      "steps = 161, loss = 3.033586025238037\n",
      "steps = 161, loss = 3.154783010482788\n",
      "steps = 161, loss = 2.4449737071990967\n",
      "steps = 161, loss = 2.7375895977020264\n",
      "steps = 161, loss = 4.282492637634277\n",
      "steps = 161, loss = 2.663386583328247\n",
      "steps = 161, loss = 3.6800336837768555\n",
      "steps = 161, loss = 3.5884599685668945\n",
      "steps = 161, loss = 4.199473857879639\n",
      "steps = 161, loss = 2.1409518718719482\n",
      "steps = 161, loss = 3.3879566192626953\n",
      "steps = 161, loss = 2.834671974182129\n",
      "steps = 161, loss = 1.9252907037734985\n",
      "steps = 161, loss = 3.3882436752319336\n",
      "steps = 161, loss = 2.6917924880981445\n",
      "steps = 161, loss = 1.9966729879379272\n",
      "steps = 161, loss = 2.607002019882202\n",
      "steps = 161, loss = 2.9304652214050293\n",
      "steps = 161, loss = 2.9551303386688232\n",
      "steps = 161, loss = 2.6684014797210693\n",
      "steps = 161, loss = 49.9853630065918\n",
      "steps = 161, loss = 3.238659381866455\n",
      "steps = 161, loss = 3.6464626789093018\n",
      "steps = 161, loss = 4.583914756774902\n",
      "steps = 161, loss = 50.001976013183594\n",
      "steps = 161, loss = 2.6376521587371826\n",
      "steps = 161, loss = 6.6033711433410645\n",
      "steps = 162, loss = 4.280005931854248\n",
      "steps = 162, loss = 3.523458480834961\n",
      "steps = 162, loss = 2.9511799812316895\n",
      "steps = 162, loss = 2.645021677017212\n",
      "steps = 162, loss = 2.967130422592163\n",
      "steps = 162, loss = 3.027885913848877\n",
      "steps = 162, loss = 2.7719953060150146\n",
      "steps = 162, loss = 2.14900279045105\n",
      "steps = 162, loss = 3.5135014057159424\n",
      "steps = 162, loss = 3.4160544872283936\n",
      "steps = 162, loss = 4.305361747741699\n",
      "steps = 162, loss = 4.62011194229126\n",
      "steps = 162, loss = 3.376516103744507\n",
      "steps = 162, loss = 3.593384265899658\n",
      "steps = 162, loss = 3.417145252227783\n",
      "steps = 162, loss = 49.9853630065918\n",
      "steps = 162, loss = 2.2853283882141113\n",
      "steps = 162, loss = 2.9462926387786865\n",
      "steps = 162, loss = 2.8726320266723633\n",
      "steps = 162, loss = 3.173379421234131\n",
      "steps = 162, loss = 2.2594752311706543\n",
      "steps = 162, loss = 4.180078983306885\n",
      "steps = 162, loss = 3.5273170471191406\n",
      "steps = 162, loss = 6.6978302001953125\n",
      "steps = 162, loss = 2.576700448989868\n",
      "steps = 162, loss = 3.0709266662597656\n",
      "steps = 162, loss = 2.730060338973999\n",
      "steps = 162, loss = 49.92323303222656\n",
      "steps = 162, loss = 2.6622743606567383\n",
      "steps = 163, loss = 50.036521911621094\n",
      "steps = 163, loss = 2.7311933040618896\n",
      "steps = 163, loss = 2.145040512084961\n",
      "steps = 163, loss = 3.010709762573242\n",
      "steps = 163, loss = 2.8754611015319824\n",
      "steps = 163, loss = 3.3990893363952637\n",
      "steps = 163, loss = 2.2822980880737305\n",
      "steps = 163, loss = 2.6206612586975098\n",
      "steps = 163, loss = 3.3730392456054688\n",
      "steps = 163, loss = 4.190842151641846\n",
      "steps = 163, loss = 2.630747079849243\n",
      "steps = 163, loss = 3.658229351043701\n",
      "steps = 163, loss = 2.9519052505493164\n",
      "steps = 163, loss = 3.5373125076293945\n",
      "steps = 163, loss = 6.72987699508667\n",
      "steps = 163, loss = 2.6753673553466797\n",
      "steps = 163, loss = 4.317063808441162\n",
      "steps = 163, loss = 50.01622772216797\n",
      "steps = 163, loss = 4.077422142028809\n",
      "steps = 163, loss = 2.63871169090271\n",
      "steps = 163, loss = 2.943035364151001\n",
      "steps = 163, loss = 3.3846194744110107\n",
      "steps = 163, loss = 4.638799667358398\n",
      "steps = 163, loss = 2.9686317443847656\n",
      "steps = 163, loss = 3.5567750930786133\n",
      "steps = 163, loss = 2.266728401184082\n",
      "steps = 163, loss = 49.9853630065918\n",
      "steps = 163, loss = 2.770251750946045\n",
      "steps = 163, loss = 3.4821395874023438\n",
      "steps = 164, loss = 2.9641048908233643\n",
      "steps = 164, loss = 3.5278401374816895\n",
      "steps = 164, loss = 2.4456307888031006\n",
      "steps = 164, loss = 2.6145777702331543\n",
      "steps = 164, loss = 4.1879963874816895\n",
      "steps = 164, loss = 2.0777347087860107\n",
      "steps = 164, loss = 2.3029024600982666\n",
      "steps = 164, loss = 2.953963279724121\n",
      "steps = 164, loss = 2.85505747795105\n",
      "steps = 164, loss = 4.3222246170043945\n",
      "steps = 164, loss = 50.005836486816406\n",
      "steps = 164, loss = 2.221933603286743\n",
      "steps = 164, loss = 2.9723899364471436\n",
      "steps = 164, loss = 3.339116096496582\n",
      "steps = 164, loss = 3.4416165351867676\n",
      "steps = 164, loss = 3.444265842437744\n",
      "steps = 164, loss = 2.713047742843628\n",
      "steps = 164, loss = 2.5782902240753174\n",
      "steps = 164, loss = 3.0180704593658447\n",
      "steps = 164, loss = 49.51689147949219\n",
      "steps = 164, loss = 3.44816255569458\n",
      "steps = 164, loss = 49.9853630065918\n",
      "steps = 164, loss = 3.6121606826782227\n",
      "steps = 164, loss = 2.7625505924224854\n",
      "steps = 164, loss = 4.6569623947143555\n",
      "steps = 164, loss = 2.7173569202423096\n",
      "steps = 164, loss = 6.755073070526123\n",
      "steps = 164, loss = 3.3903141021728516\n",
      "steps = 164, loss = 4.196880340576172\n",
      "steps = 165, loss = 6.631925582885742\n",
      "steps = 165, loss = 3.3555266857147217\n",
      "steps = 165, loss = 3.584272861480713\n",
      "steps = 165, loss = 2.787430763244629\n",
      "steps = 165, loss = 2.1388678550720215\n",
      "steps = 165, loss = 2.3863112926483154\n",
      "steps = 165, loss = 4.649367332458496\n",
      "steps = 165, loss = 2.8348770141601562\n",
      "steps = 165, loss = 2.154905319213867\n",
      "steps = 165, loss = 2.805819272994995\n",
      "steps = 165, loss = 2.945626735687256\n",
      "steps = 165, loss = 2.750429391860962\n",
      "steps = 165, loss = 4.118954658508301\n",
      "steps = 165, loss = 2.7410292625427246\n",
      "steps = 165, loss = 49.46884536743164\n",
      "steps = 165, loss = 3.376019239425659\n",
      "steps = 165, loss = 4.193082332611084\n",
      "steps = 165, loss = 3.3067476749420166\n",
      "steps = 165, loss = 2.954918622970581\n",
      "steps = 165, loss = 3.5064659118652344\n",
      "steps = 165, loss = 3.6153223514556885\n",
      "steps = 165, loss = 2.0554769039154053\n",
      "steps = 165, loss = 3.6758627891540527\n",
      "steps = 165, loss = 2.6922967433929443\n",
      "steps = 165, loss = 2.621429681777954\n",
      "steps = 165, loss = 4.3253278732299805\n",
      "steps = 165, loss = 50.0155029296875\n",
      "steps = 165, loss = 49.9853630065918\n",
      "steps = 165, loss = 3.0935721397399902\n",
      "steps = 166, loss = 49.9853630065918\n",
      "steps = 166, loss = 3.5230114459991455\n",
      "steps = 166, loss = 2.966662883758545\n",
      "steps = 166, loss = 2.648571729660034\n",
      "steps = 166, loss = 4.685230255126953\n",
      "steps = 166, loss = 2.9975898265838623\n",
      "steps = 166, loss = 4.356013774871826\n",
      "steps = 166, loss = 4.213651657104492\n",
      "steps = 166, loss = 3.128700017929077\n",
      "steps = 166, loss = 2.7712907791137695\n",
      "steps = 166, loss = 3.4666872024536133\n",
      "steps = 166, loss = 2.284452438354492\n",
      "steps = 166, loss = 2.961142063140869\n",
      "steps = 166, loss = 4.348764419555664\n",
      "steps = 166, loss = 2.2590465545654297\n",
      "steps = 166, loss = 2.778461456298828\n",
      "steps = 166, loss = 3.5053701400756836\n",
      "steps = 166, loss = 2.6519298553466797\n",
      "steps = 166, loss = 3.5723190307617188\n",
      "steps = 166, loss = 50.012081146240234\n",
      "steps = 166, loss = 6.724431991577148\n",
      "steps = 166, loss = 50.017452239990234\n",
      "steps = 166, loss = 3.426405191421509\n",
      "steps = 166, loss = 2.1944849491119385\n",
      "steps = 166, loss = 2.730350971221924\n",
      "steps = 166, loss = 3.5490217208862305\n",
      "steps = 166, loss = 2.8736298084259033\n",
      "steps = 166, loss = 3.367741823196411\n",
      "steps = 166, loss = 2.966773271560669\n",
      "steps = 167, loss = 2.854660749435425\n",
      "steps = 167, loss = 3.634183645248413\n",
      "steps = 167, loss = 3.341444253921509\n",
      "steps = 167, loss = 4.705372333526611\n",
      "steps = 167, loss = 6.778595447540283\n",
      "steps = 167, loss = 4.211571216583252\n",
      "steps = 167, loss = 2.100562810897827\n",
      "steps = 167, loss = 2.0715274810791016\n",
      "steps = 167, loss = 50.008209228515625\n",
      "steps = 167, loss = 2.786036968231201\n",
      "steps = 167, loss = 2.1947083473205566\n",
      "steps = 167, loss = 3.6575045585632324\n",
      "steps = 167, loss = 3.152062177658081\n",
      "steps = 167, loss = 2.7125136852264404\n",
      "steps = 167, loss = 4.3547515869140625\n",
      "steps = 167, loss = 3.524690628051758\n",
      "steps = 167, loss = 2.9639978408813477\n",
      "steps = 167, loss = 2.764436721801758\n",
      "steps = 167, loss = 3.5078916549682617\n",
      "steps = 167, loss = 50.017845153808594\n",
      "steps = 167, loss = 2.42759108543396\n",
      "steps = 167, loss = 3.527299642562866\n",
      "steps = 167, loss = 49.9853630065918\n",
      "steps = 167, loss = 4.289804935455322\n",
      "steps = 167, loss = 3.4377267360687256\n",
      "steps = 167, loss = 2.8642845153808594\n",
      "steps = 167, loss = 3.1455228328704834\n",
      "steps = 167, loss = 2.962806224822998\n",
      "steps = 167, loss = 2.713188648223877\n",
      "steps = 168, loss = 49.9853630065918\n",
      "steps = 168, loss = 2.5757083892822266\n",
      "steps = 168, loss = 2.04158091545105\n",
      "steps = 168, loss = 2.6924972534179688\n",
      "steps = 168, loss = 3.2657008171081543\n",
      "steps = 168, loss = 4.216998100280762\n",
      "steps = 168, loss = 4.427507400512695\n",
      "steps = 168, loss = 2.7571074962615967\n",
      "steps = 168, loss = 3.144521474838257\n",
      "steps = 168, loss = 2.1381609439849854\n",
      "steps = 168, loss = 2.7008769512176514\n",
      "steps = 168, loss = 2.9538745880126953\n",
      "steps = 168, loss = 3.404416084289551\n",
      "steps = 168, loss = 3.2019786834716797\n",
      "steps = 168, loss = 50.00336456298828\n",
      "steps = 168, loss = 6.6944475173950195\n",
      "steps = 168, loss = 4.698731899261475\n",
      "steps = 168, loss = 2.4545514583587646\n",
      "steps = 168, loss = 2.95590877532959\n",
      "steps = 168, loss = 3.3443543910980225\n",
      "steps = 168, loss = 3.4404947757720947\n",
      "steps = 168, loss = 2.7390387058258057\n",
      "steps = 168, loss = 2.834918975830078\n",
      "steps = 168, loss = 49.83823013305664\n",
      "steps = 168, loss = 3.392002820968628\n",
      "steps = 168, loss = 2.966486692428589\n",
      "steps = 168, loss = 4.358043670654297\n",
      "steps = 168, loss = 3.10333514213562\n",
      "steps = 168, loss = 2.1477198600769043\n",
      "steps = 169, loss = 2.771676778793335\n",
      "steps = 169, loss = 49.9853630065918\n",
      "steps = 169, loss = 2.7367544174194336\n",
      "steps = 169, loss = 3.656670331954956\n",
      "steps = 169, loss = 50.012977600097656\n",
      "steps = 169, loss = 4.733834743499756\n",
      "steps = 169, loss = 3.4713046550750732\n",
      "steps = 169, loss = 2.2652714252471924\n",
      "steps = 169, loss = 2.873396158218384\n",
      "steps = 169, loss = 2.9656622409820557\n",
      "steps = 169, loss = 3.4645156860351562\n",
      "steps = 169, loss = 2.80525803565979\n",
      "steps = 169, loss = 2.16817307472229\n",
      "steps = 169, loss = 3.1139402389526367\n",
      "steps = 169, loss = 4.380353927612305\n",
      "steps = 169, loss = 49.9830436706543\n",
      "steps = 169, loss = 3.4881789684295654\n",
      "steps = 169, loss = 2.9714245796203613\n",
      "steps = 169, loss = 3.4270668029785156\n",
      "steps = 169, loss = 2.8734843730926514\n",
      "steps = 169, loss = 2.2837107181549072\n",
      "steps = 169, loss = 6.821903228759766\n",
      "steps = 169, loss = 3.3889317512512207\n",
      "steps = 169, loss = 4.236995697021484\n",
      "steps = 169, loss = 4.6931023597717285\n",
      "steps = 169, loss = 2.7305281162261963\n",
      "steps = 169, loss = 2.7021267414093018\n",
      "steps = 169, loss = 2.5771214962005615\n",
      "steps = 169, loss = 3.0481817722320557\n",
      "steps = 170, loss = 4.38607120513916\n",
      "steps = 170, loss = 12.676243782043457\n",
      "steps = 170, loss = 49.9853630065918\n",
      "steps = 170, loss = 3.381357192993164\n",
      "steps = 170, loss = 2.8116860389709473\n",
      "steps = 170, loss = 2.96186900138855\n",
      "steps = 170, loss = 2.764014720916748\n",
      "steps = 170, loss = 6.72390604019165\n",
      "steps = 170, loss = 3.497474431991577\n",
      "steps = 170, loss = 2.480761766433716\n",
      "steps = 170, loss = 3.330068826675415\n",
      "steps = 170, loss = 3.3788764476776123\n",
      "steps = 170, loss = 2.8545944690704346\n",
      "steps = 170, loss = 2.9077088832855225\n",
      "steps = 170, loss = 50.0062141418457\n",
      "steps = 170, loss = 4.235449314117432\n",
      "steps = 170, loss = 3.260399103164673\n",
      "steps = 170, loss = 2.2051525115966797\n",
      "steps = 170, loss = 2.6977121829986572\n",
      "steps = 170, loss = 14.338594436645508\n",
      "steps = 170, loss = 2.713404893875122\n",
      "steps = 170, loss = 2.9831433296203613\n",
      "steps = 170, loss = 3.0637714862823486\n",
      "steps = 170, loss = 49.87311935424805\n",
      "steps = 170, loss = 2.9741439819335938\n",
      "steps = 170, loss = 2.159822463989258\n",
      "steps = 170, loss = 3.451606273651123\n",
      "steps = 170, loss = 3.3860106468200684\n",
      "steps = 170, loss = 4.75446891784668\n",
      "steps = 171, loss = 3.4633657932281494\n",
      "steps = 171, loss = 2.6927108764648438\n",
      "steps = 171, loss = 49.9853630065918\n",
      "steps = 171, loss = 2.952981472015381\n",
      "steps = 171, loss = 6.892306327819824\n",
      "steps = 171, loss = 4.240992069244385\n",
      "steps = 171, loss = 2.9705491065979004\n",
      "steps = 171, loss = 3.5599727630615234\n",
      "steps = 171, loss = 2.7249321937561035\n",
      "steps = 171, loss = 2.931807041168213\n",
      "steps = 171, loss = 2.7392261028289795\n",
      "steps = 171, loss = 2.139554977416992\n",
      "steps = 171, loss = 2.099838972091675\n",
      "steps = 171, loss = 3.678818941116333\n",
      "steps = 171, loss = 3.917518138885498\n",
      "steps = 171, loss = 4.747734069824219\n",
      "steps = 171, loss = 2.6569929122924805\n",
      "steps = 171, loss = 3.2929651737213135\n",
      "steps = 171, loss = 7.218460559844971\n",
      "steps = 171, loss = 2.397814989089966\n",
      "steps = 171, loss = 3.311131238937378\n",
      "steps = 171, loss = 3.5144100189208984\n",
      "steps = 171, loss = 4.389408111572266\n",
      "steps = 171, loss = 2.4676501750946045\n",
      "steps = 171, loss = 50.00775146484375\n",
      "steps = 171, loss = 2.966081142425537\n",
      "steps = 171, loss = 2.8349616527557373\n",
      "steps = 171, loss = 2.7309062480926514\n",
      "steps = 171, loss = 3.4485106468200684\n",
      "steps = 172, loss = 2.7307322025299072\n",
      "steps = 172, loss = 2.8734312057495117\n",
      "steps = 172, loss = 3.51305890083313\n",
      "steps = 172, loss = 3.1029324531555176\n",
      "steps = 172, loss = 2.580043315887451\n",
      "steps = 172, loss = 2.8078510761260986\n",
      "steps = 172, loss = 2.9814908504486084\n",
      "steps = 172, loss = 2.7717463970184326\n",
      "steps = 172, loss = 3.5724360942840576\n",
      "steps = 172, loss = 3.401749610900879\n",
      "steps = 172, loss = 2.7209322452545166\n",
      "steps = 172, loss = 3.001784086227417\n",
      "steps = 172, loss = 2.816748857498169\n",
      "steps = 172, loss = 2.9647622108459473\n",
      "steps = 172, loss = 6.6807990074157715\n",
      "steps = 172, loss = 2.0575170516967773\n",
      "steps = 172, loss = 3.4271161556243896\n",
      "steps = 172, loss = 3.314319610595703\n",
      "steps = 172, loss = 2.9724836349487305\n",
      "steps = 172, loss = 2.228996992111206\n",
      "steps = 172, loss = 4.411782264709473\n",
      "steps = 172, loss = 2.990570306777954\n",
      "steps = 172, loss = 4.2610578536987305\n",
      "steps = 172, loss = 2.2845029830932617\n",
      "steps = 172, loss = 4.782903671264648\n",
      "steps = 172, loss = 49.9853630065918\n",
      "steps = 172, loss = 3.355212450027466\n",
      "steps = 172, loss = 3.4503729343414307\n",
      "steps = 172, loss = 50.003570556640625\n",
      "steps = 173, loss = 3.460831642150879\n",
      "steps = 173, loss = 2.992356538772583\n",
      "steps = 173, loss = 2.9671692848205566\n",
      "steps = 173, loss = 2.713549852371216\n",
      "steps = 173, loss = 2.079620122909546\n",
      "steps = 173, loss = 2.694716691970825\n",
      "steps = 173, loss = 4.2596540451049805\n",
      "steps = 173, loss = 2.764185905456543\n",
      "steps = 173, loss = 3.400489091873169\n",
      "steps = 173, loss = 3.013824939727783\n",
      "steps = 173, loss = 3.6771254539489746\n",
      "steps = 173, loss = 3.513786554336548\n",
      "steps = 173, loss = 2.204634189605713\n",
      "steps = 173, loss = 2.85455584526062\n",
      "steps = 173, loss = 3.011173963546753\n",
      "steps = 173, loss = 4.417627811431885\n",
      "steps = 173, loss = 2.8500020503997803\n",
      "steps = 173, loss = 2.961003303527832\n",
      "steps = 173, loss = 6.744626522064209\n",
      "steps = 173, loss = 2.088505744934082\n",
      "steps = 173, loss = 3.674989938735962\n",
      "steps = 173, loss = 49.9853630065918\n",
      "steps = 173, loss = 2.513650894165039\n",
      "steps = 173, loss = 3.536583662033081\n",
      "steps = 173, loss = 2.7966270446777344\n",
      "steps = 173, loss = 2.98419451713562\n",
      "steps = 173, loss = 4.804314613342285\n",
      "steps = 173, loss = 3.341048002243042\n",
      "steps = 173, loss = 50.00164031982422\n",
      "steps = 174, loss = 3.2185778617858887\n",
      "steps = 174, loss = 3.546905517578125\n",
      "steps = 174, loss = 1.9853473901748657\n",
      "steps = 174, loss = 6.732807159423828\n",
      "steps = 174, loss = 49.9853630065918\n",
      "steps = 174, loss = 2.6414949893951416\n",
      "steps = 174, loss = 2.139632225036621\n",
      "steps = 174, loss = 3.0707075595855713\n",
      "steps = 174, loss = 2.361656904220581\n",
      "steps = 174, loss = 2.692903995513916\n",
      "steps = 174, loss = 2.684344530105591\n",
      "steps = 174, loss = 4.264791488647461\n",
      "steps = 174, loss = 3.208035469055176\n",
      "steps = 174, loss = 2.6945314407348633\n",
      "steps = 174, loss = 3.2931509017944336\n",
      "steps = 174, loss = 3.441587209701538\n",
      "steps = 174, loss = 50.00748825073242\n",
      "steps = 174, loss = 3.072007656097412\n",
      "steps = 174, loss = 4.797247886657715\n",
      "steps = 174, loss = 2.553412675857544\n",
      "steps = 174, loss = 2.952160358428955\n",
      "steps = 174, loss = 2.012115001678467\n",
      "steps = 174, loss = 3.379672050476074\n",
      "steps = 174, loss = 2.834998369216919\n",
      "steps = 174, loss = 4.420822620391846\n",
      "steps = 174, loss = 2.9760026931762695\n",
      "steps = 174, loss = 2.7395544052124023\n",
      "steps = 174, loss = 2.7183077335357666\n",
      "steps = 174, loss = 3.4426302909851074\n",
      "steps = 175, loss = 4.437010288238525\n",
      "steps = 175, loss = 2.0864713191986084\n",
      "steps = 175, loss = 3.320310115814209\n",
      "steps = 175, loss = 3.5287487506866455\n",
      "steps = 175, loss = 2.464298963546753\n",
      "steps = 175, loss = 49.9853630065918\n",
      "steps = 175, loss = 3.5001726150512695\n",
      "steps = 175, loss = 2.713606595993042\n",
      "steps = 175, loss = 3.1968624591827393\n",
      "steps = 175, loss = 3.6775851249694824\n",
      "steps = 175, loss = 2.7474756240844727\n",
      "steps = 175, loss = 2.5148873329162598\n",
      "steps = 175, loss = 3.366027593612671\n",
      "steps = 175, loss = 2.087122678756714\n",
      "steps = 175, loss = 2.853853225708008\n",
      "steps = 175, loss = 4.272899627685547\n",
      "steps = 175, loss = 3.5867371559143066\n",
      "steps = 175, loss = 2.747793674468994\n",
      "steps = 175, loss = 4.835622310638428\n",
      "steps = 175, loss = 3.603245973587036\n",
      "steps = 175, loss = 6.793589115142822\n",
      "steps = 175, loss = 3.0515711307525635\n",
      "steps = 175, loss = 2.9891858100891113\n",
      "steps = 175, loss = 2.257845640182495\n",
      "steps = 175, loss = 50.00703048706055\n",
      "steps = 175, loss = 2.7604856491088867\n",
      "steps = 175, loss = 2.9589171409606934\n",
      "steps = 175, loss = 3.1470093727111816\n",
      "steps = 175, loss = 3.4075303077697754\n",
      "steps = 176, loss = 2.9632344245910645\n",
      "steps = 176, loss = 3.452207565307617\n",
      "steps = 176, loss = 2.729862689971924\n",
      "steps = 176, loss = 3.5605368614196777\n",
      "steps = 176, loss = 2.9960289001464844\n",
      "steps = 176, loss = 2.6216511726379395\n",
      "steps = 176, loss = 3.1560211181640625\n",
      "steps = 176, loss = 2.95967960357666\n",
      "steps = 176, loss = 2.274930715560913\n",
      "steps = 176, loss = 3.4603936672210693\n",
      "steps = 176, loss = 3.4377639293670654\n",
      "steps = 176, loss = 3.491523265838623\n",
      "steps = 176, loss = 3.3202481269836426\n",
      "steps = 176, loss = 2.5677688121795654\n",
      "steps = 176, loss = 2.7708418369293213\n",
      "steps = 176, loss = 2.872293472290039\n",
      "steps = 176, loss = 49.9853630065918\n",
      "steps = 176, loss = 2.1418213844299316\n",
      "steps = 176, loss = 4.454214572906494\n",
      "steps = 176, loss = 4.851731777191162\n",
      "steps = 176, loss = 2.6258976459503174\n",
      "steps = 176, loss = 50.00559616088867\n",
      "steps = 176, loss = 3.917353868484497\n",
      "steps = 176, loss = 2.6309077739715576\n",
      "steps = 176, loss = 3.3960821628570557\n",
      "steps = 176, loss = 3.4451801776885986\n",
      "steps = 176, loss = 2.2903027534484863\n",
      "steps = 176, loss = 6.815162181854248\n",
      "steps = 176, loss = 4.291608810424805\n",
      "steps = 177, loss = 3.8574259281158447\n",
      "steps = 177, loss = 3.4264414310455322\n",
      "steps = 177, loss = 3.156630754470825\n",
      "steps = 177, loss = 2.6944077014923096\n",
      "steps = 177, loss = 3.502962350845337\n",
      "steps = 177, loss = 3.0317676067352295\n",
      "steps = 177, loss = 3.4374186992645264\n",
      "steps = 177, loss = 4.451177597045898\n",
      "steps = 177, loss = 2.722426414489746\n",
      "steps = 177, loss = 49.9853630065918\n",
      "steps = 177, loss = 4.2895917892456055\n",
      "steps = 177, loss = 3.592033624649048\n",
      "steps = 177, loss = 2.9861795902252197\n",
      "steps = 177, loss = 2.761533498764038\n",
      "steps = 177, loss = 2.951888084411621\n",
      "steps = 177, loss = 2.1388587951660156\n",
      "steps = 177, loss = 2.0595831871032715\n",
      "steps = 177, loss = 2.010349988937378\n",
      "steps = 177, loss = 2.688119888305664\n",
      "steps = 177, loss = 3.5158090591430664\n",
      "steps = 177, loss = 2.739722728729248\n",
      "steps = 177, loss = 6.880029201507568\n",
      "steps = 177, loss = 2.6049644947052\n",
      "steps = 177, loss = 4.8477783203125\n",
      "steps = 177, loss = 3.340961456298828\n",
      "steps = 177, loss = 50.00571823120117\n",
      "steps = 177, loss = 2.8359792232513428\n",
      "steps = 177, loss = 3.6919524669647217\n",
      "steps = 177, loss = 2.4066128730773926\n",
      "steps = 178, loss = 2.741609811782837\n",
      "steps = 178, loss = 4.886401653289795\n",
      "steps = 178, loss = 2.713770627975464\n",
      "steps = 178, loss = 2.1786649227142334\n",
      "steps = 178, loss = 2.7207067012786865\n",
      "steps = 178, loss = 3.447279691696167\n",
      "steps = 178, loss = 3.4032435417175293\n",
      "steps = 178, loss = 3.417649745941162\n",
      "steps = 178, loss = 2.9584217071533203\n",
      "steps = 178, loss = 4.297995090484619\n",
      "steps = 178, loss = 49.9853630065918\n",
      "steps = 178, loss = 3.551762342453003\n",
      "steps = 178, loss = 3.4310503005981445\n",
      "steps = 178, loss = 49.95180892944336\n",
      "steps = 178, loss = 4.468149185180664\n",
      "steps = 178, loss = 6.913933277130127\n",
      "steps = 178, loss = 2.8491404056549072\n",
      "steps = 178, loss = 3.494259834289551\n",
      "steps = 178, loss = 2.4475901126861572\n",
      "steps = 178, loss = 2.7641541957855225\n",
      "steps = 178, loss = 3.348646640777588\n",
      "steps = 178, loss = 2.7854435443878174\n",
      "steps = 178, loss = 3.1198229789733887\n",
      "steps = 178, loss = 2.0742392539978027\n",
      "steps = 178, loss = 3.260016918182373\n",
      "steps = 178, loss = 2.9990925788879395\n",
      "steps = 178, loss = 4.205795764923096\n",
      "steps = 178, loss = 2.166206121444702\n",
      "steps = 178, loss = 2.853895902633667\n",
      "steps = 179, loss = 3.005872964859009\n",
      "steps = 179, loss = 2.96262526512146\n",
      "steps = 179, loss = 2.946051597595215\n",
      "steps = 179, loss = 3.5154647827148438\n",
      "steps = 179, loss = 49.9853630065918\n",
      "steps = 179, loss = 4.901796340942383\n",
      "steps = 179, loss = 4.484501361846924\n",
      "steps = 179, loss = 3.42236065864563\n",
      "steps = 179, loss = 50.0127067565918\n",
      "steps = 179, loss = 2.290025234222412\n",
      "steps = 179, loss = 3.33781099319458\n",
      "steps = 179, loss = 2.6930630207061768\n",
      "steps = 179, loss = 3.5937106609344482\n",
      "steps = 179, loss = 4.316405773162842\n",
      "steps = 179, loss = 2.770777940750122\n",
      "steps = 179, loss = 3.493607759475708\n",
      "steps = 179, loss = 3.4263272285461426\n",
      "steps = 179, loss = 2.8725903034210205\n",
      "steps = 179, loss = 2.730039358139038\n",
      "steps = 179, loss = 4.004703521728516\n",
      "steps = 179, loss = 6.853961944580078\n",
      "steps = 179, loss = 2.945096254348755\n",
      "steps = 179, loss = 3.6799449920654297\n",
      "steps = 179, loss = 2.963228464126587\n",
      "steps = 179, loss = 2.613051176071167\n",
      "steps = 179, loss = 2.1277990341186523\n",
      "steps = 179, loss = 2.596780776977539\n",
      "steps = 179, loss = 2.242288112640381\n",
      "steps = 179, loss = 3.334047317504883\n",
      "steps = 180, loss = 7.023717880249023\n",
      "steps = 180, loss = 3.3011107444763184\n",
      "steps = 180, loss = 2.9513144493103027\n",
      "steps = 180, loss = 2.69473934173584\n",
      "steps = 180, loss = 3.041482448577881\n",
      "steps = 180, loss = 3.4346015453338623\n",
      "steps = 180, loss = 3.299053192138672\n",
      "steps = 180, loss = 2.911215305328369\n",
      "steps = 180, loss = 3.651259660720825\n",
      "steps = 180, loss = 2.216308832168579\n",
      "steps = 180, loss = 3.586984872817993\n",
      "steps = 180, loss = 3.205615520477295\n",
      "steps = 180, loss = 2.836155652999878\n",
      "steps = 180, loss = 2.8093314170837402\n",
      "steps = 180, loss = 2.721813678741455\n",
      "steps = 180, loss = 4.315835475921631\n",
      "steps = 180, loss = 49.9853630065918\n",
      "steps = 180, loss = 2.740370512008667\n",
      "steps = 180, loss = 2.4660284519195557\n",
      "steps = 180, loss = 1.9404150247573853\n",
      "steps = 180, loss = 4.897833824157715\n",
      "steps = 180, loss = 2.138889789581299\n",
      "steps = 180, loss = 3.519906759262085\n",
      "steps = 180, loss = 2.9959864616394043\n",
      "steps = 180, loss = 2.574413776397705\n",
      "steps = 180, loss = 3.3470489978790283\n",
      "steps = 180, loss = 3.3144354820251465\n",
      "steps = 180, loss = 4.480953216552734\n",
      "steps = 180, loss = 50.01333236694336\n",
      "steps = 181, loss = 3.6380064487457275\n",
      "steps = 181, loss = 4.5027079582214355\n",
      "steps = 181, loss = 3.4559247493743896\n",
      "steps = 181, loss = 3.011962413787842\n",
      "steps = 181, loss = 2.7314484119415283\n",
      "steps = 181, loss = 3.8439390659332275\n",
      "steps = 181, loss = 3.011160373687744\n",
      "steps = 181, loss = 3.593932628631592\n",
      "steps = 181, loss = 50.009220123291016\n",
      "steps = 181, loss = 4.336171627044678\n",
      "steps = 181, loss = 2.8742029666900635\n",
      "steps = 181, loss = 6.745604038238525\n",
      "steps = 181, loss = 2.7720706462860107\n",
      "steps = 181, loss = 2.2274417877197266\n",
      "steps = 181, loss = 2.8267245292663574\n",
      "steps = 181, loss = 49.9853630065918\n",
      "steps = 181, loss = 3.6895461082458496\n",
      "steps = 181, loss = 3.05489182472229\n",
      "steps = 181, loss = 2.2774977684020996\n",
      "steps = 181, loss = 2.625967264175415\n",
      "steps = 181, loss = 3.411954879760742\n",
      "steps = 181, loss = 3.4794375896453857\n",
      "steps = 181, loss = 3.478360414505005\n",
      "steps = 181, loss = 2.985954523086548\n",
      "steps = 181, loss = 2.58064603805542\n",
      "steps = 181, loss = 3.401021957397461\n",
      "steps = 181, loss = 4.931725978851318\n",
      "steps = 181, loss = 2.962711811065674\n",
      "steps = 181, loss = 2.13429594039917\n",
      "steps = 182, loss = 3.3538970947265625\n",
      "steps = 182, loss = 6.808065891265869\n",
      "steps = 182, loss = 50.001224517822266\n",
      "steps = 182, loss = 3.4248907566070557\n",
      "steps = 182, loss = 2.2438573837280273\n",
      "steps = 182, loss = 2.590169668197632\n",
      "steps = 182, loss = 2.387152671813965\n",
      "steps = 182, loss = 3.6389286518096924\n",
      "steps = 182, loss = 2.282957077026367\n",
      "steps = 182, loss = 2.7401328086853027\n",
      "steps = 182, loss = 4.929142475128174\n",
      "steps = 182, loss = 3.1129767894744873\n",
      "steps = 182, loss = 2.6964175701141357\n",
      "steps = 182, loss = 2.138983964920044\n",
      "steps = 182, loss = 3.6129000186920166\n",
      "steps = 182, loss = 3.498960256576538\n",
      "steps = 182, loss = 3.8519504070281982\n",
      "steps = 182, loss = 3.0019404888153076\n",
      "steps = 182, loss = 3.4355111122131348\n",
      "steps = 182, loss = 2.8374857902526855\n",
      "steps = 182, loss = 2.85662841796875\n",
      "steps = 182, loss = 2.9516446590423584\n",
      "steps = 182, loss = 4.499866962432861\n",
      "steps = 182, loss = 3.4634344577789307\n",
      "steps = 182, loss = 3.3800456523895264\n",
      "steps = 182, loss = 4.335446834564209\n",
      "steps = 182, loss = 3.0391180515289307\n",
      "steps = 182, loss = 2.702535629272461\n",
      "steps = 182, loss = 49.9853630065918\n",
      "steps = 183, loss = 2.853748321533203\n",
      "steps = 183, loss = 3.491717576980591\n",
      "steps = 183, loss = 4.516736030578613\n",
      "steps = 183, loss = 3.3822875022888184\n",
      "steps = 183, loss = 3.3532159328460693\n",
      "steps = 183, loss = 3.672748565673828\n",
      "steps = 183, loss = 6.868688106536865\n",
      "steps = 183, loss = 3.8978400230407715\n",
      "steps = 183, loss = 2.4859819412231445\n",
      "steps = 183, loss = 2.764932632446289\n",
      "steps = 183, loss = 50.00517654418945\n",
      "steps = 183, loss = 4.966925144195557\n",
      "steps = 183, loss = 2.070082902908325\n",
      "steps = 183, loss = 49.9853630065918\n",
      "steps = 183, loss = 2.6942451000213623\n",
      "steps = 183, loss = 2.2551238536834717\n",
      "steps = 183, loss = 2.4467689990997314\n",
      "steps = 183, loss = 4.341183662414551\n",
      "steps = 183, loss = 7.940268039703369\n",
      "steps = 183, loss = 2.910501003265381\n",
      "steps = 183, loss = 3.5659146308898926\n",
      "steps = 183, loss = 3.0147576332092285\n",
      "steps = 183, loss = 3.4693546295166016\n",
      "steps = 183, loss = 2.830040454864502\n",
      "steps = 183, loss = 2.713928699493408\n",
      "steps = 183, loss = 3.111684560775757\n",
      "steps = 183, loss = 2.1977221965789795\n",
      "steps = 183, loss = 2.957892894744873\n",
      "steps = 183, loss = 3.539659023284912\n",
      "steps = 184, loss = 2.9619951248168945\n",
      "steps = 184, loss = 2.8345046043395996\n",
      "steps = 184, loss = 2.6626641750335693\n",
      "steps = 184, loss = 4.360633373260498\n",
      "steps = 184, loss = 3.507918357849121\n",
      "steps = 184, loss = 2.8731279373168945\n",
      "steps = 184, loss = 2.3238120079040527\n",
      "steps = 184, loss = 3.9517080783843994\n",
      "steps = 184, loss = 2.730523109436035\n",
      "steps = 184, loss = 2.897291421890259\n",
      "steps = 184, loss = 3.310859203338623\n",
      "steps = 184, loss = 3.342184543609619\n",
      "steps = 184, loss = 4.981965065002441\n",
      "steps = 184, loss = 6.878673553466797\n",
      "steps = 184, loss = 3.4862539768218994\n",
      "steps = 184, loss = 3.0215680599212646\n",
      "steps = 184, loss = 2.7714288234710693\n",
      "steps = 184, loss = 4.533005237579346\n",
      "steps = 184, loss = 50.00138473510742\n",
      "steps = 184, loss = 2.624082326889038\n",
      "steps = 184, loss = 3.424816131591797\n",
      "steps = 184, loss = 2.1439402103424072\n",
      "steps = 184, loss = 2.2900073528289795\n",
      "steps = 184, loss = 2.988330841064453\n",
      "steps = 184, loss = 49.9853630065918\n",
      "steps = 184, loss = 3.4317939281463623\n",
      "steps = 184, loss = 2.9723567962646484\n",
      "steps = 184, loss = 2.804112434387207\n",
      "steps = 184, loss = 3.4841670989990234\n",
      "steps = 185, loss = 4.359498977661133\n",
      "steps = 185, loss = 2.6956212520599365\n",
      "steps = 185, loss = 2.589679002761841\n",
      "steps = 185, loss = 2.9507505893707275\n",
      "steps = 185, loss = 2.0478827953338623\n",
      "steps = 185, loss = 3.0117027759552\n",
      "steps = 185, loss = 2.423949956893921\n",
      "steps = 185, loss = 2.7670557498931885\n",
      "steps = 185, loss = 3.4615671634674072\n",
      "steps = 185, loss = 6.8616108894348145\n",
      "steps = 185, loss = 2.836775541305542\n",
      "steps = 185, loss = 2.1382977962493896\n",
      "steps = 185, loss = 3.5720279216766357\n",
      "steps = 185, loss = 3.8259940147399902\n",
      "steps = 185, loss = 3.2136099338531494\n",
      "steps = 185, loss = 4.52962064743042\n",
      "steps = 185, loss = 3.435584783554077\n",
      "steps = 185, loss = 4.978510856628418\n",
      "steps = 185, loss = 49.888919830322266\n",
      "steps = 185, loss = 49.9853630065918\n",
      "steps = 185, loss = 3.3567404747009277\n",
      "steps = 185, loss = 3.1921067237854004\n",
      "steps = 185, loss = 2.741325616836548\n",
      "steps = 185, loss = 2.885100841522217\n",
      "steps = 185, loss = 2.061652660369873\n",
      "steps = 185, loss = 2.6476728916168213\n",
      "steps = 185, loss = 3.6557462215423584\n",
      "steps = 185, loss = 3.0205109119415283\n",
      "steps = 185, loss = 3.357717514038086\n",
      "steps = 186, loss = 4.545980930328369\n",
      "steps = 186, loss = 2.805124282836914\n",
      "steps = 186, loss = 3.474680185317993\n",
      "steps = 186, loss = 1.9416004419326782\n",
      "steps = 186, loss = 6.853860378265381\n",
      "steps = 186, loss = 4.365823745727539\n",
      "steps = 186, loss = 3.0159552097320557\n",
      "steps = 186, loss = 2.7652792930603027\n",
      "steps = 186, loss = 3.324415683746338\n",
      "steps = 186, loss = 5.017282962799072\n",
      "steps = 186, loss = 4.058512210845947\n",
      "steps = 186, loss = 49.9853630065918\n",
      "steps = 186, loss = 3.3137338161468506\n",
      "steps = 186, loss = 2.086942195892334\n",
      "steps = 186, loss = 3.083876371383667\n",
      "steps = 186, loss = 3.557379722595215\n",
      "steps = 186, loss = 3.4012503623962402\n",
      "steps = 186, loss = 3.6901490688323975\n",
      "steps = 186, loss = 49.993709564208984\n",
      "steps = 186, loss = 3.357604503631592\n",
      "steps = 186, loss = 3.3229458332061768\n",
      "steps = 186, loss = 2.8537118434906006\n",
      "steps = 186, loss = 2.1650607585906982\n",
      "steps = 186, loss = 2.5192787647247314\n",
      "steps = 186, loss = 2.7139668464660645\n",
      "steps = 186, loss = 3.024470329284668\n",
      "steps = 186, loss = 2.676701307296753\n",
      "steps = 186, loss = 3.098803758621216\n",
      "steps = 186, loss = 2.9570698738098145\n",
      "steps = 187, loss = 50.007389068603516\n",
      "steps = 187, loss = 3.4384450912475586\n",
      "steps = 187, loss = 4.195621013641357\n",
      "steps = 187, loss = 3.4147732257843018\n",
      "steps = 187, loss = 2.634775161743164\n",
      "steps = 187, loss = 2.601442337036133\n",
      "steps = 187, loss = 6.825197696685791\n",
      "steps = 187, loss = 3.3849785327911377\n",
      "steps = 187, loss = 2.872969150543213\n",
      "steps = 187, loss = 2.2831475734710693\n",
      "steps = 187, loss = 3.6447458267211914\n",
      "steps = 187, loss = 2.7717907428741455\n",
      "steps = 187, loss = 2.7306063175201416\n",
      "steps = 187, loss = 2.1697375774383545\n",
      "steps = 187, loss = 3.6171934604644775\n",
      "steps = 187, loss = 2.9195308685302734\n",
      "steps = 187, loss = 2.144552707672119\n",
      "steps = 187, loss = 4.384734153747559\n",
      "steps = 187, loss = 3.4526166915893555\n",
      "steps = 187, loss = 3.448256492614746\n",
      "steps = 187, loss = 2.451697826385498\n",
      "steps = 187, loss = 4.561817169189453\n",
      "steps = 187, loss = 49.9853630065918\n",
      "steps = 187, loss = 2.6060807704925537\n",
      "steps = 187, loss = 2.9611570835113525\n",
      "steps = 187, loss = 5.032426834106445\n",
      "steps = 187, loss = 4.031366348266602\n",
      "steps = 187, loss = 2.9118173122406006\n",
      "steps = 187, loss = 3.031128406524658\n",
      "steps = 188, loss = 3.3392884731292725\n",
      "steps = 188, loss = 2.7141098976135254\n",
      "steps = 188, loss = 2.137887477874756\n",
      "steps = 188, loss = 4.3838725090026855\n",
      "steps = 188, loss = 3.3618149757385254\n",
      "steps = 188, loss = 2.949969530105591\n",
      "steps = 188, loss = 3.428623676300049\n",
      "steps = 188, loss = 3.4095723628997803\n",
      "steps = 188, loss = 3.248981475830078\n",
      "steps = 188, loss = 2.6954429149627686\n",
      "steps = 188, loss = 2.15622878074646\n",
      "steps = 188, loss = 2.741323471069336\n",
      "steps = 188, loss = 3.6164989471435547\n",
      "steps = 188, loss = 3.0210506916046143\n",
      "steps = 188, loss = 2.8366010189056396\n",
      "steps = 188, loss = 4.557784080505371\n",
      "steps = 188, loss = 2.7302892208099365\n",
      "steps = 188, loss = 50.0103645324707\n",
      "steps = 188, loss = 6.874669551849365\n",
      "steps = 188, loss = 3.301548957824707\n",
      "steps = 188, loss = 2.5906014442443848\n",
      "steps = 188, loss = 3.559413194656372\n",
      "steps = 188, loss = 47.27445983886719\n",
      "steps = 188, loss = 49.9853630065918\n",
      "steps = 188, loss = 3.076904296875\n",
      "steps = 188, loss = 3.0266220569610596\n",
      "steps = 188, loss = 5.029557228088379\n",
      "steps = 188, loss = 2.4362754821777344\n",
      "steps = 188, loss = 4.5904316902160645\n",
      "steps = 189, loss = 4.573716640472412\n",
      "steps = 189, loss = 2.853677988052368\n",
      "steps = 189, loss = 2.272627353668213\n",
      "steps = 189, loss = 3.033618688583374\n",
      "steps = 189, loss = 5.06831693649292\n",
      "steps = 189, loss = 2.956319808959961\n",
      "steps = 189, loss = 2.8311688899993896\n",
      "steps = 189, loss = 2.8095338344573975\n",
      "steps = 189, loss = 2.7193009853363037\n",
      "steps = 189, loss = 3.656200885772705\n",
      "steps = 189, loss = 3.532045602798462\n",
      "steps = 189, loss = 3.4087071418762207\n",
      "steps = 189, loss = 3.119403600692749\n",
      "steps = 189, loss = 49.9853630065918\n",
      "steps = 189, loss = 3.409851312637329\n",
      "steps = 189, loss = 3.524167537689209\n",
      "steps = 189, loss = 3.7850704193115234\n",
      "steps = 189, loss = 4.33200740814209\n",
      "steps = 189, loss = 2.021571397781372\n",
      "steps = 189, loss = 6.908746242523193\n",
      "steps = 189, loss = 3.6636645793914795\n",
      "steps = 189, loss = 2.7645103931427\n",
      "steps = 189, loss = 2.754141092300415\n",
      "steps = 189, loss = 4.39055871963501\n",
      "steps = 189, loss = 2.5144193172454834\n",
      "steps = 189, loss = 50.01432418823242\n",
      "steps = 189, loss = 2.7142293453216553\n",
      "steps = 189, loss = 2.1395277976989746\n",
      "steps = 189, loss = 3.539515733718872\n",
      "steps = 190, loss = 2.687317132949829\n",
      "steps = 190, loss = 2.7306065559387207\n",
      "steps = 190, loss = 2.96041202545166\n",
      "steps = 190, loss = 4.410027027130127\n",
      "steps = 190, loss = 4.36259651184082\n",
      "steps = 190, loss = 3.0403523445129395\n",
      "steps = 190, loss = 2.772020101547241\n",
      "steps = 190, loss = 2.3811326026916504\n",
      "steps = 190, loss = 2.8729162216186523\n",
      "steps = 190, loss = 3.3915281295776367\n",
      "steps = 190, loss = 3.4162464141845703\n",
      "steps = 190, loss = 3.602151393890381\n",
      "steps = 190, loss = 2.70407772064209\n",
      "steps = 190, loss = 50.00575637817383\n",
      "steps = 190, loss = 2.0519046783447266\n",
      "steps = 190, loss = 3.6072075366973877\n",
      "steps = 190, loss = 3.4234001636505127\n",
      "steps = 190, loss = 3.4021975994110107\n",
      "steps = 190, loss = 3.4842164516448975\n",
      "steps = 190, loss = 4.589696407318115\n",
      "steps = 190, loss = 2.285585880279541\n",
      "steps = 190, loss = 2.59726619720459\n",
      "steps = 190, loss = 6.958744049072266\n",
      "steps = 190, loss = 5.083397388458252\n",
      "steps = 190, loss = 3.1355793476104736\n",
      "steps = 190, loss = 49.9853630065918\n",
      "steps = 190, loss = 3.142458200454712\n",
      "steps = 190, loss = 3.029924154281616\n",
      "steps = 190, loss = 3.330596685409546\n",
      "steps = 191, loss = 4.408902645111084\n",
      "steps = 191, loss = 3.4263083934783936\n",
      "steps = 191, loss = 3.641603946685791\n",
      "steps = 191, loss = 3.6965816020965576\n",
      "steps = 191, loss = 3.409508466720581\n",
      "steps = 191, loss = 6.949197292327881\n",
      "steps = 191, loss = 2.7618770599365234\n",
      "steps = 191, loss = 4.518393039703369\n",
      "steps = 191, loss = 3.0421578884124756\n",
      "steps = 191, loss = 2.6363351345062256\n",
      "steps = 191, loss = 5.105339050292969\n",
      "steps = 191, loss = 2.226550817489624\n",
      "steps = 191, loss = 2.816723585128784\n",
      "steps = 191, loss = 2.4630346298217773\n",
      "steps = 191, loss = 2.7315030097961426\n",
      "steps = 191, loss = 50.000457763671875\n",
      "steps = 191, loss = 2.714519500732422\n",
      "steps = 191, loss = 2.956845283508301\n",
      "steps = 191, loss = 2.8545219898223877\n",
      "steps = 191, loss = 2.7643654346466064\n",
      "steps = 191, loss = 4.594592571258545\n",
      "steps = 191, loss = 3.448453903198242\n",
      "steps = 191, loss = 49.9853630065918\n",
      "steps = 191, loss = 3.5604453086853027\n",
      "steps = 191, loss = 3.3501434326171875\n",
      "steps = 191, loss = 2.182267904281616\n",
      "steps = 191, loss = 3.509417772293091\n",
      "steps = 191, loss = 2.113804578781128\n",
      "steps = 191, loss = 3.1675689220428467\n",
      "steps = 192, loss = 2.693265676498413\n",
      "steps = 192, loss = 7.0335845947265625\n",
      "steps = 192, loss = 3.6945712566375732\n",
      "steps = 192, loss = 2.7864155769348145\n",
      "steps = 192, loss = 2.8347270488739014\n",
      "steps = 192, loss = 3.359419107437134\n",
      "steps = 192, loss = 2.113477945327759\n",
      "steps = 192, loss = 2.74029541015625\n",
      "steps = 192, loss = 3.440762758255005\n",
      "steps = 192, loss = 50.01529312133789\n",
      "steps = 192, loss = 49.9853630065918\n",
      "steps = 192, loss = 3.194647789001465\n",
      "steps = 192, loss = 4.595693588256836\n",
      "steps = 192, loss = 4.413787841796875\n",
      "steps = 192, loss = 2.4677586555480957\n",
      "steps = 192, loss = 2.947955369949341\n",
      "steps = 192, loss = 3.033208131790161\n",
      "steps = 192, loss = 2.0279734134674072\n",
      "steps = 192, loss = 3.3458774089813232\n",
      "steps = 192, loss = 2.718726396560669\n",
      "steps = 192, loss = 3.5557613372802734\n",
      "steps = 192, loss = 2.9896481037139893\n",
      "steps = 192, loss = 5.09762716293335\n",
      "steps = 192, loss = 2.1409783363342285\n",
      "steps = 192, loss = 3.3554227352142334\n",
      "steps = 192, loss = 2.5951714515686035\n",
      "steps = 192, loss = 2.6087746620178223\n",
      "steps = 192, loss = 3.2757861614227295\n",
      "steps = 192, loss = 4.56308126449585\n",
      "steps = 193, loss = 3.49314546585083\n",
      "steps = 193, loss = 2.954763174057007\n",
      "steps = 193, loss = 2.169140338897705\n",
      "steps = 193, loss = 2.543440341949463\n",
      "steps = 193, loss = 3.4250080585479736\n",
      "steps = 193, loss = 2.3038787841796875\n",
      "steps = 193, loss = 4.445886611938477\n",
      "steps = 193, loss = 3.544282913208008\n",
      "steps = 193, loss = 2.0088918209075928\n",
      "steps = 193, loss = 3.127135992050171\n",
      "steps = 193, loss = 2.853675603866577\n",
      "steps = 193, loss = 3.6639227867126465\n",
      "steps = 193, loss = 2.760986089706421\n",
      "steps = 193, loss = 6.755569934844971\n",
      "steps = 193, loss = 3.0458455085754395\n",
      "steps = 193, loss = 3.397404432296753\n",
      "steps = 193, loss = 2.8153417110443115\n",
      "steps = 193, loss = 3.7153940200805664\n",
      "steps = 193, loss = 5.135166168212891\n",
      "steps = 193, loss = 4.611906051635742\n",
      "steps = 193, loss = 4.421771049499512\n",
      "steps = 193, loss = 2.8637216091156006\n",
      "steps = 193, loss = 2.3734068870544434\n",
      "steps = 193, loss = 49.9853630065918\n",
      "steps = 193, loss = 50.002925872802734\n",
      "steps = 193, loss = 3.4403162002563477\n",
      "steps = 193, loss = 2.7145721912384033\n",
      "steps = 193, loss = 2.849412441253662\n",
      "steps = 193, loss = 2.8030149936676025\n",
      "steps = 194, loss = 2.730539321899414\n",
      "steps = 194, loss = 2.282724142074585\n",
      "steps = 194, loss = 3.442274570465088\n",
      "steps = 194, loss = 6.854625701904297\n",
      "steps = 194, loss = 2.958868980407715\n",
      "steps = 194, loss = 3.024829149246216\n",
      "steps = 194, loss = 2.7719013690948486\n",
      "steps = 194, loss = 49.9853630065918\n",
      "steps = 194, loss = 4.602682590484619\n",
      "steps = 194, loss = 49.951114654541016\n",
      "steps = 194, loss = 5.149504661560059\n",
      "steps = 194, loss = 2.141937494277954\n",
      "steps = 194, loss = 3.5481066703796387\n",
      "steps = 194, loss = 3.3428702354431152\n",
      "steps = 194, loss = 2.6343913078308105\n",
      "steps = 194, loss = 3.479696273803711\n",
      "steps = 194, loss = 3.052525281906128\n",
      "steps = 194, loss = 2.8722586631774902\n",
      "steps = 194, loss = 2.64551043510437\n",
      "steps = 194, loss = 3.3993287086486816\n",
      "steps = 194, loss = 2.875006914138794\n",
      "steps = 194, loss = 2.6245248317718506\n",
      "steps = 194, loss = 3.4552173614501953\n",
      "steps = 194, loss = 4.441827774047852\n",
      "steps = 194, loss = 3.020845890045166\n",
      "steps = 194, loss = 2.2199060916900635\n",
      "steps = 194, loss = 4.626618385314941\n",
      "steps = 194, loss = 3.461395263671875\n",
      "steps = 194, loss = 2.287661552429199\n",
      "steps = 195, loss = 3.047623634338379\n",
      "steps = 195, loss = 3.473902702331543\n",
      "steps = 195, loss = 2.409121513366699\n",
      "steps = 195, loss = 2.741179943084717\n",
      "steps = 195, loss = 3.280837297439575\n",
      "steps = 195, loss = 4.622661590576172\n",
      "steps = 195, loss = 2.8916916847229004\n",
      "steps = 195, loss = 3.692805290222168\n",
      "steps = 195, loss = 3.3885977268218994\n",
      "steps = 195, loss = 2.752725839614868\n",
      "steps = 195, loss = 2.947847604751587\n",
      "steps = 195, loss = 3.042285680770874\n",
      "steps = 195, loss = 50.00811767578125\n",
      "steps = 195, loss = 6.880729675292969\n",
      "steps = 195, loss = 3.351694345474243\n",
      "steps = 195, loss = 2.6015212535858154\n",
      "steps = 195, loss = 2.0488176345825195\n",
      "steps = 195, loss = 2.022880792617798\n",
      "steps = 195, loss = 4.440377712249756\n",
      "steps = 195, loss = 2.138848066329956\n",
      "steps = 195, loss = 3.4162962436676025\n",
      "steps = 195, loss = 49.9853630065918\n",
      "steps = 195, loss = 3.4500529766082764\n",
      "steps = 195, loss = 4.617776393890381\n",
      "steps = 195, loss = 2.695082187652588\n",
      "steps = 195, loss = 2.8359341621398926\n",
      "steps = 195, loss = 2.8325178623199463\n",
      "steps = 195, loss = 5.146894454956055\n",
      "steps = 195, loss = 50.00208282470703\n",
      "steps = 196, loss = 2.9635982513427734\n",
      "steps = 196, loss = 3.4267771244049072\n",
      "steps = 196, loss = 2.573551893234253\n",
      "steps = 196, loss = 3.0105676651000977\n",
      "steps = 196, loss = 6.931207656860352\n",
      "steps = 196, loss = 4.388052463531494\n",
      "steps = 196, loss = 5.161136627197266\n",
      "steps = 196, loss = 2.1382715702056885\n",
      "steps = 196, loss = 2.9945602416992188\n",
      "steps = 196, loss = 4.449947834014893\n",
      "steps = 196, loss = 3.43426513671875\n",
      "steps = 196, loss = 3.283796548843384\n",
      "steps = 196, loss = 50.01582717895508\n",
      "steps = 196, loss = 3.68632173538208\n",
      "steps = 196, loss = 2.9472479820251465\n",
      "steps = 196, loss = 3.4003992080688477\n",
      "steps = 196, loss = 4.630532264709473\n",
      "steps = 196, loss = 49.9853630065918\n",
      "steps = 196, loss = 3.6094117164611816\n",
      "steps = 196, loss = 2.0963335037231445\n",
      "steps = 196, loss = 2.0664443969726562\n",
      "steps = 196, loss = 3.0431902408599854\n",
      "steps = 196, loss = 2.7387092113494873\n",
      "steps = 196, loss = 2.8374290466308594\n",
      "steps = 196, loss = 3.3263468742370605\n",
      "steps = 196, loss = 2.450025796890259\n",
      "steps = 196, loss = 2.7169997692108154\n",
      "steps = 196, loss = 2.696328639984131\n",
      "steps = 196, loss = 50.00505828857422\n",
      "steps = 197, loss = 4.470022678375244\n",
      "steps = 197, loss = 49.9853630065918\n",
      "steps = 197, loss = 2.5968871116638184\n",
      "steps = 197, loss = 2.2824766635894775\n",
      "steps = 197, loss = 50.00128936767578\n",
      "steps = 197, loss = 3.489903688430786\n",
      "steps = 197, loss = 3.50148606300354\n",
      "steps = 197, loss = 3.0592241287231445\n",
      "steps = 197, loss = 25.991209030151367\n",
      "steps = 197, loss = 2.9587771892547607\n",
      "steps = 197, loss = 50.01222610473633\n",
      "steps = 197, loss = 2.7324142456054688\n",
      "steps = 197, loss = 3.4429831504821777\n",
      "steps = 197, loss = 3.3720955848693848\n",
      "steps = 197, loss = 2.6780102252960205\n",
      "steps = 197, loss = 3.4063618183135986\n",
      "steps = 197, loss = 4.651702880859375\n",
      "steps = 197, loss = 6.947961807250977\n",
      "steps = 197, loss = 2.848947286605835\n",
      "steps = 197, loss = 2.157123327255249\n",
      "steps = 197, loss = 2.9279544353485107\n",
      "steps = 197, loss = 2.3230772018432617\n",
      "steps = 197, loss = 3.630711078643799\n",
      "steps = 197, loss = 2.709968090057373\n",
      "steps = 197, loss = 3.3875744342803955\n",
      "steps = 197, loss = 5.194965839385986\n",
      "steps = 197, loss = 2.873821496963501\n",
      "steps = 197, loss = 2.7153398990631104\n",
      "steps = 197, loss = 2.7737410068511963\n",
      "steps = 198, loss = 2.5343122482299805\n",
      "steps = 198, loss = 3.5465922355651855\n",
      "steps = 198, loss = 2.9554738998413086\n",
      "steps = 198, loss = 2.0516960620880127\n",
      "steps = 198, loss = 3.5360913276672363\n",
      "steps = 198, loss = 3.163144826889038\n",
      "steps = 198, loss = 3.0617899894714355\n",
      "steps = 198, loss = 3.4078845977783203\n",
      "steps = 198, loss = 4.468234062194824\n",
      "steps = 198, loss = 2.715050220489502\n",
      "steps = 198, loss = 3.6739909648895264\n",
      "steps = 198, loss = 3.405972719192505\n",
      "steps = 198, loss = 2.6217589378356934\n",
      "steps = 198, loss = 2.562159776687622\n",
      "steps = 198, loss = 6.986958980560303\n",
      "steps = 198, loss = 2.854248523712158\n",
      "steps = 198, loss = 3.458111524581909\n",
      "steps = 198, loss = 2.1918587684631348\n",
      "steps = 198, loss = 49.9853630065918\n",
      "steps = 198, loss = 5.218601226806641\n",
      "steps = 198, loss = 43.53734588623047\n",
      "steps = 198, loss = 2.979268789291382\n",
      "steps = 198, loss = 2.369666814804077\n",
      "steps = 198, loss = 2.884026288986206\n",
      "steps = 198, loss = 2.5294222831726074\n",
      "steps = 198, loss = 2.7667694091796875\n",
      "steps = 198, loss = 4.6569037437438965\n",
      "steps = 198, loss = 50.005855560302734\n",
      "steps = 198, loss = 2.862950563430786\n",
      "steps = 199, loss = 2.4568920135498047\n",
      "steps = 199, loss = 2.563246488571167\n",
      "steps = 199, loss = 2.698399305343628\n",
      "steps = 199, loss = 3.228541374206543\n",
      "steps = 199, loss = 3.481372594833374\n",
      "steps = 199, loss = 4.4738993644714355\n",
      "steps = 199, loss = 2.2662534713745117\n",
      "steps = 199, loss = 3.3644535541534424\n",
      "steps = 199, loss = 2.7622573375701904\n",
      "steps = 199, loss = 2.8789312839508057\n",
      "steps = 199, loss = 49.9853630065918\n",
      "steps = 199, loss = 3.567148208618164\n",
      "steps = 199, loss = 2.091649293899536\n",
      "steps = 199, loss = 2.7146971225738525\n",
      "steps = 199, loss = 4.6661481857299805\n",
      "steps = 199, loss = 5.553982734680176\n",
      "steps = 199, loss = 3.085240602493286\n",
      "steps = 199, loss = 6.833362579345703\n",
      "steps = 199, loss = 3.0652594566345215\n",
      "steps = 199, loss = 3.642591953277588\n",
      "steps = 199, loss = 3.3519046306610107\n",
      "steps = 199, loss = 2.9544215202331543\n",
      "steps = 199, loss = 2.603067398071289\n",
      "steps = 199, loss = 2.1162328720092773\n",
      "steps = 199, loss = 3.484182357788086\n",
      "steps = 199, loss = 5.237117767333984\n",
      "steps = 199, loss = 2.854234457015991\n",
      "steps = 199, loss = 49.98021697998047\n",
      "steps = 199, loss = 3.412318706512451\n",
      "steps = 200, loss = 2.962615728378296\n",
      "steps = 200, loss = 6.9238409996032715\n",
      "steps = 200, loss = 2.967474937438965\n",
      "steps = 200, loss = 3.436641216278076\n",
      "steps = 200, loss = 2.7711782455444336\n",
      "steps = 200, loss = 2.730438470840454\n",
      "steps = 200, loss = 2.9579601287841797\n",
      "steps = 200, loss = 3.5202226638793945\n",
      "steps = 200, loss = 2.5836212635040283\n",
      "steps = 200, loss = 2.980865001678467\n",
      "steps = 200, loss = 5.249671936035156\n",
      "steps = 200, loss = 37.7863655090332\n",
      "steps = 200, loss = 2.3321280479431152\n",
      "steps = 200, loss = 3.4271702766418457\n",
      "steps = 200, loss = 2.744971990585327\n",
      "steps = 200, loss = 2.9242031574249268\n",
      "steps = 200, loss = 3.2947726249694824\n",
      "steps = 200, loss = 3.0708699226379395\n",
      "steps = 200, loss = 4.679605007171631\n",
      "steps = 200, loss = 3.3451132774353027\n",
      "steps = 200, loss = 3.4933552742004395\n",
      "steps = 200, loss = 2.5778346061706543\n",
      "steps = 200, loss = 2.2218291759490967\n",
      "steps = 200, loss = 2.8724992275238037\n",
      "steps = 200, loss = 4.491889476776123\n",
      "steps = 200, loss = 3.4397311210632324\n",
      "steps = 200, loss = 50.01191329956055\n",
      "steps = 200, loss = 2.279832363128662\n",
      "steps = 200, loss = 49.9853630065918\n",
      "steps = 201, loss = 6.9642767906188965\n",
      "steps = 201, loss = 2.742938280105591\n",
      "steps = 201, loss = 3.320932626724243\n",
      "steps = 201, loss = 3.386420965194702\n",
      "steps = 201, loss = 4.6753387451171875\n",
      "steps = 201, loss = 2.779961585998535\n",
      "steps = 201, loss = 3.513838768005371\n",
      "steps = 201, loss = 3.355032205581665\n",
      "steps = 201, loss = 3.4768528938293457\n",
      "steps = 201, loss = 2.6950607299804688\n",
      "steps = 201, loss = 2.8357667922973633\n",
      "steps = 201, loss = 5.246646404266357\n",
      "steps = 201, loss = 2.4150094985961914\n",
      "steps = 201, loss = 3.7159814834594727\n",
      "steps = 201, loss = 2.723982095718384\n",
      "steps = 201, loss = 2.9408535957336426\n",
      "steps = 201, loss = 3.297987937927246\n",
      "steps = 201, loss = 49.49295425415039\n",
      "steps = 201, loss = 2.001445770263672\n",
      "steps = 201, loss = 2.946852684020996\n",
      "steps = 201, loss = 49.9853630065918\n",
      "steps = 201, loss = 2.6022491455078125\n",
      "steps = 201, loss = 2.442173957824707\n",
      "steps = 201, loss = 2.1201870441436768\n",
      "steps = 201, loss = 4.491230487823486\n",
      "steps = 201, loss = 2.8469903469085693\n",
      "steps = 201, loss = 3.060192346572876\n",
      "steps = 201, loss = 2.7012367248535156\n",
      "steps = 201, loss = 2.141953706741333\n",
      "steps = 202, loss = 2.7319982051849365\n",
      "steps = 202, loss = 49.9853630065918\n",
      "steps = 202, loss = 4.694607257843018\n",
      "steps = 202, loss = 2.874195098876953\n",
      "steps = 202, loss = 2.973987340927124\n",
      "steps = 202, loss = 3.4908814430236816\n",
      "steps = 202, loss = 3.0750365257263184\n",
      "steps = 202, loss = 3.4454991817474365\n",
      "steps = 202, loss = 2.1905555725097656\n",
      "steps = 202, loss = 3.6012308597564697\n",
      "steps = 202, loss = 2.675621747970581\n",
      "steps = 202, loss = 2.9580304622650146\n",
      "steps = 202, loss = 6.973318099975586\n",
      "steps = 202, loss = 2.98134446144104\n",
      "steps = 202, loss = 2.2713987827301025\n",
      "steps = 202, loss = 2.1970677375793457\n",
      "steps = 202, loss = 3.3743855953216553\n",
      "steps = 202, loss = 4.5111799240112305\n",
      "steps = 202, loss = 5.27932071685791\n",
      "steps = 202, loss = 3.4227733612060547\n",
      "steps = 202, loss = 3.4194095134735107\n",
      "steps = 202, loss = 3.3924059867858887\n",
      "steps = 202, loss = 3.0126101970672607\n",
      "steps = 202, loss = 2.7729098796844482\n",
      "steps = 202, loss = 50.02041244506836\n",
      "steps = 202, loss = 2.644904136657715\n",
      "steps = 202, loss = 3.031141519546509\n",
      "steps = 202, loss = 2.9264228343963623\n",
      "steps = 202, loss = 2.611340284347534\n",
      "steps = 203, loss = 2.847602605819702\n",
      "steps = 203, loss = 2.1348838806152344\n",
      "steps = 203, loss = 50.0174674987793\n",
      "steps = 203, loss = 3.0653746128082275\n",
      "steps = 203, loss = 4.691460609436035\n",
      "steps = 203, loss = 3.371098041534424\n",
      "steps = 203, loss = 3.1413612365722656\n",
      "steps = 203, loss = 2.9472644329071045\n",
      "steps = 203, loss = 2.6969408988952637\n",
      "steps = 203, loss = 2.262216329574585\n",
      "steps = 203, loss = 2.1203415393829346\n",
      "steps = 203, loss = 5.278356075286865\n",
      "steps = 203, loss = 2.572899580001831\n",
      "steps = 203, loss = 3.7156834602355957\n",
      "steps = 203, loss = 2.4243063926696777\n",
      "steps = 203, loss = 2.017454147338867\n",
      "steps = 203, loss = 4.512338161468506\n",
      "steps = 203, loss = 7.097236633300781\n",
      "steps = 203, loss = 3.2600834369659424\n",
      "steps = 203, loss = 3.637697219848633\n",
      "steps = 203, loss = 3.3755686283111572\n",
      "steps = 203, loss = 2.6440649032592773\n",
      "steps = 203, loss = 2.7416670322418213\n",
      "steps = 203, loss = 3.5376296043395996\n",
      "steps = 203, loss = 2.801215648651123\n",
      "steps = 203, loss = 49.9853630065918\n",
      "steps = 203, loss = 2.8041775226593018\n",
      "steps = 203, loss = 3.3893697261810303\n",
      "steps = 203, loss = 2.8373401165008545\n",
      "steps = 204, loss = 3.6224465370178223\n",
      "steps = 204, loss = 3.378793239593506\n",
      "steps = 204, loss = 5.316793441772461\n",
      "steps = 204, loss = 2.955369710922241\n",
      "steps = 204, loss = 3.078108310699463\n",
      "steps = 204, loss = 6.8312764167785645\n",
      "steps = 204, loss = 2.16208553314209\n",
      "steps = 204, loss = 2.242443561553955\n",
      "steps = 204, loss = 2.3488123416900635\n",
      "steps = 204, loss = 4.706830024719238\n",
      "steps = 204, loss = 2.984846591949463\n",
      "steps = 204, loss = 2.6765191555023193\n",
      "steps = 204, loss = 2.8895137310028076\n",
      "steps = 204, loss = 2.9536402225494385\n",
      "steps = 204, loss = 2.764746904373169\n",
      "steps = 204, loss = 4.516613006591797\n",
      "steps = 204, loss = 2.761075735092163\n",
      "steps = 204, loss = 3.389820098876953\n",
      "steps = 204, loss = 49.988826751708984\n",
      "steps = 204, loss = 2.11734676361084\n",
      "steps = 204, loss = 3.3863136768341064\n",
      "steps = 204, loss = 2.6989338397979736\n",
      "steps = 204, loss = 3.4910786151885986\n",
      "steps = 204, loss = 2.714982509613037\n",
      "steps = 204, loss = 2.8535845279693604\n",
      "steps = 204, loss = 2.310967445373535\n",
      "steps = 204, loss = 3.4414403438568115\n",
      "steps = 204, loss = 3.3770599365234375\n",
      "steps = 204, loss = 49.9853630065918\n",
      "steps = 205, loss = 3.0823397636413574\n",
      "steps = 205, loss = 2.329976797103882\n",
      "steps = 205, loss = 3.490987539291382\n",
      "steps = 205, loss = 50.00625228881836\n",
      "steps = 205, loss = 2.7149555683135986\n",
      "steps = 205, loss = 2.8732237815856934\n",
      "steps = 205, loss = 4.522360324859619\n",
      "steps = 205, loss = 2.6665916442871094\n",
      "steps = 205, loss = 2.8108744621276855\n",
      "steps = 205, loss = 2.3521881103515625\n",
      "steps = 205, loss = 2.749030590057373\n",
      "steps = 205, loss = 2.763760805130005\n",
      "steps = 205, loss = 4.716662883758545\n",
      "steps = 205, loss = 2.9530880451202393\n",
      "steps = 205, loss = 2.9482882022857666\n",
      "steps = 205, loss = 3.463078737258911\n",
      "steps = 205, loss = 2.1153171062469482\n",
      "steps = 205, loss = 5.336700916290283\n",
      "steps = 205, loss = 2.110581159591675\n",
      "steps = 205, loss = 3.4709956645965576\n",
      "steps = 205, loss = 3.5864665508270264\n",
      "steps = 205, loss = 49.9853630065918\n",
      "steps = 205, loss = 3.624948263168335\n",
      "steps = 205, loss = 2.716869592666626\n",
      "steps = 205, loss = 2.051044225692749\n",
      "steps = 205, loss = 6.877040863037109\n",
      "steps = 205, loss = 3.6341888904571533\n",
      "steps = 205, loss = 2.85400390625\n",
      "steps = 205, loss = 3.4006664752960205\n",
      "steps = 206, loss = 4.313405990600586\n",
      "steps = 206, loss = 3.4515957832336426\n",
      "steps = 206, loss = 2.8072636127471924\n",
      "steps = 206, loss = 5.330031394958496\n",
      "steps = 206, loss = 49.9853630065918\n",
      "steps = 206, loss = 3.384337902069092\n",
      "steps = 206, loss = 2.100843906402588\n",
      "steps = 206, loss = 2.775754690170288\n",
      "steps = 206, loss = 3.4967172145843506\n",
      "steps = 206, loss = 2.4471898078918457\n",
      "steps = 206, loss = 3.055778741836548\n",
      "steps = 206, loss = 6.898991107940674\n",
      "steps = 206, loss = 50.001827239990234\n",
      "steps = 206, loss = 3.6490912437438965\n",
      "steps = 206, loss = 3.0738203525543213\n",
      "steps = 206, loss = 3.4644460678100586\n",
      "steps = 206, loss = 4.529566287994385\n",
      "steps = 206, loss = 2.8348464965820312\n",
      "steps = 206, loss = 2.142199754714966\n",
      "steps = 206, loss = 3.4078316688537598\n",
      "steps = 206, loss = 2.7415990829467773\n",
      "steps = 206, loss = 2.5429327487945557\n",
      "steps = 206, loss = 4.717942714691162\n",
      "steps = 206, loss = 2.0756378173828125\n",
      "steps = 206, loss = 2.9446749687194824\n",
      "steps = 206, loss = 2.8660659790039062\n",
      "steps = 206, loss = 3.4300928115844727\n",
      "steps = 206, loss = 2.7291460037231445\n",
      "steps = 206, loss = 2.6938161849975586\n",
      "steps = 207, loss = 2.651106595993042\n",
      "steps = 207, loss = 2.9254374504089355\n",
      "steps = 207, loss = 2.7781362533569336\n",
      "steps = 207, loss = 3.0888478755950928\n",
      "steps = 207, loss = 3.6744866371154785\n",
      "steps = 207, loss = 2.6019763946533203\n",
      "steps = 207, loss = 4.548244953155518\n",
      "steps = 207, loss = 2.160804271697998\n",
      "steps = 207, loss = 3.3478331565856934\n",
      "steps = 207, loss = 2.872771739959717\n",
      "steps = 207, loss = 2.448108196258545\n",
      "steps = 207, loss = 49.9859504699707\n",
      "steps = 207, loss = 4.737105369567871\n",
      "steps = 207, loss = 3.4522898197174072\n",
      "steps = 207, loss = 2.956111192703247\n",
      "steps = 207, loss = 2.278290271759033\n",
      "steps = 207, loss = 2.731797218322754\n",
      "steps = 207, loss = 2.6472067832946777\n",
      "steps = 207, loss = 3.4129559993743896\n",
      "steps = 207, loss = 49.9853630065918\n",
      "steps = 207, loss = 2.5518078804016113\n",
      "steps = 207, loss = 6.922364711761475\n",
      "steps = 207, loss = 2.304633140563965\n",
      "steps = 207, loss = 3.340914011001587\n",
      "steps = 207, loss = 5.362418174743652\n",
      "steps = 207, loss = 2.925074338912964\n",
      "steps = 207, loss = 3.639615774154663\n",
      "steps = 207, loss = 3.3788180351257324\n",
      "steps = 207, loss = 2.7732467651367188\n",
      "steps = 208, loss = 2.773418426513672\n",
      "steps = 208, loss = 2.390411376953125\n",
      "steps = 208, loss = 3.093379020690918\n",
      "steps = 208, loss = 4.561435222625732\n",
      "steps = 208, loss = 2.791840076446533\n",
      "steps = 208, loss = 2.9122231006622314\n",
      "steps = 208, loss = 3.48980450630188\n",
      "steps = 208, loss = 2.8756444454193115\n",
      "steps = 208, loss = 2.732923984527588\n",
      "steps = 208, loss = 2.584761381149292\n",
      "steps = 208, loss = 2.264474630355835\n",
      "steps = 208, loss = 2.957711696624756\n",
      "steps = 208, loss = 6.974638938903809\n",
      "steps = 208, loss = 3.462351083755493\n",
      "steps = 208, loss = 4.746140956878662\n",
      "steps = 208, loss = 2.6408324241638184\n",
      "steps = 208, loss = 2.144913673400879\n",
      "steps = 208, loss = 3.4920995235443115\n",
      "steps = 208, loss = 2.9122204780578613\n",
      "steps = 208, loss = 2.4840950965881348\n",
      "steps = 208, loss = 3.0144307613372803\n",
      "steps = 208, loss = 2.578482151031494\n",
      "steps = 208, loss = 49.97343444824219\n",
      "steps = 208, loss = 5.3808746337890625\n",
      "steps = 208, loss = 3.498080253601074\n",
      "steps = 208, loss = 3.4924392700195312\n",
      "steps = 208, loss = 3.5357041358947754\n",
      "steps = 208, loss = 3.437838077545166\n",
      "steps = 208, loss = 49.9853630065918\n",
      "steps = 209, loss = 3.04585599899292\n",
      "steps = 209, loss = 3.082820177078247\n",
      "steps = 209, loss = 3.334289073944092\n",
      "steps = 209, loss = 4.562678337097168\n",
      "steps = 209, loss = 2.788335084915161\n",
      "steps = 209, loss = 3.468489646911621\n",
      "steps = 209, loss = 2.019536256790161\n",
      "steps = 209, loss = 2.686420440673828\n",
      "steps = 209, loss = 7.029374599456787\n",
      "steps = 209, loss = 7.2124481201171875\n",
      "steps = 209, loss = 3.344219446182251\n",
      "steps = 209, loss = 3.695669651031494\n",
      "steps = 209, loss = 2.69728422164917\n",
      "steps = 209, loss = 50.000919342041016\n",
      "steps = 209, loss = 2.816559076309204\n",
      "steps = 209, loss = 3.2488391399383545\n",
      "steps = 209, loss = 5.37853479385376\n",
      "steps = 209, loss = 3.466259241104126\n",
      "steps = 209, loss = 2.837367534637451\n",
      "steps = 209, loss = 2.781024932861328\n",
      "steps = 209, loss = 2.9190673828125\n",
      "steps = 209, loss = 49.9853630065918\n",
      "steps = 209, loss = 2.62841796875\n",
      "steps = 209, loss = 4.74233865737915\n",
      "steps = 209, loss = 2.480876922607422\n",
      "steps = 209, loss = 2.743232250213623\n",
      "steps = 209, loss = 2.1335909366607666\n",
      "steps = 209, loss = 2.9465370178222656\n",
      "steps = 209, loss = 3.4447836875915527\n",
      "steps = 210, loss = 3.4710495471954346\n",
      "steps = 210, loss = 2.8536126613616943\n",
      "steps = 210, loss = 5.416061878204346\n",
      "steps = 210, loss = 49.9853630065918\n",
      "steps = 210, loss = 2.705808162689209\n",
      "steps = 210, loss = 3.129030704498291\n",
      "steps = 210, loss = 2.952794313430786\n",
      "steps = 210, loss = 2.765056610107422\n",
      "steps = 210, loss = 3.5543787479400635\n",
      "steps = 210, loss = 4.757599353790283\n",
      "steps = 210, loss = 2.6894452571868896\n",
      "steps = 210, loss = 3.427004814147949\n",
      "steps = 210, loss = 2.7913620471954346\n",
      "steps = 210, loss = 49.958953857421875\n",
      "steps = 210, loss = 3.673938274383545\n",
      "steps = 210, loss = 2.7151005268096924\n",
      "steps = 210, loss = 2.305449962615967\n",
      "steps = 210, loss = 3.0816898345947266\n",
      "steps = 210, loss = 3.3999996185302734\n",
      "steps = 210, loss = 7.069910049438477\n",
      "steps = 210, loss = 2.2615931034088135\n",
      "steps = 210, loss = 2.12522292137146\n",
      "steps = 210, loss = 3.0952401161193848\n",
      "steps = 210, loss = 2.9250693321228027\n",
      "steps = 210, loss = 2.11562180519104\n",
      "steps = 210, loss = 4.56571102142334\n",
      "steps = 210, loss = 3.040827512741089\n",
      "steps = 210, loss = 3.3542556762695312\n",
      "steps = 210, loss = 2.4879231452941895\n",
      "steps = 211, loss = 3.6049468517303467\n",
      "steps = 211, loss = 3.521355390548706\n",
      "steps = 211, loss = 3.417606830596924\n",
      "steps = 211, loss = 49.9853630065918\n",
      "steps = 211, loss = 2.74237060546875\n",
      "steps = 211, loss = 6.93946647644043\n",
      "steps = 211, loss = 3.3764426708221436\n",
      "steps = 211, loss = 2.9806811809539795\n",
      "steps = 211, loss = 2.1894419193267822\n",
      "steps = 211, loss = 2.140772581100464\n",
      "steps = 211, loss = 2.9190189838409424\n",
      "steps = 211, loss = 2.5060067176818848\n",
      "steps = 211, loss = 2.8593215942382812\n",
      "steps = 211, loss = 5.411489486694336\n",
      "steps = 211, loss = 2.695409059524536\n",
      "steps = 211, loss = 2.944603204727173\n",
      "steps = 211, loss = 4.5738396644592285\n",
      "steps = 211, loss = 3.603999137878418\n",
      "steps = 211, loss = 4.758383750915527\n",
      "steps = 211, loss = 2.0864651203155518\n",
      "steps = 211, loss = 2.314987897872925\n",
      "steps = 211, loss = 2.75369930267334\n",
      "steps = 211, loss = 50.003936767578125\n",
      "steps = 211, loss = 3.0874006748199463\n",
      "steps = 211, loss = 2.836015462875366\n",
      "steps = 211, loss = 2.6051831245422363\n",
      "steps = 211, loss = 3.3765339851379395\n",
      "steps = 211, loss = 2.9748117923736572\n",
      "steps = 211, loss = 3.408447027206421\n",
      "steps = 212, loss = 2.4850637912750244\n",
      "steps = 212, loss = 3.496598243713379\n",
      "steps = 212, loss = 3.448841094970703\n",
      "steps = 212, loss = 4.773690700531006\n",
      "steps = 212, loss = 3.100126028060913\n",
      "steps = 212, loss = 3.5781965255737305\n",
      "steps = 212, loss = 2.9514822959899902\n",
      "steps = 212, loss = 2.9211978912353516\n",
      "steps = 212, loss = 3.311140537261963\n",
      "steps = 212, loss = 3.7297723293304443\n",
      "steps = 212, loss = 2.8831124305725098\n",
      "steps = 212, loss = 5.4494757652282715\n",
      "steps = 212, loss = 3.446115255355835\n",
      "steps = 212, loss = 2.107518434524536\n",
      "steps = 212, loss = 2.712334394454956\n",
      "steps = 212, loss = 2.853595018386841\n",
      "steps = 212, loss = 3.0049355030059814\n",
      "steps = 212, loss = 3.1320412158966064\n",
      "steps = 212, loss = 2.9883768558502197\n",
      "steps = 212, loss = 2.7051451206207275\n",
      "steps = 212, loss = 49.999534606933594\n",
      "steps = 212, loss = 2.7651054859161377\n",
      "steps = 212, loss = 3.454911708831787\n",
      "steps = 212, loss = 4.578395366668701\n",
      "steps = 212, loss = 2.0676562786102295\n",
      "steps = 212, loss = 2.7153844833374023\n",
      "steps = 212, loss = 49.9853630065918\n",
      "steps = 212, loss = 6.969928741455078\n",
      "steps = 212, loss = 2.3065266609191895\n",
      "steps = 213, loss = 1.862272024154663\n",
      "steps = 213, loss = 2.922177791595459\n",
      "steps = 213, loss = 4.7877936363220215\n",
      "steps = 213, loss = 2.9683117866516113\n",
      "steps = 213, loss = 3.2298097610473633\n",
      "steps = 213, loss = 3.226210594177246\n",
      "steps = 213, loss = 3.3959579467773438\n",
      "steps = 213, loss = 2.7312917709350586\n",
      "steps = 213, loss = 3.6531548500061035\n",
      "steps = 213, loss = 5.463601589202881\n",
      "steps = 213, loss = 2.282773017883301\n",
      "steps = 213, loss = 2.772752285003662\n",
      "steps = 213, loss = 6.940488338470459\n",
      "steps = 213, loss = 49.9853630065918\n",
      "steps = 213, loss = 3.358201026916504\n",
      "steps = 213, loss = 3.1079204082489014\n",
      "steps = 213, loss = 49.99692916870117\n",
      "steps = 213, loss = 2.872495651245117\n",
      "steps = 213, loss = 4.594842433929443\n",
      "steps = 213, loss = 3.641929864883423\n",
      "steps = 213, loss = 2.66422438621521\n",
      "steps = 213, loss = 3.4746594429016113\n",
      "steps = 213, loss = 2.1424312591552734\n",
      "steps = 213, loss = 3.1062159538269043\n",
      "steps = 213, loss = 2.955352306365967\n",
      "steps = 213, loss = 2.6287569999694824\n",
      "steps = 213, loss = 3.329815626144409\n",
      "steps = 213, loss = 2.44655442237854\n",
      "steps = 213, loss = 3.424494743347168\n",
      "steps = 214, loss = 2.185577392578125\n",
      "steps = 214, loss = 49.9853630065918\n",
      "steps = 214, loss = 2.773514747619629\n",
      "steps = 214, loss = 5.481259822845459\n",
      "steps = 214, loss = 3.4984819889068604\n",
      "steps = 214, loss = 2.7327077388763428\n",
      "steps = 214, loss = 3.0400609970092773\n",
      "steps = 214, loss = 2.8755900859832764\n",
      "steps = 214, loss = 3.4544484615325928\n",
      "steps = 214, loss = 3.109863519668579\n",
      "steps = 214, loss = 6.999983787536621\n",
      "steps = 214, loss = 2.6371893882751465\n",
      "steps = 214, loss = 3.0306665897369385\n",
      "steps = 214, loss = 3.4647648334503174\n",
      "steps = 214, loss = 3.5908334255218506\n",
      "steps = 214, loss = 1.9785171747207642\n",
      "steps = 214, loss = 2.2616629600524902\n",
      "steps = 214, loss = 4.796275615692139\n",
      "steps = 214, loss = 2.9567160606384277\n",
      "steps = 214, loss = 3.439809799194336\n",
      "steps = 214, loss = 3.5028092861175537\n",
      "steps = 214, loss = 4.607690334320068\n",
      "steps = 214, loss = 2.6232473850250244\n",
      "steps = 214, loss = 2.890256881713867\n",
      "steps = 214, loss = 50.00230026245117\n",
      "steps = 214, loss = 3.47387433052063\n",
      "steps = 214, loss = 3.509796619415283\n",
      "steps = 214, loss = 50.00404739379883\n",
      "steps = 214, loss = 2.708399772644043\n",
      "steps = 215, loss = 3.405545473098755\n",
      "steps = 215, loss = 3.1108994483947754\n",
      "steps = 215, loss = 3.4551424980163574\n",
      "steps = 215, loss = 5.502929210662842\n",
      "steps = 215, loss = 2.4769513607025146\n",
      "steps = 215, loss = 3.5094003677368164\n",
      "steps = 215, loss = 2.715546131134033\n",
      "steps = 215, loss = 3.0439209938049316\n",
      "steps = 215, loss = 3.6850717067718506\n",
      "steps = 215, loss = 2.16528582572937\n",
      "steps = 215, loss = 2.086744546890259\n",
      "steps = 215, loss = 2.953179121017456\n",
      "steps = 215, loss = 4.80015754699707\n",
      "steps = 215, loss = 7.042551040649414\n",
      "steps = 215, loss = 49.78996276855469\n",
      "steps = 215, loss = 3.5806334018707275\n",
      "steps = 215, loss = 50.00434494018555\n",
      "steps = 215, loss = 2.8172926902770996\n",
      "steps = 215, loss = 4.605779647827148\n",
      "steps = 215, loss = 2.9507663249969482\n",
      "steps = 215, loss = 2.7657575607299805\n",
      "steps = 215, loss = 2.612661123275757\n",
      "steps = 215, loss = 49.9853630065918\n",
      "steps = 215, loss = 3.309720754623413\n",
      "steps = 215, loss = 2.0484185218811035\n",
      "steps = 215, loss = 2.8547322750091553\n",
      "steps = 215, loss = 3.458695411682129\n",
      "steps = 215, loss = 2.663088798522949\n",
      "steps = 215, loss = 3.462217330932617\n",
      "steps = 216, loss = 2.404707431793213\n",
      "steps = 216, loss = 3.4765994548797607\n",
      "steps = 216, loss = 3.0275981426239014\n",
      "steps = 216, loss = 2.9440741539001465\n",
      "steps = 216, loss = 4.799990653991699\n",
      "steps = 216, loss = 3.008570909500122\n",
      "steps = 216, loss = 2.742007255554199\n",
      "steps = 216, loss = 4.6124444007873535\n",
      "steps = 216, loss = 49.440086364746094\n",
      "steps = 216, loss = 3.4113423824310303\n",
      "steps = 216, loss = 3.2957494258880615\n",
      "steps = 216, loss = 3.319502115249634\n",
      "steps = 216, loss = 3.320728302001953\n",
      "steps = 216, loss = 1.9762868881225586\n",
      "steps = 216, loss = 3.101820468902588\n",
      "steps = 216, loss = 2.5679595470428467\n",
      "steps = 216, loss = 3.113473415374756\n",
      "steps = 216, loss = 39.981773376464844\n",
      "steps = 216, loss = 3.3664143085479736\n",
      "steps = 216, loss = 3.2300782203674316\n",
      "steps = 216, loss = 2.1425485610961914\n",
      "steps = 216, loss = 2.694286346435547\n",
      "steps = 216, loss = 49.9853630065918\n",
      "steps = 216, loss = 2.5862503051757812\n",
      "steps = 216, loss = 5.496171951293945\n",
      "steps = 216, loss = 2.834984540939331\n",
      "steps = 216, loss = 7.051941394805908\n",
      "steps = 216, loss = 50.01384353637695\n",
      "steps = 216, loss = 2.732266426086426\n",
      "steps = 217, loss = 3.6137921810150146\n",
      "steps = 217, loss = 50.01182556152344\n",
      "steps = 217, loss = 2.853585720062256\n",
      "steps = 217, loss = 2.8155343532562256\n",
      "steps = 217, loss = 1.996989130973816\n",
      "steps = 217, loss = 7.086756706237793\n",
      "steps = 217, loss = 3.6923186779022217\n",
      "steps = 217, loss = 3.5445449352264404\n",
      "steps = 217, loss = 3.6025142669677734\n",
      "steps = 217, loss = 4.814398288726807\n",
      "steps = 217, loss = 4.615886688232422\n",
      "steps = 217, loss = 3.4950931072235107\n",
      "steps = 217, loss = 2.777090311050415\n",
      "steps = 217, loss = 3.3672878742218018\n",
      "steps = 217, loss = 49.99580001831055\n",
      "steps = 217, loss = 2.9648079872131348\n",
      "steps = 217, loss = 49.9853630065918\n",
      "steps = 217, loss = 3.146920919418335\n",
      "steps = 217, loss = 3.6508443355560303\n",
      "steps = 217, loss = 5.533053874969482\n",
      "steps = 217, loss = 2.715477705001831\n",
      "steps = 217, loss = 3.1139214038848877\n",
      "steps = 217, loss = 2.7139198780059814\n",
      "steps = 217, loss = 2.370051622390747\n",
      "steps = 217, loss = 2.288710355758667\n",
      "steps = 217, loss = 2.7634661197662354\n",
      "steps = 217, loss = 2.1377310752868652\n",
      "steps = 217, loss = 3.372544050216675\n",
      "steps = 217, loss = 2.9508814811706543\n",
      "steps = 218, loss = 3.5398120880126953\n",
      "steps = 218, loss = 2.6947567462921143\n",
      "steps = 218, loss = 2.915311098098755\n",
      "steps = 218, loss = 5.528262615203857\n",
      "steps = 218, loss = 3.4477243423461914\n",
      "steps = 218, loss = 3.599088191986084\n",
      "steps = 218, loss = 2.1325159072875977\n",
      "steps = 218, loss = 2.9428045749664307\n",
      "steps = 218, loss = 50.00185775756836\n",
      "steps = 218, loss = 2.742126703262329\n",
      "steps = 218, loss = 2.4283359050750732\n",
      "steps = 218, loss = 3.106044054031372\n",
      "steps = 218, loss = 2.8352792263031006\n",
      "steps = 218, loss = 3.030935764312744\n",
      "steps = 218, loss = 3.529445171356201\n",
      "steps = 218, loss = 2.2175991535186768\n",
      "steps = 218, loss = 2.771801233291626\n",
      "steps = 218, loss = 3.473752021789551\n",
      "steps = 218, loss = 4.8149871826171875\n",
      "steps = 218, loss = 49.9853630065918\n",
      "steps = 218, loss = 2.998410224914551\n",
      "steps = 218, loss = 3.3732378482818604\n",
      "steps = 218, loss = 3.500673294067383\n",
      "steps = 218, loss = 50.02409362792969\n",
      "steps = 218, loss = 2.629389524459839\n",
      "steps = 218, loss = 7.058044910430908\n",
      "steps = 218, loss = 4.624165058135986\n",
      "steps = 218, loss = 3.6461379528045654\n",
      "steps = 218, loss = 2.01835036277771\n",
      "steps = 219, loss = 2.954143762588501\n",
      "steps = 219, loss = 3.473158359527588\n",
      "steps = 219, loss = 7.133957862854004\n",
      "steps = 219, loss = 3.120673894882202\n",
      "steps = 219, loss = 49.9853630065918\n",
      "steps = 219, loss = 3.647023916244507\n",
      "steps = 219, loss = 2.2772839069366455\n",
      "steps = 219, loss = 2.9163873195648193\n",
      "steps = 219, loss = 3.0688600540161133\n",
      "steps = 219, loss = 2.773638963699341\n",
      "steps = 219, loss = 4.639321804046631\n",
      "steps = 219, loss = 5.560166358947754\n",
      "steps = 219, loss = 3.259265661239624\n",
      "steps = 219, loss = 2.6692147254943848\n",
      "steps = 219, loss = 2.261366367340088\n",
      "steps = 219, loss = 2.732393264770508\n",
      "steps = 219, loss = 3.4412856101989746\n",
      "steps = 219, loss = 2.609701156616211\n",
      "steps = 219, loss = 50.00092315673828\n",
      "steps = 219, loss = 3.004403829574585\n",
      "steps = 219, loss = 3.360224962234497\n",
      "steps = 219, loss = 2.1608517169952393\n",
      "steps = 219, loss = 2.6491403579711914\n",
      "steps = 219, loss = 3.6566221714019775\n",
      "steps = 219, loss = 4.8338751792907715\n",
      "steps = 219, loss = 3.9546587467193604\n",
      "steps = 219, loss = 3.481163740158081\n",
      "steps = 219, loss = 49.791259765625\n",
      "steps = 219, loss = 2.8731303215026855\n",
      "steps = 220, loss = 3.471846103668213\n",
      "steps = 220, loss = 3.4415388107299805\n",
      "steps = 220, loss = 3.542635202407837\n",
      "steps = 220, loss = 4.652091026306152\n",
      "steps = 220, loss = 7.022805213928223\n",
      "steps = 220, loss = 2.183911085128784\n",
      "steps = 220, loss = 2.8759541511535645\n",
      "steps = 220, loss = 2.1620125770568848\n",
      "steps = 220, loss = 2.6929497718811035\n",
      "steps = 220, loss = 2.7334535121917725\n",
      "steps = 220, loss = 3.431281089782715\n",
      "steps = 220, loss = 49.856353759765625\n",
      "steps = 220, loss = 50.01582717895508\n",
      "steps = 220, loss = 3.1250505447387695\n",
      "steps = 220, loss = 2.2643630504608154\n",
      "steps = 220, loss = 4.114027976989746\n",
      "steps = 220, loss = 2.9557693004608154\n",
      "steps = 220, loss = 2.8076231479644775\n",
      "steps = 220, loss = 4.842554092407227\n",
      "steps = 220, loss = 3.34885573387146\n",
      "steps = 220, loss = 2.562533140182495\n",
      "steps = 220, loss = 3.4151611328125\n",
      "steps = 220, loss = 49.9853630065918\n",
      "steps = 220, loss = 2.773967742919922\n",
      "steps = 220, loss = 2.882059335708618\n",
      "steps = 220, loss = 2.9919958114624023\n",
      "steps = 220, loss = 5.57883882522583\n",
      "steps = 220, loss = 2.987856149673462\n",
      "steps = 220, loss = 3.4902074337005615\n",
      "steps = 221, loss = 4.838221549987793\n",
      "steps = 221, loss = 7.177844047546387\n",
      "steps = 221, loss = 3.1768710613250732\n",
      "steps = 221, loss = 2.1320183277130127\n",
      "steps = 221, loss = 3.224825859069824\n",
      "steps = 221, loss = 2.7437682151794434\n",
      "steps = 221, loss = 2.5579264163970947\n",
      "steps = 221, loss = 3.2991905212402344\n",
      "steps = 221, loss = 4.02385950088501\n",
      "steps = 221, loss = 3.349956512451172\n",
      "steps = 221, loss = 2.837589979171753\n",
      "steps = 221, loss = 5.577442169189453\n",
      "steps = 221, loss = 2.6382317543029785\n",
      "steps = 221, loss = 3.1145060062408447\n",
      "steps = 221, loss = 2.7923450469970703\n",
      "steps = 221, loss = 3.6738860607147217\n",
      "steps = 221, loss = 2.8019638061523438\n",
      "steps = 221, loss = 3.4716100692749023\n",
      "steps = 221, loss = 3.0270638465881348\n",
      "steps = 221, loss = 2.033221483230591\n",
      "steps = 221, loss = 49.930152893066406\n",
      "steps = 221, loss = 2.436657667160034\n",
      "steps = 221, loss = 4.654751777648926\n",
      "steps = 221, loss = 2.697831869125366\n",
      "steps = 221, loss = 3.4069302082061768\n",
      "steps = 221, loss = 49.9853630065918\n",
      "steps = 221, loss = 2.2633376121520996\n",
      "steps = 221, loss = 2.9447615146636963\n",
      "steps = 221, loss = 50.00596237182617\n",
      "steps = 222, loss = 2.8811988830566406\n",
      "steps = 222, loss = 2.8708276748657227\n",
      "steps = 222, loss = 3.391779899597168\n",
      "steps = 222, loss = 6.9322710037231445\n",
      "steps = 222, loss = 4.6546454429626465\n",
      "steps = 222, loss = 3.5217864513397217\n",
      "steps = 222, loss = 2.8536312580108643\n",
      "steps = 222, loss = 49.9853630065918\n",
      "steps = 222, loss = 3.4585604667663574\n",
      "steps = 222, loss = 2.7662415504455566\n",
      "steps = 222, loss = 2.6917152404785156\n",
      "steps = 222, loss = 3.468994140625\n",
      "steps = 222, loss = 2.231950283050537\n",
      "steps = 222, loss = 5.6133928298950195\n",
      "steps = 222, loss = 2.0619990825653076\n",
      "steps = 222, loss = 3.9001996517181396\n",
      "steps = 222, loss = 50.016624450683594\n",
      "steps = 222, loss = 3.1264865398406982\n",
      "steps = 222, loss = 2.715644121170044\n",
      "steps = 222, loss = 4.852998733520508\n",
      "steps = 222, loss = 3.7011220455169678\n",
      "steps = 222, loss = 50.0184326171875\n",
      "steps = 222, loss = 2.145509719848633\n",
      "steps = 222, loss = 3.4778754711151123\n",
      "steps = 222, loss = 3.002074956893921\n",
      "steps = 222, loss = 2.722196578979492\n",
      "steps = 222, loss = 2.3529269695281982\n",
      "steps = 222, loss = 2.9510436058044434\n",
      "steps = 222, loss = 3.3448612689971924\n",
      "steps = 223, loss = 2.731875419616699\n",
      "steps = 223, loss = 2.201108694076538\n",
      "steps = 223, loss = 3.6263978481292725\n",
      "steps = 223, loss = 7.0101141929626465\n",
      "steps = 223, loss = 2.9241528511047363\n",
      "steps = 223, loss = 3.4045140743255615\n",
      "steps = 223, loss = 2.5896172523498535\n",
      "steps = 223, loss = 50.01015090942383\n",
      "steps = 223, loss = 5.6278228759765625\n",
      "steps = 223, loss = 2.954604387283325\n",
      "steps = 223, loss = 2.6149954795837402\n",
      "steps = 223, loss = 3.4217095375061035\n",
      "steps = 223, loss = 2.752061367034912\n",
      "steps = 223, loss = 2.773252248764038\n",
      "steps = 223, loss = 2.769447088241577\n",
      "steps = 223, loss = 2.637356758117676\n",
      "steps = 223, loss = 3.4410862922668457\n",
      "steps = 223, loss = 2.8736610412597656\n",
      "steps = 223, loss = 3.1612257957458496\n",
      "steps = 223, loss = 3.4907782077789307\n",
      "steps = 223, loss = 49.9853630065918\n",
      "steps = 223, loss = 3.3786160945892334\n",
      "steps = 223, loss = 4.865551948547363\n",
      "steps = 223, loss = 4.671308517456055\n",
      "steps = 223, loss = 3.132611036300659\n",
      "steps = 223, loss = 3.6405022144317627\n",
      "steps = 223, loss = 2.2760353088378906\n",
      "steps = 223, loss = 2.174323320388794\n",
      "steps = 223, loss = 50.0166015625\n",
      "steps = 224, loss = 2.3871583938598633\n",
      "steps = 224, loss = 7.057778358459473\n",
      "steps = 224, loss = 2.057661294937134\n",
      "steps = 224, loss = 3.037686824798584\n",
      "steps = 224, loss = 3.4502928256988525\n",
      "steps = 224, loss = 2.8370096683502197\n",
      "steps = 224, loss = 3.401175022125244\n",
      "steps = 224, loss = 3.1223599910736084\n",
      "steps = 224, loss = 2.743300676345825\n",
      "steps = 224, loss = 4.67483377456665\n",
      "steps = 224, loss = 4.861242294311523\n",
      "steps = 224, loss = 2.1409170627593994\n",
      "steps = 224, loss = 3.382087230682373\n",
      "steps = 224, loss = 2.7750091552734375\n",
      "steps = 224, loss = 2.7397029399871826\n",
      "steps = 224, loss = 5.626780033111572\n",
      "steps = 224, loss = 50.01912307739258\n",
      "steps = 224, loss = 49.9853630065918\n",
      "steps = 224, loss = 2.569657802581787\n",
      "steps = 224, loss = 3.2670249938964844\n",
      "steps = 224, loss = 2.6971120834350586\n",
      "steps = 224, loss = 3.5918807983398438\n",
      "steps = 224, loss = 2.6211202144622803\n",
      "steps = 224, loss = 3.524735689163208\n",
      "steps = 224, loss = 3.2822458744049072\n",
      "steps = 224, loss = 2.061882972717285\n",
      "steps = 224, loss = 3.2931928634643555\n",
      "steps = 224, loss = 2.943912982940674\n",
      "steps = 224, loss = 50.03557205200195\n",
      "steps = 225, loss = 3.3652024269104004\n",
      "steps = 225, loss = 2.0491602420806885\n",
      "steps = 225, loss = 2.7658498287200928\n",
      "steps = 225, loss = 3.726914882659912\n",
      "steps = 225, loss = 50.01763153076172\n",
      "steps = 225, loss = 2.8536109924316406\n",
      "steps = 225, loss = 3.621554136276245\n",
      "steps = 225, loss = 2.9482712745666504\n",
      "steps = 225, loss = 3.6225855350494385\n",
      "steps = 225, loss = 4.876011371612549\n",
      "steps = 225, loss = 2.7408549785614014\n",
      "steps = 225, loss = 7.1032490730285645\n",
      "steps = 225, loss = 5.6633477210998535\n",
      "steps = 225, loss = 2.065511465072632\n",
      "steps = 225, loss = 3.482048988342285\n",
      "steps = 225, loss = 3.5064473152160645\n",
      "steps = 225, loss = 2.9503848552703857\n",
      "steps = 225, loss = 3.143540143966675\n",
      "steps = 225, loss = 49.92518997192383\n",
      "steps = 225, loss = 3.8845906257629395\n",
      "steps = 225, loss = 2.411256790161133\n",
      "steps = 225, loss = 2.93208384513855\n",
      "steps = 225, loss = 2.850066661834717\n",
      "steps = 225, loss = 2.715604782104492\n",
      "steps = 225, loss = 3.1343514919281006\n",
      "steps = 225, loss = 2.2880122661590576\n",
      "steps = 225, loss = 4.674936294555664\n",
      "steps = 225, loss = 49.9853630065918\n",
      "steps = 225, loss = 3.439124584197998\n",
      "steps = 226, loss = 2.1388280391693115\n",
      "steps = 226, loss = 2.9422755241394043\n",
      "steps = 226, loss = 2.247647523880005\n",
      "steps = 226, loss = 3.250812292098999\n",
      "steps = 226, loss = 4.876004695892334\n",
      "steps = 226, loss = 3.7433907985687256\n",
      "steps = 226, loss = 5.657505512237549\n",
      "steps = 226, loss = 2.0457847118377686\n",
      "steps = 226, loss = 2.743198871612549\n",
      "steps = 226, loss = 3.4570772647857666\n",
      "steps = 226, loss = 3.6367626190185547\n",
      "steps = 226, loss = 3.5344974994659424\n",
      "steps = 226, loss = 2.969113349914551\n",
      "steps = 226, loss = 4.683915138244629\n",
      "steps = 226, loss = 3.5303118228912354\n",
      "steps = 226, loss = 2.8361151218414307\n",
      "steps = 226, loss = 2.592881917953491\n",
      "steps = 226, loss = 2.4481866359710693\n",
      "steps = 226, loss = 50.012630462646484\n",
      "steps = 226, loss = 2.880777359008789\n",
      "steps = 226, loss = 7.030375957489014\n",
      "steps = 226, loss = 49.9853630065918\n",
      "steps = 226, loss = 2.695866823196411\n",
      "steps = 226, loss = 3.1265201568603516\n",
      "steps = 226, loss = 2.61399245262146\n",
      "steps = 226, loss = 3.59043288230896\n",
      "steps = 226, loss = 2.593997001647949\n",
      "steps = 226, loss = 50.02410888671875\n",
      "steps = 226, loss = 3.4216878414154053\n",
      "steps = 227, loss = 2.2782576084136963\n",
      "steps = 227, loss = 3.4085896015167236\n",
      "steps = 227, loss = 7.104243755340576\n",
      "steps = 227, loss = 2.9534451961517334\n",
      "steps = 227, loss = 32.29393005371094\n",
      "steps = 227, loss = 3.4987869262695312\n",
      "steps = 227, loss = 49.99524688720703\n",
      "steps = 227, loss = 5.689245223999023\n",
      "steps = 227, loss = 3.6901960372924805\n",
      "steps = 227, loss = 49.954444885253906\n",
      "steps = 227, loss = 2.1457576751708984\n",
      "steps = 227, loss = 3.3657188415527344\n",
      "steps = 227, loss = 2.6183807849884033\n",
      "steps = 227, loss = 2.873750686645508\n",
      "steps = 227, loss = 2.732893466949463\n",
      "steps = 227, loss = 2.774174451828003\n",
      "steps = 227, loss = 49.9853630065918\n",
      "steps = 227, loss = 4.179680347442627\n",
      "steps = 227, loss = 2.865346908569336\n",
      "steps = 227, loss = 3.0313565731048584\n",
      "steps = 227, loss = 4.697373867034912\n",
      "steps = 227, loss = 3.141258478164673\n",
      "steps = 227, loss = 3.513054847717285\n",
      "steps = 227, loss = 3.0516648292541504\n",
      "steps = 227, loss = 3.4802048206329346\n",
      "steps = 227, loss = 3.5107297897338867\n",
      "steps = 227, loss = 2.7183141708374023\n",
      "steps = 227, loss = 4.894885063171387\n",
      "steps = 227, loss = 2.6727561950683594\n",
      "steps = 228, loss = 3.1964430809020996\n",
      "steps = 228, loss = 4.255661964416504\n",
      "steps = 228, loss = 3.3962574005126953\n",
      "steps = 228, loss = 3.4232494831085205\n",
      "steps = 228, loss = 7.155861854553223\n",
      "steps = 228, loss = 3.295311450958252\n",
      "steps = 228, loss = 3.587723970413208\n",
      "steps = 228, loss = 2.1722676753997803\n",
      "steps = 228, loss = 2.8542847633361816\n",
      "steps = 228, loss = 2.7006044387817383\n",
      "steps = 228, loss = 2.7573914527893066\n",
      "steps = 228, loss = 3.354743242263794\n",
      "steps = 228, loss = 49.9853630065918\n",
      "steps = 228, loss = 5.712497234344482\n",
      "steps = 228, loss = 1.94748055934906\n",
      "steps = 228, loss = 2.7160770893096924\n",
      "steps = 228, loss = 2.766753911972046\n",
      "steps = 228, loss = 2.0398571491241455\n",
      "steps = 228, loss = 4.89973258972168\n",
      "steps = 228, loss = 3.366816282272339\n",
      "steps = 228, loss = 50.00758361816406\n",
      "steps = 228, loss = 3.143449306488037\n",
      "steps = 228, loss = 2.950680732727051\n",
      "steps = 228, loss = 4.697615146636963\n",
      "steps = 228, loss = 3.5037248134613037\n",
      "steps = 228, loss = 49.60347366333008\n",
      "steps = 228, loss = 2.5245797634124756\n",
      "steps = 228, loss = 3.144892930984497\n",
      "steps = 228, loss = 2.766380786895752\n",
      "steps = 229, loss = 2.1821417808532715\n",
      "steps = 229, loss = 2.8739097118377686\n",
      "steps = 229, loss = 2.2766776084899902\n",
      "steps = 229, loss = 2.255284547805786\n",
      "steps = 229, loss = 3.6469979286193848\n",
      "steps = 229, loss = 2.644052743911743\n",
      "steps = 229, loss = 3.0821573734283447\n",
      "steps = 229, loss = 7.042003631591797\n",
      "steps = 229, loss = 3.3981072902679443\n",
      "steps = 229, loss = 2.8947815895080566\n",
      "steps = 229, loss = 50.01458740234375\n",
      "steps = 229, loss = 3.674567461013794\n",
      "steps = 229, loss = 5.725141525268555\n",
      "steps = 229, loss = 4.185048580169678\n",
      "steps = 229, loss = 2.970162868499756\n",
      "steps = 229, loss = 50.02184295654297\n",
      "steps = 229, loss = 4.910748481750488\n",
      "steps = 229, loss = 2.953789234161377\n",
      "steps = 229, loss = 3.4752283096313477\n",
      "steps = 229, loss = 3.444208860397339\n",
      "steps = 229, loss = 3.528759002685547\n",
      "steps = 229, loss = 2.7320444583892822\n",
      "steps = 229, loss = 2.9643847942352295\n",
      "steps = 229, loss = 2.7729623317718506\n",
      "steps = 229, loss = 49.9853630065918\n",
      "steps = 229, loss = 3.148571729660034\n",
      "steps = 229, loss = 3.3921332359313965\n",
      "steps = 229, loss = 4.71201753616333\n",
      "steps = 229, loss = 2.6477653980255127\n",
      "steps = 230, loss = 2.9430429935455322\n",
      "steps = 230, loss = 2.6820993423461914\n",
      "steps = 230, loss = 5.723944187164307\n",
      "steps = 230, loss = 3.13793683052063\n",
      "steps = 230, loss = 3.377467632293701\n",
      "steps = 230, loss = 7.2049455642700195\n",
      "steps = 230, loss = 2.6969258785247803\n",
      "steps = 230, loss = 3.647900104522705\n",
      "steps = 230, loss = 2.7652394771575928\n",
      "steps = 230, loss = 49.75244140625\n",
      "steps = 230, loss = 2.836866617202759\n",
      "steps = 230, loss = 2.1410229206085205\n",
      "steps = 230, loss = 3.461090564727783\n",
      "steps = 230, loss = 1.9989986419677734\n",
      "steps = 230, loss = 3.3680944442749023\n",
      "steps = 230, loss = 4.3702192306518555\n",
      "steps = 230, loss = 3.1995749473571777\n",
      "steps = 230, loss = 4.71653413772583\n",
      "steps = 230, loss = 4.905642986297607\n",
      "steps = 230, loss = 2.9465458393096924\n",
      "steps = 230, loss = 2.7436037063598633\n",
      "steps = 230, loss = 2.080275774002075\n",
      "steps = 230, loss = 2.5808558464050293\n",
      "steps = 230, loss = 49.9853630065918\n",
      "steps = 230, loss = 3.4250948429107666\n",
      "steps = 230, loss = 3.3414788246154785\n",
      "steps = 230, loss = 49.97208023071289\n",
      "steps = 230, loss = 2.364455461502075\n",
      "steps = 230, loss = 2.827505111694336\n",
      "steps = 231, loss = 49.90430450439453\n",
      "steps = 231, loss = 3.1093358993530273\n",
      "steps = 231, loss = 6.982848167419434\n",
      "steps = 231, loss = 50.01968765258789\n",
      "steps = 231, loss = 2.9495506286621094\n",
      "steps = 231, loss = 2.7158186435699463\n",
      "steps = 231, loss = 4.9199957847595215\n",
      "steps = 231, loss = 2.5258257389068604\n",
      "steps = 231, loss = 5.7595601081848145\n",
      "steps = 231, loss = 49.9853630065918\n",
      "steps = 231, loss = 4.375613689422607\n",
      "steps = 231, loss = 3.450745105743408\n",
      "steps = 231, loss = 3.0460896492004395\n",
      "steps = 231, loss = 3.6383395195007324\n",
      "steps = 231, loss = 2.1468846797943115\n",
      "steps = 231, loss = 2.765286922454834\n",
      "steps = 231, loss = 3.6141350269317627\n",
      "steps = 231, loss = 4.717123031616211\n",
      "steps = 231, loss = 3.149773359298706\n",
      "steps = 231, loss = 3.6943511962890625\n",
      "steps = 231, loss = 3.298337459564209\n",
      "steps = 231, loss = 2.7836811542510986\n",
      "steps = 231, loss = 3.4269893169403076\n",
      "steps = 231, loss = 2.853661060333252\n",
      "steps = 231, loss = 2.0939271450042725\n",
      "steps = 231, loss = 2.2881600856781006\n",
      "steps = 231, loss = 2.7748541831970215\n",
      "steps = 231, loss = 3.4638493061065674\n",
      "steps = 231, loss = 2.719953775405884\n",
      "steps = 232, loss = 3.526616096496582\n",
      "steps = 232, loss = 3.0856502056121826\n",
      "steps = 232, loss = 3.425868272781372\n",
      "steps = 232, loss = 2.9490606784820557\n",
      "steps = 232, loss = 2.0445027351379395\n",
      "steps = 232, loss = 3.1534488201141357\n",
      "steps = 232, loss = 49.756038665771484\n",
      "steps = 232, loss = 4.7220869064331055\n",
      "steps = 232, loss = 2.0420339107513428\n",
      "steps = 232, loss = 3.5667011737823486\n",
      "steps = 232, loss = 2.8588480949401855\n",
      "steps = 232, loss = 2.854105234146118\n",
      "steps = 232, loss = 3.54302716255188\n",
      "steps = 232, loss = 49.9853630065918\n",
      "steps = 232, loss = 3.4828951358795166\n",
      "steps = 232, loss = 2.487290143966675\n",
      "steps = 232, loss = 2.7159059047698975\n",
      "steps = 232, loss = 49.90431594848633\n",
      "steps = 232, loss = 4.928584575653076\n",
      "steps = 232, loss = 3.3697776794433594\n",
      "steps = 232, loss = 3.2984893321990967\n",
      "steps = 232, loss = 5.7780375480651855\n",
      "steps = 232, loss = 2.7636449337005615\n",
      "steps = 232, loss = 4.530517101287842\n",
      "steps = 232, loss = 3.0734105110168457\n",
      "steps = 232, loss = 2.408271551132202\n",
      "steps = 232, loss = 7.0576019287109375\n",
      "steps = 232, loss = 2.8272299766540527\n",
      "steps = 232, loss = 2.784928798675537\n",
      "steps = 233, loss = 3.476814031600952\n",
      "steps = 233, loss = 4.734890460968018\n",
      "steps = 233, loss = 7.039241313934326\n",
      "steps = 233, loss = 3.7069177627563477\n",
      "steps = 233, loss = 2.731511354446411\n",
      "steps = 233, loss = 2.9523353576660156\n",
      "steps = 233, loss = 3.6071088314056396\n",
      "steps = 233, loss = 2.772667407989502\n",
      "steps = 233, loss = 3.411313056945801\n",
      "steps = 233, loss = 5.790323257446289\n",
      "steps = 233, loss = 49.9853630065918\n",
      "steps = 233, loss = 3.4366605281829834\n",
      "steps = 233, loss = 3.5633881092071533\n",
      "steps = 233, loss = 2.163508176803589\n",
      "steps = 233, loss = 49.78569412231445\n",
      "steps = 233, loss = 2.8805668354034424\n",
      "steps = 233, loss = 3.5018820762634277\n",
      "steps = 233, loss = 2.28377103805542\n",
      "steps = 233, loss = 2.8726656436920166\n",
      "steps = 233, loss = 2.6649444103240967\n",
      "steps = 233, loss = 3.042288303375244\n",
      "steps = 233, loss = 3.0377466678619385\n",
      "steps = 233, loss = 3.158475160598755\n",
      "steps = 233, loss = 4.647591590881348\n",
      "steps = 233, loss = 2.953040599822998\n",
      "steps = 233, loss = 4.939713001251221\n",
      "steps = 233, loss = 3.4426863193511963\n",
      "steps = 233, loss = 2.623396396636963\n",
      "steps = 233, loss = 2.2515740394592285\n",
      "steps = 234, loss = 3.3647632598876953\n",
      "steps = 234, loss = 3.0898468494415283\n",
      "steps = 234, loss = 3.2147951126098633\n",
      "steps = 234, loss = 3.430450677871704\n",
      "steps = 234, loss = 5.053287506103516\n",
      "steps = 234, loss = 3.4581494331359863\n",
      "steps = 234, loss = 2.7431533336639404\n",
      "steps = 234, loss = 2.8360679149627686\n",
      "steps = 234, loss = 7.10073709487915\n",
      "steps = 234, loss = 1.9783328771591187\n",
      "steps = 234, loss = 2.6248528957366943\n",
      "steps = 234, loss = 2.427628993988037\n",
      "steps = 234, loss = 3.352238416671753\n",
      "steps = 234, loss = 2.8342556953430176\n",
      "steps = 234, loss = 3.4239301681518555\n",
      "steps = 234, loss = 3.2704827785491943\n",
      "steps = 234, loss = 5.788377285003662\n",
      "steps = 234, loss = 2.721752882003784\n",
      "steps = 234, loss = 2.9417736530303955\n",
      "steps = 234, loss = 2.741647481918335\n",
      "steps = 234, loss = 2.6962904930114746\n",
      "steps = 234, loss = 50.01342010498047\n",
      "steps = 234, loss = 49.9853630065918\n",
      "steps = 234, loss = 3.147878408432007\n",
      "steps = 234, loss = 2.0719563961029053\n",
      "steps = 234, loss = 4.934453964233398\n",
      "steps = 234, loss = 2.1423423290252686\n",
      "steps = 234, loss = 3.335520029067993\n",
      "steps = 234, loss = 4.740250110626221\n",
      "steps = 235, loss = 2.616302728652954\n",
      "steps = 235, loss = 3.1617038249969482\n",
      "steps = 235, loss = 2.733161211013794\n",
      "steps = 235, loss = 5.8181023597717285\n",
      "steps = 235, loss = 4.9520368576049805\n",
      "steps = 235, loss = 2.2291362285614014\n",
      "steps = 235, loss = 2.8745548725128174\n",
      "steps = 235, loss = 2.1859185695648193\n",
      "steps = 235, loss = 4.751626014709473\n",
      "steps = 235, loss = 3.601315975189209\n",
      "steps = 235, loss = 2.952117443084717\n",
      "steps = 235, loss = 3.4057419300079346\n",
      "steps = 235, loss = 50.01517868041992\n",
      "steps = 235, loss = 2.8603408336639404\n",
      "steps = 235, loss = 7.1030497550964355\n",
      "steps = 235, loss = 3.00044584274292\n",
      "steps = 235, loss = 2.9526498317718506\n",
      "steps = 235, loss = 2.2694504261016846\n",
      "steps = 235, loss = 2.774101734161377\n",
      "steps = 235, loss = 2.9877676963806152\n",
      "steps = 235, loss = 3.3554608821868896\n",
      "steps = 235, loss = 3.4606082439422607\n",
      "steps = 235, loss = 3.4284565448760986\n",
      "steps = 235, loss = 49.9853630065918\n",
      "steps = 235, loss = 2.7233190536499023\n",
      "steps = 235, loss = 3.6314027309417725\n",
      "steps = 235, loss = 2.709937334060669\n",
      "steps = 235, loss = 3.678485631942749\n",
      "steps = 235, loss = 3.3738746643066406\n",
      "steps = 236, loss = 2.642059087753296\n",
      "steps = 236, loss = 5.8180365562438965\n",
      "steps = 236, loss = 3.463743209838867\n",
      "steps = 236, loss = 4.7580389976501465\n",
      "steps = 236, loss = 2.0484066009521484\n",
      "steps = 236, loss = 3.54476261138916\n",
      "steps = 236, loss = 3.0047800540924072\n",
      "steps = 236, loss = 2.662801742553711\n",
      "steps = 236, loss = 2.8377392292022705\n",
      "steps = 236, loss = 3.448777198791504\n",
      "steps = 236, loss = 3.3768820762634277\n",
      "steps = 236, loss = 2.7440683841705322\n",
      "steps = 236, loss = 49.9853630065918\n",
      "steps = 236, loss = 3.1520841121673584\n",
      "steps = 236, loss = 2.136367082595825\n",
      "steps = 236, loss = 4.947805881500244\n",
      "steps = 236, loss = 2.6982126235961914\n",
      "steps = 236, loss = 3.3019497394561768\n",
      "steps = 236, loss = 2.7894437313079834\n",
      "steps = 236, loss = 2.3845584392547607\n",
      "steps = 236, loss = 2.7683522701263428\n",
      "steps = 236, loss = 3.403607130050659\n",
      "steps = 236, loss = 3.4181313514709473\n",
      "steps = 236, loss = 7.16787576675415\n",
      "steps = 236, loss = 2.8698346614837646\n",
      "steps = 236, loss = 2.9423599243164062\n",
      "steps = 236, loss = 3.360771894454956\n",
      "steps = 236, loss = 50.01219177246094\n",
      "steps = 236, loss = 2.105670928955078\n",
      "steps = 237, loss = 2.766266107559204\n",
      "steps = 237, loss = 2.719210386276245\n",
      "steps = 237, loss = 7.185991287231445\n",
      "steps = 237, loss = 2.1731972694396973\n",
      "steps = 237, loss = 2.7210652828216553\n",
      "steps = 237, loss = 2.0632288455963135\n",
      "steps = 237, loss = 3.6436610221862793\n",
      "steps = 237, loss = 2.230851650238037\n",
      "steps = 237, loss = 3.575409412384033\n",
      "steps = 237, loss = 2.9488794803619385\n",
      "steps = 237, loss = 3.114821672439575\n",
      "steps = 237, loss = 3.1641993522644043\n",
      "steps = 237, loss = 2.716090440750122\n",
      "steps = 237, loss = 49.9853630065918\n",
      "steps = 237, loss = 4.757986068725586\n",
      "steps = 237, loss = 3.458899736404419\n",
      "steps = 237, loss = 2.783862352371216\n",
      "steps = 237, loss = 4.961602210998535\n",
      "steps = 237, loss = 3.35693097114563\n",
      "steps = 237, loss = 4.153334140777588\n",
      "steps = 237, loss = 5.853084564208984\n",
      "steps = 237, loss = 3.382321357727051\n",
      "steps = 237, loss = 49.906471252441406\n",
      "steps = 237, loss = 2.551088333129883\n",
      "steps = 237, loss = 2.8537070751190186\n",
      "steps = 237, loss = 3.5240767002105713\n",
      "steps = 237, loss = 2.8560140132904053\n",
      "steps = 237, loss = 2.991544485092163\n",
      "steps = 237, loss = 3.460442543029785\n",
      "steps = 238, loss = 3.433326005935669\n",
      "steps = 238, loss = 2.8066956996917725\n",
      "steps = 238, loss = 3.0224990844726562\n",
      "steps = 238, loss = 2.49188494682312\n",
      "steps = 238, loss = 3.3777875900268555\n",
      "steps = 238, loss = 3.4778709411621094\n",
      "steps = 238, loss = 7.172737121582031\n",
      "steps = 238, loss = 3.167994499206543\n",
      "steps = 238, loss = 3.4547579288482666\n",
      "steps = 238, loss = 4.969611167907715\n",
      "steps = 238, loss = 2.03831148147583\n",
      "steps = 238, loss = 2.7160885334014893\n",
      "steps = 238, loss = 2.90054988861084\n",
      "steps = 238, loss = 2.942692279815674\n",
      "steps = 238, loss = 3.536867618560791\n",
      "steps = 238, loss = 4.7619781494140625\n",
      "steps = 238, loss = 2.9483819007873535\n",
      "steps = 238, loss = 2.7263283729553223\n",
      "steps = 238, loss = 3.652538776397705\n",
      "steps = 238, loss = 49.95296096801758\n",
      "steps = 238, loss = 5.8713579177856445\n",
      "steps = 238, loss = 49.9853630065918\n",
      "steps = 238, loss = 3.1332473754882812\n",
      "steps = 238, loss = 49.93541717529297\n",
      "steps = 238, loss = 2.764313220977783\n",
      "steps = 238, loss = 2.027168035507202\n",
      "steps = 238, loss = 3.423468589782715\n",
      "steps = 238, loss = 2.854125738143921\n",
      "steps = 238, loss = 4.120748996734619\n",
      "steps = 239, loss = 49.9853630065918\n",
      "steps = 239, loss = 3.6486098766326904\n",
      "steps = 239, loss = 2.151546001434326\n",
      "steps = 239, loss = 3.071720600128174\n",
      "steps = 239, loss = 3.0496129989624023\n",
      "steps = 239, loss = 2.3561995029449463\n",
      "steps = 239, loss = 2.5924885272979736\n",
      "steps = 239, loss = 1.7593189477920532\n",
      "steps = 239, loss = 4.969156265258789\n",
      "steps = 239, loss = 3.5139236450195312\n",
      "steps = 239, loss = 2.7434167861938477\n",
      "steps = 239, loss = 4.7698540687561035\n",
      "steps = 239, loss = 2.695247173309326\n",
      "steps = 239, loss = 2.142357349395752\n",
      "steps = 239, loss = 2.579860210418701\n",
      "steps = 239, loss = 7.163388252258301\n",
      "steps = 239, loss = 3.1741392612457275\n",
      "steps = 239, loss = 3.886850595474243\n",
      "steps = 239, loss = 3.1595265865325928\n",
      "steps = 239, loss = 2.817173480987549\n",
      "steps = 239, loss = 2.939958333969116\n",
      "steps = 239, loss = 3.7386434078216553\n",
      "steps = 239, loss = 3.359070301055908\n",
      "steps = 239, loss = 3.452086925506592\n",
      "steps = 239, loss = 5.866197109222412\n",
      "steps = 239, loss = 2.8353195190429688\n",
      "steps = 239, loss = 2.8272829055786133\n",
      "steps = 239, loss = 3.4266750812530518\n",
      "steps = 239, loss = 50.001373291015625\n",
      "steps = 240, loss = 3.4736597537994385\n",
      "steps = 240, loss = 3.425086736679077\n",
      "steps = 240, loss = 5.895444869995117\n",
      "steps = 240, loss = 2.2774593830108643\n",
      "steps = 240, loss = 2.0300168991088867\n",
      "steps = 240, loss = 2.732780933380127\n",
      "steps = 240, loss = 3.4065098762512207\n",
      "steps = 240, loss = 4.653513431549072\n",
      "steps = 240, loss = 4.986364364624023\n",
      "steps = 240, loss = 2.773653745651245\n",
      "steps = 240, loss = 3.190288543701172\n",
      "steps = 240, loss = 3.670807361602783\n",
      "steps = 240, loss = 3.3555634021759033\n",
      "steps = 240, loss = 2.9351320266723633\n",
      "steps = 240, loss = 2.9581944942474365\n",
      "steps = 240, loss = 49.9853630065918\n",
      "steps = 240, loss = 3.4539577960968018\n",
      "steps = 240, loss = 2.56286883354187\n",
      "steps = 240, loss = 3.1737818717956543\n",
      "steps = 240, loss = 49.92647171020508\n",
      "steps = 240, loss = 2.6907331943511963\n",
      "steps = 240, loss = 2.951075792312622\n",
      "steps = 240, loss = 3.5386998653411865\n",
      "steps = 240, loss = 4.780325889587402\n",
      "steps = 240, loss = 2.873253107070923\n",
      "steps = 240, loss = 1.8837122917175293\n",
      "steps = 240, loss = 2.637672185897827\n",
      "steps = 240, loss = 7.243544578552246\n",
      "steps = 240, loss = 2.6025493144989014\n",
      "steps = 241, loss = 3.4796149730682373\n",
      "steps = 241, loss = 2.952716112136841\n",
      "steps = 241, loss = 3.1779630184173584\n",
      "steps = 241, loss = 4.793323516845703\n",
      "steps = 241, loss = 2.5555145740509033\n",
      "steps = 241, loss = 3.3577780723571777\n",
      "steps = 241, loss = 4.290359020233154\n",
      "steps = 241, loss = 2.7748100757598877\n",
      "steps = 241, loss = 3.445319175720215\n",
      "steps = 241, loss = 3.535318613052368\n",
      "steps = 241, loss = 3.24045991897583\n",
      "steps = 241, loss = 49.9853630065918\n",
      "steps = 241, loss = 49.9718132019043\n",
      "steps = 241, loss = 3.002074956893921\n",
      "steps = 241, loss = 2.263967514038086\n",
      "steps = 241, loss = 7.0804948806762695\n",
      "steps = 241, loss = 5.913941383361816\n",
      "steps = 241, loss = 2.871420383453369\n",
      "steps = 241, loss = 3.424670696258545\n",
      "steps = 241, loss = 3.652132749557495\n",
      "steps = 241, loss = 4.994338035583496\n",
      "steps = 241, loss = 2.7339839935302734\n",
      "steps = 241, loss = 3.4857864379882812\n",
      "steps = 241, loss = 2.6628756523132324\n",
      "steps = 241, loss = 2.0043997764587402\n",
      "steps = 241, loss = 2.973855972290039\n",
      "steps = 241, loss = 2.876136302947998\n",
      "steps = 241, loss = 3.006871461868286\n",
      "steps = 241, loss = 2.2332587242126465\n",
      "steps = 242, loss = 49.98944091796875\n",
      "steps = 242, loss = 3.3612897396087646\n",
      "steps = 242, loss = 3.672546863555908\n",
      "steps = 242, loss = 2.744774341583252\n",
      "steps = 242, loss = 1.925085425376892\n",
      "steps = 242, loss = 3.1674227714538574\n",
      "steps = 242, loss = 2.97660493850708\n",
      "steps = 242, loss = 3.098594903945923\n",
      "steps = 242, loss = 3.3764612674713135\n",
      "steps = 242, loss = 3.4648280143737793\n",
      "steps = 242, loss = 3.5074894428253174\n",
      "steps = 242, loss = 2.160902261734009\n",
      "steps = 242, loss = 7.2370123863220215\n",
      "steps = 242, loss = 2.612365961074829\n",
      "steps = 242, loss = 2.9557621479034424\n",
      "steps = 242, loss = 2.941983699798584\n",
      "steps = 242, loss = 49.9853630065918\n",
      "steps = 242, loss = 2.6908798217773438\n",
      "steps = 242, loss = 5.9130940437316895\n",
      "steps = 242, loss = 2.704378604888916\n",
      "steps = 242, loss = 2.399348735809326\n",
      "steps = 242, loss = 2.134411334991455\n",
      "steps = 242, loss = 3.192235231399536\n",
      "steps = 242, loss = 3.4275689125061035\n",
      "steps = 242, loss = 3.307647705078125\n",
      "steps = 242, loss = 2.6987032890319824\n",
      "steps = 242, loss = 4.798486709594727\n",
      "steps = 242, loss = 2.8379430770874023\n",
      "steps = 242, loss = 4.989638805389404\n",
      "steps = 243, loss = 4.797922134399414\n",
      "steps = 243, loss = 7.052586078643799\n",
      "steps = 243, loss = 2.2537219524383545\n",
      "steps = 243, loss = 3.4723668098449707\n",
      "steps = 243, loss = 2.7421135902404785\n",
      "steps = 243, loss = 3.0971004962921143\n",
      "steps = 243, loss = 2.8538222312927246\n",
      "steps = 243, loss = 2.568155288696289\n",
      "steps = 243, loss = 2.4713377952575684\n",
      "steps = 243, loss = 3.701697826385498\n",
      "steps = 243, loss = 2.7161853313446045\n",
      "steps = 243, loss = 2.8579294681549072\n",
      "steps = 243, loss = 3.4951300621032715\n",
      "steps = 243, loss = 3.6396164894104004\n",
      "steps = 243, loss = 2.140531539916992\n",
      "steps = 243, loss = 5.003013610839844\n",
      "steps = 243, loss = 3.480541706085205\n",
      "steps = 243, loss = 49.99271774291992\n",
      "steps = 243, loss = 3.3952813148498535\n",
      "steps = 243, loss = 3.453186273574829\n",
      "steps = 243, loss = 5.946779251098633\n",
      "steps = 243, loss = 49.9853630065918\n",
      "steps = 243, loss = 2.0340170860290527\n",
      "steps = 243, loss = 3.744736433029175\n",
      "steps = 243, loss = 3.1793463230133057\n",
      "steps = 243, loss = 2.9483673572540283\n",
      "steps = 243, loss = 2.7663092613220215\n",
      "steps = 243, loss = 2.986469268798828\n",
      "steps = 243, loss = 3.0155394077301025\n",
      "steps = 244, loss = 3.4584391117095947\n",
      "steps = 244, loss = 4.809357166290283\n",
      "steps = 244, loss = 3.1848628520965576\n",
      "steps = 244, loss = 3.3758981227874756\n",
      "steps = 244, loss = 4.312537670135498\n",
      "steps = 244, loss = 2.7325053215026855\n",
      "steps = 244, loss = 3.442293405532837\n",
      "steps = 244, loss = 2.1333184242248535\n",
      "steps = 244, loss = 3.12528920173645\n",
      "steps = 244, loss = 3.350964069366455\n",
      "steps = 244, loss = 3.424569606781006\n",
      "steps = 244, loss = 2.873932361602783\n",
      "steps = 244, loss = 2.2542686462402344\n",
      "steps = 244, loss = 2.8653130531311035\n",
      "steps = 244, loss = 2.742907762527466\n",
      "steps = 244, loss = 3.7080578804016113\n",
      "steps = 244, loss = 3.363743782043457\n",
      "steps = 244, loss = 2.7736761569976807\n",
      "steps = 244, loss = 49.9853630065918\n",
      "steps = 244, loss = 3.021772861480713\n",
      "steps = 244, loss = 50.02025604248047\n",
      "steps = 244, loss = 7.129451751708984\n",
      "steps = 244, loss = 2.274712562561035\n",
      "steps = 244, loss = 5.014516353607178\n",
      "steps = 244, loss = 2.581848382949829\n",
      "steps = 244, loss = 2.609463691711426\n",
      "steps = 244, loss = 2.5913002490997314\n",
      "steps = 244, loss = 5.95996618270874\n",
      "steps = 244, loss = 2.9516758918762207\n",
      "steps = 245, loss = 3.7174675464630127\n",
      "steps = 245, loss = 1.9871594905853271\n",
      "steps = 245, loss = 3.174489974975586\n",
      "steps = 245, loss = 2.744640350341797\n",
      "steps = 245, loss = 2.467158079147339\n",
      "steps = 245, loss = 7.132526874542236\n",
      "steps = 245, loss = 2.140977621078491\n",
      "steps = 245, loss = 2.95937442779541\n",
      "steps = 245, loss = 2.9412286281585693\n",
      "steps = 245, loss = 2.8374478816986084\n",
      "steps = 245, loss = 5.959630012512207\n",
      "steps = 245, loss = 3.483271360397339\n",
      "steps = 245, loss = 3.876800298690796\n",
      "steps = 245, loss = 3.655087471008301\n",
      "steps = 245, loss = 3.4406778812408447\n",
      "steps = 245, loss = 2.9916999340057373\n",
      "steps = 245, loss = 2.8751137256622314\n",
      "steps = 245, loss = 2.5804443359375\n",
      "steps = 245, loss = 3.266714334487915\n",
      "steps = 245, loss = 3.1252970695495605\n",
      "steps = 245, loss = 2.7330737113952637\n",
      "steps = 245, loss = 3.2950522899627686\n",
      "steps = 245, loss = 2.6981277465820312\n",
      "steps = 245, loss = 49.9853630065918\n",
      "steps = 245, loss = 4.816452980041504\n",
      "steps = 245, loss = 5.009957790374756\n",
      "steps = 245, loss = 2.644099473953247\n",
      "steps = 245, loss = 49.99187469482422\n",
      "steps = 245, loss = 2.195674419403076\n",
      "steps = 246, loss = 2.976029396057129\n",
      "steps = 246, loss = 3.7013232707977295\n",
      "steps = 246, loss = 2.1912729740142822\n",
      "steps = 246, loss = 5.023684978485107\n",
      "steps = 246, loss = 3.606126546859741\n",
      "steps = 246, loss = 2.662666082382202\n",
      "steps = 246, loss = 3.4095075130462646\n",
      "steps = 246, loss = 2.716205358505249\n",
      "steps = 246, loss = 49.9853630065918\n",
      "steps = 246, loss = 3.2901196479797363\n",
      "steps = 246, loss = 2.073212146759033\n",
      "steps = 246, loss = 2.766317844390869\n",
      "steps = 246, loss = 4.815727233886719\n",
      "steps = 246, loss = 2.947798728942871\n",
      "steps = 246, loss = 3.5250988006591797\n",
      "steps = 246, loss = 2.7260966300964355\n",
      "steps = 246, loss = 3.1862316131591797\n",
      "steps = 246, loss = 2.8801472187042236\n",
      "steps = 246, loss = 2.5935332775115967\n",
      "steps = 246, loss = 2.2957494258880615\n",
      "steps = 246, loss = 2.760160446166992\n",
      "steps = 246, loss = 3.4841792583465576\n",
      "steps = 246, loss = 49.81285858154297\n",
      "steps = 246, loss = 5.99355411529541\n",
      "steps = 246, loss = 2.853828191757202\n",
      "steps = 246, loss = 7.186877250671387\n",
      "steps = 246, loss = 3.271864175796509\n",
      "steps = 246, loss = 3.942408800125122\n",
      "steps = 246, loss = 2.465120792388916\n",
      "steps = 247, loss = 5.023167133331299\n",
      "steps = 247, loss = 2.5868780612945557\n",
      "steps = 247, loss = 2.9396681785583496\n",
      "steps = 247, loss = 2.7234385013580322\n",
      "steps = 247, loss = 2.696845769882202\n",
      "steps = 247, loss = 2.8365824222564697\n",
      "steps = 247, loss = 3.648224115371704\n",
      "steps = 247, loss = 49.9853630065918\n",
      "steps = 247, loss = 50.003963470458984\n",
      "steps = 247, loss = 2.0322723388671875\n",
      "steps = 247, loss = 3.1782026290893555\n",
      "steps = 247, loss = 3.0311310291290283\n",
      "steps = 247, loss = 3.991323709487915\n",
      "steps = 247, loss = 3.3491694927215576\n",
      "steps = 247, loss = 2.5357613563537598\n",
      "steps = 247, loss = 7.09326696395874\n",
      "steps = 247, loss = 4.824237823486328\n",
      "steps = 247, loss = 2.034410238265991\n",
      "steps = 247, loss = 2.1413159370422363\n",
      "steps = 247, loss = 3.3385298252105713\n",
      "steps = 247, loss = 3.623868942260742\n",
      "steps = 247, loss = 3.3947649002075195\n",
      "steps = 247, loss = 2.745060443878174\n",
      "steps = 247, loss = 5.989027976989746\n",
      "steps = 247, loss = 2.346081018447876\n",
      "steps = 247, loss = 3.385983943939209\n",
      "steps = 247, loss = 3.5004220008850098\n",
      "steps = 247, loss = 2.8920724391937256\n",
      "steps = 247, loss = 3.3977627754211426\n",
      "steps = 248, loss = 2.301382303237915\n",
      "steps = 248, loss = 2.853867530822754\n",
      "steps = 248, loss = 49.9853630065918\n",
      "steps = 248, loss = 4.8252363204956055\n",
      "steps = 248, loss = 2.808518171310425\n",
      "steps = 248, loss = 3.4554619789123535\n",
      "steps = 248, loss = 49.64708709716797\n",
      "steps = 248, loss = 5.036644458770752\n",
      "steps = 248, loss = 3.9079325199127197\n",
      "steps = 248, loss = 3.461758613586426\n",
      "steps = 248, loss = 3.459725856781006\n",
      "steps = 248, loss = 2.232536792755127\n",
      "steps = 248, loss = 3.383408546447754\n",
      "steps = 248, loss = 6.023688316345215\n",
      "steps = 248, loss = 3.190593719482422\n",
      "steps = 248, loss = 3.1940014362335205\n",
      "steps = 248, loss = 3.1252212524414062\n",
      "steps = 248, loss = 2.119818925857544\n",
      "steps = 248, loss = 3.370029926300049\n",
      "steps = 248, loss = 3.3042795658111572\n",
      "steps = 248, loss = 2.716623306274414\n",
      "steps = 248, loss = 2.723170518875122\n",
      "steps = 248, loss = 7.151271343231201\n",
      "steps = 248, loss = 3.668138265609741\n",
      "steps = 248, loss = 3.563028335571289\n",
      "steps = 248, loss = 2.826742649078369\n",
      "steps = 248, loss = 2.9467318058013916\n",
      "steps = 248, loss = 2.7657558917999268\n",
      "steps = 248, loss = 2.4638235569000244\n",
      "steps = 249, loss = 3.6392245292663574\n",
      "steps = 249, loss = 2.7734413146972656\n",
      "steps = 249, loss = 2.9783103466033936\n",
      "steps = 249, loss = 3.4147136211395264\n",
      "steps = 249, loss = 3.4440627098083496\n",
      "steps = 249, loss = 3.19588565826416\n",
      "steps = 249, loss = 3.2051002979278564\n",
      "steps = 249, loss = 2.742124319076538\n",
      "steps = 249, loss = 3.4803009033203125\n",
      "steps = 249, loss = 6.036554336547852\n",
      "steps = 249, loss = 2.8451743125915527\n",
      "steps = 249, loss = 2.118544816970825\n",
      "steps = 249, loss = 4.83499813079834\n",
      "steps = 249, loss = 2.9501724243164062\n",
      "steps = 249, loss = 2.9326016902923584\n",
      "steps = 249, loss = 5.047738552093506\n",
      "steps = 249, loss = 7.127775192260742\n",
      "steps = 249, loss = 2.872972249984741\n",
      "steps = 249, loss = 2.282632350921631\n",
      "steps = 249, loss = 2.9966771602630615\n",
      "steps = 249, loss = 2.07971453666687\n",
      "steps = 249, loss = 50.016265869140625\n",
      "steps = 249, loss = 3.470264434814453\n",
      "steps = 249, loss = 3.383998394012451\n",
      "steps = 249, loss = 49.9853630065918\n",
      "steps = 249, loss = 19.0013427734375\n",
      "steps = 249, loss = 2.6470489501953125\n",
      "steps = 249, loss = 3.625864028930664\n",
      "steps = 249, loss = 2.7324416637420654\n",
      "steps = 250, loss = 3.515547513961792\n",
      "steps = 250, loss = 3.412144422531128\n",
      "steps = 250, loss = 5.054676532745361\n",
      "steps = 250, loss = 2.2611031532287598\n",
      "steps = 250, loss = 2.7748327255249023\n",
      "steps = 250, loss = 2.9515957832336426\n",
      "steps = 250, loss = 3.431766986846924\n",
      "steps = 250, loss = 3.396832227706909\n",
      "steps = 250, loss = 2.733882427215576\n",
      "steps = 250, loss = 3.483509063720703\n",
      "steps = 250, loss = 49.98381805419922\n",
      "steps = 250, loss = 3.327643632888794\n",
      "steps = 250, loss = 7.185635566711426\n",
      "steps = 250, loss = 2.114349365234375\n",
      "steps = 250, loss = 49.9853630065918\n",
      "steps = 250, loss = 3.3943779468536377\n",
      "steps = 250, loss = 2.132260322570801\n",
      "steps = 250, loss = 3.228006601333618\n",
      "steps = 250, loss = 2.9259958267211914\n",
      "steps = 250, loss = 6.05277681350708\n",
      "steps = 250, loss = 2.876098394393921\n",
      "steps = 250, loss = 2.701350688934326\n",
      "steps = 250, loss = 2.9880270957946777\n",
      "steps = 250, loss = 3.1995444297790527\n",
      "steps = 250, loss = 3.479816198348999\n",
      "steps = 250, loss = 2.8705673217773438\n",
      "steps = 250, loss = 4.846316814422607\n",
      "steps = 250, loss = 2.636516809463501\n",
      "steps = 250, loss = 2.5352487564086914\n",
      "CPU times: user 1h 32min 35s, sys: 11min 9s, total: 1h 43min 44s\n",
      "Wall time: 9h 7min 49s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IncrementalSearchCV(decay_rate=0,\n",
       "                    estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                    max_iter=250, n_initial_parameters=29,\n",
       "                    parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                                &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;,\n",
       "                                                       &#x27;ELU&#x27;, &#x27;PReLU&#x27;],\n",
       "                                &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                                 &#x27;xavier_normal_&#x27;,\n",
       "                                                 &#x27;kaiming_uniform_&#x27;,\n",
       "                                                 &#x27;kaiming_norm...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                                &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                                &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, ...],\n",
       "                                &#x27;train_split&#x27;: [None]},\n",
       "                    random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;IncrementalSearchCV<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>IncrementalSearchCV(decay_rate=0,\n",
       "                    estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                    max_iter=250, n_initial_parameters=29,\n",
       "                    parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                                &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;,\n",
       "                                                       &#x27;ELU&#x27;, &#x27;PReLU&#x27;],\n",
       "                                &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                                 &#x27;xavier_normal_&#x27;,\n",
       "                                                 &#x27;kaiming_uniform_&#x27;,\n",
       "                                                 &#x27;kaiming_norm...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                                &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                                &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, ...],\n",
       "                                &#x27;train_split&#x27;: [None]},\n",
       "                    random_state=42)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "IncrementalSearchCV(decay_rate=0,\n",
       "                    estimator=<class '__main__.TrimParams'>[uninitialized](\n",
       "  module=<class 'autoencoder.Autoencoder'>,\n",
       "),\n",
       "                    max_iter=250, n_initial_parameters=29,\n",
       "                    parameters={'batch_size': [32, 64, 128, 256, 512],\n",
       "                                'module__activation': ['ReLU', 'LeakyReLU',\n",
       "                                                       'ELU', 'PReLU'],\n",
       "                                'module__init': ['xavier_uniform_',\n",
       "                                                 'xavier_normal_',\n",
       "                                                 'kaiming_uniform_',\n",
       "                                                 'kaiming_norm...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                                'optimizer__nesterov': [True],\n",
       "                                'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, ...],\n",
       "                                'train_split': [None]},\n",
       "                    random_state=42)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "passive_search.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_search(passive_search, today, \"passive\", X_test.compute(), y_test.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing_stats = client.profile()\n",
    "# with open(f\"{absolutepath_to_results}/final-final-timings.json\", \"w\") as f:\n",
    "#     json.dump(timing_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* patience: `max_iter // 8` (10 epochs)\n",
    "* n_initial: `2 * num_models`\n",
    "\n",
    "This requires choosing\n",
    "\n",
    "* the explore/exploit tradeoff (`patience` vs `n_initial`)\n",
    "* some estimate on many models will take advantage of `patience` to get total number of partial fit calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import IncrementalSearchCV\n",
    "\n",
    "patience_search = IncrementalSearchCV(\n",
    "    model,\n",
    "    params,\n",
    "    decay_rate=0,\n",
    "    patience=max_iter // 10,\n",
    "    n_initial_parameters=2 * num_models,\n",
    "    max_iter=num_calls,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps = 1, loss = 0.7015392184257507\n",
      "steps = 1, loss = 2.0390679836273193\n",
      "steps = 1, loss = 1.9451401233673096\n",
      "steps = 1, loss = 1.9790714979171753\n",
      "steps = 1, loss = 49.89901351928711\n",
      "steps = 1, loss = 2.2601351737976074\n",
      "steps = 1, loss = 1.9300516843795776\n",
      "steps = 1, loss = 3.0525965690612793\n",
      "steps = 1, loss = 1.9642322063446045\n",
      "steps = 1, loss = 2.8931949138641357\n",
      "steps = 1, loss = 3.145359516143799\n",
      "steps = 1, loss = 1.9237724542617798\n",
      "steps = 1, loss = 2.664077043533325\n",
      "steps = 1, loss = 0.7161751985549927\n",
      "steps = 1, loss = 2.6207008361816406\n",
      "steps = 1, loss = 1.9601235389709473\n",
      "steps = 1, loss = 0.7597142457962036\n",
      "steps = 1, loss = 2.4687323570251465\n",
      "steps = 1, loss = 1.9262726306915283\n",
      "steps = 1, loss = 2.2864997386932373\n",
      "steps = 1, loss = 1.119533896446228\n",
      "steps = 1, loss = 2.8370532989501953\n",
      "steps = 1, loss = 2.231611967086792\n",
      "steps = 1, loss = 2.931516647338867\n",
      "steps = 1, loss = 1.5063927173614502\n",
      "steps = 1, loss = 3.0882468223571777\n",
      "steps = 1, loss = 2.370192050933838\n",
      "steps = 1, loss = 2.6160356998443604\n",
      "steps = 1, loss = 1.4761656522750854\n",
      "steps = 1, loss = 1.4569000005722046\n",
      "steps = 1, loss = 1.9884908199310303\n",
      "steps = 1, loss = 2.465902805328369\n",
      "steps = 1, loss = 2.4441633224487305\n",
      "steps = 1, loss = 1.7918369770050049\n",
      "steps = 1, loss = 2.282869577407837\n",
      "steps = 1, loss = 50.02168655395508\n",
      "steps = 1, loss = 2.0435872077941895\n",
      "steps = 1, loss = 50.02360534667969\n",
      "steps = 1, loss = 0.8441962003707886\n",
      "steps = 1, loss = 49.948604583740234\n",
      "steps = 1, loss = 1.8131247758865356\n",
      "steps = 1, loss = 2.700913429260254\n",
      "steps = 1, loss = 1.5371888875961304\n",
      "steps = 1, loss = 3.062363624572754\n",
      "steps = 1, loss = 1.9901022911071777\n",
      "steps = 1, loss = 2.2272908687591553\n",
      "steps = 1, loss = 2.1988604068756104\n",
      "steps = 1, loss = 1.8518195152282715\n",
      "steps = 1, loss = 50.02899169921875\n",
      "steps = 1, loss = 2.0213916301727295\n",
      "steps = 1, loss = 2.0055603981018066\n",
      "steps = 1, loss = 1.891239047050476\n",
      "steps = 1, loss = 2.2822697162628174\n",
      "steps = 1, loss = 1.9285523891448975\n",
      "steps = 1, loss = 1.9646786451339722\n",
      "steps = 1, loss = 2.231853485107422\n",
      "steps = 1, loss = 1.0851256847381592\n",
      "steps = 1, loss = 3.1120615005493164\n",
      "steps = 2, loss = 2.0649361610412598\n",
      "steps = 2, loss = 49.286922454833984\n",
      "steps = 2, loss = 2.219710111618042\n",
      "steps = 2, loss = 2.404712438583374\n",
      "steps = 2, loss = 2.9821219444274902\n",
      "steps = 2, loss = 2.936922550201416\n",
      "steps = 2, loss = 2.0517542362213135\n",
      "steps = 2, loss = 1.35455322265625\n",
      "steps = 2, loss = 2.4960482120513916\n",
      "steps = 2, loss = 2.1214840412139893\n",
      "steps = 2, loss = 0.7862914800643921\n",
      "steps = 2, loss = 5.033481597900391\n",
      "steps = 2, loss = 1.2296638488769531\n",
      "steps = 2, loss = 3.269145965576172\n",
      "steps = 2, loss = 2.108257293701172\n",
      "steps = 2, loss = 2.720346689224243\n",
      "steps = 2, loss = 3.1571202278137207\n",
      "steps = 2, loss = 2.9980525970458984\n",
      "steps = 2, loss = 2.1790432929992676\n",
      "steps = 2, loss = 2.750941753387451\n",
      "steps = 2, loss = 2.4493439197540283\n",
      "steps = 2, loss = 2.6125919818878174\n",
      "steps = 2, loss = 2.8706159591674805\n",
      "steps = 2, loss = 1.8386337757110596\n",
      "steps = 2, loss = 2.044407606124878\n",
      "steps = 2, loss = 1.5621674060821533\n",
      "steps = 2, loss = 2.526874303817749\n",
      "steps = 2, loss = 5.222795009613037\n",
      "steps = 2, loss = 2.819878101348877\n",
      "steps = 2, loss = 2.3319509029388428\n",
      "steps = 2, loss = 2.7237470149993896\n",
      "steps = 2, loss = 1.9905595779418945\n",
      "steps = 2, loss = 2.0769336223602295\n",
      "steps = 2, loss = 2.275181293487549\n",
      "steps = 2, loss = 2.792921304702759\n",
      "steps = 2, loss = 49.8498420715332\n",
      "steps = 2, loss = 1.969590187072754\n",
      "steps = 2, loss = 2.954298973083496\n",
      "steps = 2, loss = 1.8490006923675537\n",
      "steps = 2, loss = 2.574220657348633\n",
      "steps = 2, loss = 49.973636627197266\n",
      "steps = 2, loss = 4.053158760070801\n",
      "steps = 2, loss = 2.5500240325927734\n",
      "steps = 2, loss = 1.4387238025665283\n",
      "steps = 2, loss = 50.00247573852539\n",
      "steps = 2, loss = 2.066012382507324\n",
      "steps = 2, loss = 2.5463247299194336\n",
      "steps = 2, loss = 2.250905752182007\n",
      "steps = 2, loss = 2.6103408336639404\n",
      "steps = 2, loss = 2.668344259262085\n",
      "steps = 2, loss = 2.225985527038574\n",
      "steps = 2, loss = 50.02617263793945\n",
      "steps = 2, loss = 2.3223307132720947\n",
      "steps = 2, loss = 3.135007619857788\n",
      "steps = 2, loss = 2.093651294708252\n",
      "steps = 2, loss = 2.13775897026062\n",
      "steps = 2, loss = 2.167015790939331\n",
      "steps = 2, loss = 1.646716594696045\n",
      "steps = 3, loss = 2.8757710456848145\n",
      "steps = 3, loss = 1.7045027017593384\n",
      "steps = 3, loss = 2.6059372425079346\n",
      "steps = 3, loss = 1.7936562299728394\n",
      "steps = 3, loss = 2.518653154373169\n",
      "steps = 3, loss = 50.005252838134766\n",
      "steps = 3, loss = 49.9119873046875\n",
      "steps = 3, loss = 49.922935485839844\n",
      "steps = 3, loss = 50.027320861816406\n",
      "steps = 3, loss = 2.4380695819854736\n",
      "steps = 3, loss = 2.698162078857422\n",
      "steps = 3, loss = 2.3813552856445312\n",
      "steps = 3, loss = 2.4385786056518555\n",
      "steps = 3, loss = 2.336625814437866\n",
      "steps = 3, loss = 2.41652250289917\n",
      "steps = 3, loss = 2.166430711746216\n",
      "steps = 3, loss = 3.664792776107788\n",
      "steps = 3, loss = 1.9824475049972534\n",
      "steps = 3, loss = 2.4180397987365723\n",
      "steps = 3, loss = 1.7213953733444214\n",
      "steps = 3, loss = 2.61824107170105\n",
      "steps = 3, loss = 2.074545383453369\n",
      "steps = 3, loss = 3.378053665161133\n",
      "steps = 3, loss = 1.6481565237045288\n",
      "steps = 3, loss = 3.1845643520355225\n",
      "steps = 3, loss = 1.5599877834320068\n",
      "steps = 3, loss = 4.595548152923584\n",
      "steps = 3, loss = 2.7938125133514404\n",
      "steps = 3, loss = 3.150792121887207\n",
      "steps = 3, loss = 2.8628132343292236\n",
      "steps = 3, loss = 2.0072033405303955\n",
      "steps = 3, loss = 2.2573397159576416\n",
      "steps = 3, loss = 2.2523958683013916\n",
      "steps = 3, loss = 3.7210099697113037\n",
      "steps = 3, loss = 2.7568612098693848\n",
      "steps = 3, loss = 50.00235366821289\n",
      "steps = 3, loss = 1.0675112009048462\n",
      "steps = 3, loss = 2.1945977210998535\n",
      "steps = 3, loss = 1.902837872505188\n",
      "steps = 3, loss = 1.9859004020690918\n",
      "steps = 3, loss = 2.085496425628662\n",
      "steps = 3, loss = 2.068298816680908\n",
      "steps = 3, loss = 2.731066942214966\n",
      "steps = 3, loss = 2.638345956802368\n",
      "steps = 3, loss = 2.5932514667510986\n",
      "steps = 3, loss = 1.853012204170227\n",
      "steps = 3, loss = 2.966437578201294\n",
      "steps = 3, loss = 2.3415236473083496\n",
      "steps = 3, loss = 5.248587608337402\n",
      "steps = 3, loss = 3.0341780185699463\n",
      "steps = 3, loss = 2.4184203147888184\n",
      "steps = 3, loss = 1.9398105144500732\n",
      "steps = 3, loss = 2.172227144241333\n",
      "steps = 3, loss = 4.821182727813721\n",
      "steps = 3, loss = 2.528197765350342\n",
      "steps = 3, loss = 2.9078469276428223\n",
      "steps = 3, loss = 3.2020678520202637\n",
      "steps = 3, loss = 2.9952328205108643\n",
      "steps = 4, loss = 2.869763135910034\n",
      "steps = 4, loss = 2.5849149227142334\n",
      "steps = 4, loss = 2.9004716873168945\n",
      "steps = 4, loss = 5.2969889640808105\n",
      "steps = 4, loss = 2.553907632827759\n",
      "steps = 4, loss = 2.09587025642395\n",
      "steps = 4, loss = 2.284367799758911\n",
      "steps = 4, loss = 1.3774644136428833\n",
      "steps = 4, loss = 50.02705383300781\n",
      "steps = 4, loss = 2.4176688194274902\n",
      "steps = 4, loss = 50.00544357299805\n",
      "steps = 4, loss = 1.7633347511291504\n",
      "steps = 4, loss = 6.606234073638916\n",
      "steps = 4, loss = 2.34745717048645\n",
      "steps = 4, loss = 1.9613579511642456\n",
      "steps = 4, loss = 2.933858871459961\n",
      "steps = 4, loss = 2.4137537479400635\n",
      "steps = 4, loss = 2.7474024295806885\n",
      "steps = 4, loss = 2.2892706394195557\n",
      "steps = 4, loss = 2.272324800491333\n",
      "steps = 4, loss = 3.420466899871826\n",
      "steps = 4, loss = 2.697070360183716\n",
      "steps = 4, loss = 2.0503737926483154\n",
      "steps = 4, loss = 3.2813704013824463\n",
      "steps = 4, loss = 3.1778223514556885\n",
      "steps = 4, loss = 2.5223705768585205\n",
      "steps = 4, loss = 3.8051984310150146\n",
      "steps = 4, loss = 2.728101968765259\n",
      "steps = 4, loss = 1.8871148824691772\n",
      "steps = 4, loss = 3.267754554748535\n",
      "steps = 4, loss = 2.586529493331909\n",
      "steps = 4, loss = 2.7010061740875244\n",
      "steps = 4, loss = 2.5015203952789307\n",
      "steps = 4, loss = 2.8697495460510254\n",
      "steps = 4, loss = 49.99324417114258\n",
      "steps = 4, loss = 2.771122694015503\n",
      "steps = 4, loss = 3.7421133518218994\n",
      "steps = 4, loss = 50.00089645385742\n",
      "steps = 4, loss = 1.8178670406341553\n",
      "steps = 4, loss = 2.922982692718506\n",
      "steps = 4, loss = 1.7109428644180298\n",
      "steps = 4, loss = 2.60359525680542\n",
      "steps = 4, loss = 1.837752103805542\n",
      "steps = 4, loss = 2.6252503395080566\n",
      "steps = 4, loss = 2.8182249069213867\n",
      "steps = 4, loss = 2.2718122005462646\n",
      "steps = 4, loss = 2.3392269611358643\n",
      "steps = 4, loss = 3.164457082748413\n",
      "steps = 4, loss = 3.0908029079437256\n",
      "steps = 4, loss = 2.9083006381988525\n",
      "steps = 4, loss = 2.636613368988037\n",
      "steps = 4, loss = 2.3147690296173096\n",
      "steps = 4, loss = 2.0846071243286133\n",
      "steps = 4, loss = 50.01469421386719\n",
      "steps = 4, loss = 2.36037278175354\n",
      "steps = 4, loss = 2.008852243423462\n",
      "steps = 4, loss = 4.9609694480896\n",
      "steps = 4, loss = 2.180630683898926\n",
      "steps = 5, loss = 2.507192850112915\n",
      "steps = 5, loss = 2.136157989501953\n",
      "steps = 5, loss = 3.1107330322265625\n",
      "steps = 5, loss = 2.7396223545074463\n",
      "steps = 5, loss = 2.2598114013671875\n",
      "steps = 5, loss = 2.7305428981781006\n",
      "steps = 5, loss = 2.6856870651245117\n",
      "steps = 5, loss = 2.5234804153442383\n",
      "steps = 5, loss = 2.8630707263946533\n",
      "steps = 5, loss = 2.900348424911499\n",
      "steps = 5, loss = 1.9341168403625488\n",
      "steps = 5, loss = 2.6594841480255127\n",
      "steps = 5, loss = 2.0741024017333984\n",
      "steps = 5, loss = 2.7174839973449707\n",
      "steps = 5, loss = 1.5417507886886597\n",
      "steps = 5, loss = 2.5684804916381836\n",
      "steps = 5, loss = 2.8630623817443848\n",
      "steps = 5, loss = 2.4405791759490967\n",
      "steps = 5, loss = 2.668637990951538\n",
      "steps = 5, loss = 2.4874696731567383\n",
      "steps = 5, loss = 50.02632522583008\n",
      "steps = 5, loss = 2.5645663738250732\n",
      "steps = 5, loss = 1.875414490699768\n",
      "steps = 5, loss = 1.9661167860031128\n",
      "steps = 5, loss = 2.9050989151000977\n",
      "steps = 5, loss = 49.96736145019531\n",
      "steps = 5, loss = 50.004722595214844\n",
      "steps = 5, loss = 2.652508497238159\n",
      "steps = 5, loss = 49.777408599853516\n",
      "steps = 5, loss = 2.3283488750457764\n",
      "steps = 5, loss = 6.175302982330322\n",
      "steps = 5, loss = 4.228654861450195\n",
      "steps = 5, loss = 2.9097650051116943\n",
      "steps = 5, loss = 2.9138495922088623\n",
      "steps = 5, loss = 5.950742244720459\n",
      "steps = 5, loss = 3.2261462211608887\n",
      "steps = 5, loss = 1.9064459800720215\n",
      "steps = 5, loss = 2.0518686771392822\n",
      "steps = 5, loss = 2.8656198978424072\n",
      "steps = 5, loss = 2.0766477584838867\n",
      "steps = 5, loss = 3.423321485519409\n",
      "steps = 5, loss = 1.8701012134552002\n",
      "steps = 5, loss = 2.379734516143799\n",
      "steps = 5, loss = 3.5413577556610107\n",
      "steps = 5, loss = 2.6380293369293213\n",
      "steps = 5, loss = 3.202521562576294\n",
      "steps = 5, loss = 3.284722089767456\n",
      "steps = 5, loss = 2.214536190032959\n",
      "steps = 5, loss = 5.223073959350586\n",
      "steps = 5, loss = 2.092392683029175\n",
      "steps = 5, loss = 2.197572708129883\n",
      "steps = 5, loss = 3.796440601348877\n",
      "steps = 5, loss = 1.8261762857437134\n",
      "steps = 5, loss = 2.368847608566284\n",
      "steps = 5, loss = 1.7875397205352783\n",
      "steps = 5, loss = 2.8760852813720703\n",
      "steps = 5, loss = 2.909943103790283\n",
      "steps = 5, loss = 2.818068265914917\n",
      "steps = 6, loss = 3.1927297115325928\n",
      "steps = 6, loss = 4.76845121383667\n",
      "steps = 6, loss = 2.0739760398864746\n",
      "steps = 6, loss = 2.7167561054229736\n",
      "steps = 6, loss = 2.7454211711883545\n",
      "steps = 6, loss = 2.9254348278045654\n",
      "steps = 6, loss = 50.01953887939453\n",
      "steps = 6, loss = 2.3247945308685303\n",
      "steps = 6, loss = 2.8850719928741455\n",
      "steps = 6, loss = 2.6140666007995605\n",
      "steps = 6, loss = 7.43291711807251\n",
      "steps = 6, loss = 2.8265185356140137\n",
      "steps = 6, loss = 1.9434036016464233\n",
      "steps = 6, loss = 1.8626712560653687\n",
      "steps = 6, loss = 3.070068359375\n",
      "steps = 6, loss = 5.830057144165039\n",
      "steps = 6, loss = 2.935776948928833\n",
      "steps = 6, loss = 2.354790449142456\n",
      "steps = 6, loss = 1.6322509050369263\n",
      "steps = 6, loss = 50.00014114379883\n",
      "steps = 6, loss = 2.5805628299713135\n",
      "steps = 6, loss = 3.9815497398376465\n",
      "steps = 6, loss = 1.9393035173416138\n",
      "steps = 6, loss = 2.7255663871765137\n",
      "steps = 6, loss = 2.7996411323547363\n",
      "steps = 6, loss = 3.516786813735962\n",
      "steps = 6, loss = 1.959154486656189\n",
      "steps = 6, loss = 3.146223306655884\n",
      "steps = 6, loss = 2.20033860206604\n",
      "steps = 6, loss = 2.298417806625366\n",
      "steps = 6, loss = 2.5187020301818848\n",
      "steps = 6, loss = 1.9093722105026245\n",
      "steps = 6, loss = 3.3311328887939453\n",
      "steps = 6, loss = 2.322777271270752\n",
      "steps = 6, loss = 2.810650587081909\n",
      "steps = 6, loss = 2.8044369220733643\n",
      "steps = 6, loss = 2.4400150775909424\n",
      "steps = 6, loss = 2.0935351848602295\n",
      "steps = 6, loss = 2.628380298614502\n",
      "steps = 6, loss = 2.864506483078003\n",
      "steps = 6, loss = 2.6544721126556396\n",
      "steps = 6, loss = 2.2931809425354004\n",
      "steps = 6, loss = 2.559171676635742\n",
      "steps = 6, loss = 2.921271324157715\n",
      "steps = 6, loss = 3.0713515281677246\n",
      "steps = 6, loss = 3.1256301403045654\n",
      "steps = 6, loss = 50.004058837890625\n",
      "steps = 6, loss = 1.8311915397644043\n",
      "steps = 6, loss = 2.7619447708129883\n",
      "steps = 6, loss = 2.9662375450134277\n",
      "steps = 6, loss = 1.9604063034057617\n",
      "steps = 6, loss = 2.1009979248046875\n",
      "steps = 6, loss = 3.919163465499878\n",
      "steps = 6, loss = 2.5819852352142334\n",
      "steps = 6, loss = 2.5720412731170654\n",
      "steps = 6, loss = 1.6535793542861938\n",
      "steps = 6, loss = 2.086374282836914\n",
      "steps = 6, loss = 49.951011657714844\n",
      "steps = 7, loss = 3.2675862312316895\n",
      "steps = 7, loss = 2.367860794067383\n",
      "steps = 7, loss = 3.0102570056915283\n",
      "steps = 7, loss = 1.945683479309082\n",
      "steps = 7, loss = 2.5355284214019775\n",
      "steps = 7, loss = 2.4694983959198\n",
      "steps = 7, loss = 2.9017679691314697\n",
      "steps = 7, loss = 2.085435628890991\n",
      "steps = 7, loss = 1.9722485542297363\n",
      "steps = 7, loss = 2.098924398422241\n",
      "steps = 7, loss = 49.978981018066406\n",
      "steps = 7, loss = 2.983721971511841\n",
      "steps = 7, loss = 3.9497201442718506\n",
      "steps = 7, loss = 3.252452850341797\n",
      "steps = 7, loss = 2.6030681133270264\n",
      "steps = 7, loss = 2.734426259994507\n",
      "steps = 7, loss = 2.770885944366455\n",
      "steps = 7, loss = 3.5864787101745605\n",
      "steps = 7, loss = 1.8988540172576904\n",
      "steps = 7, loss = 1.8724186420440674\n",
      "steps = 7, loss = 2.1079177856445312\n",
      "steps = 7, loss = 8.074295043945312\n",
      "steps = 7, loss = 3.39957857131958\n",
      "steps = 7, loss = 4.63505744934082\n",
      "steps = 7, loss = 2.094320058822632\n",
      "steps = 7, loss = 2.996690511703491\n",
      "steps = 7, loss = 2.9767231941223145\n",
      "steps = 7, loss = 2.8346219062805176\n",
      "steps = 7, loss = 2.971647262573242\n",
      "steps = 7, loss = 3.020712375640869\n",
      "steps = 7, loss = 1.9977818727493286\n",
      "steps = 7, loss = 2.6060516834259033\n",
      "steps = 7, loss = 2.706450939178467\n",
      "steps = 7, loss = 3.033792018890381\n",
      "steps = 7, loss = 3.029603958129883\n",
      "steps = 7, loss = 2.3815033435821533\n",
      "steps = 7, loss = 1.6992332935333252\n",
      "steps = 7, loss = 2.0324411392211914\n",
      "steps = 7, loss = 2.9352145195007324\n",
      "steps = 7, loss = 50.005027770996094\n",
      "steps = 7, loss = 2.117724895477295\n",
      "steps = 7, loss = 2.7155771255493164\n",
      "steps = 7, loss = 2.396942377090454\n",
      "steps = 7, loss = 2.1252963542938232\n",
      "steps = 7, loss = 2.552610158920288\n",
      "steps = 7, loss = 2.481874704360962\n",
      "steps = 7, loss = 2.6567630767822266\n",
      "steps = 7, loss = 2.572456121444702\n",
      "steps = 7, loss = 50.002708435058594\n",
      "steps = 7, loss = 2.8589744567871094\n",
      "steps = 7, loss = 49.89972686767578\n",
      "steps = 7, loss = 2.4337973594665527\n",
      "steps = 7, loss = 2.887641668319702\n",
      "steps = 7, loss = 1.977953553199768\n",
      "steps = 7, loss = 6.129579067230225\n",
      "steps = 7, loss = 2.7977283000946045\n",
      "steps = 7, loss = 3.465596914291382\n",
      "steps = 7, loss = 3.3643598556518555\n",
      "steps = 8, loss = 2.843292713165283\n",
      "steps = 8, loss = 3.762173652648926\n",
      "steps = 8, loss = 2.0256459712982178\n",
      "steps = 8, loss = 2.899463415145874\n",
      "steps = 8, loss = 2.712872266769409\n",
      "steps = 8, loss = 3.0281407833099365\n",
      "steps = 8, loss = 2.191598415374756\n",
      "steps = 8, loss = 2.931323766708374\n",
      "steps = 8, loss = 2.708441972732544\n",
      "steps = 8, loss = 2.82892107963562\n",
      "steps = 8, loss = 49.83818435668945\n",
      "steps = 8, loss = 2.6285409927368164\n",
      "steps = 8, loss = 2.7201671600341797\n",
      "steps = 8, loss = 1.9952701330184937\n",
      "steps = 8, loss = 3.0815234184265137\n",
      "steps = 8, loss = 2.036494016647339\n",
      "steps = 8, loss = 2.0609288215637207\n",
      "steps = 8, loss = 49.8573112487793\n",
      "steps = 8, loss = 3.2793421745300293\n",
      "steps = 8, loss = 2.829104423522949\n",
      "steps = 8, loss = 2.9406816959381104\n",
      "steps = 8, loss = 6.0263166427612305\n",
      "steps = 8, loss = 2.4890360832214355\n",
      "steps = 8, loss = 2.431933879852295\n",
      "steps = 8, loss = 2.6645619869232178\n",
      "steps = 8, loss = 2.931305170059204\n",
      "steps = 8, loss = 3.275049924850464\n",
      "steps = 8, loss = 3.0609378814697266\n",
      "steps = 8, loss = 2.6608221530914307\n",
      "steps = 8, loss = 3.2651407718658447\n",
      "steps = 8, loss = 2.2813358306884766\n",
      "steps = 8, loss = 2.1820785999298096\n",
      "steps = 8, loss = 2.7533743381500244\n",
      "steps = 8, loss = 2.0975515842437744\n",
      "steps = 8, loss = 50.007904052734375\n",
      "steps = 8, loss = 1.7663187980651855\n",
      "steps = 8, loss = 3.087885618209839\n",
      "steps = 8, loss = 2.0935263633728027\n",
      "steps = 8, loss = 2.1898818016052246\n",
      "steps = 8, loss = 1.9480024576187134\n",
      "steps = 8, loss = 2.873795747756958\n",
      "steps = 8, loss = 2.7987618446350098\n",
      "steps = 8, loss = 4.747208595275879\n",
      "steps = 8, loss = 3.133970260620117\n",
      "steps = 8, loss = 7.5726318359375\n",
      "steps = 8, loss = 2.699868679046631\n",
      "steps = 8, loss = 2.2702789306640625\n",
      "steps = 8, loss = 2.9712185859680176\n",
      "steps = 8, loss = 2.7721426486968994\n",
      "steps = 8, loss = 1.927833080291748\n",
      "steps = 8, loss = 2.8880667686462402\n",
      "steps = 8, loss = 5.236158847808838\n",
      "steps = 8, loss = 49.97856140136719\n",
      "steps = 8, loss = 3.226499080657959\n",
      "steps = 8, loss = 2.589491128921509\n",
      "steps = 8, loss = 3.1896982192993164\n",
      "steps = 8, loss = 3.6344852447509766\n",
      "steps = 8, loss = 2.648833751678467\n",
      "steps = 9, loss = 2.6948390007019043\n",
      "steps = 9, loss = 2.423799991607666\n",
      "steps = 9, loss = 2.9663896560668945\n",
      "steps = 9, loss = 2.061859369277954\n",
      "steps = 9, loss = 3.6917755603790283\n",
      "steps = 9, loss = 3.896336317062378\n",
      "steps = 9, loss = 2.0903797149658203\n",
      "steps = 9, loss = 1.8051013946533203\n",
      "steps = 9, loss = 2.045916795730591\n",
      "steps = 9, loss = 4.966381072998047\n",
      "steps = 9, loss = 2.0714385509490967\n",
      "steps = 9, loss = 7.617615699768066\n",
      "steps = 9, loss = 3.0947201251983643\n",
      "steps = 9, loss = 1.9695758819580078\n",
      "steps = 9, loss = 2.8642613887786865\n",
      "steps = 9, loss = 1.952425479888916\n",
      "steps = 9, loss = 6.025022029876709\n",
      "steps = 9, loss = 3.0578811168670654\n",
      "steps = 9, loss = 2.266094923019409\n",
      "steps = 9, loss = 2.6036221981048584\n",
      "steps = 9, loss = 2.024412155151367\n",
      "steps = 9, loss = 2.7590689659118652\n",
      "steps = 9, loss = 2.9196200370788574\n",
      "steps = 9, loss = 5.223305702209473\n",
      "steps = 9, loss = 2.5905537605285645\n",
      "steps = 9, loss = 3.168510675430298\n",
      "steps = 9, loss = 2.188227653503418\n",
      "steps = 9, loss = 3.550602436065674\n",
      "steps = 9, loss = 43.64059066772461\n",
      "steps = 9, loss = 2.0300145149230957\n",
      "steps = 9, loss = 2.8205976486206055\n",
      "steps = 9, loss = 50.014862060546875\n",
      "steps = 9, loss = 2.8863162994384766\n",
      "steps = 9, loss = 2.783552408218384\n",
      "steps = 9, loss = 2.538752555847168\n",
      "steps = 9, loss = 2.2480087280273438\n",
      "steps = 9, loss = 3.0708749294281006\n",
      "steps = 9, loss = 2.3206803798675537\n",
      "steps = 9, loss = 2.1942880153656006\n",
      "steps = 9, loss = 2.931478261947632\n",
      "steps = 9, loss = 3.2707769870758057\n",
      "steps = 9, loss = 3.254173755645752\n",
      "steps = 9, loss = 2.8201372623443604\n",
      "steps = 9, loss = 3.2295634746551514\n",
      "steps = 9, loss = 2.2369143962860107\n",
      "steps = 9, loss = 2.9424986839294434\n",
      "steps = 9, loss = 3.0516226291656494\n",
      "steps = 9, loss = 2.640316963195801\n",
      "steps = 9, loss = 2.871965169906616\n",
      "steps = 9, loss = 2.764437198638916\n",
      "steps = 9, loss = 2.4665329456329346\n",
      "steps = 9, loss = 2.5220556259155273\n",
      "steps = 9, loss = 3.118269443511963\n",
      "steps = 9, loss = 2.8333842754364014\n",
      "steps = 9, loss = 3.4346323013305664\n",
      "steps = 9, loss = 2.9034838676452637\n",
      "steps = 9, loss = 49.97856521606445\n",
      "steps = 9, loss = 49.64495086669922\n",
      "steps = 10, loss = 3.102877616882324\n",
      "steps = 10, loss = 1.998290777206421\n",
      "steps = 10, loss = 2.8371362686157227\n",
      "steps = 10, loss = 49.978389739990234\n",
      "steps = 10, loss = 1.9846127033233643\n",
      "steps = 10, loss = 50.01826095581055\n",
      "steps = 10, loss = 2.1664412021636963\n",
      "steps = 10, loss = 2.5963661670684814\n",
      "steps = 10, loss = 2.135108232498169\n",
      "steps = 10, loss = 3.5914671421051025\n",
      "steps = 10, loss = 2.587646722793579\n",
      "steps = 10, loss = 3.455730438232422\n",
      "steps = 10, loss = 2.0699429512023926\n",
      "steps = 10, loss = 3.412931203842163\n",
      "steps = 10, loss = 2.684443950653076\n",
      "steps = 10, loss = 2.7096505165100098\n",
      "steps = 10, loss = 2.9215197563171387\n",
      "steps = 10, loss = 4.005661964416504\n",
      "steps = 10, loss = 3.099548578262329\n",
      "steps = 10, loss = 2.994440793991089\n",
      "steps = 10, loss = 2.2225091457366943\n",
      "steps = 10, loss = 7.468094348907471\n",
      "steps = 10, loss = 3.242859363555908\n",
      "steps = 10, loss = 3.1428163051605225\n",
      "steps = 10, loss = 2.975102186203003\n",
      "steps = 10, loss = 2.0943503379821777\n",
      "steps = 10, loss = 2.272334337234497\n",
      "steps = 10, loss = 2.8652751445770264\n",
      "steps = 10, loss = 1.9852395057678223\n",
      "steps = 10, loss = 3.0565383434295654\n",
      "steps = 10, loss = 49.98720932006836\n",
      "steps = 10, loss = 3.2355968952178955\n",
      "steps = 10, loss = 2.621227979660034\n",
      "steps = 10, loss = 1.8442074060440063\n",
      "steps = 10, loss = 2.351073741912842\n",
      "steps = 10, loss = 2.765108346939087\n",
      "steps = 10, loss = 3.353376865386963\n",
      "steps = 10, loss = 2.9227564334869385\n",
      "steps = 10, loss = 5.521064758300781\n",
      "steps = 10, loss = 2.890779972076416\n",
      "steps = 10, loss = 3.0427889823913574\n",
      "steps = 10, loss = 6.9440741539001465\n",
      "steps = 10, loss = 2.4596214294433594\n",
      "steps = 10, loss = 2.9537150859832764\n",
      "steps = 10, loss = 2.604849338531494\n",
      "steps = 10, loss = 49.888309478759766\n",
      "steps = 10, loss = 3.8209891319274902\n",
      "steps = 10, loss = 2.2909581661224365\n",
      "steps = 10, loss = 4.490538120269775\n",
      "steps = 10, loss = 2.8519551753997803\n",
      "steps = 10, loss = 2.779078483581543\n",
      "steps = 10, loss = 3.3658628463745117\n",
      "steps = 10, loss = 2.0586390495300293\n",
      "steps = 10, loss = 2.8146941661834717\n",
      "steps = 10, loss = 2.158473253250122\n",
      "steps = 10, loss = 2.4244906902313232\n",
      "steps = 10, loss = 3.322519302368164\n",
      "steps = 10, loss = 2.539151430130005\n",
      "steps = 11, loss = 3.1012611389160156\n",
      "steps = 11, loss = 2.0617642402648926\n",
      "steps = 11, loss = 2.9733901023864746\n",
      "steps = 11, loss = 2.3356645107269287\n",
      "steps = 11, loss = 2.6846566200256348\n",
      "steps = 11, loss = 3.1232662200927734\n",
      "steps = 11, loss = 3.040902853012085\n",
      "steps = 11, loss = 2.258586883544922\n",
      "steps = 11, loss = 2.6325440406799316\n",
      "steps = 11, loss = 2.08534574508667\n",
      "steps = 11, loss = 2.8418779373168945\n",
      "steps = 11, loss = 2.164579153060913\n",
      "steps = 11, loss = 3.2225284576416016\n",
      "steps = 11, loss = 2.9581353664398193\n",
      "steps = 11, loss = 3.4875550270080566\n",
      "steps = 11, loss = 2.2563188076019287\n",
      "steps = 11, loss = 49.98544692993164\n",
      "steps = 11, loss = 3.129474401473999\n",
      "steps = 11, loss = 2.598294734954834\n",
      "steps = 11, loss = 2.655843496322632\n",
      "steps = 11, loss = 2.896632671356201\n",
      "steps = 11, loss = 8.023770332336426\n",
      "steps = 11, loss = 2.4906668663024902\n",
      "steps = 11, loss = 2.7861390113830566\n",
      "steps = 11, loss = 49.97831726074219\n",
      "steps = 11, loss = 2.0004589557647705\n",
      "steps = 11, loss = 3.6143362522125244\n",
      "steps = 11, loss = 2.114468812942505\n",
      "steps = 11, loss = 2.975996971130371\n",
      "steps = 11, loss = 2.7062535285949707\n",
      "steps = 11, loss = 3.167548894882202\n",
      "steps = 11, loss = 2.0769906044006348\n",
      "steps = 11, loss = 6.395556449890137\n",
      "steps = 11, loss = 4.871739387512207\n",
      "steps = 11, loss = 2.5973384380340576\n",
      "steps = 11, loss = 3.3722972869873047\n",
      "steps = 11, loss = 2.0898656845092773\n",
      "steps = 11, loss = 2.907092571258545\n",
      "steps = 11, loss = 49.857540130615234\n",
      "steps = 11, loss = 3.4029791355133057\n",
      "steps = 11, loss = 2.2106354236602783\n",
      "steps = 11, loss = 2.764160633087158\n",
      "steps = 11, loss = 3.828382730484009\n",
      "steps = 11, loss = 49.9925651550293\n",
      "steps = 11, loss = 3.135998487472534\n",
      "steps = 11, loss = 2.249476432800293\n",
      "steps = 11, loss = 2.0119783878326416\n",
      "steps = 11, loss = 4.108358383178711\n",
      "steps = 11, loss = 2.875809669494629\n",
      "steps = 11, loss = 2.8779797554016113\n",
      "steps = 11, loss = 3.0085465908050537\n",
      "steps = 11, loss = 3.092287540435791\n",
      "steps = 11, loss = 3.119128465652466\n",
      "steps = 11, loss = 1.8682276010513306\n",
      "steps = 11, loss = 2.6113476753234863\n",
      "steps = 11, loss = 2.4450221061706543\n",
      "steps = 11, loss = 3.1115453243255615\n",
      "steps = 11, loss = 3.3239753246307373\n",
      "steps = 12, loss = 2.6876182556152344\n",
      "steps = 12, loss = 2.7547028064727783\n",
      "steps = 12, loss = 3.0222153663635254\n",
      "steps = 12, loss = 49.98743438720703\n",
      "steps = 12, loss = 49.87403869628906\n",
      "steps = 12, loss = 3.3500237464904785\n",
      "steps = 12, loss = 50.02107238769531\n",
      "steps = 12, loss = 2.361048698425293\n",
      "steps = 12, loss = 2.1024045944213867\n",
      "steps = 12, loss = 2.9160211086273193\n",
      "steps = 12, loss = 2.118375539779663\n",
      "steps = 12, loss = 2.3640639781951904\n",
      "steps = 12, loss = 2.1764283180236816\n",
      "steps = 12, loss = 2.0049328804016113\n",
      "steps = 12, loss = 3.1217293739318848\n",
      "steps = 12, loss = 3.067706823348999\n",
      "steps = 12, loss = 3.0365347862243652\n",
      "steps = 12, loss = 2.484968900680542\n",
      "steps = 12, loss = 2.202303409576416\n",
      "steps = 12, loss = 3.2750020027160645\n",
      "steps = 12, loss = 2.825817108154297\n",
      "steps = 12, loss = 2.7000629901885986\n",
      "steps = 12, loss = 6.262167453765869\n",
      "steps = 12, loss = 2.458239793777466\n",
      "steps = 12, loss = 2.135294198989868\n",
      "steps = 12, loss = 2.183518886566162\n",
      "steps = 12, loss = 3.4791698455810547\n",
      "steps = 12, loss = 2.5844886302948\n",
      "steps = 12, loss = 2.442201614379883\n",
      "steps = 12, loss = 4.471340179443359\n",
      "steps = 12, loss = 3.8770177364349365\n",
      "steps = 12, loss = 3.051820755004883\n",
      "steps = 12, loss = 1.8807262182235718\n",
      "steps = 12, loss = 3.049307346343994\n",
      "steps = 12, loss = 2.102280616760254\n",
      "steps = 12, loss = 2.6556854248046875\n",
      "steps = 12, loss = 2.9062130451202393\n",
      "steps = 12, loss = 3.103567361831665\n",
      "steps = 12, loss = 2.174433946609497\n",
      "steps = 12, loss = 3.002401828765869\n",
      "steps = 12, loss = 2.940290689468384\n",
      "steps = 12, loss = 2.8254778385162354\n",
      "steps = 12, loss = 2.5551211833953857\n",
      "steps = 12, loss = 2.0566301345825195\n",
      "steps = 12, loss = 50.02360153198242\n",
      "steps = 12, loss = 3.319959878921509\n",
      "steps = 12, loss = 3.1269850730895996\n",
      "steps = 12, loss = 2.870781183242798\n",
      "steps = 12, loss = 3.1236557960510254\n",
      "steps = 12, loss = 2.5281436443328857\n",
      "steps = 12, loss = 2.0159008502960205\n",
      "steps = 12, loss = 3.0580379962921143\n",
      "steps = 12, loss = 2.3964884281158447\n",
      "steps = 12, loss = 4.105146408081055\n",
      "steps = 12, loss = 3.134356737136841\n",
      "steps = 12, loss = 3.068403720855713\n",
      "steps = 12, loss = 2.659208059310913\n",
      "steps = 12, loss = 6.993444919586182\n",
      "steps = 13, loss = 2.561925172805786\n",
      "steps = 13, loss = 2.410606622695923\n",
      "steps = 13, loss = 2.246976852416992\n",
      "steps = 13, loss = 3.4355850219726562\n",
      "steps = 13, loss = 2.1245765686035156\n",
      "steps = 13, loss = 50.01811981201172\n",
      "steps = 13, loss = 3.2199788093566895\n",
      "steps = 13, loss = 2.6666533946990967\n",
      "steps = 13, loss = 4.814917087554932\n",
      "steps = 13, loss = 49.85498046875\n",
      "steps = 13, loss = 2.870708703994751\n",
      "steps = 13, loss = 3.0464160442352295\n",
      "steps = 13, loss = 4.1298370361328125\n",
      "steps = 13, loss = 2.573831081390381\n",
      "steps = 13, loss = 3.4004130363464355\n",
      "steps = 13, loss = 3.108943223953247\n",
      "steps = 13, loss = 2.0274267196655273\n",
      "steps = 13, loss = 3.715946674346924\n",
      "steps = 13, loss = 2.416231870651245\n",
      "steps = 13, loss = 2.573678731918335\n",
      "steps = 13, loss = 3.062328577041626\n",
      "steps = 13, loss = 3.057060718536377\n",
      "steps = 13, loss = 2.965954303741455\n",
      "steps = 13, loss = 3.1589999198913574\n",
      "steps = 13, loss = 2.922393560409546\n",
      "steps = 13, loss = 3.9320218563079834\n",
      "steps = 13, loss = 3.152338981628418\n",
      "steps = 13, loss = 2.130009889602661\n",
      "steps = 13, loss = 2.1606922149658203\n",
      "steps = 13, loss = 2.21895432472229\n",
      "steps = 13, loss = 2.526712417602539\n",
      "steps = 13, loss = 2.8347575664520264\n",
      "steps = 13, loss = 2.716315984725952\n",
      "steps = 13, loss = 3.0724799633026123\n",
      "steps = 13, loss = 2.1422359943389893\n",
      "steps = 13, loss = 2.695974349975586\n",
      "steps = 13, loss = 3.0477640628814697\n",
      "steps = 13, loss = 6.832388877868652\n",
      "steps = 13, loss = 2.596071243286133\n",
      "steps = 13, loss = 2.691761016845703\n",
      "steps = 13, loss = 3.071216106414795\n",
      "steps = 13, loss = 2.4494752883911133\n",
      "steps = 13, loss = 2.3506240844726562\n",
      "steps = 13, loss = 3.0888874530792236\n",
      "steps = 13, loss = 1.898645281791687\n",
      "steps = 13, loss = 3.2251012325286865\n",
      "steps = 13, loss = 3.2928988933563232\n",
      "steps = 13, loss = 2.028848171234131\n",
      "steps = 13, loss = 2.2100610733032227\n",
      "steps = 13, loss = 2.1359410285949707\n",
      "steps = 13, loss = 7.835664749145508\n",
      "steps = 13, loss = 2.287564277648926\n",
      "steps = 13, loss = 2.0199663639068604\n",
      "steps = 13, loss = 49.98232650756836\n",
      "steps = 13, loss = 49.96748352050781\n",
      "steps = 13, loss = 2.655860185623169\n",
      "steps = 13, loss = 2.7596442699432373\n",
      "steps = 13, loss = 2.834336042404175\n",
      "steps = 14, loss = 2.8245816230773926\n",
      "steps = 14, loss = 3.2170565128326416\n",
      "steps = 14, loss = 2.574479341506958\n",
      "steps = 14, loss = 2.710380792617798\n",
      "steps = 14, loss = 2.251413106918335\n",
      "steps = 14, loss = 2.7354631423950195\n",
      "steps = 14, loss = 2.2539618015289307\n",
      "steps = 14, loss = 2.2011053562164307\n",
      "steps = 14, loss = 2.7152469158172607\n",
      "steps = 14, loss = 3.0844178199768066\n",
      "steps = 14, loss = 2.559058666229248\n",
      "steps = 14, loss = 2.7473599910736084\n",
      "steps = 14, loss = 9.8505220413208\n",
      "steps = 14, loss = 2.4778847694396973\n",
      "steps = 14, loss = 2.1733362674713135\n",
      "steps = 14, loss = 2.2793290615081787\n",
      "steps = 14, loss = 49.98201370239258\n",
      "steps = 14, loss = 8.22896957397461\n",
      "steps = 14, loss = 2.920725107192993\n",
      "steps = 14, loss = 3.1657769680023193\n",
      "steps = 14, loss = 50.004825592041016\n",
      "steps = 14, loss = 2.6695101261138916\n",
      "steps = 14, loss = 7.1159586906433105\n",
      "steps = 14, loss = 2.196284532546997\n",
      "steps = 14, loss = 50.025123596191406\n",
      "steps = 14, loss = 3.4847381114959717\n",
      "steps = 14, loss = 2.2674977779388428\n",
      "steps = 14, loss = 3.1673226356506348\n",
      "steps = 14, loss = 3.0942795276641846\n",
      "steps = 14, loss = 1.9320112466812134\n",
      "steps = 14, loss = 3.403198003768921\n",
      "steps = 14, loss = 49.80181121826172\n",
      "steps = 14, loss = 2.598461389541626\n",
      "steps = 14, loss = 2.1249895095825195\n",
      "steps = 14, loss = 3.1390557289123535\n",
      "steps = 14, loss = 2.4843404293060303\n",
      "steps = 14, loss = 3.0058176517486572\n",
      "steps = 14, loss = 3.1906073093414307\n",
      "steps = 14, loss = 3.552382230758667\n",
      "steps = 14, loss = 2.9089462757110596\n",
      "steps = 14, loss = 2.8938164710998535\n",
      "steps = 14, loss = 2.056494951248169\n",
      "steps = 14, loss = 2.773266553878784\n",
      "steps = 14, loss = 5.082920551300049\n",
      "steps = 14, loss = 3.0394906997680664\n",
      "steps = 14, loss = 2.0605664253234863\n",
      "steps = 14, loss = 2.1582186222076416\n",
      "steps = 14, loss = 3.9791135787963867\n",
      "steps = 14, loss = 2.83217716217041\n",
      "steps = 14, loss = 3.3922202587127686\n",
      "steps = 14, loss = 3.1613943576812744\n",
      "steps = 14, loss = 3.3523612022399902\n",
      "steps = 14, loss = 2.156892776489258\n",
      "steps = 14, loss = 4.354515075683594\n",
      "steps = 14, loss = 3.0778870582580566\n",
      "steps = 14, loss = 3.4648385047912598\n",
      "steps = 14, loss = 2.8860669136047363\n",
      "steps = 14, loss = 3.135697364807129\n",
      "steps = 15, loss = 2.1109373569488525\n",
      "steps = 15, loss = 8.616015434265137\n",
      "steps = 15, loss = 3.1060333251953125\n",
      "steps = 15, loss = 3.0108368396759033\n",
      "steps = 15, loss = 2.8625030517578125\n",
      "steps = 15, loss = 2.570404052734375\n",
      "steps = 15, loss = 2.5191729068756104\n",
      "steps = 15, loss = 4.488426685333252\n",
      "steps = 15, loss = 3.299008846282959\n",
      "steps = 15, loss = 2.878877639770508\n",
      "steps = 15, loss = 2.201529026031494\n",
      "steps = 15, loss = 2.232065439224243\n",
      "steps = 15, loss = 3.062758445739746\n",
      "steps = 15, loss = 3.1098084449768066\n",
      "steps = 15, loss = 7.00740909576416\n",
      "steps = 15, loss = 3.143958806991577\n",
      "steps = 15, loss = 2.3111166954040527\n",
      "steps = 15, loss = 50.00859832763672\n",
      "steps = 15, loss = 2.466269016265869\n",
      "steps = 15, loss = 3.1959149837493896\n",
      "steps = 15, loss = 2.221651792526245\n",
      "steps = 15, loss = 2.987346649169922\n",
      "steps = 15, loss = 49.76908874511719\n",
      "steps = 15, loss = 1.9472872018814087\n",
      "steps = 15, loss = 2.7329258918762207\n",
      "steps = 15, loss = 2.889645576477051\n",
      "steps = 15, loss = 2.146571397781372\n",
      "steps = 15, loss = 3.463639497756958\n",
      "steps = 15, loss = 3.1550209522247314\n",
      "steps = 15, loss = 2.673595428466797\n",
      "steps = 15, loss = 3.9497640132904053\n",
      "steps = 15, loss = 3.3116629123687744\n",
      "steps = 15, loss = 2.187351703643799\n",
      "steps = 15, loss = 4.937313556671143\n",
      "steps = 15, loss = 2.258702278137207\n",
      "steps = 15, loss = 2.225647449493408\n",
      "steps = 15, loss = 7.25535249710083\n",
      "steps = 15, loss = 3.355714797973633\n",
      "steps = 15, loss = 50.005550384521484\n",
      "steps = 15, loss = 3.45809006690979\n",
      "steps = 15, loss = 3.0063745975494385\n",
      "steps = 15, loss = 3.155074119567871\n",
      "steps = 15, loss = 2.0698249340057373\n",
      "steps = 15, loss = 2.227130174636841\n",
      "steps = 15, loss = 3.308821439743042\n",
      "steps = 15, loss = 2.769657850265503\n",
      "steps = 15, loss = 49.98201370239258\n",
      "steps = 15, loss = 2.8845598697662354\n",
      "steps = 15, loss = 2.069385528564453\n",
      "steps = 15, loss = 2.8023879528045654\n",
      "steps = 15, loss = 2.945326566696167\n",
      "steps = 15, loss = 2.602475881576538\n",
      "steps = 15, loss = 2.5451736450195312\n",
      "steps = 15, loss = 2.737489938735962\n",
      "steps = 15, loss = 1.900665283203125\n",
      "steps = 15, loss = 3.266216278076172\n",
      "steps = 15, loss = 3.045522451400757\n",
      "steps = 15, loss = 3.192013740539551\n",
      "steps = 16, loss = 3.213912010192871\n",
      "steps = 16, loss = 2.890648365020752\n",
      "steps = 16, loss = 2.516252040863037\n",
      "steps = 16, loss = 2.9281623363494873\n",
      "steps = 16, loss = 2.2616453170776367\n",
      "steps = 16, loss = 3.485438585281372\n",
      "steps = 16, loss = 1.9679415225982666\n",
      "steps = 16, loss = 3.504931926727295\n",
      "steps = 16, loss = 2.562523603439331\n",
      "steps = 16, loss = 2.088923215866089\n",
      "steps = 16, loss = 3.353464365005493\n",
      "steps = 16, loss = 2.7867398262023926\n",
      "steps = 16, loss = 2.2625796794891357\n",
      "steps = 16, loss = 2.576206684112549\n",
      "steps = 16, loss = 2.353614330291748\n",
      "steps = 16, loss = 3.1191341876983643\n",
      "steps = 16, loss = 2.3004050254821777\n",
      "steps = 16, loss = 2.27079439163208\n",
      "steps = 16, loss = 3.492610216140747\n",
      "steps = 16, loss = 7.093342304229736\n",
      "steps = 16, loss = 3.244225025177002\n",
      "steps = 16, loss = 3.090008020401001\n",
      "steps = 16, loss = 4.438423156738281\n",
      "steps = 16, loss = 2.2207205295562744\n",
      "steps = 16, loss = 2.8345625400543213\n",
      "steps = 16, loss = 2.8002452850341797\n",
      "steps = 16, loss = 3.2897250652313232\n",
      "steps = 16, loss = 2.8970625400543213\n",
      "steps = 16, loss = 50.020240783691406\n",
      "steps = 16, loss = 3.0760045051574707\n",
      "steps = 16, loss = 2.6577932834625244\n",
      "steps = 16, loss = 3.2424845695495605\n",
      "steps = 16, loss = 49.882240295410156\n",
      "steps = 16, loss = 2.2322957515716553\n",
      "steps = 16, loss = 2.0916764736175537\n",
      "steps = 16, loss = 2.7286415100097656\n",
      "steps = 16, loss = 49.67563247680664\n",
      "steps = 16, loss = 3.5139455795288086\n",
      "steps = 16, loss = 3.186962366104126\n",
      "steps = 16, loss = 2.256559371948242\n",
      "steps = 16, loss = 2.6021177768707275\n",
      "steps = 16, loss = 2.6611554622650146\n",
      "steps = 16, loss = 2.5642080307006836\n",
      "steps = 16, loss = 2.226651191711426\n",
      "steps = 16, loss = 2.4929556846618652\n",
      "steps = 16, loss = 2.7189180850982666\n",
      "steps = 16, loss = 5.673459053039551\n",
      "steps = 16, loss = 3.2164783477783203\n",
      "steps = 16, loss = 2.973392963409424\n",
      "steps = 16, loss = 3.1733720302581787\n",
      "steps = 16, loss = 4.036111831665039\n",
      "steps = 16, loss = 2.805302381515503\n",
      "steps = 16, loss = 2.904160261154175\n",
      "steps = 16, loss = 3.27253794670105\n",
      "steps = 16, loss = 49.98201370239258\n",
      "steps = 16, loss = 10.703156471252441\n",
      "steps = 16, loss = 8.752373695373535\n",
      "steps = 16, loss = 2.203538656234741\n",
      "steps = 17, loss = 2.971449613571167\n",
      "steps = 17, loss = 2.7371432781219482\n",
      "steps = 17, loss = 3.2038726806640625\n",
      "steps = 17, loss = 2.8337230682373047\n",
      "steps = 17, loss = 3.505112648010254\n",
      "steps = 17, loss = 3.13600754737854\n",
      "steps = 17, loss = 2.9531779289245605\n",
      "steps = 17, loss = 4.022747993469238\n",
      "steps = 17, loss = 2.246251106262207\n",
      "steps = 17, loss = 3.1270411014556885\n",
      "steps = 17, loss = 6.552212715148926\n",
      "steps = 17, loss = 3.195345640182495\n",
      "steps = 17, loss = 2.2265844345092773\n",
      "steps = 17, loss = 2.6837236881256104\n",
      "steps = 17, loss = 1.967326283454895\n",
      "steps = 17, loss = 2.8623507022857666\n",
      "steps = 17, loss = 2.0877773761749268\n",
      "steps = 17, loss = 2.028839111328125\n",
      "steps = 17, loss = 2.628732681274414\n",
      "steps = 17, loss = 3.085937261581421\n",
      "steps = 17, loss = 3.324000358581543\n",
      "steps = 17, loss = 2.8459274768829346\n",
      "steps = 17, loss = 4.7720770835876465\n",
      "steps = 17, loss = 3.1071619987487793\n",
      "steps = 17, loss = 2.0840306282043457\n",
      "steps = 17, loss = 2.6812422275543213\n",
      "steps = 17, loss = 49.5657844543457\n",
      "steps = 17, loss = 50.0180549621582\n",
      "steps = 17, loss = 2.5251717567443848\n",
      "steps = 17, loss = 2.188375234603882\n",
      "steps = 17, loss = 2.1383628845214844\n",
      "steps = 17, loss = 2.8094308376312256\n",
      "steps = 17, loss = 3.3568363189697266\n",
      "steps = 17, loss = 4.21324348449707\n",
      "steps = 17, loss = 3.0538246631622314\n",
      "steps = 17, loss = 50.00963592529297\n",
      "steps = 17, loss = 2.514415979385376\n",
      "steps = 17, loss = 2.365649938583374\n",
      "steps = 17, loss = 3.017382860183716\n",
      "steps = 17, loss = 2.5617902278900146\n",
      "steps = 17, loss = 2.4683055877685547\n",
      "steps = 17, loss = 2.2906432151794434\n",
      "steps = 17, loss = 3.2993874549865723\n",
      "steps = 17, loss = 2.748641014099121\n",
      "steps = 17, loss = 2.2234976291656494\n",
      "steps = 17, loss = 8.1475248336792\n",
      "steps = 17, loss = 2.2621586322784424\n",
      "steps = 17, loss = 2.0929689407348633\n",
      "steps = 17, loss = 2.1422648429870605\n",
      "steps = 17, loss = 3.2137956619262695\n",
      "steps = 17, loss = 3.1913259029388428\n",
      "steps = 17, loss = 3.401850461959839\n",
      "steps = 17, loss = 7.422162055969238\n",
      "steps = 17, loss = 3.470935821533203\n",
      "steps = 17, loss = 50.01982498168945\n",
      "steps = 17, loss = 2.7115132808685303\n",
      "steps = 17, loss = 2.582547187805176\n",
      "steps = 17, loss = 3.244493246078491\n",
      "steps = 18, loss = 2.479257822036743\n",
      "steps = 18, loss = 2.610960006713867\n",
      "steps = 18, loss = 2.9778506755828857\n",
      "steps = 18, loss = 2.283322811126709\n",
      "steps = 18, loss = 2.8749163150787354\n",
      "steps = 18, loss = 1.9880399703979492\n",
      "steps = 18, loss = 3.2047572135925293\n",
      "steps = 18, loss = 2.4151175022125244\n",
      "steps = 18, loss = 3.512734889984131\n",
      "steps = 18, loss = 3.2447030544281006\n",
      "steps = 18, loss = 3.404524326324463\n",
      "steps = 18, loss = 4.909353256225586\n",
      "steps = 18, loss = 2.6137592792510986\n",
      "steps = 18, loss = 50.020503997802734\n",
      "steps = 18, loss = 2.2529287338256836\n",
      "steps = 18, loss = 2.094993829727173\n",
      "steps = 18, loss = 2.6801233291625977\n",
      "steps = 18, loss = 49.50071334838867\n",
      "steps = 18, loss = 4.016269683837891\n",
      "steps = 18, loss = 2.074047803878784\n",
      "steps = 18, loss = 7.896333694458008\n",
      "steps = 18, loss = 2.3604769706726074\n",
      "steps = 18, loss = 3.0023038387298584\n",
      "steps = 18, loss = 2.315981864929199\n",
      "steps = 18, loss = 2.6734888553619385\n",
      "steps = 18, loss = 10.178664207458496\n",
      "steps = 18, loss = 3.2545225620269775\n",
      "steps = 18, loss = 3.168175458908081\n",
      "steps = 18, loss = 2.843742847442627\n",
      "steps = 18, loss = 2.8821425437927246\n",
      "steps = 18, loss = 3.2495639324188232\n",
      "steps = 18, loss = 2.636237621307373\n",
      "steps = 18, loss = 3.0640311241149902\n",
      "steps = 18, loss = 3.114203929901123\n",
      "steps = 18, loss = 2.8135485649108887\n",
      "steps = 18, loss = 2.7189526557922363\n",
      "steps = 18, loss = 1.9897888898849487\n",
      "steps = 18, loss = 2.616983413696289\n",
      "steps = 18, loss = 3.2061595916748047\n",
      "steps = 18, loss = 2.527163028717041\n",
      "steps = 18, loss = 3.409008264541626\n",
      "steps = 18, loss = 3.262800693511963\n",
      "steps = 18, loss = 2.106484889984131\n",
      "steps = 18, loss = 2.235382556915283\n",
      "steps = 18, loss = 2.1145987510681152\n",
      "steps = 18, loss = 3.231790542602539\n",
      "steps = 18, loss = 2.795353412628174\n",
      "steps = 18, loss = 3.1730101108551025\n",
      "steps = 18, loss = 2.878051996231079\n",
      "steps = 18, loss = 2.543097734451294\n",
      "steps = 18, loss = 50.02204513549805\n",
      "steps = 18, loss = 6.989780426025391\n",
      "steps = 18, loss = 4.87566614151001\n",
      "steps = 18, loss = 49.97947692871094\n",
      "steps = 18, loss = 2.9913573265075684\n",
      "steps = 18, loss = 2.0861294269561768\n",
      "steps = 18, loss = 2.8729660511016846\n",
      "steps = 18, loss = 3.5928001403808594\n",
      "steps = 19, loss = 3.0955276489257812\n",
      "steps = 19, loss = 4.832042694091797\n",
      "steps = 19, loss = 49.98353958129883\n",
      "steps = 19, loss = 2.617300271987915\n",
      "steps = 19, loss = 2.312309741973877\n",
      "steps = 19, loss = 2.3612020015716553\n",
      "steps = 19, loss = 3.530660390853882\n",
      "steps = 19, loss = 2.5943119525909424\n",
      "steps = 19, loss = 2.623847484588623\n",
      "steps = 19, loss = 2.4197747707366943\n",
      "steps = 19, loss = 3.4270594120025635\n",
      "steps = 19, loss = 49.97262191772461\n",
      "steps = 19, loss = 3.2711498737335205\n",
      "steps = 19, loss = 2.7261106967926025\n",
      "steps = 19, loss = 2.223641872406006\n",
      "steps = 19, loss = 3.315500259399414\n",
      "steps = 19, loss = 50.005619049072266\n",
      "steps = 19, loss = 2.258985757827759\n",
      "steps = 19, loss = 2.682187557220459\n",
      "steps = 19, loss = 2.9980154037475586\n",
      "steps = 19, loss = 49.82206726074219\n",
      "steps = 19, loss = 2.6768290996551514\n",
      "steps = 19, loss = 2.5700900554656982\n",
      "steps = 19, loss = 3.2160089015960693\n",
      "steps = 19, loss = 2.504345417022705\n",
      "steps = 19, loss = 8.589319229125977\n",
      "steps = 19, loss = 2.2586312294006348\n",
      "steps = 19, loss = 2.2120397090911865\n",
      "steps = 19, loss = 5.558412551879883\n",
      "steps = 19, loss = 3.2590014934539795\n",
      "steps = 19, loss = 2.694624662399292\n",
      "steps = 19, loss = 3.0056886672973633\n",
      "steps = 19, loss = 2.8277196884155273\n",
      "steps = 19, loss = 2.453519582748413\n",
      "steps = 19, loss = 2.127410888671875\n",
      "steps = 19, loss = 3.5214216709136963\n",
      "steps = 19, loss = 3.271970272064209\n",
      "steps = 19, loss = 2.0063905715942383\n",
      "steps = 19, loss = 2.4565837383270264\n",
      "steps = 19, loss = 3.231616973876953\n",
      "steps = 19, loss = 3.265338659286499\n",
      "steps = 19, loss = 3.287374973297119\n",
      "steps = 19, loss = 3.1816904544830322\n",
      "steps = 19, loss = 3.1844210624694824\n",
      "steps = 19, loss = 7.316338539123535\n",
      "steps = 19, loss = 2.8739101886749268\n",
      "steps = 19, loss = 3.390514612197876\n",
      "steps = 19, loss = 2.8996336460113525\n",
      "steps = 19, loss = 2.8951406478881836\n",
      "steps = 19, loss = 2.8982250690460205\n",
      "steps = 19, loss = 2.761552572250366\n",
      "steps = 19, loss = 7.350788116455078\n",
      "steps = 19, loss = 2.122011661529541\n",
      "steps = 19, loss = 4.094474792480469\n",
      "steps = 19, loss = 2.7379555702209473\n",
      "steps = 19, loss = 3.577601194381714\n",
      "steps = 19, loss = 2.1373291015625\n",
      "steps = 19, loss = 2.695518732070923\n",
      "steps = 20, loss = 3.0799591541290283\n",
      "steps = 20, loss = 3.243208408355713\n",
      "steps = 20, loss = 3.49092698097229\n",
      "steps = 20, loss = 6.893733501434326\n",
      "steps = 20, loss = 3.263181686401367\n",
      "steps = 20, loss = 2.258927583694458\n",
      "steps = 20, loss = 49.98271942138672\n",
      "steps = 20, loss = 2.1580488681793213\n",
      "steps = 20, loss = 2.1344566345214844\n",
      "steps = 20, loss = 2.838606119155884\n",
      "steps = 20, loss = 8.023662567138672\n",
      "steps = 20, loss = 2.4786324501037598\n",
      "steps = 20, loss = 2.630061388015747\n",
      "steps = 20, loss = 2.324843645095825\n",
      "steps = 20, loss = 2.453526496887207\n",
      "steps = 20, loss = 2.937232494354248\n",
      "steps = 20, loss = 50.012908935546875\n",
      "steps = 20, loss = 50.02340316772461\n",
      "steps = 20, loss = 2.4854483604431152\n",
      "steps = 20, loss = 3.2338035106658936\n",
      "steps = 20, loss = 30.18525505065918\n",
      "steps = 20, loss = 3.357447624206543\n",
      "steps = 20, loss = 2.362236738204956\n",
      "steps = 20, loss = 2.132192611694336\n",
      "steps = 20, loss = 3.6437416076660156\n",
      "steps = 20, loss = 2.25111985206604\n",
      "steps = 20, loss = 2.6507582664489746\n",
      "steps = 20, loss = 2.551241397857666\n",
      "steps = 20, loss = 2.467144727706909\n",
      "steps = 20, loss = 3.05442214012146\n",
      "steps = 20, loss = 3.1978302001953125\n",
      "steps = 20, loss = 3.184356927871704\n",
      "steps = 20, loss = 2.872889995574951\n",
      "steps = 20, loss = 3.2150750160217285\n",
      "steps = 20, loss = 2.632530689239502\n",
      "steps = 20, loss = 2.4702272415161133\n",
      "steps = 20, loss = 3.089869976043701\n",
      "steps = 20, loss = 2.691112995147705\n",
      "steps = 20, loss = 2.854017972946167\n",
      "steps = 20, loss = 2.728518486022949\n",
      "steps = 20, loss = 4.081196308135986\n",
      "steps = 20, loss = 2.8584237098693848\n",
      "steps = 20, loss = 2.7348525524139404\n",
      "steps = 20, loss = 5.075322151184082\n",
      "steps = 20, loss = 2.1254308223724365\n",
      "steps = 20, loss = 2.2258834838867188\n",
      "steps = 20, loss = 2.003399610519409\n",
      "steps = 20, loss = 2.687807083129883\n",
      "steps = 20, loss = 3.218172073364258\n",
      "steps = 20, loss = 49.98353958129883\n",
      "steps = 20, loss = 2.899970769882202\n",
      "steps = 20, loss = 4.829484939575195\n",
      "steps = 20, loss = 3.2984158992767334\n",
      "steps = 20, loss = 7.062668323516846\n",
      "steps = 20, loss = 3.6213338375091553\n",
      "steps = 20, loss = 3.1523377895355225\n",
      "steps = 20, loss = 2.482241630554199\n",
      "steps = 20, loss = 2.165959358215332\n",
      "steps = 21, loss = 3.2210304737091064\n",
      "steps = 21, loss = 7.114541530609131\n",
      "steps = 21, loss = 2.8824257850646973\n",
      "steps = 21, loss = 2.23629093170166\n",
      "steps = 21, loss = 4.825845718383789\n",
      "steps = 21, loss = 7.73872709274292\n",
      "steps = 21, loss = 2.250373363494873\n",
      "steps = 21, loss = 2.861694812774658\n",
      "steps = 21, loss = 3.10792875289917\n",
      "steps = 21, loss = 2.0223052501678467\n",
      "steps = 21, loss = 3.2608187198638916\n",
      "steps = 21, loss = 2.9010870456695557\n",
      "steps = 21, loss = 1.9465618133544922\n",
      "steps = 21, loss = 2.151083469390869\n",
      "steps = 21, loss = 2.584774971008301\n",
      "steps = 21, loss = 3.338252544403076\n",
      "steps = 21, loss = 2.924725294113159\n",
      "steps = 21, loss = 3.334416627883911\n",
      "steps = 21, loss = 1.9545551538467407\n",
      "steps = 21, loss = 2.5140974521636963\n",
      "steps = 21, loss = 3.4833171367645264\n",
      "steps = 21, loss = 4.061997413635254\n",
      "steps = 21, loss = 3.2475266456604004\n",
      "steps = 21, loss = 2.650240898132324\n",
      "steps = 21, loss = 2.8741250038146973\n",
      "steps = 21, loss = 8.614625930786133\n",
      "steps = 21, loss = 2.9561686515808105\n",
      "steps = 21, loss = 3.406585693359375\n",
      "steps = 21, loss = 2.488189220428467\n",
      "steps = 21, loss = 3.4426538944244385\n",
      "steps = 21, loss = 3.216867685317993\n",
      "steps = 21, loss = 2.355708122253418\n",
      "steps = 21, loss = 3.3282155990600586\n",
      "steps = 21, loss = 2.534604072570801\n",
      "steps = 21, loss = 49.98353958129883\n",
      "steps = 21, loss = 2.6831488609313965\n",
      "steps = 21, loss = 2.4054720401763916\n",
      "steps = 21, loss = 3.5064423084259033\n",
      "steps = 21, loss = 2.884105682373047\n",
      "steps = 21, loss = 49.43680953979492\n",
      "steps = 21, loss = 3.4189836978912354\n",
      "steps = 21, loss = 2.7396421432495117\n",
      "steps = 21, loss = 3.2323834896087646\n",
      "steps = 21, loss = 2.870504140853882\n",
      "steps = 21, loss = 2.479667901992798\n",
      "steps = 21, loss = 2.3597798347473145\n",
      "steps = 21, loss = 3.6415674686431885\n",
      "steps = 21, loss = 2.957827091217041\n",
      "steps = 21, loss = 3.5792670249938965\n",
      "steps = 21, loss = 2.162209987640381\n",
      "steps = 21, loss = 50.00745391845703\n",
      "steps = 21, loss = 2.7161951065063477\n",
      "steps = 21, loss = 2.205878734588623\n",
      "steps = 21, loss = 2.8361144065856934\n",
      "steps = 21, loss = 5.17114782333374\n",
      "steps = 21, loss = 2.7410409450531006\n",
      "steps = 21, loss = 2.5420596599578857\n",
      "steps = 21, loss = 2.243959665298462\n",
      "steps = 22, loss = 2.1803042888641357\n",
      "steps = 22, loss = 4.863548278808594\n",
      "steps = 22, loss = 7.772350788116455\n",
      "steps = 22, loss = 2.649562358856201\n",
      "steps = 22, loss = 2.5991621017456055\n",
      "steps = 22, loss = 5.399074077606201\n",
      "steps = 22, loss = 3.612586736679077\n",
      "steps = 22, loss = 2.892467498779297\n",
      "steps = 22, loss = 2.3850276470184326\n",
      "steps = 22, loss = 3.0035271644592285\n",
      "steps = 22, loss = 2.925172805786133\n",
      "steps = 22, loss = 2.1067349910736084\n",
      "steps = 22, loss = 3.2077372074127197\n",
      "steps = 22, loss = 3.459641695022583\n",
      "steps = 22, loss = 2.673492908477783\n",
      "steps = 22, loss = 5.115263938903809\n",
      "steps = 22, loss = 2.886517286300659\n",
      "steps = 22, loss = 2.233278274536133\n",
      "steps = 22, loss = 3.0618488788604736\n",
      "steps = 22, loss = 3.253730058670044\n",
      "steps = 22, loss = 8.226149559020996\n",
      "steps = 22, loss = 2.236180067062378\n",
      "steps = 22, loss = 3.1067276000976562\n",
      "steps = 22, loss = 2.49080753326416\n",
      "steps = 22, loss = 2.5660150051116943\n",
      "steps = 22, loss = 3.268202304840088\n",
      "steps = 22, loss = 3.560532569885254\n",
      "steps = 22, loss = 2.430626153945923\n",
      "steps = 22, loss = 3.2721621990203857\n",
      "steps = 22, loss = 2.3893818855285645\n",
      "steps = 22, loss = 2.196506977081299\n",
      "steps = 22, loss = 2.6845319271087646\n",
      "steps = 22, loss = 3.3381288051605225\n",
      "steps = 22, loss = 3.269486665725708\n",
      "steps = 22, loss = 3.875506639480591\n",
      "steps = 22, loss = 2.5462117195129395\n",
      "steps = 22, loss = 2.237175226211548\n",
      "steps = 22, loss = 1.8209424018859863\n",
      "steps = 22, loss = 49.987850189208984\n",
      "steps = 22, loss = 3.2979202270507812\n",
      "steps = 22, loss = 4.084139347076416\n",
      "steps = 22, loss = 2.8200600147247314\n",
      "steps = 22, loss = 2.7747578620910645\n",
      "steps = 22, loss = 2.194258689880371\n",
      "steps = 22, loss = 3.3658382892608643\n",
      "steps = 22, loss = 2.8738386631011963\n",
      "steps = 22, loss = 2.8544328212738037\n",
      "steps = 22, loss = 2.619746208190918\n",
      "steps = 22, loss = 3.529252529144287\n",
      "steps = 22, loss = 2.4803671836853027\n",
      "steps = 22, loss = 3.1636247634887695\n",
      "steps = 22, loss = 2.034181833267212\n",
      "steps = 22, loss = 2.9180502891540527\n",
      "steps = 22, loss = 2.1692349910736084\n",
      "steps = 22, loss = 2.751199960708618\n",
      "steps = 22, loss = 50.0214729309082\n",
      "steps = 22, loss = 49.98353958129883\n",
      "steps = 22, loss = 2.9167323112487793\n",
      "steps = 23, loss = 8.944123268127441\n",
      "steps = 23, loss = 2.666088342666626\n",
      "steps = 23, loss = 3.300123453140259\n",
      "steps = 23, loss = 2.257408380508423\n",
      "steps = 23, loss = 3.234840154647827\n",
      "steps = 23, loss = 3.2586989402770996\n",
      "steps = 23, loss = 2.4102518558502197\n",
      "steps = 23, loss = 2.2025978565216064\n",
      "steps = 23, loss = 2.5172348022460938\n",
      "steps = 23, loss = 3.5796494483947754\n",
      "steps = 23, loss = 2.8918466567993164\n",
      "steps = 23, loss = 2.0491302013397217\n",
      "steps = 23, loss = 2.7273201942443848\n",
      "steps = 23, loss = 2.3781464099884033\n",
      "steps = 23, loss = 2.171912431716919\n",
      "steps = 23, loss = 2.8102786540985107\n",
      "steps = 23, loss = 3.297971248626709\n",
      "steps = 23, loss = 3.391237258911133\n",
      "steps = 23, loss = 3.3613343238830566\n",
      "steps = 23, loss = 2.192838191986084\n",
      "steps = 23, loss = 50.018272399902344\n",
      "steps = 23, loss = 5.643082141876221\n",
      "steps = 23, loss = 2.939277172088623\n",
      "steps = 23, loss = 2.893148899078369\n",
      "steps = 23, loss = 2.583361864089966\n",
      "steps = 23, loss = 3.5166003704071045\n",
      "steps = 23, loss = 2.6159918308258057\n",
      "steps = 23, loss = 50.00554656982422\n",
      "steps = 23, loss = 2.6514527797698975\n",
      "steps = 23, loss = 2.7844936847686768\n",
      "steps = 23, loss = 2.8275907039642334\n",
      "steps = 23, loss = 3.1998016834259033\n",
      "steps = 23, loss = 2.253986358642578\n",
      "steps = 23, loss = 4.139030456542969\n",
      "steps = 23, loss = 4.893231391906738\n",
      "steps = 23, loss = 3.3080735206604004\n",
      "steps = 23, loss = 8.027283668518066\n",
      "steps = 23, loss = 2.699885606765747\n",
      "steps = 23, loss = 2.9374537467956543\n",
      "steps = 23, loss = 3.3963820934295654\n",
      "steps = 23, loss = 2.2596704959869385\n",
      "steps = 23, loss = 2.8504040241241455\n",
      "steps = 23, loss = 2.7342746257781982\n",
      "steps = 23, loss = 2.494170665740967\n",
      "steps = 23, loss = 3.200753927230835\n",
      "steps = 23, loss = 3.426516056060791\n",
      "steps = 23, loss = 2.900845766067505\n",
      "steps = 23, loss = 3.093278169631958\n",
      "steps = 23, loss = 2.163994312286377\n",
      "steps = 23, loss = 2.562349796295166\n",
      "steps = 23, loss = 2.4735286235809326\n",
      "steps = 23, loss = 3.5482940673828125\n",
      "steps = 23, loss = 2.91416072845459\n",
      "steps = 23, loss = 2.8986945152282715\n",
      "steps = 23, loss = 5.288036346435547\n",
      "steps = 23, loss = 2.972992420196533\n",
      "steps = 23, loss = 50.00435256958008\n",
      "steps = 23, loss = 3.0029828548431396\n",
      "steps = 24, loss = 2.47503924369812\n",
      "steps = 24, loss = 2.9322381019592285\n",
      "steps = 24, loss = 3.044694185256958\n",
      "steps = 24, loss = 3.1685168743133545\n",
      "steps = 24, loss = 2.8411059379577637\n",
      "steps = 24, loss = 2.04396915435791\n",
      "steps = 24, loss = 2.4600677490234375\n",
      "steps = 24, loss = 3.293618679046631\n",
      "steps = 24, loss = 2.1947805881500244\n",
      "steps = 24, loss = 2.4968690872192383\n",
      "steps = 24, loss = 2.2947187423706055\n",
      "steps = 24, loss = 4.115182399749756\n",
      "steps = 24, loss = 2.126706123352051\n",
      "steps = 24, loss = 3.2972192764282227\n",
      "steps = 24, loss = 50.010562896728516\n",
      "steps = 24, loss = 2.4920828342437744\n",
      "steps = 24, loss = 2.3023104667663574\n",
      "steps = 24, loss = 2.8539772033691406\n",
      "steps = 24, loss = 2.908396005630493\n",
      "steps = 24, loss = 2.703507661819458\n",
      "steps = 24, loss = 3.1313581466674805\n",
      "steps = 24, loss = 2.4203057289123535\n",
      "steps = 24, loss = 5.389432430267334\n",
      "steps = 24, loss = 49.997989654541016\n",
      "steps = 24, loss = 2.436610698699951\n",
      "steps = 24, loss = 5.563391208648682\n",
      "steps = 24, loss = 2.8544039726257324\n",
      "steps = 24, loss = 2.7081291675567627\n",
      "steps = 24, loss = 3.4687488079071045\n",
      "steps = 24, loss = 3.264061450958252\n",
      "steps = 24, loss = 3.570704460144043\n",
      "steps = 24, loss = 3.399409532546997\n",
      "steps = 24, loss = 2.015714168548584\n",
      "steps = 24, loss = 3.3290743827819824\n",
      "steps = 24, loss = 2.227088451385498\n",
      "steps = 24, loss = 3.3982911109924316\n",
      "steps = 24, loss = 2.737651824951172\n",
      "steps = 24, loss = 2.860715627670288\n",
      "steps = 24, loss = 4.302393913269043\n",
      "steps = 24, loss = 7.777547836303711\n",
      "steps = 24, loss = 2.589143753051758\n",
      "steps = 24, loss = 2.6870365142822266\n",
      "steps = 24, loss = 3.222510576248169\n",
      "steps = 24, loss = 2.201712131500244\n",
      "steps = 24, loss = 2.6795074939727783\n",
      "steps = 24, loss = 2.8206372261047363\n",
      "steps = 24, loss = 2.9017722606658936\n",
      "steps = 24, loss = 2.5396568775177\n",
      "steps = 24, loss = 3.0963656902313232\n",
      "steps = 24, loss = 2.9955387115478516\n",
      "steps = 24, loss = 50.00544357299805\n",
      "steps = 24, loss = 2.201213836669922\n",
      "steps = 24, loss = 8.042521476745605\n",
      "steps = 24, loss = 3.384673833847046\n",
      "steps = 24, loss = 2.6881167888641357\n",
      "steps = 24, loss = 3.1662373542785645\n",
      "steps = 24, loss = 2.138047218322754\n",
      "steps = 24, loss = 3.004866600036621\n",
      "steps = 25, loss = 3.1990504264831543\n",
      "steps = 25, loss = 2.865659713745117\n",
      "steps = 25, loss = 2.1391563415527344\n",
      "steps = 25, loss = 2.7051713466644287\n",
      "steps = 25, loss = 3.0337038040161133\n",
      "steps = 25, loss = 3.3611466884613037\n",
      "steps = 25, loss = 2.8512768745422363\n",
      "steps = 25, loss = 2.713458776473999\n",
      "steps = 25, loss = 3.3500850200653076\n",
      "steps = 25, loss = 2.8563740253448486\n",
      "steps = 25, loss = 2.6502833366394043\n",
      "steps = 25, loss = 2.701530933380127\n",
      "steps = 25, loss = 2.6153404712677\n",
      "steps = 25, loss = 3.0431458950042725\n",
      "steps = 25, loss = 1.9923282861709595\n",
      "steps = 25, loss = 50.012081146240234\n",
      "steps = 25, loss = 2.2295081615448\n",
      "steps = 25, loss = 3.190162181854248\n",
      "steps = 25, loss = 5.691205978393555\n",
      "steps = 25, loss = 2.9849448204040527\n",
      "steps = 25, loss = 3.4213976860046387\n",
      "steps = 25, loss = 2.9939589500427246\n",
      "steps = 25, loss = 7.4689106941223145\n",
      "steps = 25, loss = 50.0119514465332\n",
      "steps = 25, loss = 3.4488658905029297\n",
      "steps = 25, loss = 3.306396722793579\n",
      "steps = 25, loss = 3.2758002281188965\n",
      "steps = 25, loss = 2.3938279151916504\n",
      "steps = 25, loss = 5.230583667755127\n",
      "steps = 25, loss = 2.210747241973877\n",
      "steps = 25, loss = 2.3374369144439697\n",
      "steps = 25, loss = 2.4409053325653076\n",
      "steps = 25, loss = 1.3915868997573853\n",
      "steps = 25, loss = 2.9431283473968506\n",
      "steps = 25, loss = 2.5475940704345703\n",
      "steps = 25, loss = 2.850417375564575\n",
      "steps = 25, loss = 2.2155227661132812\n",
      "steps = 25, loss = 4.4176859855651855\n",
      "steps = 25, loss = 3.7062742710113525\n",
      "steps = 25, loss = 7.3145527839660645\n",
      "steps = 25, loss = 3.3328492641448975\n",
      "steps = 25, loss = 3.057342052459717\n",
      "steps = 25, loss = 2.5344078540802\n",
      "steps = 25, loss = 3.3771140575408936\n",
      "steps = 25, loss = 4.133566856384277\n",
      "steps = 25, loss = 2.040147542953491\n",
      "steps = 25, loss = 2.8554811477661133\n",
      "steps = 25, loss = 50.00498962402344\n",
      "steps = 25, loss = 2.506192207336426\n",
      "steps = 25, loss = 3.3133232593536377\n",
      "steps = 25, loss = 2.227912425994873\n",
      "steps = 25, loss = 3.2625417709350586\n",
      "steps = 25, loss = 2.661848783493042\n",
      "steps = 25, loss = 3.1223936080932617\n",
      "steps = 25, loss = 2.767017364501953\n",
      "steps = 25, loss = 2.049860715866089\n",
      "steps = 25, loss = 2.495840549468994\n",
      "steps = 25, loss = 2.6877102851867676\n",
      "steps = 26, loss = 2.905437469482422\n",
      "steps = 26, loss = 50.000633239746094\n",
      "steps = 26, loss = 2.554112195968628\n",
      "steps = 26, loss = 2.1376750469207764\n",
      "steps = 26, loss = 50.0119514465332\n",
      "steps = 26, loss = 50.00606155395508\n",
      "steps = 26, loss = 3.3176798820495605\n",
      "steps = 26, loss = 2.214604377746582\n",
      "steps = 26, loss = 3.207472085952759\n",
      "steps = 26, loss = 2.5653412342071533\n",
      "steps = 26, loss = 2.986691474914551\n",
      "steps = 26, loss = 2.299739122390747\n",
      "steps = 26, loss = 3.176551580429077\n",
      "steps = 26, loss = 2.9741291999816895\n",
      "CPU times: user 15min 40s, sys: 1min 59s, total: 17min 40s\n",
      "Wall time: 1h 34min 24s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IncrementalSearchCV(decay_rate=0,\n",
       "                    estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                    max_iter=250, n_initial_parameters=58,\n",
       "                    parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                                &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;,\n",
       "                                                       &#x27;ELU&#x27;, &#x27;PReLU&#x27;],\n",
       "                                &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                                 &#x27;xavier_normal_&#x27;,\n",
       "                                                 &#x27;kaiming_uniform_&#x27;,\n",
       "                                                 &#x27;kaiming_norm...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                                &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                                &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, ...],\n",
       "                                &#x27;train_split&#x27;: [None]},\n",
       "                    patience=25, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;IncrementalSearchCV<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>IncrementalSearchCV(decay_rate=0,\n",
       "                    estimator=&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       "),\n",
       "                    max_iter=250, n_initial_parameters=58,\n",
       "                    parameters={&#x27;batch_size&#x27;: [32, 64, 128, 256, 512],\n",
       "                                &#x27;module__activation&#x27;: [&#x27;ReLU&#x27;, &#x27;LeakyReLU&#x27;,\n",
       "                                                       &#x27;ELU&#x27;, &#x27;PReLU&#x27;],\n",
       "                                &#x27;module__init&#x27;: [&#x27;xavier_uniform_&#x27;,\n",
       "                                                 &#x27;xavier_normal_&#x27;,\n",
       "                                                 &#x27;kaiming_uniform_&#x27;,\n",
       "                                                 &#x27;kaiming_norm...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                                &#x27;optimizer__nesterov&#x27;: [True],\n",
       "                                &#x27;optimizer__weight_decay&#x27;: [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, ...],\n",
       "                                &#x27;train_split&#x27;: [None]},\n",
       "                    patience=25, random_state=42)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">TrimParams</label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;__main__.TrimParams&#x27;&gt;[uninitialized](\n",
       "  module=&lt;class &#x27;autoencoder.Autoencoder&#x27;&gt;,\n",
       ")</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "IncrementalSearchCV(decay_rate=0,\n",
       "                    estimator=<class '__main__.TrimParams'>[uninitialized](\n",
       "  module=<class 'autoencoder.Autoencoder'>,\n",
       "),\n",
       "                    max_iter=250, n_initial_parameters=58,\n",
       "                    parameters={'batch_size': [32, 64, 128, 256, 512],\n",
       "                                'module__activation': ['ReLU', 'LeakyReLU',\n",
       "                                                       'ELU', 'PReLU'],\n",
       "                                'module__init': ['xavier_uniform_',\n",
       "                                                 'xavier_normal_',\n",
       "                                                 'kaiming_uniform_',\n",
       "                                                 'kaiming_norm...\n",
       "       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,\n",
       "       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,\n",
       "       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,\n",
       "       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),\n",
       "                                'optimizer__nesterov': [True],\n",
       "                                'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, 0, 0, 0, 0, 0,\n",
       "                                                            0, 0, ...],\n",
       "                                'train_split': [None]},\n",
       "                    patience=25, random_state=42)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "patience_search.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9923282861709595"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patience_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_search(patience_search, today, \"patience\", X_test.compute(), y_test.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_stats = client.profile()\n",
    "with open(f\"{absolutepath_to_results}/final-timings.json\", \"w\") as f:\n",
    "    json.dump(timing_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, fig = client.get_task_stream(plot=\"save\", filename=f\"{absolutepath_to_results}/task_stream.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>worker</th>\n",
       "      <th>nbytes</th>\n",
       "      <th>type</th>\n",
       "      <th>typename</th>\n",
       "      <th>metadata</th>\n",
       "      <th>thread</th>\n",
       "      <th>startstops</th>\n",
       "      <th>status</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>task-finished-1707309902.2195184</td>\n",
       "      <td>tcp://127.0.0.1:35239</td>\n",
       "      <td>2148</td>\n",
       "      <td>b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>tuple</td>\n",
       "      <td>{}</td>\n",
       "      <td>140008744806144</td>\n",
       "      <td>({'action': 'compute', 'start': 1707309898.278...</td>\n",
       "      <td>OK</td>\n",
       "      <td>_partial_fit-04849501-587d-4e65-81b0-661db8c57329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>task-finished-1707309906.142285</td>\n",
       "      <td>tcp://127.0.0.1:40671</td>\n",
       "      <td>2153</td>\n",
       "      <td>b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>tuple</td>\n",
       "      <td>{}</td>\n",
       "      <td>140395937789696</td>\n",
       "      <td>({'action': 'compute', 'start': 1707309895.645...</td>\n",
       "      <td>OK</td>\n",
       "      <td>_partial_fit-9af09ab7-1d0d-44c4-8411-bee171e6bda0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>task-finished-1707309908.2306275</td>\n",
       "      <td>tcp://127.0.0.1:35239</td>\n",
       "      <td>2146</td>\n",
       "      <td>b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>tuple</td>\n",
       "      <td>{}</td>\n",
       "      <td>140008744806144</td>\n",
       "      <td>({'action': 'compute', 'start': 1707309902.224...</td>\n",
       "      <td>OK</td>\n",
       "      <td>_partial_fit-285c7a7a-b62b-4519-aeb9-f36dd20f66a8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>task-finished-1707309910.1030731</td>\n",
       "      <td>tcp://127.0.0.1:40671</td>\n",
       "      <td>2147</td>\n",
       "      <td>b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>tuple</td>\n",
       "      <td>{}</td>\n",
       "      <td>140395937789696</td>\n",
       "      <td>({'action': 'compute', 'start': 1707309906.147...</td>\n",
       "      <td>OK</td>\n",
       "      <td>_partial_fit-7a2c1360-3b0e-4048-8304-e653b228b1b7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>task-finished-1707309912.542559</td>\n",
       "      <td>tcp://127.0.0.1:35239</td>\n",
       "      <td>2148</td>\n",
       "      <td>b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>tuple</td>\n",
       "      <td>{}</td>\n",
       "      <td>140008744806144</td>\n",
       "      <td>({'action': 'compute', 'start': 1707309908.234...</td>\n",
       "      <td>OK</td>\n",
       "      <td>_partial_fit-12462be2-5d9e-4f0a-8c96-45691442c2b7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        stimulus_id                 worker  nbytes  \\\n",
       "0  task-finished-1707309902.2195184  tcp://127.0.0.1:35239    2148   \n",
       "1   task-finished-1707309906.142285  tcp://127.0.0.1:40671    2153   \n",
       "2  task-finished-1707309908.2306275  tcp://127.0.0.1:35239    2146   \n",
       "3  task-finished-1707309910.1030731  tcp://127.0.0.1:40671    2147   \n",
       "4   task-finished-1707309912.542559  tcp://127.0.0.1:35239    2148   \n",
       "\n",
       "                                                type typename metadata  \\\n",
       "0  b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...    tuple       {}   \n",
       "1  b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...    tuple       {}   \n",
       "2  b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...    tuple       {}   \n",
       "3  b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...    tuple       {}   \n",
       "4  b'\\x80\\x05\\x95\\x16\\x00\\x00\\x00\\x00\\x00\\x00\\x00...    tuple       {}   \n",
       "\n",
       "            thread                                         startstops status  \\\n",
       "0  140008744806144  ({'action': 'compute', 'start': 1707309898.278...     OK   \n",
       "1  140395937789696  ({'action': 'compute', 'start': 1707309895.645...     OK   \n",
       "2  140008744806144  ({'action': 'compute', 'start': 1707309902.224...     OK   \n",
       "3  140395937789696  ({'action': 'compute', 'start': 1707309906.147...     OK   \n",
       "4  140008744806144  ({'action': 'compute', 'start': 1707309908.234...     OK   \n",
       "\n",
       "                                                 key  \n",
       "0  _partial_fit-04849501-587d-4e65-81b0-661db8c57329  \n",
       "1  _partial_fit-9af09ab7-1d0d-44c4-8411-bee171e6bda0  \n",
       "2  _partial_fit-285c7a7a-b62b-4519-aeb9-f36dd20f66a8  \n",
       "3  _partial_fit-7a2c1360-3b0e-4048-8304-e653b228b1b7  \n",
       "4  _partial_fit-12462be2-5d9e-4f0a-8c96-45691442c2b7  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 24 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_hat = patience_search.best_estimator_.predict(noisy_test)\n",
    "\n",
    "cols = 8\n",
    "w = 1.0\n",
    "fig, axs = plt.subplots(figsize=(cols*w, 3*w), ncols=cols, nrows=3)\n",
    "\n",
    "rng = check_random_state(42)\n",
    "for col, (upper, middle, lower) in enumerate(zip(axs[0], axs[1], axs[2])):\n",
    "    if col == 0:\n",
    "        upper.text(-28, 14, 'ground\\ntruth')\n",
    "        middle.text(-28, 14, 'input')\n",
    "        lower.text(-28, 14, 'output')\n",
    "    i = rng.choice(len(X_test))\n",
    "    noisy = X_test[i].reshape(28, 28)\n",
    "    clean = y_test[i].reshape(28, 28)\n",
    "    clean_hat_i = clean_hat[i][1].reshape(28,28)\n",
    "    kwargs = {'cbar': False, 'xticklabels': False, 'yticklabels': False, 'cmap': 'gray_r'}\n",
    "    sns.heatmap(noisy, ax=middle, **kwargs)\n",
    "    sns.heatmap(clean, ax=upper, **kwargs)\n",
    "    sns.heatmap(clean_hat_i, ax=lower, **kwargs)\n",
    "plt.savefig(f\"{absolutepath_to_results}/patience-best-out.svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This notebook demonstrates hyperparameter optimization for image denoising using Dask-ML. It compares different algorithms and highlights the importance of choosing appropriate parameters for each approach.\n",
    "\n",
    "This enhanced script provides clear explanations, code comments, and visualizations to effectively teach hyperparameter optimization concepts and best practices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
