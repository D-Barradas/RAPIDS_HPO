{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D-Barradas/RAPIDS_HPO/blob/main/notebooks/HPO_cuML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciMBynkBaCLF"
      },
      "source": [
        "# Hyper-parameter Optimization with Sklearn + RAPIDS\n",
        "\n",
        "## Introduction\n",
        "\n",
        "&emsp; &emsp; &emsp; [Hyperparameter optimization](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview) is the task of picking the values for the hyperparameters of the model that provide the optimal results for the problem, as measured on a specific test dataset. This is often a crucial step and can help boost the model accuracy when done correctly. Cross-validation is often used to more accurately estimate the performance of the models in the search process. Cross-validation is the method of splitting the training set into complementary subsets and performing training on one of the subsets, then predicting the models performance on the other. This is a potential indication of how the model will generalise to data it has not seen before.\n",
        "\n",
        "Despite its theoretical importance, HPO has been difficult to implement in practical applications because of the resources needed to run so many distinct training jobs.\n",
        "\n",
        "The two approaches that we will be exploring in this notebook are :\n",
        "\n",
        "\n",
        "#### 1. GridSearch\n",
        "\n",
        "&emsp; &emsp; &emsp; As the name suggests, the \"search\" is done over each possible combination in a grid of parameters that the user provides. The user must manually define this grid.. For each parameter that needs to be tuned, a set of values are given and the final grid search is performed with tuple having one element from each set, thus resulting in a Catersian Product of the elements.\n",
        "\n",
        "&emsp; &emsp; &emsp;For example, assume we want to perform HPO on XGBoost. For simplicity lets tune only `n_estimators` and `max_depth`\n",
        "\n",
        "&emsp; &emsp; &emsp;`n_estimators: [50, 100, 150]`\n",
        "\n",
        "&emsp; &emsp; &emsp;`max_depth: [6, 7, ,8]`\n",
        "    \n",
        "&emsp; &emsp; &emsp; The grid search will take place over |n_estimators| x |max_depth| which is 3 x 3 = 9. As you have probably guessed, the grid size grows rapidly as the number of parameters and their search space increases.\n",
        "\n",
        "#### 2. RandomSearch\n",
        "\n",
        "\n",
        "&emsp; &emsp; &emsp; [Random Search](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) replaces the exhaustive nature of the search from before with a random selection of parameters over the specified space. This method can outperform GridSearch in cases where the number of parameters affecting the model's performance is small (low-dimension optimization problems). Since this does not pick every tuple from the cartesian product, it tends to yield results faster, and the performance can be comparable to that of the Grid Search approach. It's worth keeping in mind that the random nature of this search means, the results with each run might differ.\n",
        "\n",
        "Some of the other methods used for HPO include:\n",
        "\n",
        "1. Bayesian Optimization\n",
        "\n",
        "2. Gradient-based Optimization\n",
        "\n",
        "3. Evolutionary Optimization\n",
        "\n",
        "To learn more about HPO, some papers are linked to at the end of the notebook for further reading.\n",
        "\n",
        "Now that we have a basic understanding of what HPO is, let's discuss what we wish to achieve with this demo. The aim of this notebook is to show the importance of hyper parameter optimisation and the performance of dask-ml GPU for xgboost and cuML-RF.\n",
        "\n",
        "For this demo, we will be using the [Airline dataset](http://kt.ijs.si/elena_ikonomovska/data.html). The aim of the problem is to predict the arrival delay. It has about 116 million entries with 13 attributes that are used to determine the delay for a given airline. We have modified this problem to serve as a binary classification problem to determine if the airline will be delayed (True) or not.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHGD6r__aCLH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cudf\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.model_selection import KFold, ParameterSampler, RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "from cuml.model_selection import train_test_split\n",
        "from cuml.metrics import accuracy_score\n",
        "\n",
        "import time\n",
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "import gzip\n",
        "import glob\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XalFHZjdDyC"
      },
      "source": [
        "## Mount the google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQr3DXqadC8O"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIqKQqcBaCLI"
      },
      "source": [
        "## Setup parameters  \n",
        "NOTE: You must execute the download_data notebook before. It migth take up to 15 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjBitq_ZaCLI"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_rows = 2500000  # number of rows to be used in this notebook\n",
        "\n",
        "# the parrent dir path is important to place the data and results in the correct flder\n",
        "# parent_dir = \"/\".join(os.getcwd().split(\"/\")[:-1])\n",
        "parent_dir = \"/content/drive/MyDrive/\"\n",
        "\n",
        "data_dir = os.path.join(parent_dir, \"data\", \"airline-data\")\n",
        "\n",
        "# Create data and airline-data directories if they don't exist\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "orc_name = os.path.join(data_dir, \"airline-data\" + str(num_rows) + \".orc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRX0KBvHWyd2"
      },
      "outputs": [],
      "source": [
        "# Download the file from Google Drive\n",
        "# This link is for the airline-data2500000.orc file\n",
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=1DVuLjbHu-oAc-IkGJMlCk44gD1nBpQkC\"\n",
        "gdown.download(url, orc_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKUaIASeaCLJ"
      },
      "source": [
        "## Check the data is downloaded , otherwise execute the download_data.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsxBQCOFaCLJ"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset():\n",
        "    try:\n",
        "        if os.path.isfile(orc_name):\n",
        "            df = cudf.read_orc(orc_name)\n",
        "            df = df.drop(\"index\",axis=1)\n",
        "                # encode categoricals as numeric\n",
        "            for col in df.select_dtypes([\"object\"]).columns:\n",
        "                df[col] = df[col].astype(\"category\").cat.codes.astype(np.int32)\n",
        "\n",
        "            # cast all columns to int32\n",
        "            for col in df.columns:\n",
        "                df[col] = df[col].astype(np.float32)  # needed for random forest\n",
        "            # reduce the size of the data for colab\n",
        "            # we will use 10 K rows\n",
        "            # df = df.head(10000)\n",
        "\n",
        "            return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Archive '{orc_name}' not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pq0lCjuQaCLJ"
      },
      "outputs": [],
      "source": [
        "df = prepare_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czOcJ_7VaCLJ"
      },
      "outputs": [],
      "source": [
        "type(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKGb67XWaCLK"
      },
      "outputs": [],
      "source": [
        "# Double check the precesence of Nan\n",
        "for col in df.columns.to_list() :\n",
        "    nan_vals = len ( df[df[col].isna()== True ])\n",
        "    print (col , nan_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Phd2OYxaaCLK"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from contextlib import contextmanager\n",
        "# Helping time blocks of code\n",
        "@contextmanager\n",
        "def timed(txt):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    t1 = time.time()\n",
        "    print(\"%32s time:  %8.5f\" % (txt, t1 - t0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpXRYjh1aCLK"
      },
      "outputs": [],
      "source": [
        "# Define some default values to make use of across the notebook for a fair comparison\n",
        "N_FOLDS = 3\n",
        "N_ITER = 10  # The number of times that search combinations of parameters\n",
        "SEED = check_random_state(73)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e5XBYSsaCLK"
      },
      "outputs": [],
      "source": [
        "label = 'ArrDelayBinary'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2-8FbijaCLK"
      },
      "source": [
        "## Splitting Data\n",
        "\n",
        "We split the data randomnly into train and test sets using the [cuml train_test_split](https://rapidsai.github.io/projects/cuml/en/0.12.0/api.html#cuml.preprocessing.model_selection.train_test_split) and create CPU versions of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh-YdszLaCLK"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df, label, test_size=0.2, random_state=SEED )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe2fqN6naCLK"
      },
      "source": [
        "#### Get the data into the CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wj8Q1gPaCLL"
      },
      "outputs": [],
      "source": [
        "X_cpu = X_train.to_pandas()\n",
        "y_cpu = y_train.to_numpy()\n",
        "\n",
        "X_test_cpu = X_test.to_pandas()\n",
        "y_test_cpu = y_test.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ16w2lDaCLL"
      },
      "source": [
        "## Setup Custom cuML scorers\n",
        "\n",
        "The search functions (such as GridSearchCV) for scikit-learn and dask-ml expect the metric functions (such as accuracy_score) to match the “scorer” API. This can be achieved using the scikit-learn's [make_scorer](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) function.\n",
        "\n",
        "We will generate a `cuml_scorer` with the cuML `accuracy_score` function.  You'll also notice an `accuracy_score_wrapper` which primarily converts the y label into a `float32` type. This is because some cuML models only accept this type for now and in order to make it compatible, we perform this conversion.\n",
        "\n",
        "We also create helper functions for performing HPO in 2 different modes:\n",
        "1. `gpu-grid`: Perform GPU based GridSearchCV\n",
        "2. `gpu-random`: Perform GPU based RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-jVuQ1YaCLL"
      },
      "outputs": [],
      "source": [
        "def accuracy_score_wrapper(y, y_hat):\n",
        "    \"\"\"\n",
        "        A wrapper function to convert labels to float32,\n",
        "        and pass it to accuracy_score.\n",
        "\n",
        "        Params:\n",
        "        - y: The y labels that need to be converted\n",
        "        - y_hat: The predictions made by the model\n",
        "    \"\"\"\n",
        "    y = y.astype(\"float32\") # cuML RandomForest needs the y labels to be float32\n",
        "    return accuracy_score(y, y_hat)\n",
        "\n",
        "accuracy_wrapper_scorer = make_scorer(accuracy_score_wrapper)\n",
        "cuml_accuracy_scorer = make_scorer(accuracy_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N0a2mHZaCLL"
      },
      "source": [
        "#### This creates a wrapper to help us select the details of the HPO we want to compare and run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ml0aepc6aCLL"
      },
      "outputs": [],
      "source": [
        "def do_HPO(model, gridsearch_params, scorer, X, y, mode='gpu-Grid', n_iter=10):\n",
        "    \"\"\"\n",
        "        Perform HPO based on the mode specified\n",
        "\n",
        "        mode: default gpu-Grid. The possible options are:\n",
        "        1. gpu-grid: Perform GPU based GridSearchCV\n",
        "        2. gpu-random: Perform GPU based RandomizedSearchCV\n",
        "\n",
        "        n_iter: specified with Random option for number of parameter settings sampled\n",
        "\n",
        "        Returns the best estimator and the results of the search\n",
        "    \"\"\"\n",
        "    if mode == 'gpu-grid':\n",
        "        print(\"gpu-grid selected\")\n",
        "        clf = GridSearchCV(model,\n",
        "                               gridsearch_params,\n",
        "                               cv=N_FOLDS,\n",
        "                               scoring=scorer)\n",
        "    elif mode == 'gpu-random':\n",
        "        print(\"gpu-random selected\")\n",
        "        clf = RandomizedSearchCV(model,\n",
        "                               gridsearch_params,\n",
        "                               cv=N_FOLDS,\n",
        "                               scoring=scorer,\n",
        "                               n_iter=n_iter)\n",
        "\n",
        "    else:\n",
        "        print(\"Unknown Option, please choose one of [gpu-grid, gpu-random]\")\n",
        "        return None, None\n",
        "    res = clf.fit(X, y)\n",
        "    print(\"Best clf and score {} {}\\n---\\n\".format(res.best_estimator_, res.best_score_))\n",
        "    return res.best_estimator_, res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48b10855"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def manual_random_search_xgb(model, param_distributions, X, y, n_iter=10, n_splits=N_FOLDS, random_state=SEED):\n",
        "    \"\"\"\n",
        "    Performs manual randomized search cross-validation for XGBoost using KFold.\n",
        "\n",
        "    Args:\n",
        "        model: The cuML/XGBoost model to train.\n",
        "        param_distributions: Dictionary with parameters names (string) as keys and distributions\n",
        "            or lists of parameters to sample from as values.\n",
        "        X: cuDF DataFrame of features.\n",
        "        y: cuDF Series of labels.\n",
        "        n_iter: Number of parameter settings that are sampled.\n",
        "        n_splits: Number of folds for cross-validation.\n",
        "        random_state: Random state for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the best estimator and a dictionary of results.\n",
        "    \"\"\"\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    param_sampler = ParameterSampler(param_distributions, n_iter=n_iter, random_state=random_state)\n",
        "\n",
        "    best_score = -float('inf')\n",
        "    best_params = None\n",
        "    cv_results = []\n",
        "\n",
        "    # Need to compute the splits once as they are dask delayed objects\n",
        "    splits = list(kf.split(X))\n",
        "\n",
        "    for i, params in enumerate(param_sampler):\n",
        "        print(f\"Fitting iteration {i+1}/{n_iter} with parameters: {params}\")\n",
        "        fold_scores = []\n",
        "\n",
        "        for fold_idx, (train_idx, val_idx) in enumerate(splits):\n",
        "            # Use cuDF indexing with computed indices\n",
        "            # Compute the indices once per iteration to avoid recomputing in each fold\n",
        "            train_idx_computed = train_idx\n",
        "            val_idx_computed = val_idx\n",
        "\n",
        "            X_train_fold = X.iloc[train_idx_computed]\n",
        "            y_train_fold = y.iloc[train_idx_computed]\n",
        "            X_val_fold = X.iloc[val_idx_computed]\n",
        "            y_val_fold = y.iloc[val_idx_computed]\n",
        "\n",
        "            # Ensure y is float32 for cuML\n",
        "            y_train_fold = y_train_fold.astype('float32')\n",
        "            y_val_fold = y_val_fold.astype('float32')\n",
        "\n",
        "\n",
        "            # Train the model with current parameters\n",
        "            current_model = model.set_params(**params)\n",
        "            current_model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "            # Evaluate on the validation fold\n",
        "            y_pred_fold = current_model.predict(X_val_fold)\n",
        "            score = accuracy_score(y_val_fold, y_pred_fold)\n",
        "            fold_scores.append(score)\n",
        "\n",
        "        mean_score = np.mean(fold_scores)\n",
        "        cv_results.append({'params': params, 'mean_test_score': mean_score, 'fold_scores': fold_scores})\n",
        "\n",
        "        if mean_score > best_score:\n",
        "            best_score = mean_score\n",
        "            best_params = params\n",
        "            # Store the best estimator found during CV (optional, but can be useful)\n",
        "            best_cv_estimator = current_model\n",
        "\n",
        "\n",
        "    print(\"\\n---\")\n",
        "    print(f\"Best parameters found: {best_params}\")\n",
        "    print(f\"Best cross-validation score: {best_score}\")\n",
        "    print(\"---\")\n",
        "\n",
        "    # Train the final model on the full training data with best params\n",
        "    final_model = model.set_params(**best_params)\n",
        "    final_model.fit(X, y.astype('float32')) # Ensure y is float32 for final training\n",
        "\n",
        "    results_dict = {\n",
        "        'best_estimator_': final_model,\n",
        "        'best_score_': best_score,\n",
        "        'best_params_': best_params,\n",
        "        'cv_results_': cv_results\n",
        "    }\n",
        "\n",
        "    return final_model, results_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGMLjuLmaCLL"
      },
      "outputs": [],
      "source": [
        "def print_acc(model, X_train, y_train, X_test, y_test, mode_str=\"Default\"):\n",
        "    \"\"\"\n",
        "        Trains a model on the train data provided, and prints the accuracy of the trained model.\n",
        "        mode_str: User specifies what model it is to print the value\n",
        "    \"\"\"\n",
        "    y_pred = model.fit(X_train, y_train).predict(X_test)\n",
        "    score = accuracy_score(y_pred, y_test.astype('float32'))\n",
        "    print(\"{} model accuracy: {}\".format(mode_str, score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMtc_cGxaCLL"
      },
      "source": [
        "#### Look at trainin set shape, we are going to read 200k rows and 13 columns as features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKWfITTuaCLL"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoB-8gZzaCLM"
      },
      "source": [
        "## Launch HPO\n",
        "\n",
        "We will first see the model's performances without the gridsearch and then compare it with the performance after searching.\n",
        "\n",
        "### XGBoost\n",
        "\n",
        "To perform the Hyperparameter Optimization, we make use of the sklearn version of the [XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn).We're making use of this version to make it compatible and easily comparable to the scikit-learn version. The model takes a set of parameters that can be found in the documentation. We're primarily interested in the `max_depth`, `learning_rate`, `min_child_weight`, `reg_alpha` and `num_round` as these affect the performance of XGBoost the most.\n",
        "\n",
        "Read more about what these parameters are useful for [here](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
        "\n",
        "#### Default Performance\n",
        "\n",
        "We first use the model with it's default parameters and see the accuracy of the model. In this case, it is 84%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yriOoj6aCLM"
      },
      "outputs": [],
      "source": [
        "model_gpu_xgb_ = xgb.XGBClassifier(tree_method='hist',device = 'cuda',random_state=SEED)\n",
        "print_acc(model_gpu_xgb_, X_train, y_cpu, X_test, y_test_cpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vJ98HDLaCLM"
      },
      "outputs": [],
      "source": [
        "# ## Random Forest\n",
        "# model_rf_ = RandomForestClassifier()\n",
        "\n",
        "# print(\"Default acc: \",accuracy_score(model_rf_.fit(X_train, y_train).predict(X_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuDWYRy4aCLM"
      },
      "source": [
        "#### Lets define a set of parameters to explore with model of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYjjydPuaCLM"
      },
      "outputs": [],
      "source": [
        "# # For rf_model\n",
        "# model_rf = RandomForestClassifier()\n",
        "\n",
        "# # range\n",
        "# params_rf = {\n",
        "#     \"max_depth\": np.arange(start=3, stop = 15, step = 2), # Default = 6\n",
        "#     \"max_features\": [0.1, 0.50, 0.75, 'auto'], #default = 0.3\n",
        "#     \"n_estimators\": [100, 200, 1000]\n",
        "#             }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt2iMSJQaCLM"
      },
      "source": [
        "#### Parameter Distributions\n",
        "\n",
        "The way we define the grid to perform the search is by including ranges of parameters that need to be used for the search. In this example we make use of [np.arange](https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html) which returns an ndarray of even spaced values, [np.logspace](https://docs.scipy.org/doc/numpy/reference/generated/numpy.logspace.html#numpy.logspace) returns a specified number of ssamples that are equally spaced on the log scale. We can also specify as lists, NumPy arrays or make use of any random variate sample that gives a sample when called. SciPy provides various functions for this too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMVjdORJaCLM"
      },
      "outputs": [],
      "source": [
        "# For xgb_model\n",
        "model_gpu_xgb = xgb.XGBClassifier(tree_method='hist',device = 'cuda',random_state=SEED)\n",
        "\n",
        "# More range\n",
        "params_xgb = {\n",
        "    \"max_depth\": np.arange(start=3, stop = 12, step = 3), # Default = 6\n",
        "    \"alpha\" : np.logspace(-3, -1, 5), # default = 0\n",
        "    \"learning_rate\": [0.05, 0.1, 0.15], #default = 0.3\n",
        "    \"min_child_weight\" : np.arange(start=2, stop=10, step=3), # default = 1\n",
        "    \"n_estimators\": [100, 200, 1000]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHxu-ZzQaCLM"
      },
      "source": [
        "#### RandomizedSearchCV\n",
        "\n",
        "We'll now try [RandomizedSearchCV](https://dask-ml.readthedocs.io/en/latest/modules/generated/dask_ml.model_selection.RandomizedSearchCV.html).\n",
        "`n_iter` specifies the number of parameters points theat the search needs to perform. Here we will search `N_ITER` (defined earlier) points for the best performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnOgeSbvaCLM"
      },
      "outputs": [],
      "source": [
        "mode = \"manual-gpu-random-xgb\"\n",
        "\n",
        "with timed(\"XGB-\"+mode):\n",
        "    # Use the manual random search function\n",
        "    res, results = manual_random_search_xgb(model_gpu_xgb,\n",
        "                                   params_xgb,\n",
        "                                   X_train, # Pass cuDF DataFrame\n",
        "                                   y_train, # Pass cuDF Series\n",
        "                                   n_iter=N_ITER,\n",
        "                                   n_splits=N_FOLDS,\n",
        "                                   random_state=SEED)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRpSjDUOaCLM"
      },
      "outputs": [],
      "source": [
        "print_acc(model_gpu_xgb_, X_train, y_cpu, X_test, y_test_cpu)\n",
        "print_acc(res, X_train, y_cpu, X_test, y_test_cpu, mode_str=mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z__uDWuxCUbN"
      },
      "outputs": [],
      "source": [
        "mode = \"gpu-random\"\n",
        "\n",
        "with timed(\"XGB-\"+mode):\n",
        "    res, results = do_HPO(model_gpu_xgb,\n",
        "                                   params_xgb,\n",
        "                                   cuml_accuracy_scorer,\n",
        "                                   X_train,\n",
        "                                   y_cpu,\n",
        "                                   mode=mode,\n",
        "                                   n_iter=N_ITER)\n",
        "print(\"Searched over {} parameters\".format(len(results.cv_results_['mean_test_score'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYzpWAweXmVY"
      },
      "outputs": [],
      "source": [
        "print_acc(model_gpu_xgb_, X_train, y_cpu, X_test, y_test_cpu)\n",
        "print_acc(res, X_train, y_cpu, X_test, y_test_cpu, mode_str=mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5-62cbFaCLM"
      },
      "source": [
        "#### XgBoost improves\n",
        "With a very small set of parameter and with a random search we have a 0.863 accuracy vs 0.848 baseline accuracy a gain of 1% in about 2 minutes of testing, not bad. The main target for HPO is to explore wider ranges of parameters and usually with more data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4YyxxppaCLM"
      },
      "outputs": [],
      "source": [
        "# mode = \"gpu-random\"\n",
        "\n",
        "# with timed(\"RF-\"+mode):\n",
        "#     res, results = do_HPO(model_rf,\n",
        "#                           params_rf,\n",
        "#                           cuml_accuracy_scorer,\n",
        "#                           X_train.to_cupy().get(),\n",
        "#                           y_cpu,\n",
        "#                           mode=mode,\n",
        "#                          n_iter = N_ITER)\n",
        "# print(\"Searched over {} parameters\".format(len(results.cv_results_['mean_test_score'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5vEtTplaCLN"
      },
      "outputs": [],
      "source": [
        "# print(\"Improved acc: \",accuracy_score(res.predict(X_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WOx8XjsaCLS"
      },
      "source": [
        "#### Heatmaps\n",
        "   - Between parameter pairs (we can do a combination of all possible pairs, but only one are shown in this notebook)\n",
        "   - This gives a visual representation of how the pair affect the test score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-_mbumAAI5w"
      },
      "outputs": [],
      "source": [
        "def plot_heatmap(df, col1, col2):\n",
        "    \"\"\"\n",
        "    Plots a heatmap of the mean test score for combinations of two parameters.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing the results of a GridSearchCV or RandomizedSearchCV.\n",
        "        col1: Name of the first parameter column.\n",
        "        col2: Name of the second parameter column.\n",
        "    \"\"\"\n",
        "    # Exclude the 'params' column before grouping and aggregating\n",
        "    df_filtered = df.drop(columns=['params'], errors='ignore')\n",
        "    max_scores = df_filtered.groupby([col1, col2]).max()\n",
        "    max_scores = max_scores.unstack()[[\"mean_test_score\"]]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(max_scores.mean_test_score, annot=True, fmt=\".3g\")\n",
        "    plt.title(f'Heatmap of Mean Test Score for {col1} and {col2}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HIoj6E2-vUa"
      },
      "outputs": [],
      "source": [
        "df_randomsearch = pd.DataFrame(results.cv_results_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c1vgyB7aCLS"
      },
      "outputs": [],
      "source": [
        "plot_heatmap(df_randomsearch, \"param_max_depth\", \"param_n_estimators\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xSzpn51aCLS"
      },
      "source": [
        "### Now lets perfrom the Grid-search strategy. This will explore all the the possible combinations , we should skip this for the sake of time (takes about 1 hrs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3_ch2jLaCLS"
      },
      "outputs": [],
      "source": [
        "mode = \"gpu-grid\"\n",
        "\n",
        "# For xgb_model\n",
        "model_gpu_xgb = xgb.XGBClassifier(tree_method='gpu_hist',random_state=SEED)\n",
        "\n",
        "with timed(\"XGB-\"+mode):\n",
        "    res, results = do_HPO(model_gpu_xgb,\n",
        "                            params_xgb,\n",
        "                                   cuml_accuracy_scorer,\n",
        "                                   X_train,\n",
        "                                   y_cpu,\n",
        "                                   mode=mode)\n",
        "print(\"Searched over {} parameters\".format(len(results.cv_results_['mean_test_score'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDikalh7aCLT"
      },
      "outputs": [],
      "source": [
        "# mode = \"gpu-grid\"\n",
        "\n",
        "# # For rf_model\n",
        "# model_rf = RandomForestClassifier()\n",
        "\n",
        "# with timed(\"RF-\"+mode):\n",
        "#     res, results = do_HPO(model_rf,\n",
        "#                           params_rf,\n",
        "#                           cuml_accuracy_scorer,\n",
        "#                           X_train.to_cupy().get(),\n",
        "#                           y_cpu,\n",
        "#                           mode=mode,\n",
        "#                          n_iter = N_ITER)\n",
        "# print(\"Searched over {} parameters\".format(len(results.cv_results_['mean_test_score'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhwjNDUvaCLT"
      },
      "outputs": [],
      "source": [
        "print_acc(model_gpu_xgb_, X_train, y_cpu, X_test, y_test_cpu)\n",
        "print_acc(res, X_train, y_cpu, X_test, y_test_cpu, mode_str=mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6suf92JfaCLT"
      },
      "source": [
        "#### We have run the classifiers with two strategies its up to you decide which one is the best\n",
        "* **Xgboost baseline accuracy is 0.848**\n",
        "* **Xgboost with Random search is 0.869 accuracy -> 5 minutes**\n",
        "* **Xgboost with Grid search is 0.866 accuracy  -> 1 hour**\n",
        "\n",
        "#### Mean/Std of test scores\n",
        "\n",
        "We fix all parameters except one for each of these graphs and plot the effect the parameter has on the mean test score with the error bar indicating the standard deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6hGhApjaCLT"
      },
      "outputs": [],
      "source": [
        "df_gridsearch = pd.DataFrame(results.cv_results_)\n",
        "plot_heatmap(df_gridsearch, \"param_max_depth\", \"param_n_estimators\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fv2u8eaaCLT"
      },
      "source": [
        "## Conclusion and Next Steps\n",
        "\n",
        "We notice improvements in the performance for a really basic version of the GridSearch and RandomizedSearch. Generally, the more data we use, the better the model performs, so you are encouraged to try for larger data and broader range of parameters.\n",
        "\n",
        "This experiment can also be repeated with different classifiers and different ranges of parameters to notice how HPO can help improve the performance metric. In this example, we have chosen a basic metric - accuracy, but you can use more interesting metrics that help in determining the usefulness of a model. You can even send a list of parameters to the scoring function. This makes HPO really powerful, and it can add a significant boost to the model that we generate.\n",
        "\n",
        "\n",
        "#### Further Reading\n",
        "\n",
        "- [The 5 Classification Evaluation Metrics You Must Know](https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226)\n",
        "- [11 Important Model Evaluation Metrics for Machine Learning Everyone should know](https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/)\n",
        "- [Algorithms for Hyper-Parameter Optimisation](http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)\n",
        "- [Forward and Reverse Gradient-Based Hyperparameter Optimization](http://proceedings.mlr.press/v70/franceschi17a/franceschi17a-supp.pdf)\n",
        "- [Practical Bayesian Optimization of Machine\n",
        "Learning Algorithms](http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)\n",
        "- [Random Search for Hyper-Parameter Optimization](http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
