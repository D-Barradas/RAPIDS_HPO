{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tarfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import cudf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to only see the selected GPU \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \"/\".join(os.getcwd().split(\"/\")[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict =    {\n",
    "    \"airline-data.2005-2015.tar.gz\" : \"https://diybigdata.net/?sdm_process_download=1&download_id=392\",\n",
    "    \"airline-data.2003-2018.tar.gz\" : \"https://diybigdata.net/?sdm_process_download=1&download_id=821\",\n",
    "    \"airline-data.1998-2018.tar.gz\" : \"https://diybigdata.net/?sdm_process_download=1&download_id=832\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_data(key):\n",
    "    url = url_dict[key]\n",
    "    # Send GET request to the link\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Get filename from response headers (assuming Content-Disposition header is present)\n",
    "        filename = response.headers.get('Content-Disposition')\n",
    "        if filename:\n",
    "          filename = filename.split(\"=\")[1].strip('\"')\n",
    "        else:\n",
    "          # If filename unavailable, use a key as filename\n",
    "          filename = key\n",
    "\n",
    "        # Open a file for writing in binary mode\n",
    "        with open(f\"{parent_dir}/data/{filename}\", \"wb\") as f:\n",
    "          for chunk in response.iter_content(1024):\n",
    "            # Write downloaded data in chunks\n",
    "            f.write(chunk)\n",
    "        print(f\"File downloaded successfully: {filename}\")\n",
    "    else:\n",
    "        print(f\"Download failed with status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(fname):\n",
    "    if fname.endswith(\"tar.gz\"):\n",
    "        tar = tarfile.open(fname, \"r:gz\")\n",
    "        tar.extractall(filter=\"data\", path=f\"{parent_dir}/data/\")\n",
    "        tar.close()\n",
    "    elif fname.endswith(\"tar\"):\n",
    "        tar = tarfile.open(fname, \"r:\")\n",
    "        tar.extractall(filter=\"data\", path=f\"{parent_dir}/data/\")\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_small_data(key):\n",
    "\n",
    "    # Define archive filename\n",
    "    archive_filename = f\"{parent_dir}/data/{key}\"\n",
    "\n",
    "    # Open archive in read mode\n",
    "    try:\n",
    "        with tarfile.open(archive_filename, \"r:gz\") as tar:\n",
    "            # Extract all filenames\n",
    "            all_filenames = tar.getnames()\n",
    "            # print (all_filenames)\n",
    "\n",
    "            # Filter files starting with \"On_Time_On_Time_Performance_2003\"\n",
    "            filtered_files = [f for f in all_filenames if f.split(\"/\")[-1].startswith(\"On_Time_On_Time_Performance_2003\")]\n",
    "\n",
    "            if filtered_files:\n",
    "                print(\"Extracting files:\")\n",
    "                for member in filtered_files:\n",
    "                # Extract member (file) to output directory (if provided)\n",
    "                    tar.extract(member, filter=\"data\" ,path=f\"{parent_dir}/data/\") \n",
    "                    print(f\"- {member}\")  # Print extracted filename\n",
    "            else:\n",
    "                print(\"No files starting with 'On_Time_On_Time_Performance_2003' found\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Archive '{archive_filename}' not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(use_full_dataset=False):\n",
    "    all_data_frames = []\n",
    "\n",
    "    if use_full_dataset:\n",
    "        key = \"airline-data.1998-2018.tar.gz\"\n",
    "        if os.path.isfile (f\"{parent_dir}/data/{key}\" )== False:\n",
    "            download_data(key=key)\n",
    "            extract_data(fname=f\"{parent_dir}/data/{key}\")\n",
    "        else:\n",
    "            extract_data(fname=f\"{parent_dir}/data/{key}\")\n",
    "\n",
    "        for m in glob.glob(\n",
    "            f\"{parent_dir}/data/airline-data/On_Time_On_Time_Performance*.csv\"\n",
    "        ):\n",
    "            all_data_frames.append(cudf.read_csv(m))\n",
    "    else:\n",
    "        # key = \"airline-data.2005-2015.tar.gz\"\n",
    "        key = \"airline-data.1998-2018.tar.gz\"\n",
    "\n",
    "        if os.path.isfile( f\"{parent_dir}/data/{key}\") == False:\n",
    "            download_data(key=key)\n",
    "            extract_small_data(key=key)\n",
    "        else : \n",
    "            extract_small_data(key=key)\n",
    "\n",
    "        for m in glob.glob(\n",
    "            f\"{parent_dir}/data/airline-data/On_Time_On_Time_Performance_2003_*.csv\"\n",
    "        ):\n",
    "            all_data_frames.append(cudf.read_csv(m))\n",
    "\n",
    "    # colect all the dataframes\n",
    "    dataset = cudf.concat(all_data_frames)\n",
    "\n",
    "    # pass all column names to Capital\n",
    "    capital_names = [x.upper() for x in dataset.columns.to_list() ]\n",
    "    dataset.columns = capital_names\n",
    "    # print (capital_names)\n",
    "\n",
    "\n",
    "    # define the features specific for 2003 files\n",
    "    input_cols = ['YEAR','MONTH','DAY_OF_MONTH','DAY_OF_WEEK','CRS_DEP_TIME','CRS_ARR_TIME',\n",
    "                  'OP_UNIQUE_CARRIER','OP_CARRIER_FL_NUM','ACTUAL_ELAPSED_TIME','ORIGIN',\n",
    "                  'DEST','DISTANCE','DIVERTED']\n",
    "    \n",
    "    # NOTE: Beware the columns name on the full data set for example these are the names for 2005 files\n",
    "    # input_cols = ['YEAR', 'MONTH', 'DAYOFMONTH', 'DAYOFWEEK', 'CRSDEPTIME', 'CRSARRTIME', \n",
    "    #               'CARRIER', 'FLIGHTNUM', 'ACTUALELAPSEDTIME' ,'ORIGIN', \n",
    "    #               'DEST', 'DISTANCE','DIVERTED']\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # encode categoricals as numeric\n",
    "    for col in dataset.select_dtypes([\"object\"]).columns:\n",
    "        dataset[col] = dataset[col].astype(\"category\").cat.codes.astype(np.int32)\n",
    "\n",
    "    # cast all columns to int32\n",
    "    for col in dataset.columns:\n",
    "        dataset[col] = dataset[col].astype(np.float32)  # needed for random forest\n",
    "\n",
    "    # define the label\n",
    "    dataset[\"ArrDelayBinary\"] = 1.0 * (dataset[\"ARR_DELAY\"] > 10)\n",
    "\n",
    "    # put target/label column first [ classic XGBoost standard ]\n",
    "    output_cols = [\"ArrDelayBinary\"] + input_cols\n",
    "\n",
    "    # select the columns of interest\n",
    "    dataset = dataset[output_cols]\n",
    "\n",
    "    # drop the nan values\n",
    "    dataset.dropna(axis=0, inplace=True)\n",
    "\n",
    "    dataset = dataset.reindex(columns=output_cols)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files:\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_10.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_7.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_2.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_8.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_12.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_4.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_9.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_5.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_3.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_6.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_11.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_1.csv\n",
      "['YEAR', 'QUARTER', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'OP_UNIQUE_CARRIER', 'OP_CARRIER_AIRLINE_ID', 'OP_CARRIER', 'TAIL_NUM', 'OP_CARRIER_FL_NUM', 'ORIGIN_AIRPORT_ID', 'ORIGIN_AIRPORT_SEQ_ID', 'ORIGIN_CITY_MARKET_ID', 'ORIGIN', 'ORIGIN_CITY_NAME', 'ORIGIN_STATE_ABR', 'ORIGIN_STATE_FIPS', 'ORIGIN_STATE_NM', 'ORIGIN_WAC', 'DEST_AIRPORT_ID', 'DEST_AIRPORT_SEQ_ID', 'DEST_CITY_MARKET_ID', 'DEST', 'DEST_CITY_NAME', 'DEST_STATE_ABR', 'DEST_STATE_FIPS', 'DEST_STATE_NM', 'DEST_WAC', 'CRS_DEP_TIME', 'DEP_TIME', 'DEP_DELAY', 'DEP_DELAY_NEW', 'DEP_DEL15', 'DEP_DELAY_GROUP', 'DEP_TIME_BLK', 'TAXI_OUT', 'WHEELS_OFF', 'WHEELS_ON', 'TAXI_IN', 'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY', 'ARR_DELAY_NEW', 'ARR_DEL15', 'ARR_DELAY_GROUP', 'ARR_TIME_BLK', 'CANCELLED', 'CANCELLATION_CODE', 'DIVERTED', 'CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'FLIGHTS', 'DISTANCE', 'DISTANCE_GROUP', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'FIRST_DEP_TIME', 'TOTAL_ADD_GTIME', 'LONGEST_ADD_GTIME', 'DIV_AIRPORT_LANDINGS', 'DIV_REACHED_DEST', 'DIV_ACTUAL_ELAPSED_TIME', 'DIV_ARR_DELAY', 'DIV_DISTANCE', 'DIV1_AIRPORT', 'DIV1_AIRPORT_ID', 'DIV1_AIRPORT_SEQ_ID', 'DIV1_WHEELS_ON', 'DIV1_TOTAL_GTIME', 'DIV1_LONGEST_GTIME', 'DIV1_WHEELS_OFF', 'DIV1_TAIL_NUM', 'DIV2_AIRPORT', 'DIV2_AIRPORT_ID', 'DIV2_AIRPORT_SEQ_ID', 'DIV2_WHEELS_ON', 'DIV2_TOTAL_GTIME', 'DIV2_LONGEST_GTIME', 'DIV2_WHEELS_OFF', 'DIV2_TAIL_NUM', 'DIV3_AIRPORT', 'DIV3_AIRPORT_ID', 'DIV3_AIRPORT_SEQ_ID', 'DIV3_WHEELS_ON', 'DIV3_TOTAL_GTIME', 'DIV3_LONGEST_GTIME', 'DIV3_WHEELS_OFF', 'DIV3_TAIL_NUM', 'DIV4_AIRPORT', 'DIV4_AIRPORT_ID', 'DIV4_AIRPORT_SEQ_ID', 'DIV4_WHEELS_ON', 'DIV4_TOTAL_GTIME', 'DIV4_LONGEST_GTIME', 'DIV4_WHEELS_OFF', 'DIV4_TAIL_NUM', 'DIV5_AIRPORT', 'DIV5_AIRPORT_ID', 'DIV5_AIRPORT_SEQ_ID', 'DIV5_WHEELS_ON', 'DIV5_TOTAL_GTIME', 'DIV5_LONGEST_GTIME', 'DIV5_WHEELS_OFF', 'DIV5_TAIL_NUM', 'UNNAMED: 109']\n",
      "CPU times: user 4min 56s, sys: 10.1 s, total: 5min 6s\n",
      "Wall time: 5min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "df = prepare_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
