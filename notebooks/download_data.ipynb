{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tarfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import cudf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We download the Airline dataset and save it to local directory specific by `data_dir` and `orc_name`. In this step, we also want to convert the input data into appropriate dtypes. For this, we will use the `prepare_dataset` function.\n",
    "\n",
    "Note: To ensure that this example runs quickly on a modest machine, we default to using a small subset of the airline dataset. To use the full dataset, pass the argument `use_full_dataset=True` to the `prepare_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to only see the selected GPU \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "num_rows = 2500000  # number of rows to be used in this notebook\n",
    "\n",
    "parent_dir = \"/\".join(os.getcwd().split(\"/\")[:-1])\n",
    "\n",
    "data_dir = os.path.join(parent_dir, \"data\",\"airline-data\")\n",
    "\n",
    "os.makedirs(data_dir,exist_ok = True )\n",
    "\n",
    "orc_name = os.path.join(data_dir, \"airline-data\"+ str(num_rows) + \".orc\")\n",
    "\n",
    "orc_name_full = os.path.join(data_dir, \"airline-data-full-2003.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict =    {\n",
    "    \"airline-data.2005-2015.tar.gz\" : \"https://diybigdata.net/?sdm_process_download=1&download_id=392\",\n",
    "    \"airline-data.2003-2018.tar.gz\" : \"https://diybigdata.net/?sdm_process_download=1&download_id=821\",\n",
    "    \"airline-data.1998-2018.tar.gz\" : \"https://diybigdata.net/?sdm_process_download=1&download_id=832\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_data(key):\n",
    "    url = url_dict[key]\n",
    "    # Send GET request to the link\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Get filename from response headers (assuming Content-Disposition header is present)\n",
    "        filename = response.headers.get('Content-Disposition')\n",
    "        if filename:\n",
    "          filename = filename.split(\"=\")[1].strip('\"')\n",
    "        else:\n",
    "          # If filename unavailable, use a key as filename\n",
    "          filename = key\n",
    "\n",
    "        # Open a file for writing in binary mode\n",
    "        with open(f\"{parent_dir}/data/{filename}\", \"wb\") as f:\n",
    "          for chunk in response.iter_content(1024):\n",
    "            # Write downloaded data in chunks\n",
    "            f.write(chunk)\n",
    "        print(f\"File downloaded successfully: {filename}\")\n",
    "    else:\n",
    "        print(f\"Download failed with status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(fname):\n",
    "    if fname.endswith(\"tar.gz\"):\n",
    "        tar = tarfile.open(fname, \"r:gz\")\n",
    "        tar.extractall(filter=\"data\", path=f\"{parent_dir}/data/\")\n",
    "        tar.close()\n",
    "    elif fname.endswith(\"tar\"):\n",
    "        tar = tarfile.open(fname, \"r:\")\n",
    "        tar.extractall(filter=\"data\", path=f\"{parent_dir}/data/\")\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_small_data(key):\n",
    "\n",
    "    # Define archive filename\n",
    "    archive_filename = f\"{parent_dir}/data/{key}\"\n",
    "\n",
    "    # Open archive in read mode\n",
    "    try:\n",
    "        with tarfile.open(archive_filename, \"r:gz\") as tar:\n",
    "            # Extract all filenames\n",
    "            all_filenames = tar.getnames()\n",
    "            # print (all_filenames)\n",
    "\n",
    "            # Filter files starting with \"On_Time_On_Time_Performance_2003\"\n",
    "            filtered_files = [f for f in all_filenames if f.split(\"/\")[-1].startswith(\"On_Time_On_Time_Performance_2003\")]\n",
    "\n",
    "            if filtered_files:\n",
    "                print(\"Extracting files:\")\n",
    "                for member in filtered_files:\n",
    "                # Extract member (file) to output directory (if provided)\n",
    "                    tar.extract(member, filter=\"data\" ,path=f\"{parent_dir}/data/\") \n",
    "                    print(f\"- {member}\")  # Print extracted filename\n",
    "            else:\n",
    "                print(\"No files starting with 'On_Time_On_Time_Performance_2003' found\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Archive '{archive_filename}' not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(use_full_dataset=False):\n",
    "    all_data_frames = []\n",
    "\n",
    "    if use_full_dataset:\n",
    "        key = \"airline-data.1998-2018.tar.gz\"\n",
    "        if os.path.isfile (f\"{parent_dir}/data/{key}\" )== False:\n",
    "            download_data(key=key)\n",
    "            extract_data(fname=f\"{parent_dir}/data/{key}\")\n",
    "        else:\n",
    "            extract_data(fname=f\"{parent_dir}/data/{key}\")\n",
    "\n",
    "        for m in glob.glob(\n",
    "            f\"{parent_dir}/data/airline-data/On_Time_On_Time_Performance*.csv\"\n",
    "        ):\n",
    "            all_data_frames.append(cudf.read_csv(m))\n",
    "    else:\n",
    "        # key = \"airline-data.2005-2015.tar.gz\"\n",
    "        key = \"airline-data.1998-2018.tar.gz\"\n",
    "\n",
    "        if os.path.isfile( f\"{parent_dir}/data/{key}\") == False:\n",
    "            download_data(key=key)\n",
    "            extract_small_data(key=key)\n",
    "        else : \n",
    "            extract_small_data(key=key)\n",
    "\n",
    "        for m in glob.glob(\n",
    "            f\"{parent_dir}/data/airline-data/On_Time_On_Time_Performance_2003_*.csv\"\n",
    "        ):\n",
    "            all_data_frames.append(cudf.read_csv(m))\n",
    "\n",
    "    # colect all the dataframes\n",
    "    dataset = cudf.concat(all_data_frames)\n",
    "\n",
    "    # pass all column names to Capital\n",
    "    capital_names = [x.upper() for x in dataset.columns.to_list() ]\n",
    "    dataset.columns = capital_names\n",
    "    # print (capital_names)\n",
    "\n",
    "\n",
    "    # define the features specific for 2003 files\n",
    "    input_cols = ['YEAR','MONTH','DAY_OF_MONTH','DAY_OF_WEEK','CRS_DEP_TIME','CRS_ARR_TIME',\n",
    "                  'OP_UNIQUE_CARRIER','OP_CARRIER_FL_NUM','ACTUAL_ELAPSED_TIME','ORIGIN',\n",
    "                  'DEST','DISTANCE','DIVERTED']\n",
    "    \n",
    "    # NOTE: Beware the columns name on the full data set for example these are the names for 2005 files\n",
    "    # input_cols = ['YEAR', 'MONTH', 'DAYOFMONTH', 'DAYOFWEEK', 'CRSDEPTIME', 'CRSARRTIME', \n",
    "    #               'CARRIER', 'FLIGHTNUM', 'ACTUALELAPSEDTIME' ,'ORIGIN', \n",
    "    #               'DEST', 'DISTANCE','DIVERTED']\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # encode categoricals as numeric\n",
    "    for col in dataset.select_dtypes([\"object\"]).columns:\n",
    "        dataset[col] = dataset[col].astype(\"category\").cat.codes.astype(np.int32)\n",
    "\n",
    "    # cast all columns to int32\n",
    "    for col in dataset.columns:\n",
    "        dataset[col] = dataset[col].astype(np.float32)  # needed for random forest\n",
    "\n",
    "    # define the label\n",
    "    dataset[\"ArrDelayBinary\"] = 1.0 * (dataset[\"ARR_DELAY\"] > 10)\n",
    "\n",
    "    # put target/label column first [ classic XGBoost standard ]\n",
    "    output_cols = [\"ArrDelayBinary\"] + input_cols\n",
    "\n",
    "    # select the columns of interest\n",
    "    dataset = dataset[output_cols]\n",
    "\n",
    "    # drop the nan values\n",
    "    dataset.dropna(axis=0, inplace=True)\n",
    "\n",
    "    dataset = dataset.reindex(columns=output_cols)\n",
    "\n",
    "    # convert to ORC\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files:\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_10.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_7.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_2.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_8.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_12.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_4.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_9.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_5.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_3.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_6.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_11.csv\n",
      "- ./airline-data/On_Time_On_Time_Performance_2003_1.csv\n",
      "CPU times: user 6min 34s, sys: 34.7 s, total: 7min 9s\n",
      "Wall time: 17min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "df = prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = df.sample(n=num_rows,random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.orc as orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = df.to_arrow()\n",
    "orc.write_table(table, orc_name_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = df_rows.to_arrow()\n",
    "orc.write_table(table, orc_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
