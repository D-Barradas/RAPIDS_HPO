{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D-Barradas/RAPIDS_HPO/blob/main/notebooks/download_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use a GPU in Google Colab, you need to change the runtime type. Here's how:\n",
        "\n",
        "Go to the \"Runtime\" menu at the top of the page.\n",
        "Select \"Change runtime type\".\n",
        "In the \"Hardware accelerator\" dropdown, choose \"GPU\".\n",
        "Click \"Save\"."
      ],
      "metadata": {
        "id": "Sg_1i1SYQ0Da"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_06AxdnrPv94"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import tarfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import cudf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beNg9FVEPv95"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "We download the Airline dataset and save it to local directory specific by `data_dir` and `orc_name`. In this step, we also want to convert the input data into appropriate dtypes. For this, we will use the `prepare_dataset` function.\n",
        "\n",
        "Note: To ensure that this example runs quickly on a modest machine, we default to using a small subset of the airline dataset. To use the full dataset, pass the argument `use_full_dataset=True` to the `prepare_dataset` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IRarWLQ0Pv96"
      },
      "outputs": [],
      "source": [
        "# Set environment variable to only see the selected GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "047004a3",
        "outputId": "e3461790-7a00-4218-c5dd-66b3ce861295"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHqSeUHQPv96",
        "outputId": "7fd55481-f1b8-4937-8038-1950eb85fefd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory created at: /content/drive/MyDrive/data/airline-data\n"
          ]
        }
      ],
      "source": [
        "rng = np.random.RandomState(42)\n",
        "num_rows = 2500000  # number of rows to be used in this notebook\n",
        "\n",
        "#parent_dir = \"/\".join(os.getcwd().split(\"/\")[:-1])\n",
        "parent_dir = \"/content/drive/MyDrive/\"\n",
        "\n",
        "data_dir = os.path.join(parent_dir, \"data\",\"airline-data\")\n",
        "\n",
        "os.makedirs(data_dir,exist_ok = True )\n",
        "print(f\"Data directory created at: {data_dir}\")\n",
        "\n",
        "orc_name = os.path.join(data_dir, \"airline-data\"+ str(num_rows) + \".orc\")\n",
        "\n",
        "orc_name_full = os.path.join(data_dir, \"airline-data-full-2003.orc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v6_ATmJMPv96"
      },
      "outputs": [],
      "source": [
        "url_dict =    {\n",
        "    \"airline-data.2005-2015.tar.gz\" : \"https://diybigdata.net/?sdm_process_download=1&download_id=392\",\n",
        "    \"airline-data.2003-2018.tar.gz\" : \"https://diybigdata.net/?sdm_process_download=1&download_id=821\",\n",
        "    \"airline-data.1998-2018.tar.gz\" : \"https://diybigdata.net/?sdm_process_download=1&download_id=832\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n0yIWKFdPv97"
      },
      "outputs": [],
      "source": [
        "\n",
        "def download_data(key):\n",
        "    url = url_dict[key]\n",
        "    # Send GET request to the link\n",
        "    response = requests.get(url, stream=True)\n",
        "\n",
        "    # Check if request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Get filename from response headers (assuming Content-Disposition header is present)\n",
        "        filename = response.headers.get('Content-Disposition')\n",
        "        if filename:\n",
        "          filename = filename.split(\"=\")[1].strip('\"')\n",
        "        else:\n",
        "          # If filename unavailable, use a key as filename\n",
        "          filename = key\n",
        "\n",
        "        # Open a file for writing in binary mode\n",
        "        with open(f\"{parent_dir}/data/{filename}\", \"wb\") as f:\n",
        "          for chunk in response.iter_content(1024):\n",
        "            # Write downloaded data in chunks\n",
        "            f.write(chunk)\n",
        "        print(f\"File downloaded successfully: {filename}\")\n",
        "    else:\n",
        "        print(f\"Download failed with status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xd0qHwJQPv97"
      },
      "outputs": [],
      "source": [
        "def extract_data(fname):\n",
        "    if fname.endswith(\"tar.gz\"):\n",
        "        tar = tarfile.open(fname, \"r:gz\")\n",
        "        tar.extractall(filter=\"data\", path=f\"{parent_dir}/data/\")\n",
        "        tar.close()\n",
        "    elif fname.endswith(\"tar\"):\n",
        "        tar = tarfile.open(fname, \"r:\")\n",
        "        tar.extractall(filter=\"data\", path=f\"{parent_dir}/data/\")\n",
        "        tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fNc6RclNPv97"
      },
      "outputs": [],
      "source": [
        "def extract_small_data(key):\n",
        "\n",
        "    # Define archive filename\n",
        "    archive_filename = f\"{parent_dir}/data/{key}\"\n",
        "\n",
        "    # Open archive in read mode\n",
        "    try:\n",
        "        with tarfile.open(archive_filename, \"r:gz\") as tar:\n",
        "            # Extract all filenames\n",
        "            all_filenames = tar.getnames()\n",
        "            # print (all_filenames)\n",
        "\n",
        "            # Filter files starting with \"On_Time_On_Time_Performance_2003\"\n",
        "            filtered_files = [f for f in all_filenames if f.split(\"/\")[-1].startswith(\"On_Time_On_Time_Performance_2003\")]\n",
        "\n",
        "            if filtered_files:\n",
        "                print(\"Extracting files:\")\n",
        "                for member in filtered_files:\n",
        "                # Extract member (file) to output directory (if provided)\n",
        "                    tar.extract(member, filter=\"data\" ,path=f\"{parent_dir}/data/\")\n",
        "                    print(f\"- {member}\")  # Print extracted filename\n",
        "            else:\n",
        "                print(\"No files starting with 'On_Time_On_Time_Performance_2003' found\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Archive '{archive_filename}' not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AMBLo_jQPv97"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(use_full_dataset=False):\n",
        "    all_data_frames = []\n",
        "\n",
        "    if use_full_dataset:\n",
        "        key = \"airline-data.1998-2018.tar.gz\"\n",
        "        if os.path.isfile (f\"{parent_dir}/data/{key}\" )== False:\n",
        "            download_data(key=key)\n",
        "            extract_data(fname=f\"{parent_dir}/data/{key}\")\n",
        "        else:\n",
        "            extract_data(fname=f\"{parent_dir}/data/{key}\")\n",
        "\n",
        "        for m in glob.glob(\n",
        "            f\"{parent_dir}/data/airline-data/On_Time_On_Time_Performance*.csv\"\n",
        "        ):\n",
        "            all_data_frames.append(cudf.read_csv(m))\n",
        "    else:\n",
        "        # key = \"airline-data.2005-2015.tar.gz\"\n",
        "        key = \"airline-data.1998-2018.tar.gz\"\n",
        "\n",
        "        if os.path.isfile( f\"{parent_dir}/data/{key}\") == False:\n",
        "            download_data(key=key)\n",
        "            extract_small_data(key=key)\n",
        "        else :\n",
        "            extract_small_data(key=key)\n",
        "\n",
        "        for m in glob.glob(\n",
        "            f\"{parent_dir}/data/airline-data/On_Time_On_Time_Performance_2003_*.csv\"\n",
        "        ):\n",
        "            all_data_frames.append(cudf.read_csv(m))\n",
        "\n",
        "    # colect all the dataframes\n",
        "    dataset = cudf.concat(all_data_frames)\n",
        "\n",
        "    # pass all column names to Capital\n",
        "    capital_names = [x.upper() for x in dataset.columns.to_list() ]\n",
        "    dataset.columns = capital_names\n",
        "    # print (capital_names)\n",
        "\n",
        "\n",
        "    # define the features specific for 2003 files\n",
        "    input_cols = ['YEAR','MONTH','DAY_OF_MONTH','DAY_OF_WEEK','CRS_DEP_TIME','CRS_ARR_TIME',\n",
        "                  'OP_UNIQUE_CARRIER','OP_CARRIER_FL_NUM','ACTUAL_ELAPSED_TIME','ORIGIN',\n",
        "                  'DEST','DISTANCE','DIVERTED']\n",
        "\n",
        "    # NOTE: Beware the columns name on the full data set for example these are the names for 2005 files\n",
        "    # input_cols = ['YEAR', 'MONTH', 'DAYOFMONTH', 'DAYOFWEEK', 'CRSDEPTIME', 'CRSARRTIME',\n",
        "    #               'CARRIER', 'FLIGHTNUM', 'ACTUALELAPSEDTIME' ,'ORIGIN',\n",
        "    #               'DEST', 'DISTANCE','DIVERTED']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # encode categoricals as numeric\n",
        "    for col in dataset.select_dtypes([\"object\"]).columns:\n",
        "        dataset[col] = dataset[col].astype(\"category\").cat.codes.astype(np.int32)\n",
        "\n",
        "    # cast all columns to int32\n",
        "    for col in dataset.columns:\n",
        "        dataset[col] = dataset[col].astype(np.float32)  # needed for random forest\n",
        "\n",
        "    # define the label\n",
        "    dataset[\"ArrDelayBinary\"] = 1.0 * (dataset[\"ARR_DELAY\"] > 10)\n",
        "\n",
        "    # put target/label column first [ classic XGBoost standard ]\n",
        "    output_cols = [\"ArrDelayBinary\"] + input_cols\n",
        "\n",
        "    # select the columns of interest\n",
        "    dataset = dataset[output_cols]\n",
        "\n",
        "    # drop the nan values\n",
        "    dataset.dropna(axis=0, inplace=True)\n",
        "\n",
        "    dataset = dataset.reindex(columns=output_cols)\n",
        "\n",
        "    # convert to ORC\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfxLAcjGPv97",
        "outputId": "7dbc6696-b7a3-44f8-8378-302e44715fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files:\n",
            "- ./airline-data/On_Time_On_Time_Performance_2003_10.csv\n",
            "- ./airline-data/On_Time_On_Time_Performance_2003_7.csv\n",
            "- ./airline-data/On_Time_On_Time_Performance_2003_2.csv\n",
            "- ./airline-data/On_Time_On_Time_Performance_2003_8.csv\n",
            "- ./airline-data/On_Time_On_Time_Performance_2003_12.csv\n",
            "- ./airline-data/On_Time_On_Time_Performance_2003_4.csv\n",
            "- ./airline-data/On_Time_On_Time_Performance_2003_9.csv\n",
            "- ./airline-data/On_Time_On_Time_Performance_2003_5.csv\n",
            "- ./airline-data/On_Time_On_Time_Performance_2003_3.csv\n",
            "- ./airline-data/On_Time_On_Time_Performance_2003_6.csv\n",
            "- ./airline-data/On_Time_On_Time_Performance_2003_11.csv\n",
            "- ./airline-data/On_Time_On_Time_Performance_2003_1.csv\n",
            "CPU times: user 6min 41s, sys: 15.7 s, total: 6min 57s\n",
            "Wall time: 7min 43s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "df = prepare_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y_1xQkkMPv98"
      },
      "outputs": [],
      "source": [
        "df_rows = df.sample(n=num_rows,random_state=rng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rDghkFL7Pv98"
      },
      "outputs": [],
      "source": [
        "import pyarrow.orc as orc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WooCPBCuPv98"
      },
      "outputs": [],
      "source": [
        "table = df.to_arrow()\n",
        "orc.write_table(table, orc_name_full)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "M-K2LmXoPv98"
      },
      "outputs": [],
      "source": [
        "table = df_rows.to_arrow()\n",
        "orc.write_table(table, orc_name)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}