{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D-Barradas/RAPIDS_HPO/blob/main/notebooks/HPO_Ray_RAPIDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFuXuS9fappe"
      },
      "source": [
        "# Hyper-parameter Optimization with RayTune + RAPIDS\n",
        "\n",
        "### Introduction\n",
        "\n",
        "&emsp; &emsp; &emsp; [Hyperparameter optimization](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview) is the task of picking the values for the hyperparameters of the model that provide the optimal results for the problem, as measured on a test dataset. This is often a crucial step and can help boost the model performance when done correctly. Despite its theoretical importance, HPO has been difficult to implement in practical applications because of the resources needed to run so many distinct training jobs. In this notebook, we explore the combination of RAPIDS and RayTune to perform HyperOpt on GPU models to understand the importance that selecting the right parameters has on the model performance. We are trying to illustrate that RayTune + RAPIDS can be useful in order to perform HPO in an efficient and practical manner.\n",
        "\n",
        "#### What is RayTune?\n",
        "\n",
        "&emsp; &emsp; &emsp; [RayTune](https://ray.readthedocs.io/en/latest/index.html) is a scalable Hyperparameter optimization library. It allows distributed HPO, provides various [search algorithms](https://ray.readthedocs.io/en/latest/tune-searchalg.html) to allow different optimization techniques to be explored with ease. The library also provides [scheduling algorithms](https://ray.readthedocs.io/en/latest/tune-schedulers.html) that allows a smarter way to schedule the different parameter sweep instead of the basic First In-First Out method which is followed by other libraries (Scikit-Learn, Dask-ml) that support HPO. The different scheduling algorithms can make the HPO process resource efficient and help arrive at the best parameters much faster.\n",
        "\n",
        "#### What is RAPIDS?\n",
        "\n",
        "&emsp; &emsp; &emsp; The [RAPIDS](https://rapids.ai/start.html) framework provides a library suite that can execute end-to-end data science pipelines entirely on GPUs. One of the libraries in this framework is [cuML](https://docs.rapids.ai/api/cuml/stable/), which contains various Machine Learning algorithms that take advantage of GPU to run. You can learn more about RAPIDS [here](https://rapids.ai/about.html).\n",
        "\n",
        "For this demo, we will be using the [Airline dataset](http://kt.ijs.si/elena_ikonomovska/data.html). The aim of the problem is to predict the arrival delay. It has about 116 million entries with 13 attributes that are used to determine the delay for a given airline. We'll demonstrate the use of a cuML [RandomForestClassifier](https://rapidsai.github.io/projects/cuml/en/0.12.0/api.html#cuml.ensemble.RandomForestClassifier) model with RayTune and try to understand how HPO improves the performance, and make a case for why it is useful to perform.\n",
        "\n",
        "The notebook is structured as follows:\n",
        "\n",
        "&emsp; &emsp; &emsp; 1. Loading and preparing the dataset\n",
        "\n",
        "&emsp; &emsp; &emsp; 2. Setting the experiment parameters (like selecting scheduling & search alg, parameters ranges, etc)\n",
        "\n",
        "&emsp; &emsp; &emsp; 3. Running and visualizing the results of the experiments\n",
        "\n",
        "By the end of this notebook, you'll be familiar with running HPO experiments on RayTune with different scheduling algorithms and visualizing the results to study the importance of the process and how it affects the metric used to evaluate the model.\n",
        "\n",
        "Note: The notebook also includes the scikit-learn RandomForest for the CPU version - this should be used only for the performance comparison and is <b>not</b> recommended with large data size and ranges.\n",
        "\n",
        "Let's get started with the notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmlm6i5Aappf"
      },
      "outputs": [],
      "source": [
        "# # Uncomment this line to install the packages\n",
        "\n",
        "!pip install -U ray\n",
        "!pip install ray[tune]\n",
        "!pip install bayesian-optimization==1.5.1\n",
        "!pip install optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWaiGSwBappf"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import logging\n",
        "import math\n",
        "import multiprocessing\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import cudf\n",
        "import cuml\n",
        "import cupy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ray\n",
        "from cuml.model_selection import train_test_split\n",
        "from sklearn.model_selection import train_test_split as sktrain_test_split\n",
        "import sklearn\n",
        "from ray import tune\n",
        "from ray.tune.experiment import trial\n",
        "from ray.tune.logger import TBXLogger\n",
        "from ray.tune.schedulers import AsyncHyperBandScheduler, MedianStoppingRule\n",
        "\n",
        "_DEBUG = bool(os.environ.get(\"_DEBUG\", False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWcb2C5Gappf"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")  # Reduce number of messages/warnings displayed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive and Download data"
      ],
      "metadata": {
        "id": "r-tgLdFiEMP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "okz5mYfZETzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = 2500000  # number of rows to be used in this notebook\n",
        "\n",
        "# the parrent dir path is important to place the data and results in the correct flder\n",
        "# parent_dir = \"/\".join(os.getcwd().split(\"/\")[:-1])\n",
        "parent_dir = \"/content/drive/MyDrive/\"\n",
        "\n",
        "data_dir = os.path.join(parent_dir, \"data\", \"airline-data\")\n",
        "\n",
        "# Create data and airline-data directories if they don't exist\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "orc_name = os.path.join(data_dir, \"airline-data\" + str(num_rows) + \".orc\")"
      ],
      "metadata": {
        "id": "gXKyGS-sEUji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file from Google Drive\n",
        "# This link is for the airline-data2500000.orc file\n",
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=1DVuLjbHu-oAc-IkGJMlCk44gD1nBpQkC\"\n",
        "gdown.download(url, orc_name)"
      ],
      "metadata": {
        "id": "EwUmmUpFEY20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYuruAU_appf"
      },
      "source": [
        "# Check the data is downloaded , otherwise execute the download_data.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcniqI2Pappf"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset():\n",
        "    try:\n",
        "        if os.path.isfile(orc_name):\n",
        "            df = cudf.read_orc(orc_name)\n",
        "            df = df.drop(\"index\", axis=1)\n",
        "            return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Archive '{orc_name}' not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi7txYxnappg"
      },
      "source": [
        "## Utility Functions and Class\n",
        "\n",
        "1. `get_trial_name`(function) : Generates the trial name over an iterator variable to store results.\n",
        "\n",
        "2. `PerfTimer`(class) : A high resolution timer class in order to support the timing of operations to report during the experiment runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mTecrAEappg"
      },
      "outputs": [],
      "source": [
        "trial_num = 0\n",
        "\n",
        "\n",
        "def get_trial_name(trial: ray.tune.experiment.trial.Trial):\n",
        "    # Returns the trial number over an iterator variable trail_num\n",
        "    global trial_num\n",
        "    trial_num = trial_num + 1\n",
        "    trial_name = trial.trainable_name + \"_\" + str(trial_num)\n",
        "    return trial_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFHuA7noappg"
      },
      "outputs": [],
      "source": [
        "class PerfTimer:\n",
        "    # High resolution timer for reporting training and inference time.\n",
        "    def __init__(self):\n",
        "        self.start = None\n",
        "        self.duration = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.start = time.perf_counter()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.duration = time.perf_counter() - self.start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-Ey1ry1appg"
      },
      "source": [
        "## Setting Up Tune\n",
        "\n",
        "We will set up the Tune training with the [Trainable API](https://ray.readthedocs.io/en/latest/tune-usage.html#trainable-api) because it is stateful and allows checkpointing, which is useful for complex algorithms.\n",
        "\n",
        "To do this, let's subclass the `tune.Trainable` class and define the functions for initial setup, training, saving and restoring the models.\n",
        "\n",
        "### Member functions\n",
        "1. `_setup` : Load the dataset and set the necessary parameters to config values (uses `_build`)\n",
        "\n",
        "2. `_train` : Splitting the data into train and test sets, training a CPU or GPU model depending on `compute` mode. Evaluate the model and store the test accuracy.\n",
        "\n",
        "3. `_save` and `_restore` : Checkpoint the test accuracy periodically (specified by the scheduling algorithms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv2PIgiNappg"
      },
      "outputs": [],
      "source": [
        "class BaseTrainTransformer(tune.Trainable):\n",
        "    @property\n",
        "    def static_config(self) -> dict:\n",
        "        return getattr(self, \"_static_config\", {})\n",
        "\n",
        "    def setup(self, config: dict):\n",
        "        CSP_paths = {\"train_data\": data_dir}\n",
        "\n",
        "        if self.static_config[\"compute\"] == \"GPU\":\n",
        "            self._gpu_id = ray.get_gpu_ids()[0]\n",
        "\n",
        "        self._dataset, self._col_labels, self._y_label = (\n",
        "            ray.get(data_id),\n",
        "            None,\n",
        "            \"ArrDelayBinary\",\n",
        "        )\n",
        "\n",
        "        # classification objective requires int32 label for cuml random forest\n",
        "        self._dataset[self._y_label] = self._dataset[self._y_label].astype(\"int32\")\n",
        "        self.rf_model = None\n",
        "        self._build(config)\n",
        "\n",
        "    def _build(self, new_config):\n",
        "        self._model_params = {\n",
        "            \"max_depth\": int(new_config[\"max_depth\"]),\n",
        "            \"n_estimators\": int(new_config[\"n_estimators\"]),\n",
        "            \"max_features\": float(new_config[\"max_features\"]),\n",
        "            \"n_bins\": 16,  # args.n_bins,\n",
        "            \"seed\": time.time(),\n",
        "        }\n",
        "        self._global_best_model = None\n",
        "        self._global_best_test_accuracy = 0\n",
        "\n",
        "    def step(self):\n",
        "        iteration = getattr(self, \"iteration\", 0)\n",
        "\n",
        "        if self.static_config[\"compute\"] == \"GPU\":\n",
        "            # split data\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X=self._dataset,\n",
        "                y=self._y_label,\n",
        "                train_size=0.8,\n",
        "                shuffle=True,\n",
        "                random_state=iteration,\n",
        "            )\n",
        "            self.rf_model = cuml.ensemble.RandomForestClassifier(\n",
        "                n_estimators=self._model_params[\"n_estimators\"],\n",
        "                max_depth=self._model_params[\"max_depth\"],\n",
        "                n_bins=self._model_params[\"n_bins\"],\n",
        "                max_features=self._model_params[\"max_features\"],\n",
        "            )\n",
        "        elif self.static_config[\"compute\"] == \"CPU\":\n",
        "            # Optionally allow CPU version for performance comparison\n",
        "\n",
        "            X_train, X_test, y_train, y_test = sktrain_test_split(\n",
        "                self._dataset.loc[:, self._dataset.columns != self._y_label],\n",
        "                self._dataset[self._y_label],\n",
        "                train_size=0.8,\n",
        "                shuffle=True,\n",
        "                random_state=iteration,\n",
        "            )\n",
        "\n",
        "            self.rf_model = sklearn.ensemble.RandomForestClassifier(\n",
        "                n_estimators=self._model_params[\"n_estimators\"],\n",
        "                max_depth=self._model_params[\"max_depth\"],\n",
        "                max_features=self._model_params[\"max_features\"],\n",
        "                n_jobs=-1,\n",
        "            )\n",
        "        else:\n",
        "            print(\"Unknown option. Please select CPU or GPU\")\n",
        "            return\n",
        "\n",
        "        # train model\n",
        "        with PerfTimer() as train_timer:\n",
        "            trained_model = self.rf_model.fit(X_train, y_train)\n",
        "\n",
        "        training_time = train_timer.duration\n",
        "\n",
        "        # evaluate perf\n",
        "        with PerfTimer() as inference_timer:\n",
        "            test_accuracy = trained_model.score(X_test, y_test.astype(\"int32\"))\n",
        "        infer_time = inference_timer.duration\n",
        "\n",
        "        # update best model [ assumes maximization of perf metric ]\n",
        "        if test_accuracy > self._global_best_test_accuracy:\n",
        "            self._global_best_test_accuracy = test_accuracy\n",
        "            self._global_best_model = trained_model\n",
        "\n",
        "        return {\n",
        "            \"test_accuracy\": test_accuracy,\n",
        "            \"train_time\": round(training_time, 4),\n",
        "            \"infer_time\": round(infer_time, 4),\n",
        "            \"is_bad\": not math.isfinite(test_accuracy),\n",
        "            \"should_checkpoint\": False,\n",
        "        }\n",
        "\n",
        "    def _save(self, checkpoint):\n",
        "        self._global_best_test_accuracy = checkpoint[\"test_accuracy\"]\n",
        "\n",
        "    def _restore(self, checkpoint):\n",
        "        return {\n",
        "            \"test_accuracy\": self._global_best_test_accuracy,\n",
        "        }\n",
        "\n",
        "    def save_checkpoint(self, checkpoint_dir):\n",
        "        pass\n",
        "\n",
        "    def reset_config(self, new_config):\n",
        "        # Rebuild the config dependent stuff\n",
        "        del self.rf_model\n",
        "        self._build(new_config)\n",
        "        self.config = new_config\n",
        "        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s19wsl_appg"
      },
      "source": [
        "Wrap our Trainable class that can be passed to `tune.run` as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcPqsH_2appg"
      },
      "outputs": [],
      "source": [
        "class WrappedTrainable(BaseTrainTransformer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self._static_config = static_config\n",
        "\n",
        "        super().__init__(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GSMyoItappg"
      },
      "source": [
        "## Search Algorithm\n",
        "\n",
        "This function enables us to select a search algorithm. These are the two options in available in this notebook:\n",
        "\n",
        "### 1. [BayesOptSearch](https://ray.readthedocs.io/en/latest/tune-searchalg.html#bayesopt-search)\n",
        "\n",
        "This is backed by [bayesian optimization](https://github.com/fmfn/BayesianOptimization), which attempts to find the best performing parameters in as few iterations as possible. The optimization technique is based on bayesian inference and Gaussian processes. It attempts to find regions in the hyperparameter space that are worth exploring. At each step a Gaussian Process is fitted to the known samples, and the posterior distribution, combined with an exploration strategy are used to determine the next point that should be explored. Eventually, it finds the combination of parameters that yield results that are close to the optimal results.\n",
        "\n",
        "To learn more about Bayesian Optimization, refer to the [Practical Bayesian Optimization of Machine\n",
        "Learning Algorithms](http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)\n",
        "\n",
        "Make sure you run `pip install bayesian-optimization` before running this option.\n",
        "\n",
        "### 2. [Optuna](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.search.optuna.OptunaSearch.html#ray.tune.search.optuna.OptunaSearch)\n",
        "\n",
        "is a hyperparameter optimization library. In contrast to other libraries, it employs define-by-run style hyperparameter definitions.Optuna is framework agnostic. You can use it with any machine learning or deep learning framework.\n",
        "\n",
        "It requires you to install [optuna](https://optuna.org) using `pip install optuna`\n",
        "\n",
        "Check out other search algorithms in RayTune [here](https://ray.readthedocs.io/en/latest/tune-searchalg.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8bPQwUyappg"
      },
      "outputs": [],
      "source": [
        "def build_search_alg(search_alg, param_ranges: dict):\n",
        "    \"\"\"\n",
        "    Initialize a search algorithm that is selected using 'search_alg'\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        search_alg   : str; Selecting the search algorithm. Possible values\n",
        "                       [Optuna, SkOpt]\n",
        "        param_ranges : dictionary of parameter ranges over which the search\n",
        "                       should be performed\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        alg : Object of the RayTune search algorithm selected\n",
        "    \"\"\"\n",
        "\n",
        "    alg = None\n",
        "\n",
        "    if search_alg == \"BayesOpt\":\n",
        "      from ray.tune.search.bayesopt import BayesOptSearch\n",
        "\n",
        "      alg = BayesOptSearch(\n",
        "            param_ranges,\n",
        "            metric=\"test_accuracy\",\n",
        "            mode=\"max\",\n",
        "            utility_kwargs={\"kind\": \"ucb\", \"kappa\": 2.5, \"xi\": 0.0},\n",
        "      )\n",
        "\n",
        "    elif search_alg == \"Optuna\":\n",
        "      from ray.tune.search.optuna import OptunaSearch\n",
        "      import optuna\n",
        "\n",
        "      # Convert tuple ranges to Optuna distributions\n",
        "      optuna_param_ranges = {}\n",
        "      for key, value in param_ranges.items():\n",
        "          if isinstance(value, tuple) and len(value) == 2:\n",
        "              # Assuming the ranges are for numerical values (int or float)\n",
        "              if isinstance(value[0], int) and isinstance(value[1], int):\n",
        "                  optuna_param_ranges[key] = optuna.distributions.IntDistribution(value[0], value[1])\n",
        "              elif isinstance(value[0], float) and isinstance(value[1], float):\n",
        "                   optuna_param_ranges[key] = optuna.distributions.FloatDistribution(value[0], value[1])\n",
        "              else:\n",
        "                  # Handle other potential types if needed, or raise an error\n",
        "                  raise TypeError(f\"Unsupported range type for parameter {key}: {type(value[0])}, {type(value[1])}\")\n",
        "          else:\n",
        "              # If not a tuple of length 2, pass the value directly\n",
        "              optuna_param_ranges[key] = value\n",
        "\n",
        "\n",
        "      alg = OptunaSearch(\n",
        "            optuna_param_ranges,\n",
        "            metric=\"test_accuracy\", # Changed to test_accuracy to match the objective\n",
        "            mode=\"max\" # Changed to max to match the objective\n",
        "      )\n",
        "\n",
        "    else:\n",
        "        print(\"Unknown Option. Select BayesOpt or Optuna\")\n",
        "    return alg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2en6JIG7appg"
      },
      "source": [
        "## Scheduling algorithm\n",
        "\n",
        "The default scheduling is the First-In-First-Out method. RayTune allows us to select a different method to schedule trials in order to perform early stopping or perturb parameters. These can be more efficient in comparison to the FIFOScheduler. Schedulers take in a `metric` which needs to be optimized during the experiment runs.\n",
        "\n",
        "### 1. [Median Stopping Rule](https://ray.readthedocs.io/en/latest/tune-schedulers.html#median-stopping-rule)\n",
        "\n",
        "This method stops a trial if its performance falls below the median of the performances of other trials at similar points.\n",
        "\n",
        "### 2. [Async HyperBand](https://ray.readthedocs.io/en/latest/tune-schedulers.html#asynchronous-hyperband)\n",
        "\n",
        "This enables early stopping using the HyperBand optimization algorithm which divides the trials into brackets of varying sizes, early-stopping low-performing trials within each bracket periodically. RayTune also provides an implementation of standard [HyperBand](https://ray.readthedocs.io/en/latest/tune-schedulers.html#hyperband). The Asynchronous version of the algorithm provides more parallelism and it is recommended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgVBS9F3appg"
      },
      "outputs": [],
      "source": [
        "def select_sched_alg(sched_alg):\n",
        "    \"\"\"\n",
        "     Initialize a scheduling algorithm that is selected using 'sched_alg'\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        sched_alg   : str; Selecting the search algorithm. Possible values\n",
        "                       [MedianStop, AsyncHyperBand]\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        alg : Object of the RayTune scheduling algorithm selected\n",
        "    \"\"\"\n",
        "    sched = None\n",
        "    if sched_alg == \"AsyncHyperBand\":\n",
        "        sched = AsyncHyperBandScheduler(\n",
        "            time_attr=\"training_iteration\",\n",
        "            metric=\"test_accuracy\",\n",
        "            mode=\"max\",\n",
        "            max_t=50,\n",
        "            grace_period=1,\n",
        "            reduction_factor=3,\n",
        "            brackets=3,\n",
        "        )\n",
        "    elif sched_alg == \"MedianStop\":\n",
        "        sched = MedianStoppingRule(\n",
        "            time_attr=\"time_total_s\",\n",
        "            metric=\"test_accuracy\",\n",
        "            mode=\"max\",\n",
        "            grace_period=1,\n",
        "            min_samples_required=3,\n",
        "        )\n",
        "    else:\n",
        "        print(\"Unknown Option. Select MedianStop or AsyncHyperBand\")\n",
        "    return sched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7c9K8LBappg"
      },
      "source": [
        "## Setting up the experiment\n",
        "\n",
        "Selecting our search and scheduling algorithm and setting up the experiment name for saving the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yl8O46uappg"
      },
      "outputs": [],
      "source": [
        "num_samples = 5  # this is the number of trials. Lower this number for demos/testing\n",
        "compute = (\n",
        "    \"GPU\"  # Can take a CPU value (only for performance comparison. Not recommended)\n",
        ")\n",
        "CV_folds = 3  # The number of Cross-Validation folds to be performed\n",
        "\n",
        "# NOTE: selection of algoritm\n",
        "search_alg = \"BayesOpt\"  # Options: Optuna or BayesOpt\n",
        "\n",
        "# NOTE: selection of search strategy for the scheduler\n",
        "sched_alg = \"AsyncHyperBand\"  # Options: AsyncHyperBand or MedianStop\n",
        "\n",
        "max_concurrent = (\n",
        "    -1\n",
        ")  # Number of concurrent samples; if -1, determined by number of available GPU for GPU tasks.\n",
        "\n",
        "# HPO Param ranges\n",
        "# NOTE: Depending on the GPU memory we might need to adjust the parameter range for a successful run\n",
        "n_estimators_range = (50, 1000)\n",
        "max_depth_range = (2, 15)\n",
        "max_features_range = (0.1, 0.8)\n",
        "\n",
        "hpo_ranges = {\n",
        "    \"n_estimators\": n_estimators_range,\n",
        "    \"max_depth\": max_depth_range,\n",
        "    \"max_features\": max_features_range,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGKEl30kappg"
      },
      "outputs": [],
      "source": [
        "if _DEBUG:\n",
        "    # Only use 1 GPU when debugging\n",
        "    ray.init(local_mode=True, num_gpus=1)\n",
        "    num_samples = 3\n",
        "else:\n",
        "    ray.init(dashboard_host=\"0.0.0.0\" , ignore_reinit_error=True)\n",
        "\n",
        "if max_concurrent == -1:\n",
        "    if compute == \"GPU\":\n",
        "        max_concurrent = cupy.cuda.runtime.getDeviceCount()\n",
        "    else:\n",
        "        raise Exception(\"For CPU, must specify max_concurrent value\")\n",
        "\n",
        "cpu_per_sample = int(multiprocessing.cpu_count() / max_concurrent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo-SGV8_appg"
      },
      "outputs": [],
      "source": [
        "cdf = prepare_dataset()\n",
        "\n",
        "# for shared access across processes\n",
        "data_id = ray.put(cdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LjA8kYmapph"
      },
      "outputs": [],
      "source": [
        "# what is data_id ?\n",
        "type(data_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4FEmA-tapph"
      },
      "outputs": [],
      "source": [
        "search = build_search_alg(search_alg, hpo_ranges)\n",
        "\n",
        "sched = select_sched_alg(sched_alg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwqePzjFapph"
      },
      "outputs": [],
      "source": [
        "exp_name = None\n",
        "\n",
        "if exp_name is not None:\n",
        "    exp_name += exp_name\n",
        "else:\n",
        "    exp_name = \"\"\n",
        "    exp_name += \"{}_{}_CV-{}_{}M_SAMP-{}\".format(\n",
        "        \"RF\", compute, CV_folds, int(num_rows / 1000000), num_samples\n",
        "    )\n",
        "\n",
        "    exp_name += \"_{}\".format(\"Random\" if search_alg is None else search_alg)\n",
        "\n",
        "    if sched_alg is not None:\n",
        "        exp_name += \"_{}\".format(sched_alg)\n",
        "\n",
        "static_config = {\n",
        "    \"dataset_filename\": os.path.basename(orc_name),\n",
        "    \"compute\": compute,\n",
        "    \"results\": None,\n",
        "    \"num_workers\": cpu_per_sample if compute == \"CPU\" else 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qnWs0k0apph"
      },
      "source": [
        "## Running the experiment\n",
        "\n",
        "We have all that we need in order to run the experiment, let us set up the `tune.run` call. We pass our selected searching and scheduling algorithm, pass the `WrappedTrainable` class. The other options include stopping conditions for the trials, configuration ranges, checkpointing specifics, etc. Read more about the parameters [here](https://ray.readthedocs.io/en/latest/tune/api_docs/execution.html#tune-run)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzxK7sKyapph"
      },
      "outputs": [],
      "source": [
        "# Create data and airline-data directories if they don't exist\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "local_dir = data_dir\n",
        "print(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwVPrZsoapph"
      },
      "outputs": [],
      "source": [
        "analysis = tune.run(\n",
        "    WrappedTrainable,\n",
        "    name=exp_name,\n",
        "    scheduler=sched,\n",
        "    search_alg=search,\n",
        "    stop={\n",
        "        \"training_iteration\": CV_folds,\n",
        "        \"is_bad\": True,\n",
        "    },\n",
        "    resources_per_trial={\"cpu\": cpu_per_sample, \"gpu\": int(compute == \"GPU\")},\n",
        "    num_samples=num_samples,\n",
        "    checkpoint_at_end=True,\n",
        "    keep_checkpoints_num=1,\n",
        "    storage_path=local_dir,\n",
        "    trial_name_creator=get_trial_name,\n",
        "    checkpoint_score_attr=\"test_accuracy\",\n",
        "    verbose=1,\n",
        "    raise_on_failed_trial=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa4lzQBfapph"
      },
      "outputs": [],
      "source": [
        "# Save results for plotting\n",
        "csv_path = os.path.join(data_dir, f\"{search_alg}_{sched_alg}_trials.csv\")\n",
        "\n",
        "analysis.dataframe().to_csv(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHk5J_9Gapph"
      },
      "outputs": [],
      "source": [
        "print(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZhkgRndNa__f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl91KErXapph"
      },
      "source": [
        "## Plotting\n",
        "\n",
        "Let's see how our search performed with different options, to do this we will read from different experiment results that we have (including previous runs of the notebook).\n",
        "\n",
        "There are four plots; in the first two, we will be plotting the test accuracies we were able to achieve with the experiments for BayesOpt and Optuna separately (if both were run at least once). The next graph compares the average per optimization run and finally, we compare the cumulative maximum of the performance.\n",
        "\n",
        "These plots can be useful in helping us understand how performing HPO helps and the effectiveness of the individual search methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqmMiNo8apph"
      },
      "outputs": [],
      "source": [
        "from scipy.interpolate import interp1d\n",
        "from scipy.signal import savgol_filter\n",
        "import matplotlib.lines as mlines\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.markers as mmarkers\n",
        "\n",
        "\n",
        "trials_glob = os.path.join(data_dir, \"*.csv\")\n",
        "\n",
        "\n",
        "\n",
        "trials_csvs = glob.glob(trials_glob)\n",
        "csvs_by_type = {}\n",
        "\n",
        "for trial in trials_csvs:\n",
        "    csv_name = os.path.split(trial)[-1]\n",
        "\n",
        "    trial_type = csv_name.split(\"_\")[0]\n",
        "    if trial_type not in csvs_by_type:\n",
        "        csvs_by_type[trial_type] = []\n",
        "\n",
        "    csvs_by_type[trial_type].append(trial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH55n9qEapph"
      },
      "outputs": [],
      "source": [
        "combined_dfs = {}\n",
        "\n",
        "max_val = 0.0\n",
        "min_val = 1.0\n",
        "n_rows = num_samples\n",
        "for type_name, trials in csvs_by_type.items():\n",
        "    temp_dfs = []\n",
        "    max_val = 0.0\n",
        "    min_val = 1.0\n",
        "\n",
        "    for trial in trials:\n",
        "        imported_trial = pd.read_csv(trial)\n",
        "\n",
        "        filtered_df: pd.DataFrame = imported_trial[\"test_accuracy\"]\n",
        "\n",
        "        max_val = max(max_val, filtered_df.values.max())\n",
        "        min_val = min(min_val, filtered_df.values.min())\n",
        "\n",
        "        temp_dfs.append(filtered_df)\n",
        "\n",
        "    comb_df = pd.concat(temp_dfs, axis=1)\n",
        "\n",
        "    mean = comb_df.mean(axis=1)\n",
        "    std = comb_df.std(axis=1)\n",
        "    max_col = comb_df.max(axis=1)\n",
        "    min_col = comb_df.min(axis=1)\n",
        "    upper = pd.concat([mean + std, max_col], axis=1).min(axis=1)\n",
        "    lower = pd.concat([mean - std, min_col], axis=1).max(axis=1)\n",
        "    cummax = comb_df.cummax()\n",
        "\n",
        "    comb_df[\"Mean\"] = mean\n",
        "    comb_df[\"Std\"] = std\n",
        "    comb_df[\"Max\"] = max_col\n",
        "    comb_df[\"Min\"] = min_col\n",
        "    comb_df[\"Upper\"] = upper\n",
        "    comb_df[\"Lower\"] = lower\n",
        "    # comb_df[\"CumMax\"] = cummax\n",
        "\n",
        "    combined_dfs[type_name] = comb_df\n",
        "    if len(comb_df) < n_rows:\n",
        "        n_rows = len(comb_df)\n",
        "    print(\n",
        "        \"{} (min, max): {}, {}\".format(type_name, round(min_val, 4), round(max_val, 4))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o0J4ROhapph"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "x = np.arange(0, n_rows, 1)\n",
        "x_smoothed = np.linspace(x.min(), x.max(), 150)\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "\n",
        "fig, axes = None, None\n",
        "if len(combined_dfs) == 2:\n",
        "    fig, axes = plt.subplots(4, 1, sharex=True, sharey=True, figsize=(10, 28))\n",
        "elif len(combined_dfs) == 1:\n",
        "    fig, axes = plt.subplots(3, 1, sharex=True, sharey=True, figsize=(10, 28))\n",
        "all_axes = {}\n",
        "i = 0\n",
        "for key in combined_dfs.keys():\n",
        "    all_axes[key] = axes[i]\n",
        "    i += 1\n",
        "all_axes[\"All\"] = axes[i]\n",
        "i += 1\n",
        "all_axes[\"Max\"] = axes[i]\n",
        "\n",
        "comb_axes = axes[i - 1]\n",
        "max_axes = axes[i]\n",
        "\n",
        "colors = {\n",
        "    \"BayesOpt\": \"C2\",\n",
        "    \"Optuna\": \"C1\",\n",
        "}\n",
        "\n",
        "comb_legend_handles = []\n",
        "max_legend_handles = []\n",
        "\n",
        "for name, df in combined_dfs.items():\n",
        "    df = df[:n_rows]\n",
        "    ymean = df[\"Mean\"]\n",
        "    ystd = df[\"Std\"]\n",
        "    ystd_up = df[\"Upper\"]\n",
        "    ystd_dn = df[\"Lower\"]\n",
        "\n",
        "    ax = all_axes[name]\n",
        "\n",
        "    # Smooth the means to make it easier to read\n",
        "    itp = interp1d(x, ymean, kind=\"linear\")\n",
        "    window_size, poly_order = 17, 3\n",
        "    ymean_smoothed = savgol_filter(itp(x_smoothed), window_size, poly_order)\n",
        "\n",
        "    ax.plot(x_smoothed, ymean_smoothed, \"-\", color=colors[name])\n",
        "    ax.plot(x, df[\"test_accuracy\"], \"o\", color=colors[name], markersize=1.0)\n",
        "    ax.fill_between(x, ystd_up, ystd_dn, alpha=0.2, color=colors[name])\n",
        "    ax.set_title(name)\n",
        "\n",
        "    mean_line = mlines.Line2D(\n",
        "        [], [], color=colors[name], markersize=1, label=\"Smoothed Mean\"\n",
        "    )\n",
        "    points = mlines.Line2D(\n",
        "        [],\n",
        "        [],\n",
        "        color=colors[name],\n",
        "        marker=\"o\",\n",
        "        markersize=1,\n",
        "        linestyle=\"\",\n",
        "        label=\"Achieved Accuracy\",\n",
        "    )\n",
        "    std_patch = mpatches.Patch(color=colors[name], alpha=0.2, label=\"Std. Dev\")\n",
        "    ax.legend(handles=[mean_line, points, std_patch], loc=\"lower right\")\n",
        "\n",
        "    comb_axes.set_title(\"Comparison of Average Output Per Optimization\")\n",
        "    comb_axes.plot(x_smoothed, ymean_smoothed, \"-\", color=colors[name])\n",
        "    comb_axes.plot(x, ymean, \"o\", color=colors[name], markersize=2)\n",
        "    comb_axes.fill_between(x, ystd_up, ystd_dn, alpha=0.2, color=colors[name])\n",
        "\n",
        "    comb_legend_handles.append(\n",
        "        mlines.Line2D([], [], color=colors[name], marker=\"o\", markersize=2, label=name)\n",
        "    )\n",
        "\n",
        "    max_axes.set_title(\"Comparison of Average Cumulative Max Value Per Optimization\")\n",
        "    max_axes.plot(\n",
        "        x,\n",
        "        df[[\"test_accuracy\"]].cummax().mean(axis=1),\n",
        "        \"-\",\n",
        "        color=colors[name],\n",
        "        alpha=1.0,\n",
        "    )\n",
        "\n",
        "    max_legend_handles.append(mlines.Line2D([], [], color=colors[name], label=name))\n",
        "\n",
        "comb_axes.legend(handles=comb_legend_handles, loc=\"lower right\")\n",
        "max_axes.legend(handles=max_legend_handles, loc=\"lower right\")\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "fig.savefig(\"SearchComparison.png\", facecolor=\"white\", transparent=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT_Mk3bhapph"
      },
      "source": [
        "In order to get informative plots, run the notebook with `BayesOpt` once and then with `Optuna`.\n",
        "\n",
        "These graphs show the mean accuracy for each trial across multiple runs along with standard deviation.\n",
        "\n",
        "The comparison graphs show the two different optimizations on the same one to help see how each of them performed at similar points in optimizations.\n",
        "\n",
        "The last graph shows the cumulative maximum for each trial per optimization. This is useful in understanding which of the optimization techniques arrive at the \"best\" parameters quickly and how well each of them perform.\n",
        "\n",
        "## Conclusion and Next Steps\n",
        "We notice an improvement in performance with just `2500000` rows out of a total 115M available in the whole dataset. Generally HPO yields better results with larger datasets and wider parameter ranges. It is encouraged to repeat this experiment with different models, swapping out different search and scheduling algorithms to understand how RayTune works. In this notebook, we use a rather basic metric `test_accuracy` for evaluating the performance. You can use a more informative metric or even use a combination of metrics to evaluate. These tweaks can help boost the performance of a model greatly and make HPO quite powerful.\n",
        "\n",
        "RayTune provides a clean way to run experiments and it provides powerful tools like scheduling algorithms and efficient optimization techniques to help us arrive at the results quicker and in an efficient way. Hopefully, this notebook can serve as a good starting point to explore [RayTune](https://ray.readthedocs.io/en/latest/index.html) + [RAPIDS](https://rapids.ai/) for HPO experiments.\n",
        "\n",
        "#### Learn More\n",
        "- [Algorithms for Hyper-Parameter Optimization](http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)\n",
        "- [5 Classification Evaluation metrics](https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226)\n",
        "- [RayTune Examples](https://github.com/ray-project/ray/tree/master/python/ray/tune/examples)\n",
        "- [RAPIDS on GitHub](https://github.com/rapidsai)\n",
        "- Anyscale has series of online events this Summer, called Ray Summit Connect, where you can learn more about Ray. For information, visit the [events page](https://anyscale.com/events)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3FstiV7apph"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}