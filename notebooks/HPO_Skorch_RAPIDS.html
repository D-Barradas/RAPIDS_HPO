<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>HPO_Skorch_RAPIDS</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '•';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        const { svg } = await mermaid.render(id, raw, el);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Adaptive-Hyperparameter-Optimization-with-Dask-ML">Adaptive Hyperparameter Optimization with Dask-ML<a class="anchor-link" href="#Adaptive-Hyperparameter-Optimization-with-Dask-ML">¶</a></h3><p>This notebook demonstrates Hyperband, a model selection algorithm now part of Dask-ML. It showcases:</p>
<ul>
<li><strong>Problem:</strong> Tuning hyperparameters for good model performance.</li>
<li><strong>Realistic Use Case:</strong> Image denoising with a deep learning model.</li>
<li><strong>Hyperparameter Optimization:</strong><ul>
<li>Explores parameter space with Hyperband.</li>
<li>Compares with early stopping techniques.</li>
</ul>
</li>
<li><strong>Results Visualization:</strong><ul>
<li>Analyzes input/output data.</li>
<li>Visualizes best model's performance.</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Importance-of-Hyperparameter-Tuning">Importance of Hyperparameter Tuning<a class="anchor-link" href="#Importance-of-Hyperparameter-Tuning">¶</a></h3><p>Any machine learning model requires tuning hyperparameters for optimal performance. This notebook focuses on image denoising, where crucial parameters include:</p>
<div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'module__init'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'xavier_uniform_'</span><span class="p">,</span> <span class="s1">'xavier_normal_'</span><span class="p">,</span> <span class="s1">'kaiming_uniform_'</span><span class="p">,</span> <span class="s1">'kaiming_normal_'</span><span class="p">],</span>
    <span class="s1">'module__activation'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'ReLU'</span><span class="p">,</span> <span class="s1">'LeakyReLU'</span><span class="p">,</span> <span class="s1">'ELU'</span><span class="p">,</span> <span class="s1">'PReLU'</span><span class="p">],</span>
    <span class="s1">'optimizer'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"SGD"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"Adam"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">'batch_size'</span><span class="p">:</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
    <span class="s1">'optimizer__lr'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
    <span class="s1">'optimizer__weight_decay'</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">200</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="s1">'optimizer__nesterov'</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">],</span>
    <span class="s1">'optimizer__momentum'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
    <span class="s1">'train_split'</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">],</span>
<span class="p">}</span>


<span class="n">These</span> <span class="n">parameters</span> <span class="n">control</span><span class="p">:</span>

<span class="o">*</span> <span class="n">Model</span> <span class="n">initialization</span>
<span class="o">*</span> <span class="n">Activation</span> <span class="n">function</span>
<span class="o">*</span> <span class="n">Optimizer</span> <span class="ow">and</span> <span class="n">its</span> <span class="n">hyperparameters</span> <span class="p">(</span><span class="n">learning</span> <span class="n">rate</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">etc</span><span class="o">.</span><span class="p">)</span>
<span class="o">*</span> <span class="n">Batch</span> <span class="n">size</span>
</pre></div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span> <span class="c1"># Reduce number of messages/warnings displayed</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># !export PYTHONPATH=../src:$PYTHONPATH</span>
<span class="c1"># !echo $PYTHONPATH</span>
<span class="o">!</span>cp<span class="w"> </span>-u<span class="w"> </span>../src/noisy_mnist.py<span class="w"> </span>.<span class="w"> </span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">date</span>

<span class="n">today</span> <span class="o">=</span> <span class="n">date</span><span class="o">.</span><span class="n">today</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Today's date:"</span><span class="p">,</span> <span class="n">today</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Today's date: 2024-02-07
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">path</span> <span class="o">=</span> <span class="s2">"/"</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"/"</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">results_folder</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"data/results/skorch_run/</span><span class="si">{</span><span class="n">today</span><span class="si">}</span><span class="s2">"</span>
<span class="n">absolutepath_to_results</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span><span class="n">results_folder</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">absolutepath_to_results</span><span class="p">,</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">socket</span>
<span class="n">s</span><span class="o">=</span><span class="n">socket</span><span class="o">.</span><span class="n">socket</span><span class="p">()</span>
<span class="n">s</span><span class="o">.</span><span class="n">bind</span><span class="p">((</span><span class="s2">""</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">port</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">getsockname</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">s</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span><span class="p">,</span> <span class="n">wait</span>
<span class="kn">from</span> <span class="nn">dask_cuda</span> <span class="kn">import</span> <span class="n">LocalCUDACluster</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">cluster</span> <span class="o">=</span> <span class="n">LocalCUDACluster</span><span class="p">(</span><span class="n">dashboard_address</span><span class="o">=</span><span class="sa">f</span><span class="s2">"127.0.0.1:</span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>

<span class="n">client</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<div style="width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;"> </div>
<div style="margin-left: 48px;">
<h3 style="margin-bottom: 0px;">Client</h3>
<p style="color: #9D9D9D; margin-bottom: 0px;">Client-2af7bbc1-c5a9-11ee-914e-ac1f6b685fd7</p>
<table style="width: 100%; text-align: left;">
<tr>
<td style="text-align: left;"><strong>Connection method:</strong> Cluster object</td>
<td style="text-align: left;"><strong>Cluster type:</strong> dask_cuda.LocalCUDACluster</td>
</tr>
<tr>
<td style="text-align: left;">
<strong>Dashboard: </strong> <a href="http://127.0.0.1:33325/status" target="_blank">http://127.0.0.1:33325/status</a>
</td>
<td style="text-align: left;"></td>
</tr>
</table>
<details>
<summary style="margin-bottom: 20px;"><h3 style="display: inline;">Cluster Info</h3></summary>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output" tabindex="0">
<div style="width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;">
</div>
<div style="margin-left: 48px;">
<h3 style="margin-bottom: 0px; margin-top: 0px;">LocalCUDACluster</h3>
<p style="color: #9D9D9D; margin-bottom: 0px;">3c2976bb</p>
<table style="width: 100%; text-align: left;">
<tr>
<td style="text-align: left;">
<strong>Dashboard:</strong> <a href="http://127.0.0.1:33325/status" target="_blank">http://127.0.0.1:33325/status</a>
</td>
<td style="text-align: left;">
<strong>Workers:</strong> 2
                </td>
</tr>
<tr>
<td style="text-align: left;">
<strong>Total threads:</strong> 2
                </td>
<td style="text-align: left;">
<strong>Total memory:</strong> 187.57 GiB
                </td>
</tr>
<tr>
<td style="text-align: left;"><strong>Status:</strong> running</td>
<td style="text-align: left;"><strong>Using processes:</strong> True</td>
</tr>
</table>
<details>
<summary style="margin-bottom: 20px;">
<h3 style="display: inline;">Scheduler Info</h3>
</summary>
<div style="">
<div>
<div style="width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;"> </div>
<div style="margin-left: 48px;">
<h3 style="margin-bottom: 0px;">Scheduler</h3>
<p style="color: #9D9D9D; margin-bottom: 0px;">Scheduler-fe918e87-9d8d-4973-aa82-328cd0c2cd3d</p>
<table style="width: 100%; text-align: left;">
<tr>
<td style="text-align: left;">
<strong>Comm:</strong> tcp://127.0.0.1:45399
                    </td>
<td style="text-align: left;">
<strong>Workers:</strong> 2
                    </td>
</tr>
<tr>
<td style="text-align: left;">
<strong>Dashboard:</strong> <a href="http://127.0.0.1:33325/status" target="_blank">http://127.0.0.1:33325/status</a>
</td>
<td style="text-align: left;">
<strong>Total threads:</strong> 2
                    </td>
</tr>
<tr>
<td style="text-align: left;">
<strong>Started:</strong> Just now
                    </td>
<td style="text-align: left;">
<strong>Total memory:</strong> 187.57 GiB
                    </td>
</tr>
</table>
</div>
</div>
<details style="margin-left: 48px;">
<summary style="margin-bottom: 20px;">
<h3 style="display: inline;">Workers</h3>
</summary>
<div style="margin-bottom: 20px;">
<div style="width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;"> </div>
<div style="margin-left: 48px;">
<details>
<summary>
<h4 style="margin-bottom: 0px; display: inline;">Worker: 0</h4>
</summary>
<table style="width: 100%; text-align: left;">
<tr>
<td style="text-align: left;">
<strong>Comm: </strong> tcp://127.0.0.1:40671
                        </td>
<td style="text-align: left;">
<strong>Total threads: </strong> 1
                        </td>
</tr>
<tr>
<td style="text-align: left;">
<strong>Dashboard: </strong> <a href="http://127.0.0.1:33789/status" target="_blank">http://127.0.0.1:33789/status</a>
</td>
<td style="text-align: left;">
<strong>Memory: </strong> 93.78 GiB
                        </td>
</tr>
<tr>
<td style="text-align: left;">
<strong>Nanny: </strong> tcp://127.0.0.1:42051
                        </td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td colspan="2" style="text-align: left;">
<strong>Local directory: </strong> /tmp/dask-scratch-space/worker-55bloyxu
                        </td>
</tr>
<tr>
<td style="text-align: left;">
<strong>GPU: </strong>Quadro GV100
                        </td>
<td style="text-align: left;">
<strong>GPU memory: </strong> 31.74 GiB
                        </td>
</tr>
</table>
</details>
</div>
</div>
<div style="margin-bottom: 20px;">
<div style="width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;"> </div>
<div style="margin-left: 48px;">
<details>
<summary>
<h4 style="margin-bottom: 0px; display: inline;">Worker: 1</h4>
</summary>
<table style="width: 100%; text-align: left;">
<tr>
<td style="text-align: left;">
<strong>Comm: </strong> tcp://127.0.0.1:35239
                        </td>
<td style="text-align: left;">
<strong>Total threads: </strong> 1
                        </td>
</tr>
<tr>
<td style="text-align: left;">
<strong>Dashboard: </strong> <a href="http://127.0.0.1:41265/status" target="_blank">http://127.0.0.1:41265/status</a>
</td>
<td style="text-align: left;">
<strong>Memory: </strong> 93.78 GiB
                        </td>
</tr>
<tr>
<td style="text-align: left;">
<strong>Nanny: </strong> tcp://127.0.0.1:44341
                        </td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td colspan="2" style="text-align: left;">
<strong>Local directory: </strong> /tmp/dask-scratch-space/worker-4qzwcpg8
                        </td>
</tr>
<tr>
<td style="text-align: left;">
<strong>GPU: </strong>Quadro GV100
                        </td>
<td style="text-align: left;">
<strong>GPU memory: </strong> 31.75 GiB
                        </td>
</tr>
</table>
</details>
</div>
</div>
</details>
</div>
</details>
</div>
</div>
</details>
</div>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">time</span> client.upload_file('../models/autoencoder.py')
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>CPU times: user 1.64 s, sys: 343 ms, total: 1.98 s
Wall time: 3.93 s
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>{'tcp://127.0.0.1:35239': {'status': 'OK'},
 'tcp://127.0.0.1:40671': {'status': 'OK'}}</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">dask_ml</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s1">'Dask ml version : </span><span class="si">{</span><span class="n">dask_ml</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s1">'</span> <span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Dask ml version : 2023.3.24
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Dataset-and-Model">Dataset and Model<a class="anchor-link" href="#Dataset-and-Model">¶</a></h3><ul>
<li><strong>Dataset:</strong> Noisy MNIST images for denoising.</li>
<li><strong>Model:</strong> Deep learning autoencoder with a latent dimension of 49.</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">noisy_mnist</span>
<span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">70_000</span> <span class="o">//</span> <span class="mi">3</span>
<span class="n">_X</span><span class="p">,</span> <span class="n">_y</span> <span class="o">=</span> <span class="n">noisy_mnist</span><span class="o">.</span><span class="n">dataset</span><span class="p">()</span>
<span class="n">_X</span> <span class="o">=</span> <span class="n">_X</span><span class="p">[:</span><span class="n">chunk_size</span> <span class="o">*</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">_y</span> <span class="o">=</span> <span class="n">_y</span><span class="p">[:</span><span class="n">chunk_size</span> <span class="o">*</span> <span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">_X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">_X</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">_X</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">_X</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>((69999, 784), dtype('float32'), 0.0, 1.0)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">_y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">_y</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">_y</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">_y</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>((69999, 784), dtype('float32'), 0.0, 1.0)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">dask.array</span> <span class="k">as</span> <span class="nn">da</span>
<span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">_X</span><span class="o">.</span><span class="n">shape</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">da</span><span class="o">.</span><span class="n">from_array</span><span class="p">(</span><span class="n">_X</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="p">(</span><span class="n">n</span> <span class="o">//</span> <span class="mi">3</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">da</span><span class="o">.</span><span class="n">from_array</span><span class="p">(</span><span class="n">_y</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="n">n</span> <span class="o">//</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(dask.array&lt;array, shape=(69999, 784), dtype=float32, chunksize=(23333, 784), chunktype=numpy.ndarray&gt;,
 dask.array&lt;array, shape=(69999, 784), dtype=float32, chunksize=(23333, 784), chunktype=numpy.ndarray&gt;)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">cols</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">w</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">cols</span><span class="o">*</span><span class="n">w</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">w</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="n">cols</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="p">(</span><span class="n">upper</span><span class="p">,</span> <span class="n">lower</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])):</span>
    <span class="k">if</span> <span class="n">col</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">upper</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">28</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="s1">'ground</span><span class="se">\n</span><span class="s1">truth'</span><span class="p">)</span>
        <span class="n">lower</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">28</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="s1">'input'</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="n">noisy</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
    <span class="n">clean</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'cbar'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">'xticklabels'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">'yticklabels'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">'cmap'</span><span class="p">:</span> <span class="s1">'gray_r'</span><span class="p">}</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">noisy</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">clean</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">upper</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">absolutepath_to_results</span><span class="si">}</span><span class="s2">/input-output.svg"</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">"tight"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Model-Architecture">Model Architecture<a class="anchor-link" href="#Model-Architecture">¶</a></h3><p>The autoencoder model uses PyTorch through the scikit-learn interface <a href="https://github.com/dnouri/skorch">skorch</a>.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">autoencoder</span> <span class="kn">import</span> <span class="n">Autoencoder</span><span class="p">,</span> <span class="n">NegLossScore</span>
<span class="kn">import</span> <span class="nn">torch</span>


<span class="k">def</span> <span class="nf">trim_params</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">'optimizer'</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">'Adam'</span><span class="p">:</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">'optimizer__amsgrad'</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">'optimizer'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">'Adam'</span><span class="p">:</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">'optimizer__lr'</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">'optimizer'</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">'SGD'</span><span class="p">:</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">'optimizer__nesterov'</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">'optimizer__momentum'</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s1">'optimizer'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">'optimizer'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">kwargs</span>

<span class="k">class</span> <span class="nc">TrimParams</span><span class="p">(</span><span class="n">NegLossScore</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="n">trim_params</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">TrimParams</span><span class="p">(</span>
    <span class="n">module</span><span class="o">=</span><span class="n">Autoencoder</span><span class="p">,</span>
    <span class="n">criterion</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">,</span>
    <span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">train_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">max_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>I don't show it here; I'd rather concentrate on tuning hyperparameters. But briefly, it's a simple fully connected 3 hidden layer autoencoder with a latent dimension of 49.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Hyperparameter-Optimization-Setup">Hyperparameter Optimization Setup<a class="anchor-link" href="#Hyperparameter-Optimization-Setup">¶</a></h3><ul>
<li>Uses Dask for distributed computing.</li>
<li>Defines parameter search space (<code>params</code>).</li>
<li>Sets random state for reproducibility.</li>
</ul>
<h5 id="Parameters">Parameters<a class="anchor-link" href="#Parameters">¶</a></h5><p>The parameters interesting in tuning are</p>
<ul>
<li>model<ul>
<li>initialization</li>
<li>activation function</li>
<li>weight decay (which is similar to $\ell_2$ regularization)</li>
</ul>
</li>
<li>optimizer<ul>
<li>which optimizer to use (e.g., Adam, SGD)</li>
<li>batch size used to approximate gradient</li>
<li>learning rate (but not for Adam)</li>
<li>momentum for SGD</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'module__init'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'xavier_uniform_'</span><span class="p">,</span>
                     <span class="s1">'xavier_normal_'</span><span class="p">,</span>
                     <span class="s1">'kaiming_uniform_'</span><span class="p">,</span>
                     <span class="s1">'kaiming_normal_'</span><span class="p">,</span>
                    <span class="p">],</span>
    <span class="s1">'module__activation'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'ReLU'</span><span class="p">,</span> <span class="s1">'LeakyReLU'</span><span class="p">,</span> <span class="s1">'ELU'</span><span class="p">,</span> <span class="s1">'PReLU'</span><span class="p">],</span>
    <span class="s1">'optimizer'</span><span class="p">:</span> <span class="p">[</span><span class="s2">"SGD"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"Adam"</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">'batch_size'</span><span class="p">:</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
    <span class="s1">'optimizer__lr'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
    <span class="s1">'optimizer__weight_decay'</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">200</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
    <span class="s1">'optimizer__nesterov'</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">],</span>
    <span class="s1">'optimizer__momentum'</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
    <span class="s1">'train_split'</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">],</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<p>Testing <code>optimizer</code> to be <code>SGD</code> or <code>Adam</code> comes from here "<a href="https://arxiv.org/pdf/1705.08292.pdf">The Marginal Value of Adaptive Gradient Methods in Machine Learning</a>". From their abstract,</p>
<blockquote>
<p>We observe that the solutions found by adaptive methods generalize worse (often sig- nificantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.</p>
</blockquote>
<p>Their experiments in Figure 1b show that non-adaptive methods (SGD and heavy ball) perform much better than adaptive methods.</p>
<p>They have to do some tuning for this. <strong>Can we replicate their result?</strong></p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># # for debugging; ignore this cell</span>
<span class="c1"># from sklearn.linear_model import SGDClassifier</span>
<span class="c1"># from sklearn.datasets import make_classification</span>
<span class="c1"># from sklearn.model_selection import ParameterSampler</span>
<span class="c1"># import dask.array as da</span>
<span class="c1"># import numpy as np</span>
<span class="c1"># model = SGDClassifier()</span>
<span class="c1"># params = {'alpha': np.logspace(-7, 0, num=int(1e6))}</span>

<span class="c1"># n, d = int(10e3), 700</span>
<span class="c1"># _X, _y = make_classification(n_samples=n, n_features=d,</span>
<span class="c1">#                              random_state=1)</span>
<span class="c1"># X = da.from_array(_X, chunks=(n // 10, d))</span>
<span class="c1"># y = da.from_array(_y, chunks=n // 10)</span>
<span class="c1"># X, y</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>

<span class="k">def</span> <span class="nf">fmt</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">fmt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">obj</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">fmt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">obj</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">obj</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">obj</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">msgpack</span>

<span class="k">def</span> <span class="nf">save_search</span><span class="p">(</span><span class="n">search</span><span class="p">,</span> <span class="n">today</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># create the folder into results and direct the results over there</span>
    <span class="n">path</span> <span class="o">=</span> <span class="s2">"/"</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"/"</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">results_folder</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"results/skorch_run/</span><span class="si">{</span><span class="n">today</span><span class="si">}</span><span class="s2">"</span>
    <span class="n">absolutepath_to_results</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span><span class="n">results_folder</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">absolutepath_to_results</span><span class="p">,</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">pre</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">absolutepath_to_results</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">today</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">-"</span>


    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">pre</span> <span class="o">+</span> <span class="s2">"test.npz"</span><span class="p">,</span> <span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">y_hat</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
        <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat</span><span class="o">=</span><span class="n">y_hat</span><span class="p">)</span>
    <span class="c1"># skorch models aren't pickable</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">pre</span> <span class="o">+</span> <span class="s2">"params.json"</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">fmt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">search</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">"estimator"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span> <span class="ow">and</span> <span class="s2">"param_distribution"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">k</span><span class="p">}</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
    <span class="c1"># with open(pre + "best-model.joblib", "wb") as f:</span>
    <span class="c1">#     joblib.dump(search.best_estimator_, f)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">pre</span> <span class="o">+</span> <span class="s2">"best-params-and-score.json"</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">({</span><span class="s2">"params"</span><span class="p">:</span> <span class="n">search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="s2">"score"</span><span class="p">:</span> <span class="n">search</span><span class="o">.</span><span class="n">best_score_</span><span class="p">},</span> <span class="n">f</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">pre</span> <span class="o">+</span> <span class="s2">"history.json"</span><span class="p">,</span> <span class="s1">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">search</span><span class="o">.</span><span class="n">history_</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">pre</span> <span class="o">+</span> <span class="s2">"cv_results.json"</span><span class="p">,</span> <span class="s1">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">fmt</span><span class="p">(</span><span class="n">search</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">),</span> <span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Hyperparameter-Optimization-Algorithms">Hyperparameter Optimization Algorithms<a class="anchor-link" href="#Hyperparameter-Optimization-Algorithms">¶</a></h3><p>Compares different optimization algorithms:</p>
<ul>
<li><strong>Hyperband:</strong> Efficiently explores and exploits the parameter space.</li>
<li><strong>Hyperband with Successive Overfitting Prevention (SOP):</strong> Early stops models that don't improve, potentially reducing wasted evaluations.</li>
<li><strong>Incremental Search with Patience:</strong> Adaptively adds and removes models based on performance.</li>
</ul>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h4 id="Data-Exploration">Data Exploration<a class="anchor-link" href="#Data-Exploration">¶</a></h4><p>Visualizes noisy and clean images to understand the task.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">X</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<table>
<tr>
<td>
<table style="border-collapse: collapse;">
<thead>
<tr>
<td> </td>
<th> Array </th>
<th> Chunk </th>
</tr>
</thead>
<tbody>
<tr>
<th> Bytes </th>
<td> 209.35 MiB </td>
<td> 69.78 MiB </td>
</tr>
<tr>
<th> Shape </th>
<td> (69999, 784) </td>
<td> (23333, 784) </td>
</tr>
<tr>
<th> Dask graph </th>
<td colspan="2"> 3 chunks in 1 graph layer </td>
</tr>
<tr>
<th> Data type </th>
<td colspan="2"> float32 numpy.ndarray </td>
</tr>
</tbody>
</table>
</td>
<td>
<svg height="170" style="stroke:rgb(0,0,0);stroke-width:1" width="75">
<!-- Horizontal lines -->
<line style="stroke-width:2" x1="0" x2="25" y1="0" y2="0"></line>
<line x1="0" x2="25" y1="40" y2="40"></line>
<line x1="0" x2="25" y1="80" y2="80"></line>
<line style="stroke-width:2" x1="0" x2="25" y1="120" y2="120"></line>
<!-- Vertical lines -->
<line style="stroke-width:2" x1="0" x2="0" y1="0" y2="120"></line>
<line style="stroke-width:2" x1="25" x2="25" y1="0" y2="120"></line>
<!-- Colored Rectangle -->
<polygon points="0.0,0.0 25.96349288680034,0.0 25.96349288680034,120.0 0.0,120.0" style="fill:#ECB172A0;stroke-width:0"></polygon>
<!-- Text -->
<text font-size="1.0rem" font-weight="100" text-anchor="middle" x="12.981746" y="140.000000">784</text>
<text font-size="1.0rem" font-weight="100" text-anchor="middle" transform="rotate(-90,45.963493,60.000000)" x="45.963493" y="60.000000">69999</text>
</svg>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">dask_ml.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(dask.array&lt;concatenate, shape=(62997, 784), dtype=float32, chunksize=(20999, 784), chunktype=numpy.ndarray&gt;,
 dask.array&lt;concatenate, shape=(7002, 784), dtype=float32, chunksize=(2334, 784), chunktype=numpy.ndarray&gt;)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">y_train</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<table>
<tr>
<td>
<table style="border-collapse: collapse;">
<thead>
<tr>
<td> </td>
<th> Array </th>
<th> Chunk </th>
</tr>
</thead>
<tbody>
<tr>
<th> Bytes </th>
<td> 188.41 MiB </td>
<td> 62.80 MiB </td>
</tr>
<tr>
<th> Shape </th>
<td> (62997, 784) </td>
<td> (20999, 784) </td>
</tr>
<tr>
<th> Dask graph </th>
<td colspan="2"> 3 chunks in 12 graph layers </td>
</tr>
<tr>
<th> Data type </th>
<td colspan="2"> float32 numpy.ndarray </td>
</tr>
</tbody>
</table>
</td>
<td>
<svg height="170" style="stroke:rgb(0,0,0);stroke-width:1" width="76">
<!-- Horizontal lines -->
<line style="stroke-width:2" x1="0" x2="26" y1="0" y2="0"></line>
<line x1="0" x2="26" y1="40" y2="40"></line>
<line x1="0" x2="26" y1="80" y2="80"></line>
<line style="stroke-width:2" x1="0" x2="26" y1="120" y2="120"></line>
<!-- Vertical lines -->
<line style="stroke-width:2" x1="0" x2="0" y1="0" y2="120"></line>
<line style="stroke-width:2" x1="26" x2="26" y1="0" y2="120"></line>
<!-- Colored Rectangle -->
<polygon points="0.0,0.0 26.490396879174003,0.0 26.490396879174003,120.0 0.0,120.0" style="fill:#ECB172A0;stroke-width:0"></polygon>
<!-- Text -->
<text font-size="1.0rem" font-weight="100" text-anchor="middle" x="13.245198" y="140.000000">784</text>
<text font-size="1.0rem" font-weight="100" text-anchor="middle" transform="rotate(-90,46.490397,60.000000)" x="46.490397" y="60.000000">62997</text>
</svg>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>

<span class="n">max_iter</span> <span class="o">=</span> <span class="mi">250</span> <span class="c1"># originally this number was 250 to make a decent denoising </span>
<span class="n">history</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">searches</span> <span class="o">=</span> <span class="p">{}</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">dask_ml.model_selection</span> <span class="kn">import</span> <span class="n">HyperbandSearchCV</span>

<span class="n">fit_params</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">SGDClassifier</span><span class="p">):</span>
    <span class="n">fit_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'classes'</span><span class="p">:</span> <span class="n">da</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">compute</span><span class="p">()}</span>
   
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Hyperparameter-Tuning-with-Hyperband">Hyperparameter Tuning with Hyperband<a class="anchor-link" href="#Hyperparameter-Tuning-with-Hyperband">¶</a></h3><p>Trains the model with Hyperband and analyzes the results:</p>
<ul>
<li>Best model parameters</li>
<li>Best model score</li>
<li>Visualizations of the best model's output</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">search</span> <span class="o">=</span> <span class="n">HyperbandSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(62997, 784)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>
<span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>steps = 1, loss = 2.202204704284668
steps = 1, loss = 1.0604509115219116
steps = 1, loss = 2.0787689685821533
steps = 1, loss = 1.9983878135681152
steps = 1, loss = 2.9078874588012695
steps = 1, loss = 2.1339221000671387
steps = 1, loss = 49.96958923339844
steps = 1, loss = 1.6027408838272095
steps = 1, loss = 1.6714401245117188
steps = 83, loss = 2.4591171741485596
steps = 83, loss = 2.6524462699890137
steps = 83, loss = 2.4868273735046387
steps = 83, loss = 3.1829371452331543
steps = 83, loss = 2.051856279373169
steps = 83, loss = 4.839390277862549
steps = 1, loss = 2.002286911010742
steps = 1, loss = 1.0298068523406982
steps = 1, loss = 0.7160890102386475
steps = 1, loss = 1.9761333465576172
steps = 1, loss = 2.0289371013641357
steps = 1, loss = 1.9753522872924805
steps = 83, loss = 3.903987169265747
steps = 83, loss = 2.9585912227630615
steps = 83, loss = 50.01108932495117
steps = 250, loss = 2.5863118171691895
steps = 250, loss = 5.067054271697998
steps = 250, loss = 3.46138858795166
steps = 250, loss = 3.228792667388916
steps = 250, loss = 3.7207000255584717
steps = 250, loss = 2.7746689319610596
steps = 1, loss = 1.9999332427978516
steps = 1, loss = 1.7857023477554321
steps = 1, loss = 1.8042144775390625
steps = 1, loss = 0.970885157585144
steps = 1, loss = 4.930239677429199
steps = 1, loss = 1.2963420152664185
steps = 1, loss = 2.734524726867676
steps = 1, loss = 2.3401176929473877
steps = 1, loss = 49.84938049316406
steps = 1, loss = 4.534388542175293
steps = 1, loss = 41.324462890625
steps = 249, loss = 2.065657138824463
steps = 1, loss = 2.1478776931762695
steps = 1, loss = 1.9174538850784302
steps = 1, loss = 1.5926084518432617
steps = 1, loss = 1.8130954504013062
steps = 1, loss = 1.9271926879882812
steps = 1, loss = 2.45357084274292
steps = 1, loss = 2.5355639457702637
steps = 1, loss = 2.143592119216919
steps = 1, loss = 2.098397970199585
steps = 1, loss = 2.0105671882629395
steps = 1, loss = 2.563915491104126
steps = 1, loss = 2.2251832485198975
steps = 1, loss = 2.529712200164795
steps = 1, loss = 2.350214719772339
steps = 1, loss = 0.9975816011428833
steps = 1, loss = 2.2631888389587402
steps = 1, loss = 3.120868444442749
steps = 1, loss = 0.7099869251251221
steps = 1, loss = 2.0806734561920166
steps = 1, loss = 49.98344039916992
steps = 1, loss = 2.4708755016326904
steps = 1, loss = 1.7782317399978638
steps = 1, loss = 1.8757960796356201
steps = 1, loss = 1.949628233909607
steps = 1, loss = 2.087263822555542
steps = 1, loss = 0.8405274152755737
steps = 1, loss = 1.7991031408309937
steps = 1, loss = 2.2714898586273193
steps = 1, loss = 2.034625291824341
steps = 1, loss = 1.2400376796722412
steps = 249, loss = 2.473175525665283
steps = 9, loss = 3.046734094619751
steps = 9, loss = 2.4863338470458984
steps = 9, loss = 3.6132848262786865
steps = 9, loss = 8.022050857543945
steps = 9, loss = 1.9232337474822998
steps = 9, loss = 2.603837490081787
steps = 9, loss = 3.3170547485351562
steps = 9, loss = 3.4651875495910645
steps = 9, loss = 2.7739338874816895
steps = 9, loss = 2.6896982192993164
steps = 9, loss = 49.94734191894531
steps = 9, loss = 2.9884090423583984
steps = 9, loss = 2.912804126739502
steps = 9, loss = 2.6405251026153564
steps = 9, loss = 2.610262393951416
steps = 9, loss = 2.366626262664795
steps = 9, loss = 3.5408451557159424
steps = 9, loss = 2.761626958847046
steps = 9, loss = 2.204603433609009
steps = 9, loss = 2.4619081020355225
steps = 9, loss = 1.966184377670288
steps = 9, loss = 2.283750057220459
steps = 9, loss = 2.4842588901519775
steps = 9, loss = 3.049499034881592
steps = 9, loss = 1.8599188327789307
steps = 9, loss = 2.2713217735290527
steps = 9, loss = 2.11029052734375
steps = 9, loss = 2.4884209632873535
steps = 9, loss = 2.6915760040283203
steps = 9, loss = 50.00514602661133
steps = 9, loss = 2.1197168827056885
steps = 9, loss = 3.3964104652404785
steps = 9, loss = 3.387035608291626
steps = 9, loss = 2.735473155975342
steps = 9, loss = 2.7994236946105957
steps = 9, loss = 1.9990870952606201
steps = 9, loss = 3.4109694957733154
steps = 9, loss = 2.3063104152679443
steps = 9, loss = 2.1797211170196533
steps = 9, loss = 2.0038392543792725
steps = 9, loss = 1.9318982362747192
steps = 27, loss = 2.2164974212646484
steps = 27, loss = 2.064678430557251
steps = 27, loss = 2.0301594734191895
steps = 27, loss = 2.1418800354003906
steps = 27, loss = 2.377723217010498
steps = 27, loss = 2.1374242305755615
steps = 249, loss = 2.605091094970703
steps = 27, loss = 2.448249101638794
steps = 27, loss = 2.295426368713379
steps = 27, loss = 2.577955484390259
steps = 27, loss = 2.1817567348480225
steps = 27, loss = 2.195711612701416
steps = 27, loss = 2.465963840484619
steps = 27, loss = 2.685737371444702
steps = 1, loss = 50.032718658447266
steps = 1, loss = 4.679951190948486
steps = 1, loss = 2.371753215789795
steps = 1, loss = 2.6846392154693604
steps = 1, loss = 1.9993358850479126
steps = 1, loss = 0.8417843580245972
steps = 1, loss = 6.803238391876221
steps = 1, loss = 49.93523406982422
steps = 1, loss = 1.7455096244812012
steps = 1, loss = 50.002685546875
steps = 1, loss = 1.9234718084335327
steps = 1, loss = 2.177426338195801
steps = 1, loss = 2.333813428878784
steps = 1, loss = 2.0608274936676025
steps = 1, loss = 2.187516689300537
steps = 1, loss = 2.0072691440582275
steps = 1, loss = 2.83785080909729
steps = 1, loss = 2.7498059272766113
steps = 1, loss = 2.0378077030181885
steps = 1, loss = 1.6024774312973022
steps = 1, loss = 1.5122641324996948
steps = 1, loss = 50.01776123046875
steps = 1, loss = 50.011173248291016
steps = 1, loss = 2.351951837539673
steps = 1, loss = 50.03340530395508
steps = 1, loss = 0.7019572257995605
steps = 1, loss = 2.1238062381744385
steps = 1, loss = 2.3906102180480957
steps = 1, loss = 2.399613618850708
steps = 1, loss = 1.7182542085647583
steps = 1, loss = 1.9222230911254883
steps = 1, loss = 1.8773568868637085
steps = 1, loss = 1.349562406539917
steps = 1, loss = 2.0149919986724854
steps = 1, loss = 0.6951484084129333
steps = 1, loss = 50.007179260253906
steps = 1, loss = 1.9817843437194824
steps = 1, loss = 3.028855323791504
steps = 1, loss = 2.018341302871704
steps = 1, loss = 2.851391077041626
steps = 1, loss = 1.2481611967086792
steps = 1, loss = 1.9083788394927979
steps = 1, loss = 2.2493174076080322
steps = 1, loss = 1.6219161748886108
steps = 1, loss = 4.589573860168457
steps = 1, loss = 2.643268346786499
steps = 1, loss = 1.9534186124801636
steps = 1, loss = 1.8166946172714233
steps = 1, loss = 1.8177889585494995
steps = 1, loss = 2.0946853160858154
steps = 1, loss = 2.4243786334991455
steps = 1, loss = 1.975293517112732
steps = 1, loss = 4.203957557678223
steps = 1, loss = 1.938413143157959
steps = 1, loss = 49.779136657714844
steps = 1, loss = 2.4975955486297607
steps = 1, loss = 2.4511969089508057
steps = 1, loss = 0.7130215764045715
steps = 1, loss = 0.7310213446617126
steps = 81, loss = 2.083894729614258
steps = 1, loss = 3.1736738681793213
steps = 1, loss = 1.4476425647735596
steps = 1, loss = 1.4477850198745728
steps = 1, loss = 3.602219820022583
steps = 1, loss = 2.537437677383423
steps = 1, loss = 2.053941488265991
steps = 1, loss = 2.157195806503296
steps = 1, loss = 1.8389759063720703
steps = 1, loss = 1.780927300453186
steps = 1, loss = 2.0095810890197754
steps = 1, loss = 49.94819641113281
steps = 1, loss = 49.85722732543945
steps = 1, loss = 0.9066066145896912
steps = 1, loss = 1.9939522743225098
steps = 1, loss = 2.0564301013946533
steps = 1, loss = 50.005184173583984
steps = 1, loss = 2.9199306964874268
steps = 1, loss = 1.298521876335144
steps = 1, loss = 1.8664628267288208
steps = 1, loss = 50.017005920410156
steps = 1, loss = 49.9835205078125
steps = 1, loss = 2.5795366764068604
steps = 1, loss = 2.143399477005005
steps = 1, loss = 50.015769958496094
steps = 1, loss = 2.3679003715515137
steps = 1, loss = 1.6881660223007202
steps = 1, loss = 1.1935145854949951
steps = 1, loss = 49.98673629760742
steps = 1, loss = 1.831294298171997
steps = 1, loss = 50.00932693481445
steps = 1, loss = 0.9960564374923706
steps = 1, loss = 1.9433393478393555
steps = 1, loss = 1.928364872932434
steps = 1, loss = 2.4101498126983643
steps = 1, loss = 2.2825045585632324
steps = 1, loss = 2.1272799968719482
steps = 1, loss = 49.99524688720703
steps = 1, loss = 0.7660915851593018
steps = 1, loss = 1.6854805946350098
steps = 1, loss = 2.0361595153808594
steps = 1, loss = 2.573859214782715
steps = 1, loss = 0.755517840385437
steps = 1, loss = 2.683659315109253
steps = 1, loss = 0.7916966080665588
steps = 1, loss = 2.337462902069092
steps = 1, loss = 2.000476598739624
steps = 1, loss = 3.623856782913208
steps = 1, loss = 0.9026542901992798
steps = 1, loss = 47.03567123413086
steps = 1, loss = 1.2482517957687378
steps = 1, loss = 2.701814651489258
steps = 1, loss = 2.462686777114868
steps = 1, loss = 2.5515925884246826
steps = 1, loss = 50.008033752441406
steps = 1, loss = 2.4103238582611084
steps = 1, loss = 2.158217430114746
steps = 1, loss = 2.3994967937469482
steps = 1, loss = 1.736550211906433
steps = 1, loss = 0.7259045243263245
steps = 1, loss = 2.0171821117401123
steps = 1, loss = 50.01250457763672
steps = 1, loss = 50.02962112426758
steps = 1, loss = 3.010918140411377
steps = 1, loss = 4.5069661140441895
steps = 1, loss = 2.3052752017974854
steps = 1, loss = 2.283346176147461
steps = 1, loss = 3.183513641357422
steps = 1, loss = 2.1472208499908447
steps = 1, loss = 2.670454740524292
steps = 1, loss = 2.328421115875244
steps = 1, loss = 2.5880024433135986
steps = 1, loss = 2.07446026802063
steps = 1, loss = 2.0590479373931885
steps = 1, loss = 2.057513475418091
steps = 1, loss = 50.00571823120117
steps = 1, loss = 50.01814270019531
steps = 1, loss = 1.8505221605300903
steps = 1, loss = 1.661538004875183
steps = 1, loss = 2.5104684829711914
steps = 1, loss = 0.6967596411705017
steps = 1, loss = 49.9971809387207
steps = 1, loss = 2.0733237266540527
steps = 81, loss = 2.7337915897369385
steps = 1, loss = 1.8673076629638672
steps = 1, loss = 2.461028814315796
steps = 1, loss = 2.4548168182373047
steps = 1, loss = 0.6952893733978271
steps = 1, loss = 2.335280179977417
steps = 1, loss = 2.665414333343506
steps = 81, loss = 2.113412618637085
steps = 1, loss = 0.7112677097320557
steps = 81, loss = 2.164539098739624
steps = 1, loss = 1.369569182395935
steps = 1, loss = 1.708787202835083
steps = 1, loss = 1.8556537628173828
steps = 1, loss = 2.9766201972961426
steps = 1, loss = 1.8417705297470093
steps = 1, loss = 1.8450491428375244
steps = 1, loss = 2.7239503860473633
steps = 1, loss = 1.4235416650772095
steps = 1, loss = 50.01491928100586
steps = 1, loss = 50.02812576293945
steps = 1, loss = 1.6192883253097534
steps = 1, loss = 2.242403268814087
steps = 1, loss = 1.7629363536834717
steps = 1, loss = 1.934279203414917
steps = 1, loss = 2.241748094558716
steps = 1, loss = 1.4127460718154907
steps = 1, loss = 2.6958422660827637
steps = 1, loss = 2.064692735671997
steps = 1, loss = 0.7383877038955688
steps = 1, loss = 1.9448133707046509
steps = 1, loss = 2.176271438598633
steps = 1, loss = 1.8676103353500366
steps = 1, loss = 2.086650848388672
steps = 1, loss = 0.7096153497695923
steps = 1, loss = 2.766810655593872
steps = 1, loss = 2.2200021743774414
steps = 1, loss = 0.7304058074951172
steps = 1, loss = 2.1341066360473633
steps = 1, loss = 1.9358752965927124
steps = 1, loss = 1.9757051467895508
steps = 1, loss = 2.6487958431243896
steps = 1, loss = 2.4897818565368652
steps = 1, loss = 2.4055421352386475
steps = 1, loss = 49.99837875366211
steps = 1, loss = 2.806938886642456
steps = 1, loss = 1.5914547443389893
steps = 1, loss = 50.02394485473633
steps = 1, loss = 50.00922775268555
steps = 1, loss = 2.8377413749694824
steps = 1, loss = 2.586256504058838
steps = 1, loss = 2.5059974193573
steps = 1, loss = 2.669041633605957
steps = 1, loss = 2.5779237747192383
steps = 1, loss = 49.99397277832031
steps = 1, loss = 1.6641597747802734
steps = 1, loss = 2.3250558376312256
steps = 1, loss = 0.7033761143684387
steps = 1, loss = 1.8845086097717285
steps = 1, loss = 49.991722106933594
steps = 1, loss = 2.6523284912109375
steps = 1, loss = 2.0764777660369873
steps = 1, loss = 2.2022101879119873
steps = 1, loss = 2.3414504528045654
steps = 1, loss = 2.52833890914917
steps = 1, loss = 2.2169203758239746
steps = 1, loss = 0.7007049918174744
steps = 1, loss = 2.0439469814300537
steps = 1, loss = 2.2491633892059326
steps = 1, loss = 2.9611589908599854
steps = 1, loss = 2.9285905361175537
steps = 1, loss = 48.82188034057617
steps = 1, loss = 2.1010689735412598
steps = 1, loss = 49.987693786621094
steps = 1, loss = 2.1108415126800537
steps = 1, loss = 2.2771811485290527
steps = 1, loss = 5.8920392990112305
steps = 1, loss = 2.8315727710723877
steps = 1, loss = 2.3254363536834717
steps = 1, loss = 2.351330280303955
steps = 27, loss = 1.9543825387954712
steps = 1, loss = 1.5809504985809326
steps = 1, loss = 0.6957688927650452
steps = 27, loss = 1.9616429805755615
steps = 1, loss = 2.2617785930633545
steps = 27, loss = 3.567753553390503
steps = 1, loss = 2.0650789737701416
steps = 1, loss = 2.821685791015625
steps = 27, loss = 3.261350154876709
steps = 1, loss = 50.02217483520508
steps = 27, loss = 4.1289496421813965
steps = 1, loss = 3.806715965270996
steps = 243, loss = 2.147695779800415
steps = 1, loss = 2.4797587394714355
steps = 27, loss = 1.9735368490219116
steps = 27, loss = 13.213842391967773
steps = 1, loss = 2.08221697807312
steps = 1, loss = 0.7122755646705627
steps = 1, loss = 2.763974189758301
steps = 27, loss = 2.909888744354248
steps = 27, loss = 2.2022624015808105
steps = 1, loss = 48.63536834716797
steps = 1, loss = 2.721494197845459
steps = 1, loss = 2.9249207973480225
steps = 27, loss = 50.00017547607422
steps = 27, loss = 3.2620136737823486
steps = 1, loss = 1.8854249715805054
steps = 1, loss = 2.7605111598968506
steps = 1, loss = 49.84431076049805
steps = 1, loss = 3.0921695232391357
steps = 1, loss = 1.682149887084961
steps = 1, loss = 1.9631861448287964
steps = 1, loss = 2.0245823860168457
steps = 1, loss = 2.8585917949676514
steps = 1, loss = 2.479102849960327
steps = 1, loss = 1.1743870973587036
steps = 1, loss = 2.693814754486084
steps = 1, loss = 1.8361246585845947
steps = 1, loss = 3.2615468502044678
steps = 1, loss = 1.9490408897399902
steps = 1, loss = 49.99388885498047
steps = 1, loss = 2.032689094543457
steps = 1, loss = 1.788748025894165
steps = 1, loss = 2.767390012741089
steps = 1, loss = 1.7479889392852783
steps = 1, loss = 2.196789503097534
steps = 1, loss = 1.9676495790481567
steps = 1, loss = 1.6745113134384155
steps = 1, loss = 3.1350226402282715
steps = 1, loss = 50.0130729675293
steps = 1, loss = 2.598280906677246
steps = 1, loss = 0.780847430229187
steps = 1, loss = 3.5017411708831787
steps = 1, loss = 2.02815580368042
steps = 1, loss = 1.8911844491958618
steps = 1, loss = 2.754382371902466
steps = 3, loss = 1.6917210817337036
steps = 3, loss = 3.5514140129089355
steps = 3, loss = 1.9639184474945068
steps = 3, loss = 1.8505604267120361
steps = 3, loss = 1.8375787734985352
steps = 3, loss = 1.9967200756072998
steps = 3, loss = 2.0443854331970215
steps = 3, loss = 2.099405527114868
steps = 3, loss = 2.137349843978882
steps = 3, loss = 1.8542428016662598
steps = 3, loss = 1.916664481163025
steps = 3, loss = 1.9990984201431274
steps = 3, loss = 2.053607702255249
steps = 3, loss = 1.6322160959243774
steps = 3, loss = 2.047433614730835
steps = 3, loss = 1.8303972482681274
steps = 3, loss = 2.2528042793273926
steps = 3, loss = 2.182697296142578
steps = 3, loss = 0.8745545744895935
steps = 3, loss = 3.992319107055664
steps = 3, loss = 1.632623314857483
steps = 3, loss = 2.120809555053711
steps = 3, loss = 1.914734959602356
steps = 27, loss = 49.989131927490234
steps = 3, loss = 2.266648530960083
steps = 3, loss = 1.8691051006317139
steps = 3, loss = 1.279175043106079
steps = 3, loss = 1.726184368133545
steps = 3, loss = 2.56681752204895
steps = 3, loss = 2.2105562686920166
steps = 3, loss = 2.014338254928589
steps = 27, loss = 2.6886889934539795
steps = 3, loss = 0.8145648837089539
steps = 3, loss = 2.1167337894439697
steps = 3, loss = 2.2593181133270264
steps = 3, loss = 2.5913989543914795
steps = 3, loss = 2.0845744609832764
steps = 3, loss = 1.6139880418777466
steps = 3, loss = 1.6066166162490845
steps = 3, loss = 1.1751691102981567
steps = 3, loss = 0.8565540909767151
steps = 3, loss = 0.8174631595611572
steps = 3, loss = 2.175570249557495
steps = 3, loss = 0.7541115283966064
steps = 3, loss = 1.9867124557495117
steps = 3, loss = 2.0774238109588623
steps = 3, loss = 2.0266976356506348
steps = 3, loss = 0.7017974853515625
steps = 3, loss = 2.1302568912506104
steps = 3, loss = 2.0982284545898438
steps = 3, loss = 2.3737053871154785
steps = 3, loss = 1.9574030637741089
steps = 3, loss = 2.1273245811462402
steps = 3, loss = 1.5338908433914185
steps = 3, loss = 1.9446614980697632
steps = 3, loss = 1.7213034629821777
steps = 3, loss = 2.3107826709747314
steps = 3, loss = 2.4434316158294678
steps = 3, loss = 1.7615303993225098
steps = 3, loss = 1.8977525234222412
steps = 3, loss = 1.9457521438598633
steps = 3, loss = 1.81900954246521
steps = 3, loss = 2.108482837677002
steps = 3, loss = 2.3145909309387207
steps = 3, loss = 1.975504755973816
steps = 27, loss = 3.150441884994507
steps = 27, loss = 2.277653217315674
steps = 27, loss = 3.5635459423065186
steps = 3, loss = 2.5264015197753906
steps = 3, loss = 2.020634889602661
steps = 3, loss = 1.5007058382034302
steps = 3, loss = 2.221799850463867
steps = 3, loss = 2.136094093322754
steps = 3, loss = 0.8219884037971497
steps = 27, loss = 2.790250539779663
steps = 3, loss = 2.401764154434204
steps = 3, loss = 2.941251277923584
steps = 3, loss = 1.9531548023223877
steps = 3, loss = 1.8765037059783936
steps = 3, loss = 1.970449447631836
steps = 3, loss = 1.7807493209838867
steps = 3, loss = 2.191854953765869
steps = 3, loss = 2.0426976680755615
steps = 3, loss = 2.0298943519592285
steps = 3, loss = 2.1882474422454834
steps = 3, loss = 0.9475615620613098
steps = 3, loss = 2.086829423904419
steps = 27, loss = 3.617429494857788
steps = 9, loss = 1.6479161977767944
steps = 9, loss = 2.1398658752441406
steps = 9, loss = 1.6500917673110962
steps = 9, loss = 1.940901756286621
steps = 9, loss = 2.1139087677001953
steps = 9, loss = 1.5651419162750244
steps = 9, loss = 1.6419389247894287
steps = 9, loss = 2.1011672019958496
steps = 81, loss = 1.5750352144241333
steps = 81, loss = 2.777022361755371
steps = 9, loss = 1.9688149690628052
steps = 81, loss = 4.401637554168701
steps = 81, loss = 2.290764093399048
steps = 9, loss = 2.108937978744507
steps = 9, loss = 1.8205446004867554
steps = 81, loss = 2.5024101734161377
steps = 81, loss = 2.301663398742676
steps = 9, loss = 1.9400818347930908
steps = 9, loss = 1.8524816036224365
steps = 9, loss = 1.958072543144226
steps = 9, loss = 1.816413164138794
steps = 9, loss = 1.9768017530441284
steps = 9, loss = 2.005716323852539
steps = 9, loss = 2.100964307785034
steps = 9, loss = 2.0131993293762207
steps = 9, loss = 1.7022590637207031
steps = 9, loss = 1.6080759763717651
steps = 9, loss = 1.9013375043869019
steps = 9, loss = 2.040311098098755
steps = 9, loss = 1.5553107261657715
steps = 9, loss = 2.3723278045654297
steps = 1, loss = 1.988043189048767
steps = 1, loss = 1.4598603248596191
steps = 1, loss = 2.864659070968628
steps = 1, loss = 1.882656455039978
steps = 1, loss = 1.1321369409561157
steps = 1, loss = 2.3613624572753906
steps = 1, loss = 2.0466296672821045
steps = 1, loss = 0.748729944229126
steps = 1, loss = 49.91206359863281
steps = 1, loss = 1.8162975311279297
steps = 1, loss = 3.0930423736572266
steps = 1, loss = 1.6710162162780762
steps = 1, loss = 5.314520359039307
steps = 1, loss = 3.4586241245269775
steps = 1, loss = 0.718695342540741
steps = 1, loss = 1.96505868434906
steps = 1, loss = 2.538666009902954
steps = 1, loss = 1.8495349884033203
steps = 1, loss = 2.456063747406006
steps = 1, loss = 3.384243965148926
steps = 1, loss = 49.55873489379883
steps = 1, loss = 2.084094285964966
steps = 1, loss = 1.5956233739852905
steps = 1, loss = 2.0206804275512695
steps = 1, loss = 49.998802185058594
steps = 1, loss = 2.390878438949585
steps = 1, loss = 0.7082163095474243
steps = 1, loss = 1.4829858541488647
steps = 1, loss = 2.7847604751586914
steps = 1, loss = 1.946048378944397
steps = 1, loss = 1.9663532972335815
steps = 1, loss = 1.676689863204956
steps = 1, loss = 2.829538106918335
steps = 1, loss = 2.0278213024139404
steps = 1, loss = 0.7155572772026062
steps = 1, loss = 1.1391602754592896
steps = 1, loss = 1.7052273750305176
steps = 1, loss = 2.334047555923462
steps = 1, loss = 2.6980667114257812
steps = 1, loss = 4.827888488769531
steps = 1, loss = 50.02153778076172
steps = 1, loss = 1.9534801244735718
steps = 1, loss = 1.7400643825531006
steps = 1, loss = 2.843251943588257
steps = 1, loss = 1.7459667921066284
steps = 1, loss = 2.0712480545043945
steps = 1, loss = 3.107914686203003
steps = 1, loss = 1.827426552772522
steps = 1, loss = 1.775277018547058
steps = 9, loss = 2.1103503704071045
steps = 1, loss = 2.511686325073242
steps = 1, loss = 1.2025749683380127
steps = 9, loss = 2.0397396087646484
steps = 1, loss = 49.99958801269531
steps = 1, loss = 2.414820671081543
steps = 1, loss = 2.7962520122528076
steps = 1, loss = 3.9332029819488525
steps = 1, loss = 2.4446113109588623
steps = 1, loss = 2.1588196754455566
steps = 1, loss = 2.022237539291382
steps = 1, loss = 1.9274396896362305
steps = 1, loss = 2.7266290187835693
steps = 1, loss = 3.6347999572753906
steps = 1, loss = 1.7811264991760254
steps = 1, loss = 1.9192476272583008
steps = 1, loss = 0.9130846858024597
steps = 1, loss = 2.1966991424560547
steps = 1, loss = 1.7206494808197021
steps = 1, loss = 1.9540541172027588
steps = 1, loss = 2.577918529510498
steps = 1, loss = 50.01491928100586
steps = 1, loss = 1.9960544109344482
steps = 1, loss = 2.6618423461914062
steps = 1, loss = 2.1823067665100098
steps = 1, loss = 5.159860610961914
steps = 1, loss = 2.1380157470703125
steps = 1, loss = 1.7392140626907349
steps = 1, loss = 1.7864032983779907
steps = 1, loss = 2.2831456661224365
steps = 1, loss = 49.98958969116211
steps = 1, loss = 1.4303823709487915
steps = 1, loss = 2.1813876628875732
steps = 1, loss = 1.9446218013763428
steps = 1, loss = 2.55891489982605
steps = 1, loss = 2.3727331161499023
steps = 1, loss = 2.417633533477783
steps = 1, loss = 4.508089542388916
steps = 1, loss = 2.312091588973999
steps = 1, loss = 2.1902389526367188
steps = 1, loss = 49.998756408691406
steps = 1, loss = 2.074096202850342
steps = 1, loss = 1.7678146362304688
steps = 1, loss = 3.0235111713409424
steps = 1, loss = 2.6503663063049316
steps = 1, loss = 1.99490487575531
steps = 1, loss = 49.98341751098633
steps = 1, loss = 2.0490405559539795
steps = 1, loss = 1.588681697845459
steps = 27, loss = 1.9530421495437622
steps = 27, loss = 1.902204155921936
steps = 27, loss = 1.9828373193740845
steps = 1, loss = 50.015445709228516
steps = 1, loss = 2.257749080657959
steps = 3, loss = 49.552921295166016
steps = 3, loss = 2.6728978157043457
steps = 3, loss = 1.926630973815918
steps = 3, loss = 1.8545657396316528
steps = 3, loss = 1.5050550699234009
steps = 3, loss = 49.99422073364258
steps = 3, loss = 2.0101373195648193
steps = 3, loss = 2.47819185256958
steps = 3, loss = 3.0087037086486816
steps = 3, loss = 8.058249473571777
steps = 3, loss = 2.711496591567993
steps = 3, loss = 2.643068790435791
steps = 3, loss = 2.533818006515503
steps = 3, loss = 2.18186354637146
steps = 3, loss = 5.296260356903076
steps = 3, loss = 3.5580012798309326
steps = 3, loss = 2.089505672454834
steps = 3, loss = 2.2816920280456543
steps = 3, loss = 1.7493727207183838
steps = 3, loss = 2.7419967651367188
steps = 3, loss = 2.0205020904541016
steps = 3, loss = 2.4196906089782715
steps = 3, loss = 2.6094398498535156
steps = 3, loss = 49.99393844604492
steps = 3, loss = 2.4720845222473145
steps = 3, loss = 3.157740831375122
steps = 3, loss = 2.2218692302703857
steps = 3, loss = 9.135293960571289
steps = 3, loss = 2.1960935592651367
steps = 3, loss = 2.0453128814697266
steps = 3, loss = 2.090693712234497
steps = 3, loss = 2.5889527797698975
steps = 3, loss = 49.98958969116211
steps = 3, loss = 1.8821475505828857
steps = 3, loss = 2.2565972805023193
steps = 3, loss = 2.2815539836883545
steps = 3, loss = 2.744636058807373
steps = 3, loss = 2.3036484718322754
steps = 3, loss = 2.7280781269073486
steps = 3, loss = 6.3437042236328125
steps = 3, loss = 2.4101102352142334
steps = 3, loss = 2.2122151851654053
steps = 3, loss = 49.31107711791992
steps = 3, loss = 2.5970654487609863
steps = 3, loss = 2.06896710395813
steps = 3, loss = 3.3772706985473633
steps = 3, loss = 2.763056755065918
steps = 3, loss = 2.440821886062622
steps = 3, loss = 49.990760803222656
steps = 3, loss = 2.64194393157959
steps = 243, loss = 2.9970028400421143
steps = 3, loss = 1.9779887199401855
steps = 27, loss = 2.0856871604919434
steps = 27, loss = 1.9381581544876099
steps = 27, loss = 1.94623601436615
steps = 27, loss = 1.9385390281677246
steps = 3, loss = 2.286997079849243
steps = 3, loss = 3.2281620502471924
steps = 3, loss = 3.3155646324157715
steps = 3, loss = 1.7414875030517578
steps = 3, loss = 2.4223239421844482
steps = 3, loss = 2.4171290397644043
steps = 3, loss = 49.99544143676758
steps = 3, loss = 3.529979705810547
steps = 3, loss = 2.0710606575012207
steps = 3, loss = 6.202219486236572
steps = 3, loss = 0.9250831007957458
steps = 3, loss = 2.3244833946228027
steps = 3, loss = 2.6848907470703125
steps = 3, loss = 2.1510684490203857
steps = 3, loss = 3.0188961029052734
steps = 3, loss = 3.466869831085205
steps = 3, loss = 50.005775451660156
steps = 3, loss = 2.1997628211975098
steps = 3, loss = 1.9370272159576416
steps = 3, loss = 2.3828299045562744
steps = 3, loss = 49.99879455566406
steps = 3, loss = 2.431885004043579
steps = 3, loss = 1.7777316570281982
steps = 3, loss = 1.770989179611206
steps = 3, loss = 2.853121042251587
steps = 3, loss = 2.132418632507324
steps = 3, loss = 2.3347856998443604
steps = 3, loss = 2.4079298973083496
steps = 3, loss = 3.172027111053467
steps = 3, loss = 2.2928285598754883
steps = 3, loss = 0.7375800609588623
steps = 3, loss = 1.8691169023513794
steps = 3, loss = 1.9781814813613892
steps = 3, loss = 2.6028101444244385
steps = 3, loss = 49.99436569213867
steps = 3, loss = 4.656055450439453
steps = 3, loss = 50.020790100097656
steps = 3, loss = 2.3491809368133545
steps = 3, loss = 1.9913694858551025
steps = 3, loss = 3.3406429290771484
steps = 3, loss = 1.940172553062439
steps = 3, loss = 2.6768643856048584
steps = 3, loss = 3.6461634635925293
steps = 3, loss = 2.016941547393799
steps = 3, loss = 2.0781447887420654
steps = 3, loss = 2.3339521884918213
steps = 3, loss = 1.8425755500793457
steps = 9, loss = 2.0260090827941895
steps = 9, loss = 2.4877641201019287
steps = 9, loss = 2.260096549987793
steps = 9, loss = 2.1165499687194824
steps = 9, loss = 1.9113595485687256
steps = 9, loss = 2.269951105117798
steps = 9, loss = 2.1714670658111572
steps = 9, loss = 2.930767774581909
steps = 9, loss = 1.9066249132156372
steps = 9, loss = 2.502833366394043
steps = 9, loss = 1.716842770576477
steps = 9, loss = 2.0640578269958496
steps = 9, loss = 2.2915515899658203
steps = 9, loss = 2.28495717048645
steps = 9, loss = 2.117328405380249
steps = 9, loss = 2.0883705615997314
steps = 9, loss = 2.280385732650757
steps = 9, loss = 1.398707628250122
steps = 9, loss = 2.5525670051574707
steps = 9, loss = 2.2128031253814697
steps = 9, loss = 2.692267894744873
steps = 9, loss = 1.922197937965393
steps = 9, loss = 2.3192384243011475
steps = 9, loss = 2.4779279232025146
steps = 9, loss = 2.405194044113159
steps = 9, loss = 2.6589369773864746
steps = 9, loss = 2.7105565071105957
steps = 9, loss = 1.9156721830368042
steps = 9, loss = 2.2701592445373535
steps = 9, loss = 2.3251867294311523
steps = 9, loss = 2.0768959522247314
steps = 9, loss = 2.20330548286438
steps = 27, loss = 2.017820358276367
steps = 27, loss = 2.0844035148620605
steps = 27, loss = 2.6684670448303223
steps = 27, loss = 2.0784270763397217
steps = 81, loss = 2.2164738178253174
steps = 81, loss = 2.190943717956543
steps = 27, loss = 1.8969391584396362
steps = 27, loss = 2.8054726123809814
steps = 27, loss = 2.495670795440674
steps = 27, loss = 2.165088415145874
steps = 27, loss = 2.250701904296875
steps = 27, loss = 2.123720407485962
steps = 27, loss = 2.6180331707000732
steps = 27, loss = 2.0308263301849365
steps = 81, loss = 2.202436923980713
steps = 81, loss = 2.2496259212493896
steps = 81, loss = 2.0358080863952637
steps = 81, loss = 2.4858267307281494
steps = 243, loss = 2.2632761001586914
steps = 243, loss = 2.8700709342956543
steps = 243, loss = 1.9401624202728271
steps = 747, loss = 2.319584608078003
CPU times: user 1h 23min 54s, sys: 11min 3s, total: 1h 34min 57s
Wall time: 9h 38min 49s
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div class="sk-top-container" id="sk-container-id-1"><div class="sk-text-repr-fallback"><pre>HyperbandSearchCV(estimator=&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
),
                  max_iter=250,
                  parameters={'batch_size': [32, 64, 128, 256, 512],
                              'module__activation': ['ReLU', 'LeakyReLU', 'ELU',
                                                     'PReLU'],
                              'module__init': ['xavier_uniform_',
                                               'xavier_normal_',
                                               'kaiming_uniform_',
                                               'kaiming_normal_'],
                              'optimizer': ['SGD', 'SGD', 'SGD', 'S...
       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,
       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,
       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,
       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),
                              'optimizer__nesterov': [True],
                              'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, ...],
                              'train_split': [None]},
                  random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox"/><label class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted" for="sk-estimator-id-1"> HyperbandSearchCV<span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>HyperbandSearchCV(estimator=&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
),
                  max_iter=250,
                  parameters={'batch_size': [32, 64, 128, 256, 512],
                              'module__activation': ['ReLU', 'LeakyReLU', 'ELU',
                                                     'PReLU'],
                              'module__init': ['xavier_uniform_',
                                               'xavier_normal_',
                                               'kaiming_uniform_',
                                               'kaiming_normal_'],
                              'optimizer': ['SGD', 'SGD', 'SGD', 'S...
       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,
       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,
       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,
       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),
                              'optimizer__nesterov': [True],
                              'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, ...],
                              'train_split': [None]},
                  random_state=42)</pre></div> </div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox"/><label class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted" for="sk-estimator-id-2">estimator: TrimParams</label><div class="sk-toggleable__content fitted"><pre>&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
)</pre></div> </div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox"/><label class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted" for="sk-estimator-id-3">TrimParams</label><div class="sk-toggleable__content fitted"><pre>&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
)</pre></div> </div></div></div></div></div></div></div></div></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># timing_stats = client.profile(filename="hyperband.html")</span>
<span class="c1"># with open(f"{absolutepath_to_results}/hyperband-timing.json", "w") as f:</span>
<span class="c1">#     json.dump(timing_stats[0], f)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>&lt;class '__main__.TrimParams'&gt;[initialized](
  module_=Autoencoder(
    (encoder): Sequential(
      (0): Linear(in_features=784, out_features=784, bias=True)
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
      (2): Linear(in_features=784, out_features=196, bias=True)
      (3): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (decoder): Sequential(
      (0): Linear(in_features=196, out_features=784, bias=True)
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
      (2): Linear(in_features=784, out_features=784, bias=True)
      (3): Sigmoid()
    )
  ),
)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">search</span><span class="o">.</span><span class="n">best_score_</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>-1.9401624202728271</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">search</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>{'train_split': None,
 'optimizer__weight_decay': 0.0009119267598459299,
 'optimizer__nesterov': True,
 'optimizer__momentum': 0.25725725725725723,
 'optimizer__lr': 6.915758828738525,
 'optimizer': 'SGD',
 'module__init': 'xavier_normal_',
 'module__activation': 'LeakyReLU',
 'batch_size': 256}</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">save_search</span><span class="p">(</span><span class="n">search</span><span class="p">,</span> <span class="n">today</span><span class="p">,</span> <span class="s2">"hyperband"</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">compute</span><span class="p">(),</span> <span class="n">y_test</span><span class="o">.</span><span class="n">compute</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Hyperband-with-SOP">Hyperband with SOP<a class="anchor-link" href="#Hyperband-with-SOP">¶</a></h3><p>Trains the model with Hyperband with SOP and analyzes the results.</p>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">search_patience</span> <span class="o">=</span> <span class="n">HyperbandSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>
<span class="n">search_patience</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>steps = 1, loss = 1.976102590560913
steps = 1, loss = 2.0065276622772217
steps = 1, loss = 2.022500514984131
steps = 1, loss = 1.1510610580444336
steps = 1, loss = 1.9715073108673096
steps = 1, loss = 0.7229595184326172
steps = 84, loss = 2.7753853797912598
steps = 84, loss = 2.7443578243255615
steps = 84, loss = 3.6286063194274902
steps = 84, loss = 3.589615821838379
steps = 1, loss = 2.0105245113372803
steps = 1, loss = 2.005730390548706
steps = 1, loss = 1.9459617137908936
steps = 1, loss = 1.0126945972442627
steps = 1, loss = 1.6624188423156738
steps = 1, loss = 49.70864486694336
steps = 1, loss = 1.7215481996536255
steps = 1, loss = 1.5606186389923096
steps = 1, loss = 3.1231038570404053
steps = 84, loss = 2.5648086071014404
steps = 84, loss = 3.58955454826355
steps = 83, loss = 4.845966339111328
steps = 83, loss = 2.49174165725708
steps = 83, loss = 2.6515438556671143
steps = 83, loss = 3.897847890853882
steps = 83, loss = 3.4700748920440674
steps = 83, loss = 2.441875696182251
steps = 1, loss = 1.7992393970489502
steps = 1, loss = 1.7342123985290527
steps = 1, loss = 2.506152868270874
steps = 1, loss = 0.8883376121520996
steps = 1, loss = 2.082169771194458
steps = 1, loss = 3.865260362625122
steps = 1, loss = 2.399545192718506
steps = 1, loss = 2.0154409408569336
steps = 1, loss = 0.7719258666038513
steps = 1, loss = 1.7636090517044067
steps = 1, loss = 1.9718873500823975
steps = 1, loss = 1.9374303817749023
steps = 1, loss = 1.9311585426330566
steps = 1, loss = 1.8002336025238037
steps = 1, loss = 2.5854711532592773
steps = 1, loss = 0.9661195874214172
steps = 1, loss = 50.03294372558594
steps = 1, loss = 1.7750695943832397
steps = 1, loss = 2.5110323429107666
steps = 1, loss = 2.032252311706543
steps = 1, loss = 2.273576021194458
steps = 1, loss = 1.9864051342010498
steps = 1, loss = 2.0215888023376465
steps = 1, loss = 2.4246129989624023
steps = 1, loss = 1.8213036060333252
steps = 83, loss = 2.6944053173065186
steps = 1, loss = 2.022845506668091
steps = 1, loss = 1.5835665464401245
steps = 1, loss = 1.8579168319702148
steps = 1, loss = 2.503469467163086
steps = 1, loss = 2.9389548301696777
steps = 1, loss = 2.431325674057007
steps = 1, loss = 1.2279740571975708
steps = 1, loss = 1.793084740638733
steps = 1, loss = 2.597764015197754
steps = 1, loss = 49.999183654785156
steps = 1, loss = 2.750908374786377
steps = 1, loss = 2.1595799922943115
steps = 1, loss = 2.102537155151367
steps = 1, loss = 0.7211442589759827
steps = 1, loss = 2.0993521213531494
steps = 1, loss = 2.36973237991333
steps = 9, loss = 2.0741641521453857
steps = 9, loss = 2.4624252319335938
steps = 9, loss = 3.027205228805542
steps = 9, loss = 6.602343559265137
steps = 9, loss = 3.3072092533111572
steps = 9, loss = 2.00130295753479
steps = 9, loss = 2.1756234169006348
steps = 9, loss = 2.8150124549865723
steps = 9, loss = 2.486617088317871
steps = 9, loss = 2.538470506668091
steps = 83, loss = 2.5678772926330566
steps = 9, loss = 3.5834124088287354
steps = 9, loss = 1.93047297000885
steps = 9, loss = 2.587425947189331
steps = 9, loss = 2.0498063564300537
steps = 9, loss = 50.03324890136719
steps = 9, loss = 1.9639228582382202
steps = 9, loss = 2.2143383026123047
steps = 9, loss = 2.7151761054992676
steps = 83, loss = 3.2396674156188965
steps = 9, loss = 2.8679144382476807
steps = 9, loss = 2.382416009902954
steps = 9, loss = 3.6186742782592773
steps = 9, loss = 2.592621088027954
steps = 9, loss = 3.376551389694214
steps = 9, loss = 3.638550043106079
steps = 9, loss = 2.2905359268188477
steps = 9, loss = 3.2603542804718018
steps = 9, loss = 3.3332860469818115
steps = 9, loss = 2.8528757095336914
steps = 9, loss = 2.1080071926116943
steps = 9, loss = 2.309544086456299
steps = 9, loss = 2.6895992755889893
steps = 9, loss = 3.4739747047424316
steps = 9, loss = 49.99017333984375
steps = 9, loss = 2.959926128387451
steps = 9, loss = 2.7450175285339355
steps = 9, loss = 2.98881459236145
steps = 9, loss = 2.430205821990967
steps = 9, loss = 2.331500291824341
steps = 9, loss = 1.8820430040359497
steps = 9, loss = 2.6268491744995117
steps = 9, loss = 2.2821807861328125
steps = 27, loss = 2.429812431335449
steps = 27, loss = 2.6939525604248047
steps = 27, loss = 2.177184820175171
steps = 27, loss = 2.36126446723938
steps = 27, loss = 2.4908735752105713
steps = 27, loss = 2.040205240249634
steps = 27, loss = 2.148772954940796
steps = 27, loss = 2.192610740661621
steps = 27, loss = 2.292963743209839
steps = 27, loss = 2.419462203979492
steps = 27, loss = 2.2230629920959473
steps = 1, loss = 0.7132876515388489
steps = 1, loss = 0.6947764754295349
steps = 27, loss = 2.1621830463409424
steps = 1, loss = 1.3921881914138794
steps = 1, loss = 49.97151565551758
steps = 1, loss = 1.9595893621444702
steps = 1, loss = 1.93499755859375
steps = 27, loss = 2.545840263366699
steps = 1, loss = 2.0536389350891113
steps = 1, loss = 2.0565898418426514
steps = 1, loss = 1.8445072174072266
steps = 1, loss = 2.3870468139648438
steps = 1, loss = 1.5400933027267456
steps = 1, loss = 2.081611394882202
steps = 1, loss = 50.006195068359375
steps = 1, loss = 2.466464042663574
steps = 1, loss = 2.7470710277557373
steps = 1, loss = 7.467029571533203
steps = 1, loss = 0.6960509419441223
steps = 1, loss = 1.4138689041137695
steps = 27, loss = 1.9595837593078613
steps = 81, loss = 2.062436819076538
steps = 81, loss = 2.1939871311187744
steps = 27, loss = 1.964181661605835
steps = 81, loss = 2.0864391326904297
steps = 27, loss = 2.7682650089263916
steps = 81, loss = 2.76813006401062
steps = 27, loss = 3.1566121578216553
steps = 27, loss = 3.2393152713775635
steps = 27, loss = 3.1296322345733643
steps = 27, loss = 2.9063143730163574
steps = 27, loss = 3.691540002822876
steps = 27, loss = 3.562567710876465
steps = 27, loss = 2.7324419021606445
steps = 27, loss = 3.5421502590179443
steps = 27, loss = 49.769107818603516
steps = 27, loss = 2.201664447784424
steps = 27, loss = 3.1907079219818115
steps = 27, loss = 2.202320098876953
steps = 27, loss = 49.576107025146484
steps = 27, loss = 4.145674705505371
steps = 27, loss = 12.067633628845215
steps = 1, loss = 1.9640547037124634
steps = 1, loss = 1.4937371015548706
steps = 1, loss = 1.8534736633300781
steps = 1, loss = 1.9374562501907349
steps = 1, loss = 0.7024468779563904
steps = 1, loss = 2.965411424636841
steps = 1, loss = 1.0770316123962402
steps = 1, loss = 49.99958038330078
steps = 1, loss = 2.196505546569824
steps = 1, loss = 2.7478020191192627
steps = 1, loss = 50.02484130859375
steps = 1, loss = 1.9063485860824585
steps = 1, loss = 1.8170002698898315
steps = 1, loss = 1.729100227355957
steps = 1, loss = 2.595726728439331
steps = 1, loss = 1.56155264377594
steps = 1, loss = 2.2690179347991943
steps = 1, loss = 2.781534433364868
steps = 1, loss = 2.4304428100585938
steps = 1, loss = 2.302105665206909
steps = 1, loss = 1.7136836051940918
steps = 1, loss = 2.020298480987549
steps = 1, loss = 2.021958827972412
steps = 1, loss = 2.692499876022339
steps = 1, loss = 2.5109686851501465
steps = 1, loss = 50.01271438598633
steps = 1, loss = 1.7070250511169434
steps = 1, loss = 2.2566511631011963
steps = 1, loss = 50.01339340209961
steps = 1, loss = 2.7648661136627197
steps = 164, loss = 2.119516372680664
steps = 1, loss = 1.4949305057525635
steps = 1, loss = 2.4018468856811523
steps = 1, loss = 1.6415451765060425
steps = 1, loss = 1.932300329208374
steps = 1, loss = 1.904356837272644
steps = 1, loss = 50.032047271728516
steps = 1, loss = 3.740142822265625
steps = 81, loss = 2.288606882095337
steps = 1, loss = 2.0409111976623535
steps = 1, loss = 1.9937933683395386
steps = 1, loss = 2.415637254714966
steps = 1, loss = 1.9492605924606323
steps = 1, loss = 1.9835436344146729
steps = 1, loss = 3.5833349227905273
steps = 1, loss = 49.996463775634766
steps = 1, loss = 2.1819510459899902
steps = 1, loss = 2.1049506664276123
steps = 1, loss = 2.0917744636535645
steps = 1, loss = 2.8436737060546875
steps = 1, loss = 1.6908988952636719
steps = 1, loss = 2.320211887359619
steps = 1, loss = 2.180023193359375
steps = 1, loss = 4.232182502746582
steps = 1, loss = 1.7135908603668213
steps = 1, loss = 49.992408752441406
steps = 1, loss = 4.103856563568115
steps = 1, loss = 1.957754135131836
steps = 1, loss = 2.9487197399139404
steps = 1, loss = 1.996193766593933
steps = 1, loss = 49.99165725708008
steps = 1, loss = 2.249173641204834
steps = 1, loss = 2.436624526977539
steps = 1, loss = 2.86205792427063
steps = 81, loss = 2.300462007522583
steps = 1, loss = 3.752155065536499
steps = 1, loss = 3.680802583694458
steps = 1, loss = 2.0377774238586426
steps = 1, loss = 2.0586116313934326
steps = 1, loss = 1.8457367420196533
steps = 1, loss = 2.2953426837921143
steps = 1, loss = 50.0015983581543
steps = 1, loss = 1.5599546432495117
steps = 1, loss = 1.8391436338424683
steps = 1, loss = 3.007870674133301
steps = 1, loss = 2.3317830562591553
steps = 1, loss = 2.539541244506836
steps = 1, loss = 1.3261024951934814
steps = 1, loss = 2.2253730297088623
steps = 1, loss = 2.107239246368408
steps = 1, loss = 0.6953843832015991
steps = 1, loss = 0.7304484844207764
steps = 1, loss = 3.0402910709381104
steps = 1, loss = 0.7138105630874634
steps = 81, loss = 1.8000520467758179
steps = 1, loss = 3.02960467338562
steps = 1, loss = 50.01515197753906
steps = 1, loss = 2.2845160961151123
steps = 1, loss = 28.899627685546875
steps = 1, loss = 1.910409927368164
steps = 1, loss = 2.691826105117798
steps = 1, loss = 50.01597595214844
steps = 1, loss = 1.7868666648864746
steps = 1, loss = 2.7539455890655518
steps = 1, loss = 2.0828702449798584
steps = 1, loss = 2.156998634338379
steps = 1, loss = 0.9955209493637085
steps = 81, loss = 4.2192888259887695
steps = 1, loss = 2.1009914875030518
steps = 1, loss = 2.9833433628082275
steps = 1, loss = 0.7053812146186829
steps = 1, loss = 1.8699783086776733
steps = 1, loss = 50.00740432739258
steps = 1, loss = 2.1582610607147217
steps = 1, loss = 2.110292434692383
steps = 1, loss = 2.1345510482788086
steps = 1, loss = 2.348071813583374
steps = 1, loss = 0.7261946201324463
steps = 1, loss = 2.4299185276031494
steps = 1, loss = 2.0131545066833496
steps = 1, loss = 2.1797330379486084
steps = 1, loss = 2.479950428009033
steps = 1, loss = 3.055056571960449
steps = 1, loss = 1.9773783683776855
steps = 1, loss = 2.4471547603607178
steps = 1, loss = 2.6044278144836426
steps = 1, loss = 50.00352478027344
steps = 81, loss = 3.344578981399536
steps = 1, loss = 3.086059331893921
steps = 1, loss = 49.95124435424805
steps = 1, loss = 1.936660647392273
steps = 1, loss = 2.306732416152954
steps = 1, loss = 1.980758786201477
steps = 1, loss = 50.01052474975586
steps = 1, loss = 0.7021657824516296
steps = 1, loss = 2.219506025314331
steps = 1, loss = 3.30248761177063
steps = 1, loss = 1.6418321132659912
steps = 1, loss = 49.99880599975586
steps = 1, loss = 1.7480353116989136
steps = 1, loss = 1.7537955045700073
steps = 1, loss = 2.7315175533294678
steps = 1, loss = 2.7604799270629883
steps = 1, loss = 1.935832142829895
steps = 1, loss = 49.979820251464844
steps = 1, loss = 0.7108551263809204
steps = 1, loss = 2.685119867324829
steps = 1, loss = 49.98785400390625
steps = 1, loss = 49.81474685668945
steps = 1, loss = 2.552600860595703
steps = 1, loss = 1.8847416639328003
steps = 1, loss = 2.211468458175659
steps = 1, loss = 50.00260543823242
steps = 1, loss = 2.4355108737945557
steps = 1, loss = 2.359182834625244
steps = 1, loss = 2.904634714126587
steps = 1, loss = 1.8730379343032837
steps = 1, loss = 2.7279160022735596
steps = 1, loss = 2.289216995239258
steps = 1, loss = 2.057981014251709
steps = 1, loss = 2.5657103061676025
steps = 1, loss = 49.98379135131836
steps = 1, loss = 2.4205379486083984
steps = 1, loss = 1.8661773204803467
steps = 1, loss = 1.5764427185058594
steps = 1, loss = 50.00696563720703
steps = 1, loss = 3.1020309925079346
steps = 1, loss = 1.835217833518982
steps = 1, loss = 49.96995544433594
steps = 1, loss = 2.6811013221740723
steps = 1, loss = 2.195013999938965
steps = 1, loss = 3.123790979385376
steps = 1, loss = 2.1571567058563232
steps = 1, loss = 1.0047521591186523
steps = 1, loss = 50.00632095336914
steps = 1, loss = 5.9599151611328125
steps = 1, loss = 2.025648593902588
steps = 1, loss = 2.0817062854766846
steps = 1, loss = 2.5837910175323486
steps = 1, loss = 1.1213078498840332
steps = 1, loss = 2.458371877670288
steps = 1, loss = 0.9341937899589539
steps = 1, loss = 2.903982639312744
steps = 1, loss = 2.004361391067505
steps = 1, loss = 0.7118794322013855
steps = 1, loss = 2.4500463008880615
steps = 1, loss = 2.000138998031616
steps = 1, loss = 0.7548988461494446
steps = 1, loss = 3.040233850479126
steps = 1, loss = 0.7893089056015015
steps = 1, loss = 2.5804455280303955
steps = 1, loss = 1.462814211845398
steps = 1, loss = 3.0782651901245117
steps = 1, loss = 2.083484172821045
steps = 1, loss = 2.4949681758880615
steps = 1, loss = 2.5063560009002686
steps = 1, loss = 2.004786491394043
steps = 1, loss = 2.7360849380493164
steps = 1, loss = 2.4008426666259766
steps = 1, loss = 3.2977101802825928
steps = 1, loss = 50.03006362915039
steps = 81, loss = 2.8245906829833984
steps = 1, loss = 2.0515758991241455
steps = 1, loss = 1.8366618156433105
steps = 1, loss = 2.8082947731018066
steps = 1, loss = 50.010684967041016
steps = 1, loss = 1.7888903617858887
steps = 1, loss = 1.6812955141067505
steps = 1, loss = 1.7457085847854614
steps = 1, loss = 2.835407257080078
steps = 1, loss = 1.8007515668869019
steps = 1, loss = 2.24324631690979
steps = 1, loss = 2.146970510482788
steps = 1, loss = 1.949595332145691
steps = 1, loss = 2.5386223793029785
steps = 1, loss = 2.689239740371704
steps = 1, loss = 0.8053096532821655
steps = 1, loss = 1.8670347929000854
steps = 1, loss = 2.236208200454712
steps = 1, loss = 2.68196439743042
steps = 1, loss = 1.9637664556503296
steps = 1, loss = 5.564571857452393
steps = 1, loss = 1.6344884634017944
steps = 1, loss = 2.567379951477051
steps = 1, loss = 2.3932995796203613
steps = 1, loss = 1.2144558429718018
steps = 1, loss = 49.961692810058594
steps = 1, loss = 1.7180464267730713
steps = 1, loss = 2.8724544048309326
steps = 1, loss = 50.00546646118164
steps = 1, loss = 49.933860778808594
steps = 1, loss = 2.3920938968658447
steps = 1, loss = 2.154179096221924
steps = 1, loss = 2.113872766494751
steps = 1, loss = 2.0259921550750732
steps = 1, loss = 2.603501081466675
steps = 1, loss = 2.086256980895996
steps = 1, loss = 4.052460670471191
steps = 1, loss = 2.846071243286133
steps = 1, loss = 2.0662474632263184
steps = 1, loss = 0.7034077644348145
steps = 1, loss = 1.177841305732727
steps = 1, loss = 2.678959608078003
steps = 1, loss = 0.7826295495033264
steps = 1, loss = 0.6958963871002197
steps = 1, loss = 2.7747793197631836
steps = 1, loss = 2.537787675857544
steps = 1, loss = 2.012955904006958
steps = 1, loss = 0.8227781057357788
steps = 1, loss = 49.98824691772461
steps = 1, loss = 2.3321025371551514
steps = 1, loss = 1.8884458541870117
steps = 1, loss = 50.01002502441406
steps = 1, loss = 1.8061846494674683
steps = 1, loss = 1.9322619438171387
steps = 1, loss = 1.6438653469085693
steps = 1, loss = 2.763143301010132
steps = 1, loss = 2.785893201828003
steps = 1, loss = 1.472103476524353
steps = 1, loss = 0.7571992874145508
steps = 3, loss = 1.855315089225769
steps = 3, loss = 2.2547543048858643
steps = 3, loss = 2.085477828979492
steps = 3, loss = 2.4827325344085693
steps = 3, loss = 3.012885093688965
steps = 3, loss = 2.091968059539795
steps = 3, loss = 1.9492478370666504
steps = 3, loss = 3.173290729522705
steps = 3, loss = 1.8359686136245728
steps = 3, loss = 2.0092945098876953
steps = 3, loss = 2.1317827701568604
steps = 3, loss = 2.085400104522705
steps = 3, loss = 2.400439739227295
steps = 3, loss = 0.8477190136909485
steps = 3, loss = 1.862398624420166
steps = 3, loss = 1.1943060159683228
steps = 3, loss = 2.0647828578948975
steps = 3, loss = 0.7676287889480591
steps = 3, loss = 1.7244269847869873
steps = 3, loss = 0.7210447788238525
steps = 3, loss = 1.2954738140106201
steps = 3, loss = 1.9802824258804321
steps = 3, loss = 1.9367320537567139
steps = 3, loss = 2.0760107040405273
steps = 3, loss = 2.362640619277954
steps = 3, loss = 1.8456590175628662
steps = 3, loss = 2.3816022872924805
steps = 3, loss = 3.0321216583251953
steps = 3, loss = 2.126497983932495
steps = 3, loss = 2.083153009414673
steps = 3, loss = 2.4066925048828125
steps = 3, loss = 2.359405517578125
steps = 3, loss = 1.6079941987991333
steps = 3, loss = 1.9611597061157227
steps = 3, loss = 2.024406909942627
steps = 3, loss = 2.005197048187256
steps = 3, loss = 1.7224575281143188
steps = 3, loss = 2.152313709259033
steps = 3, loss = 2.1922245025634766
steps = 3, loss = 2.098339319229126
steps = 3, loss = 1.883231282234192
steps = 3, loss = 1.9455828666687012
steps = 3, loss = 1.6191914081573486
steps = 3, loss = 1.7660757303237915
steps = 3, loss = 0.8094541430473328
steps = 3, loss = 1.524651050567627
steps = 3, loss = 1.6460269689559937
steps = 3, loss = 1.8905798196792603
steps = 3, loss = 2.2412705421447754
steps = 3, loss = 1.9770148992538452
steps = 164, loss = 2.716118812561035
steps = 3, loss = 2.1688249111175537
steps = 3, loss = 1.6521267890930176
steps = 3, loss = 2.017876386642456
steps = 3, loss = 1.8900762796401978
steps = 3, loss = 2.1431329250335693
steps = 3, loss = 1.9306631088256836
steps = 3, loss = 2.0830466747283936
steps = 3, loss = 0.7037591338157654
steps = 3, loss = 2.2497189044952393
steps = 3, loss = 2.040952682495117
steps = 3, loss = 2.112875461578369
steps = 3, loss = 2.212273597717285
steps = 3, loss = 1.831473708152771
steps = 3, loss = 3.336906671524048
steps = 3, loss = 2.006594181060791
steps = 3, loss = 2.610340118408203
steps = 3, loss = 2.0473783016204834
steps = 3, loss = 2.217535972595215
steps = 3, loss = 1.9574196338653564
steps = 3, loss = 1.9342753887176514
steps = 3, loss = 2.0015599727630615
steps = 3, loss = 0.8698888421058655
steps = 3, loss = 1.8197076320648193
steps = 3, loss = 0.9935616254806519
steps = 3, loss = 0.8688770532608032
steps = 3, loss = 1.6892887353897095
steps = 3, loss = 2.071824312210083
steps = 3, loss = 2.0368661880493164
steps = 3, loss = 2.22698712348938
steps = 3, loss = 1.9211931228637695
steps = 3, loss = 1.511291742324829
steps = 9, loss = 1.6607881784439087
steps = 9, loss = 2.120249032974243
steps = 9, loss = 1.7270313501358032
steps = 9, loss = 2.1209592819213867
steps = 9, loss = 1.5881067514419556
steps = 9, loss = 2.0426547527313232
steps = 9, loss = 2.009592056274414
steps = 9, loss = 1.7027232646942139
steps = 9, loss = 2.086422920227051
steps = 9, loss = 2.0096709728240967
steps = 9, loss = 1.9636528491973877
steps = 9, loss = 2.049146890640259
steps = 9, loss = 1.8633979558944702
steps = 9, loss = 1.952848196029663
steps = 9, loss = 1.9769742488861084
steps = 9, loss = 1.6348720788955688
steps = 9, loss = 1.651846170425415
steps = 9, loss = 2.11975359916687
steps = 9, loss = 1.8377265930175781
steps = 9, loss = 1.9322645664215088
steps = 9, loss = 1.8323373794555664
steps = 9, loss = 1.4832422733306885
steps = 9, loss = 1.5780925750732422
steps = 9, loss = 1.9759665727615356
steps = 9, loss = 2.1082050800323486
steps = 9, loss = 1.8985896110534668
steps = 9, loss = 2.02573299407959
steps = 27, loss = 2.1040382385253906
steps = 27, loss = 1.9051071405410767
steps = 27, loss = 1.933126449584961
steps = 27, loss = 1.9565768241882324
steps = 27, loss = 1.9646590948104858
steps = 27, loss = 2.108924627304077
steps = 27, loss = 1.932251214981079
steps = 1, loss = 1.895315408706665
steps = 1, loss = 2.393123149871826
steps = 1, loss = 3.5921008586883545
steps = 1, loss = 1.8387248516082764
steps = 1, loss = 1.7891409397125244
steps = 1, loss = 1.7044051885604858
steps = 27, loss = 2.024177312850952
steps = 27, loss = 1.9838441610336304
steps = 1, loss = 2.0396249294281006
steps = 1, loss = 5.046157360076904
steps = 1, loss = 1.6774009466171265
steps = 1, loss = 0.7628449201583862
steps = 1, loss = 2.5139739513397217
steps = 1, loss = 1.7412034273147583
steps = 1, loss = 0.7005345821380615
steps = 1, loss = 4.369958400726318
steps = 1, loss = 2.0256917476654053
steps = 1, loss = 2.871305465698242
steps = 1, loss = 1.9939134120941162
steps = 1, loss = 2.476270914077759
steps = 1, loss = 1.9548014402389526
steps = 1, loss = 3.0630900859832764
steps = 1, loss = 2.106520414352417
steps = 1, loss = 0.8362001776695251
steps = 1, loss = 1.844232439994812
steps = 1, loss = 2.083726406097412
steps = 1, loss = 49.52632141113281
steps = 1, loss = 50.00410461425781
steps = 1, loss = 4.463320732116699
steps = 1, loss = 2.4617886543273926
steps = 1, loss = 2.676065444946289
steps = 1, loss = 2.562161445617676
steps = 1, loss = 3.7054591178894043
steps = 1, loss = 1.7264755964279175
steps = 1, loss = 2.1423280239105225
steps = 1, loss = 3.276416063308716
steps = 1, loss = 2.0452189445495605
steps = 1, loss = 1.8593363761901855
steps = 1, loss = 1.9370205402374268
steps = 1, loss = 2.37599778175354
steps = 1, loss = 3.033787727355957
steps = 1, loss = 1.0351457595825195
steps = 1, loss = 1.9453344345092773
steps = 1, loss = 3.028352737426758
steps = 1, loss = 2.071754217147827
steps = 81, loss = 2.2148025035858154
steps = 1, loss = 1.0793598890304565
steps = 1, loss = 1.7056211233139038
steps = 1, loss = 2.1204028129577637
steps = 1, loss = 3.019948959350586
steps = 1, loss = 2.191282033920288
steps = 1, loss = 3.033582925796509
steps = 1, loss = 1.9430780410766602
steps = 1, loss = 2.496718168258667
steps = 1, loss = 1.9281487464904785
steps = 1, loss = 49.993125915527344
steps = 1, loss = 1.5872251987457275
steps = 1, loss = 2.5271575450897217
steps = 1, loss = 1.7236967086791992
steps = 1, loss = 2.3550612926483154
steps = 1, loss = 2.349792718887329
steps = 1, loss = 2.72906494140625
steps = 1, loss = 50.01737594604492
steps = 1, loss = 2.552330255508423
steps = 1, loss = 1.7441860437393188
steps = 1, loss = 0.7178204655647278
steps = 1, loss = 49.98950958251953
steps = 1, loss = 1.4244718551635742
steps = 1, loss = 1.7995884418487549
steps = 1, loss = 1.924985647201538
steps = 1, loss = 2.5777511596679688
steps = 1, loss = 2.9048030376434326
steps = 1, loss = 2.1250522136688232
steps = 1, loss = 1.772114872932434
steps = 1, loss = 4.099021911621094
steps = 1, loss = 2.548985481262207
steps = 1, loss = 1.8010423183441162
steps = 1, loss = 1.0331693887710571
steps = 1, loss = 50.030277252197266
steps = 1, loss = 2.918372869491577
steps = 1, loss = 1.8175452947616577
steps = 1, loss = 1.454298496246338
steps = 1, loss = 2.0928523540496826
steps = 1, loss = 49.9900016784668
steps = 1, loss = 1.7071654796600342
steps = 1, loss = 1.9996201992034912
steps = 1, loss = 2.467400074005127
steps = 1, loss = 0.7289989590644836
steps = 1, loss = 1.999109148979187
steps = 1, loss = 2.0680341720581055
steps = 1, loss = 49.99685287475586
steps = 1, loss = 50.00291061401367
steps = 81, loss = 2.202017307281494
steps = 1, loss = 2.208463430404663
steps = 1, loss = 1.6046885251998901
steps = 1, loss = 1.9615029096603394
steps = 1, loss = 1.99561607837677
steps = 1, loss = 1.4283978939056396
steps = 1, loss = 2.8841774463653564
steps = 1, loss = 2.3737595081329346
steps = 1, loss = 2.3560969829559326
steps = 1, loss = 50.011226654052734
steps = 3, loss = 3.192354679107666
steps = 3, loss = 2.0808093547821045
steps = 3, loss = 2.182337522506714
steps = 3, loss = 2.6706807613372803
steps = 3, loss = 2.5108275413513184
steps = 3, loss = 2.751431941986084
steps = 3, loss = 2.6858725547790527
steps = 3, loss = 49.99565124511719
steps = 3, loss = 2.685760021209717
steps = 3, loss = 50.00397491455078
steps = 3, loss = 2.0177133083343506
steps = 3, loss = 50.020172119140625
steps = 3, loss = 3.231910228729248
steps = 3, loss = 2.02769136428833
steps = 3, loss = 2.2751128673553467
steps = 3, loss = 1.744831919670105
steps = 3, loss = 1.7370233535766602
steps = 3, loss = 2.128261089324951
steps = 3, loss = 2.1947312355041504
steps = 3, loss = 2.1020779609680176
steps = 3, loss = 2.303501605987549
steps = 3, loss = 2.4379518032073975
steps = 3, loss = 1.9313313961029053
steps = 3, loss = 2.792066812515259
steps = 3, loss = 50.01904296875
steps = 3, loss = 2.0061841011047363
steps = 3, loss = 2.320230722427368
steps = 3, loss = 2.4446866512298584
steps = 3, loss = 1.9082404375076294
steps = 3, loss = 4.088883876800537
steps = 3, loss = 4.8705949783325195
steps = 3, loss = 8.433829307556152
steps = 3, loss = 2.0264554023742676
steps = 3, loss = 1.5250083208084106
steps = 3, loss = 2.1829025745391846
steps = 3, loss = 2.059101104736328
steps = 3, loss = 6.445107936859131
steps = 3, loss = 6.032118320465088
steps = 3, loss = 2.1029958724975586
steps = 3, loss = 1.8396025896072388
steps = 3, loss = 50.02533721923828
steps = 3, loss = 3.3168938159942627
steps = 3, loss = 50.0108642578125
steps = 3, loss = 2.308790922164917
steps = 3, loss = 0.7523163557052612
steps = 3, loss = 4.172092437744141
steps = 3, loss = 3.0280823707580566
steps = 3, loss = 3.1509923934936523
steps = 3, loss = 2.7160305976867676
steps = 3, loss = 7.003838539123535
steps = 3, loss = 2.24890398979187
steps = 3, loss = 2.6372578144073486
steps = 3, loss = 1.9144850969314575
steps = 3, loss = 3.395068407058716
steps = 3, loss = 2.6224653720855713
steps = 81, loss = 2.036036491394043
steps = 3, loss = 2.156270742416382
steps = 3, loss = 2.1467459201812744
steps = 3, loss = 2.028358221054077
steps = 3, loss = 2.5080618858337402
steps = 3, loss = 3.3055343627929688
steps = 3, loss = 1.86795973777771
steps = 3, loss = 2.4270966053009033
steps = 3, loss = 3.107546806335449
steps = 3, loss = 2.580911159515381
steps = 3, loss = 1.7390267848968506
steps = 3, loss = 2.3826911449432373
steps = 3, loss = 2.5610923767089844
steps = 3, loss = 3.3957626819610596
steps = 3, loss = 2.249329090118408
steps = 3, loss = 3.3558335304260254
steps = 3, loss = 2.3448314666748047
steps = 3, loss = 2.3414411544799805
steps = 3, loss = 2.3409276008605957
steps = 3, loss = 2.1981630325317383
steps = 3, loss = 49.993125915527344
steps = 3, loss = 1.929463267326355
steps = 3, loss = 2.843702554702759
steps = 3, loss = 1.9720687866210938
steps = 3, loss = 2.9017293453216553
steps = 3, loss = 2.212996006011963
steps = 3, loss = 2.1328182220458984
steps = 3, loss = 49.798336029052734
steps = 3, loss = 2.6117916107177734
steps = 164, loss = 2.2924604415893555
steps = 3, loss = 49.85498046875
steps = 164, loss = 2.190166711807251
steps = 3, loss = 1.9933873414993286
steps = 3, loss = 0.9839263558387756
steps = 3, loss = 2.279383659362793
steps = 3, loss = 2.6321487426757812
steps = 3, loss = 2.489528179168701
steps = 3, loss = 49.99576950073242
steps = 3, loss = 2.749919891357422
steps = 3, loss = 49.99607849121094
steps = 3, loss = 2.468491554260254
steps = 3, loss = 1.9832215309143066
steps = 3, loss = 2.3394975662231445
steps = 3, loss = 2.4848289489746094
steps = 3, loss = 3.2615303993225098
steps = 3, loss = 1.8845576047897339
steps = 9, loss = 2.243142604827881
steps = 9, loss = 2.017806053161621
steps = 9, loss = 2.058734178543091
steps = 9, loss = 2.3040060997009277
steps = 9, loss = 2.504309892654419
steps = 9, loss = 2.0755722522735596
steps = 9, loss = 2.180574655532837
steps = 9, loss = 2.2753186225891113
steps = 9, loss = 2.796196699142456
steps = 9, loss = 2.46132230758667
steps = 9, loss = 2.30694580078125
steps = 9, loss = 2.6695594787597656
steps = 9, loss = 2.4428586959838867
steps = 9, loss = 2.1143031120300293
steps = 9, loss = 2.2868926525115967
steps = 9, loss = 2.3466382026672363
steps = 9, loss = 1.745879888534546
steps = 9, loss = 2.222287178039551
steps = 9, loss = 2.481391429901123
steps = 9, loss = 1.9006377458572388
steps = 9, loss = 2.3799445629119873
steps = 9, loss = 2.0893688201904297
steps = 9, loss = 2.1625735759735107
steps = 9, loss = 2.5292351245880127
steps = 9, loss = 2.9985969066619873
steps = 9, loss = 2.520145893096924
steps = 9, loss = 2.1878502368927
steps = 9, loss = 1.4346052408218384
steps = 9, loss = 1.9777668714523315
steps = 9, loss = 1.92705500125885
steps = 9, loss = 1.9138002395629883
steps = 9, loss = 2.4519124031066895
steps = 27, loss = 2.160358190536499
steps = 27, loss = 2.063892364501953
steps = 27, loss = 2.6203665733337402
steps = 27, loss = 2.9051148891448975
steps = 27, loss = 2.6540467739105225
steps = 27, loss = 2.2422797679901123
steps = 27, loss = 1.8925540447235107
steps = 27, loss = 2.1033377647399902
steps = 27, loss = 2.0571844577789307
steps = 27, loss = 2.443009853363037
steps = 81, loss = 2.246981382369995
steps = 81, loss = 2.166259765625
steps = 81, loss = 2.5439112186431885
steps = 164, loss = 2.5308125019073486
CPU times: user 31min 33s, sys: 3min 53s, total: 35min 26s
Wall time: 3h 8min 24s
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<style>#sk-container-id-2 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-2 {
  color: var(--sklearn-color-text);
}

#sk-container-id-2 pre {
  padding: 0;
}

#sk-container-id-2 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-2 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-2 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-2 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-2 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-2 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-2 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-2 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-2 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-2 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-2 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-2 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-2 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-2 div.sk-label label.sk-toggleable__label,
#sk-container-id-2 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-2 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-2 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-2 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-2 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-2 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-2 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-2 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-2 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div class="sk-top-container" id="sk-container-id-2"><div class="sk-text-repr-fallback"><pre>HyperbandSearchCV(estimator=&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
),
                  max_iter=250,
                  parameters={'batch_size': [32, 64, 128, 256, 512],
                              'module__activation': ['ReLU', 'LeakyReLU', 'ELU',
                                                     'PReLU'],
                              'module__init': ['xavier_uniform_',
                                               'xavier_normal_',
                                               'kaiming_uniform_',
                                               'kaiming_normal_'],
                              'optimizer': ['SGD', 'SGD', 'SGD', 'S...
       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,
       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,
       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,
       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),
                              'optimizer__nesterov': [True],
                              'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, ...],
                              'train_split': [None]},
                  patience=True, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox"/><label class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted" for="sk-estimator-id-4"> HyperbandSearchCV<span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>HyperbandSearchCV(estimator=&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
),
                  max_iter=250,
                  parameters={'batch_size': [32, 64, 128, 256, 512],
                              'module__activation': ['ReLU', 'LeakyReLU', 'ELU',
                                                     'PReLU'],
                              'module__init': ['xavier_uniform_',
                                               'xavier_normal_',
                                               'kaiming_uniform_',
                                               'kaiming_normal_'],
                              'optimizer': ['SGD', 'SGD', 'SGD', 'S...
       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,
       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,
       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,
       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),
                              'optimizer__nesterov': [True],
                              'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, 0, 0, 0, 0, 0,
                                                          0, 0, ...],
                              'train_split': [None]},
                  patience=True, random_state=42)</pre></div> </div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox"/><label class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted" for="sk-estimator-id-5">estimator: TrimParams</label><div class="sk-toggleable__content fitted"><pre>&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
)</pre></div> </div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox"/><label class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted" for="sk-estimator-id-6">TrimParams</label><div class="sk-toggleable__content fitted"><pre>&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
)</pre></div> </div></div></div></div></div></div></div></div></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">save_search</span><span class="p">(</span><span class="n">search_patience</span><span class="p">,</span> <span class="n">today</span><span class="p">,</span> <span class="s2">"hyperband-w-patience"</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">compute</span><span class="p">(),</span> <span class="n">y_test</span><span class="o">.</span><span class="n">compute</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># timing_stats = timing_stats = client.profile(filename="hyperband.html")</span>
<span class="c1"># with open(f"{absolutepath_to_results}/hyperband+sop-timing.json", "w") as f:</span>
<span class="c1">#     json.dump(timing_stats[0], f)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">search_patience</span><span class="o">.</span><span class="n">best_estimator_</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>&lt;class '__main__.TrimParams'&gt;[initialized](
  module_=Autoencoder(
    (encoder): Sequential(
      (0): Linear(in_features=784, out_features=784, bias=True)
      (1): ELU(alpha=1.0, inplace=True)
      (2): Linear(in_features=784, out_features=196, bias=True)
      (3): ELU(alpha=1.0, inplace=True)
    )
    (decoder): Sequential(
      (0): Linear(in_features=196, out_features=784, bias=True)
      (1): ELU(alpha=1.0, inplace=True)
      (2): Linear(in_features=784, out_features=784, bias=True)
      (3): Sigmoid()
    )
  ),
)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">search_patience</span><span class="o">.</span><span class="n">best_score_</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>-2.119516372680664</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">search_patience</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>{'train_split': None,
 'optimizer__weight_decay': 0.0007479522515621821,
 'optimizer__nesterov': True,
 'optimizer__momentum': 0.4544544544544544,
 'optimizer__lr': 3.6271002523306466,
 'optimizer': 'SGD',
 'module__init': 'xavier_uniform_',
 'module__activation': 'ELU',
 'batch_size': 128}</pre>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Visualizing-output-of-best-estimator">Visualizing output of best estimator<a class="anchor-link" href="#Visualizing-output-of-best-estimator">¶</a></h3>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">noisy_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">clean_hat</span> <span class="o">=</span> <span class="n">search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">noisy_test</span><span class="p">)</span>
<span class="n">clean_hat</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(7002, 2, 784)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">cols</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">w</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">cols</span><span class="o">*</span><span class="n">w</span><span class="p">,</span> <span class="mi">3</span><span class="o">*</span><span class="n">w</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="n">cols</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="p">(</span><span class="n">upper</span><span class="p">,</span> <span class="n">middle</span><span class="p">,</span> <span class="n">lower</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">])):</span>
    <span class="k">if</span> <span class="n">col</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">upper</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">28</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="s1">'ground</span><span class="se">\n</span><span class="s1">truth'</span><span class="p">)</span>
        <span class="n">middle</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">28</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="s1">'input'</span><span class="p">)</span>
        <span class="n">lower</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">28</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="s1">'output'</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
    <span class="n">noisy</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
    <span class="n">clean</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
    <span class="n">clean_hat_i</span> <span class="o">=</span> <span class="n">clean_hat</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'cbar'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">'xticklabels'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">'yticklabels'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">'cmap'</span><span class="p">:</span> <span class="s1">'gray_r'</span><span class="p">}</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">noisy</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">middle</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">clean</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">upper</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">clean_hat_i</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">absolutepath_to_results</span><span class="si">}</span><span class="s2">/best-out.svg"</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">"tight"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Setting-Parameters-for-Hyperband">Setting Parameters for Hyperband<a class="anchor-link" href="#Setting-Parameters-for-Hyperband">¶</a></h3><p>Tuning Hyperband requires setting:</p>
<ul>
<li><code>max_iter</code>: Number of epochs or data passes for training.</li>
<li>Chunk size for the data array.</li>
</ul>
<p>These determine the number of models evaluated and the overall search complexity.</p>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Comparison-with-Early-Stopping-Techniques">Comparison with Early Stopping Techniques<a class="anchor-link" href="#Comparison-with-Early-Stopping-Techniques">¶</a></h3><p>Compares Hyperband with:</p>
<ul>
<li><strong>Passive Search:</strong> No early stopping, evaluates all models.</li>
<li><strong>Incremental Search with Patience:</strong> Early stops models that don't improve on a validation set.</li>
</ul>
<p>Analyzes:</p>
<ul>
<li>Best model scores</li>
<li>Visualizations of the best model's output</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">total_calls</span> <span class="o">=</span> <span class="n">search</span><span class="o">.</span><span class="n">metadata_</span><span class="p">[</span><span class="s1">'partial_fit_calls'</span><span class="p">]</span>
<span class="n">num_calls</span> <span class="o">=</span> <span class="n">max_iter</span>

<span class="c1"># n_workers = 32 or len(client.cluster.workers)</span>
<span class="n">n_workers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">client</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">workers</span><span class="p">)</span>
<span class="n">num_models</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">n_workers</span><span class="p">,</span> <span class="n">total_calls</span> <span class="o">//</span> <span class="n">num_calls</span><span class="p">)</span>
<span class="n">num_calls</span><span class="p">,</span> <span class="n">num_models</span><span class="p">,</span> <span class="n">total_calls</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>(250, 29, 7401)</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">dask_ml.model_selection</span> <span class="kn">import</span> <span class="n">IncrementalSearchCV</span>

<span class="n">passive_search</span> <span class="o">=</span> <span class="n">IncrementalSearchCV</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">decay_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">patience</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">n_initial_parameters</span><span class="o">=</span><span class="n">num_models</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="n">num_calls</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>
<span class="n">passive_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>steps = 1, loss = 0.7092676758766174
steps = 1, loss = 0.7851949334144592
steps = 1, loss = 2.9281046390533447
steps = 1, loss = 2.0382602214813232
steps = 1, loss = 1.7686915397644043
steps = 1, loss = 2.0014758110046387
steps = 1, loss = 2.4612090587615967
steps = 1, loss = 2.0172791481018066
steps = 1, loss = 2.428661346435547
steps = 1, loss = 1.8866915702819824
steps = 1, loss = 2.4018678665161133
steps = 1, loss = 1.0489938259124756
steps = 1, loss = 49.450504302978516
steps = 1, loss = 2.036716938018799
steps = 1, loss = 2.4377639293670654
steps = 1, loss = 49.915061950683594
steps = 1, loss = 2.382110834121704
steps = 1, loss = 1.9739691019058228
steps = 1, loss = 1.9683659076690674
steps = 1, loss = 1.9700000286102295
steps = 1, loss = 0.7204102277755737
steps = 1, loss = 2.392211437225342
steps = 1, loss = 2.286267042160034
steps = 1, loss = 1.928123950958252
steps = 1, loss = 1.1285806894302368
steps = 1, loss = 1.7917665243148804
steps = 1, loss = 1.9771867990493774
steps = 1, loss = 2.61761736869812
steps = 1, loss = 49.983497619628906
steps = 2, loss = 2.283470630645752
steps = 2, loss = 1.2986968755722046
steps = 2, loss = 3.688019037246704
steps = 2, loss = 2.2805848121643066
steps = 2, loss = 1.4390194416046143
steps = 2, loss = 1.9770604372024536
steps = 2, loss = 2.324592351913452
steps = 2, loss = 2.7795321941375732
steps = 2, loss = 2.771923303604126
steps = 2, loss = 2.9924490451812744
steps = 2, loss = 2.0872504711151123
steps = 2, loss = 1.4930354356765747
steps = 2, loss = 50.0167350769043
steps = 2, loss = 2.463853120803833
steps = 2, loss = 2.665360689163208
steps = 2, loss = 2.597959041595459
steps = 2, loss = 49.92780303955078
steps = 2, loss = 2.1742429733276367
steps = 2, loss = 2.7490172386169434
steps = 2, loss = 2.4847848415374756
steps = 2, loss = 2.234210729598999
steps = 2, loss = 2.164482593536377
steps = 2, loss = 0.8171289563179016
steps = 2, loss = 2.1029469966888428
steps = 2, loss = 1.6517900228500366
steps = 2, loss = 49.98497772216797
steps = 2, loss = 1.953392505645752
steps = 2, loss = 3.001072645187378
steps = 2, loss = 2.110507011413574
steps = 3, loss = 2.498430013656616
steps = 3, loss = 2.272951126098633
steps = 3, loss = 2.855734348297119
steps = 3, loss = 49.97137451171875
steps = 3, loss = 3.0477211475372314
steps = 3, loss = 2.644404172897339
steps = 3, loss = 1.9225727319717407
steps = 3, loss = 2.444079637527466
steps = 3, loss = 2.939403772354126
steps = 3, loss = 2.6448261737823486
steps = 3, loss = 4.150546073913574
steps = 3, loss = 2.4208104610443115
steps = 3, loss = 2.687669515609741
steps = 3, loss = 1.7920019626617432
steps = 3, loss = 2.42437744140625
steps = 3, loss = 2.0425071716308594
steps = 3, loss = 2.6016268730163574
steps = 3, loss = 2.5177087783813477
steps = 3, loss = 50.02452087402344
steps = 3, loss = 1.5882035493850708
steps = 3, loss = 2.229506015777588
steps = 3, loss = 2.0686490535736084
steps = 3, loss = 2.63291597366333
steps = 3, loss = 2.0986995697021484
steps = 3, loss = 1.7211902141571045
steps = 3, loss = 49.9853630065918
steps = 3, loss = 1.1351513862609863
steps = 3, loss = 2.1509923934936523
steps = 3, loss = 1.6638615131378174
steps = 4, loss = 2.176088571548462
steps = 4, loss = 1.7656171321868896
steps = 4, loss = 1.847576379776001
steps = 4, loss = 1.8835585117340088
steps = 4, loss = 2.6987571716308594
steps = 4, loss = 1.7220323085784912
steps = 4, loss = 3.158294916152954
steps = 4, loss = 2.2945172786712646
steps = 4, loss = 49.9853630065918
steps = 4, loss = 1.4219131469726562
steps = 4, loss = 2.627537965774536
steps = 4, loss = 2.342996835708618
steps = 4, loss = 2.8625943660736084
steps = 4, loss = 2.033984899520874
steps = 4, loss = 2.9437384605407715
steps = 4, loss = 2.71840500831604
steps = 4, loss = 2.3450934886932373
steps = 4, loss = 2.905958414077759
steps = 4, loss = 50.018341064453125
steps = 4, loss = 49.93511199951172
steps = 4, loss = 2.7668113708496094
steps = 4, loss = 2.5238282680511475
steps = 4, loss = 2.604020118713379
steps = 4, loss = 2.518935203552246
steps = 4, loss = 2.4513299465179443
steps = 4, loss = 3.2771856784820557
steps = 4, loss = 2.7267062664031982
steps = 4, loss = 2.5456979274749756
steps = 4, loss = 2.1125948429107666
steps = 5, loss = 2.591975688934326
steps = 5, loss = 2.9962072372436523
steps = 5, loss = 2.7171456813812256
steps = 5, loss = 2.9979095458984375
steps = 5, loss = 2.2575876712799072
steps = 5, loss = 1.5709632635116577
steps = 5, loss = 2.7865188121795654
steps = 5, loss = 1.7926945686340332
steps = 5, loss = 2.624917984008789
steps = 5, loss = 2.7797257900238037
steps = 5, loss = 2.08762788772583
steps = 5, loss = 3.184272050857544
steps = 5, loss = 2.538855791091919
steps = 5, loss = 1.9321497678756714
steps = 5, loss = 2.9009666442871094
steps = 5, loss = 49.9853630065918
steps = 5, loss = 2.466801881790161
steps = 5, loss = 2.5644659996032715
steps = 5, loss = 2.5636532306671143
steps = 5, loss = 2.9108669757843018
steps = 5, loss = 49.99802017211914
steps = 5, loss = 1.9802407026290894
steps = 5, loss = 49.90399169921875
steps = 5, loss = 2.0888888835906982
steps = 5, loss = 1.9140475988388062
steps = 5, loss = 2.8158926963806152
steps = 5, loss = 1.8236297369003296
steps = 5, loss = 3.255171775817871
steps = 5, loss = 2.845951557159424
steps = 6, loss = 2.843693733215332
steps = 6, loss = 3.0205156803131104
steps = 6, loss = 3.113156795501709
steps = 6, loss = 2.7823352813720703
steps = 6, loss = 2.080512285232544
steps = 6, loss = 2.8774542808532715
steps = 6, loss = 49.9853630065918
steps = 6, loss = 1.6527352333068848
steps = 6, loss = 2.817758321762085
steps = 6, loss = 2.7850255966186523
steps = 6, loss = 2.739043951034546
steps = 6, loss = 2.604259490966797
steps = 6, loss = 50.0073356628418
steps = 6, loss = 2.830275774002075
steps = 6, loss = 1.8583101034164429
steps = 6, loss = 2.1268110275268555
steps = 6, loss = 1.9572749137878418
steps = 6, loss = 2.5525870323181152
steps = 6, loss = 2.072033643722534
steps = 6, loss = 2.3177404403686523
steps = 6, loss = 2.434527635574341
steps = 6, loss = 2.763185739517212
steps = 6, loss = 50.02309036254883
steps = 6, loss = 1.8316380977630615
steps = 6, loss = 2.613645553588867
steps = 6, loss = 4.232259273529053
steps = 6, loss = 1.9454008340835571
steps = 6, loss = 2.693899154663086
steps = 6, loss = 2.575777053833008
steps = 7, loss = 2.766954183578491
steps = 7, loss = 2.9180660247802734
steps = 7, loss = 1.982602834701538
steps = 7, loss = 2.881363868713379
steps = 7, loss = 2.437645435333252
steps = 7, loss = 2.5377602577209473
steps = 7, loss = 2.3888442516326904
steps = 7, loss = 2.134072780609131
steps = 7, loss = 3.140765428543091
steps = 7, loss = 1.713534951210022
steps = 7, loss = 2.6024136543273926
steps = 7, loss = 50.01506042480469
steps = 7, loss = 1.9929887056350708
steps = 7, loss = 3.0322647094726562
steps = 7, loss = 2.979257583618164
steps = 7, loss = 2.603130340576172
steps = 7, loss = 2.6066625118255615
steps = 7, loss = 2.0705363750457764
steps = 7, loss = 1.8936560153961182
steps = 7, loss = 3.334826707839966
steps = 7, loss = 2.8801498413085938
steps = 7, loss = 2.3018651008605957
steps = 7, loss = 2.6584277153015137
steps = 7, loss = 2.9496219158172607
steps = 7, loss = 49.9853630065918
steps = 7, loss = 1.8714599609375
steps = 7, loss = 3.0587573051452637
steps = 7, loss = 2.106884002685547
steps = 7, loss = 50.0030403137207
steps = 8, loss = 2.5007314682006836
steps = 8, loss = 2.806699752807617
steps = 8, loss = 2.4854001998901367
steps = 8, loss = 49.960594177246094
steps = 8, loss = 2.292832612991333
steps = 8, loss = 3.2753729820251465
steps = 8, loss = 3.20439076423645
steps = 8, loss = 2.4783575534820557
steps = 8, loss = 2.7478487491607666
steps = 8, loss = 1.776298999786377
steps = 8, loss = 3.5627968311309814
steps = 8, loss = 3.0353875160217285
steps = 8, loss = 49.95314407348633
steps = 8, loss = 2.0399186611175537
steps = 8, loss = 2.9788830280303955
steps = 8, loss = 2.204698085784912
steps = 8, loss = 2.927579164505005
steps = 8, loss = 3.0640900135040283
steps = 8, loss = 3.0189855098724365
steps = 8, loss = 1.9266670942306519
steps = 8, loss = 2.7846274375915527
steps = 8, loss = 2.9159653186798096
steps = 8, loss = 1.9413307905197144
steps = 8, loss = 2.1412928104400635
steps = 8, loss = 2.052168369293213
steps = 8, loss = 2.6599910259246826
steps = 8, loss = 3.052593469619751
steps = 8, loss = 2.588672637939453
steps = 8, loss = 49.9853630065918
steps = 9, loss = 2.204284191131592
steps = 9, loss = 2.377063751220703
steps = 9, loss = 2.6483991146087646
steps = 9, loss = 1.9667856693267822
steps = 9, loss = 2.9818665981292725
steps = 9, loss = 2.7982590198516846
steps = 9, loss = 2.1702675819396973
steps = 9, loss = 1.9536094665527344
steps = 9, loss = 49.9853630065918
steps = 9, loss = 2.0669426918029785
steps = 9, loss = 3.011249542236328
steps = 9, loss = 1.8122304677963257
steps = 9, loss = 2.7557694911956787
steps = 9, loss = 2.0640196800231934
steps = 9, loss = 2.661135673522949
steps = 9, loss = 2.94327974319458
steps = 9, loss = 50.00458526611328
steps = 9, loss = 3.4573073387145996
steps = 9, loss = 2.557910203933716
steps = 9, loss = 3.1820521354675293
steps = 9, loss = 2.658606767654419
steps = 9, loss = 3.2971720695495605
steps = 9, loss = 2.5273728370666504
steps = 9, loss = 2.787742853164673
steps = 9, loss = 2.0807101726531982
steps = 9, loss = 3.0637612342834473
steps = 9, loss = 2.8993730545043945
steps = 9, loss = 2.9104559421539307
steps = 9, loss = 49.807987213134766
steps = 10, loss = 2.1780946254730225
steps = 10, loss = 49.9853630065918
steps = 10, loss = 1.9862208366394043
steps = 10, loss = 2.5848631858825684
steps = 10, loss = 3.49436092376709
steps = 10, loss = 2.999671697616577
steps = 10, loss = 3.074965000152588
steps = 10, loss = 2.997954845428467
steps = 10, loss = 3.390896797180176
steps = 10, loss = 2.8113229274749756
steps = 10, loss = 1.9985440969467163
steps = 10, loss = 2.680097818374634
steps = 10, loss = 2.5925276279449463
steps = 10, loss = 2.8026058673858643
steps = 10, loss = 2.9399893283843994
steps = 10, loss = 2.911876916885376
steps = 10, loss = 36.0801887512207
steps = 10, loss = 2.8856160640716553
steps = 10, loss = 2.0952250957489014
steps = 10, loss = 1.8494077920913696
steps = 10, loss = 3.198085069656372
steps = 10, loss = 3.0966250896453857
steps = 10, loss = 2.917884349822998
steps = 10, loss = 2.1612792015075684
steps = 10, loss = 49.661773681640625
steps = 10, loss = 2.659083843231201
steps = 10, loss = 2.296572208404541
steps = 10, loss = 2.842698097229004
steps = 10, loss = 2.1202855110168457
steps = 11, loss = 2.0052847862243652
steps = 11, loss = 2.1490187644958496
steps = 11, loss = 2.346592664718628
steps = 11, loss = 50.0150260925293
steps = 11, loss = 3.3929293155670166
steps = 11, loss = 2.7793667316436768
steps = 11, loss = 3.6797287464141846
steps = 11, loss = 2.0122408866882324
steps = 11, loss = 3.5735881328582764
steps = 11, loss = 2.6631577014923096
steps = 11, loss = 2.513608694076538
steps = 11, loss = 2.020322561264038
steps = 11, loss = 3.0461089611053467
steps = 11, loss = 2.7978644371032715
steps = 11, loss = 2.621613025665283
steps = 11, loss = 3.1397578716278076
steps = 11, loss = 3.2546637058258057
steps = 11, loss = 3.0887210369110107
steps = 11, loss = 2.684490203857422
steps = 11, loss = 2.894408941268921
steps = 11, loss = 2.2161006927490234
steps = 11, loss = 2.9578893184661865
steps = 11, loss = 3.0977671146392822
steps = 11, loss = 1.872293472290039
steps = 11, loss = 2.667140245437622
steps = 11, loss = 2.1136417388916016
steps = 11, loss = 49.9853630065918
steps = 11, loss = 20.934431076049805
steps = 11, loss = 2.709932804107666
steps = 12, loss = 2.548823118209839
steps = 12, loss = 2.0584654808044434
steps = 12, loss = 2.7356576919555664
steps = 12, loss = 3.1637802124023438
steps = 12, loss = 2.6446692943573
steps = 12, loss = 2.7755355834960938
steps = 12, loss = 1.8840566873550415
steps = 12, loss = 3.3767926692962646
steps = 12, loss = 2.7088255882263184
steps = 12, loss = 2.625074625015259
steps = 12, loss = 2.115837574005127
steps = 12, loss = 2.0110437870025635
steps = 12, loss = 2.1653144359588623
steps = 12, loss = 2.7520570755004883
steps = 12, loss = 2.980480194091797
steps = 12, loss = 3.0847010612487793
steps = 12, loss = 2.034013509750366
steps = 12, loss = 9.64026165008545
steps = 12, loss = 2.8663125038146973
steps = 12, loss = 2.663911819458008
steps = 12, loss = 3.0310800075531006
steps = 12, loss = 50.01127624511719
steps = 12, loss = 3.4007151126861572
steps = 12, loss = 2.760413885116577
steps = 12, loss = 2.624009847640991
steps = 12, loss = 3.0541162490844727
steps = 12, loss = 2.1537790298461914
steps = 12, loss = 2.132988452911377
steps = 12, loss = 49.9853630065918
steps = 13, loss = 49.92717742919922
steps = 13, loss = 2.4862701892852783
steps = 13, loss = 3.510712146759033
steps = 13, loss = 2.565135955810547
steps = 13, loss = 2.156400203704834
steps = 13, loss = 3.210444450378418
steps = 13, loss = 3.3814759254455566
steps = 13, loss = 2.6668701171875
steps = 13, loss = 2.605642080307007
steps = 13, loss = 2.672523021697998
steps = 13, loss = 2.137491226196289
steps = 13, loss = 1.9016013145446777
steps = 13, loss = 2.1993868350982666
steps = 13, loss = 3.1398844718933105
steps = 13, loss = 3.092039108276367
steps = 13, loss = 6.199883460998535
steps = 13, loss = 2.028501033782959
steps = 13, loss = 3.0680625438690186
steps = 13, loss = 2.7861485481262207
steps = 13, loss = 2.05822491645813
steps = 13, loss = 2.6811954975128174
steps = 13, loss = 49.9853630065918
steps = 13, loss = 3.833543062210083
steps = 13, loss = 2.079693555831909
steps = 13, loss = 2.1322925090789795
steps = 13, loss = 2.866452932357788
steps = 13, loss = 2.7620978355407715
steps = 13, loss = 2.9633679389953613
steps = 13, loss = 2.5244157314300537
steps = 14, loss = 2.7832694053649902
steps = 14, loss = 2.809636354446411
steps = 14, loss = 3.396346092224121
steps = 14, loss = 49.940189361572266
steps = 14, loss = 3.832576036453247
steps = 14, loss = 1.934689998626709
steps = 14, loss = 2.739224910736084
steps = 14, loss = 2.664121627807617
steps = 14, loss = 3.1840808391571045
steps = 14, loss = 2.9772205352783203
steps = 14, loss = 2.907240152359009
steps = 14, loss = 2.7091281414031982
steps = 14, loss = 2.0995211601257324
steps = 14, loss = 2.884490728378296
steps = 14, loss = 2.067467212677002
steps = 14, loss = 3.113041877746582
steps = 14, loss = 49.9853630065918
steps = 14, loss = 2.1836917400360107
steps = 14, loss = 2.630082607269287
steps = 14, loss = 49.99591827392578
steps = 14, loss = 3.555476665496826
steps = 14, loss = 3.1740214824676514
steps = 14, loss = 5.482105731964111
steps = 14, loss = 2.2567784786224365
steps = 14, loss = 2.2872557640075684
steps = 14, loss = 2.1887094974517822
steps = 14, loss = 2.812370777130127
steps = 14, loss = 2.952502727508545
steps = 14, loss = 2.900676965713501
steps = 15, loss = 3.6440391540527344
steps = 15, loss = 2.4930920600891113
steps = 15, loss = 3.1637792587280273
steps = 15, loss = 3.3191323280334473
steps = 15, loss = 2.7623391151428223
steps = 15, loss = 2.084801197052002
steps = 15, loss = 3.315541982650757
steps = 15, loss = 2.124105215072632
steps = 15, loss = 2.2068512439727783
steps = 15, loss = 2.2429561614990234
steps = 15, loss = 49.9853630065918
steps = 15, loss = 3.5268428325653076
steps = 15, loss = 1.9496510028839111
steps = 15, loss = 3.0065600872039795
steps = 15, loss = 2.8832855224609375
steps = 15, loss = 2.290470600128174
steps = 15, loss = 2.7609829902648926
steps = 15, loss = 2.058344602584839
steps = 15, loss = 3.0274147987365723
steps = 15, loss = 2.994286060333252
steps = 15, loss = 2.7441937923431396
steps = 15, loss = 3.194983959197998
steps = 15, loss = 2.2166073322296143
steps = 15, loss = 2.6687910556793213
steps = 15, loss = 2.6876022815704346
steps = 15, loss = 4.289825916290283
steps = 15, loss = 48.762962341308594
steps = 15, loss = 3.345649480819702
steps = 15, loss = 2.755363941192627
steps = 16, loss = 2.256756544113159
steps = 16, loss = 2.798131227493286
steps = 16, loss = 2.1706960201263428
steps = 16, loss = 8.052592277526855
steps = 16, loss = 4.845584869384766
steps = 16, loss = 2.7163496017456055
steps = 16, loss = 2.8521029949188232
steps = 16, loss = 2.9874114990234375
steps = 16, loss = 50.0142822265625
steps = 16, loss = 49.9853630065918
steps = 16, loss = 3.473845958709717
steps = 16, loss = 2.751112937927246
steps = 16, loss = 3.5429744720458984
steps = 16, loss = 2.896865129470825
steps = 16, loss = 2.109086751937866
steps = 16, loss = 2.329084634780884
steps = 16, loss = 3.1314148902893066
steps = 16, loss = 3.57318115234375
steps = 16, loss = 1.970076084136963
steps = 16, loss = 2.9740233421325684
steps = 16, loss = 3.1630935668945312
steps = 16, loss = 2.5829885005950928
steps = 16, loss = 3.2229270935058594
steps = 16, loss = 2.713580846786499
steps = 16, loss = 2.2816097736358643
steps = 16, loss = 2.154445171356201
steps = 16, loss = 2.0673916339874268
steps = 16, loss = 3.282722234725952
steps = 16, loss = 2.776073694229126
steps = 17, loss = 3.079655885696411
steps = 17, loss = 2.272899627685547
steps = 17, loss = 2.799485683441162
steps = 17, loss = 2.726808786392212
steps = 17, loss = 2.5822248458862305
steps = 17, loss = 3.2572884559631348
steps = 17, loss = 2.7949023246765137
steps = 17, loss = 2.1086487770080566
steps = 17, loss = 2.1662697792053223
steps = 17, loss = 1.9405739307403564
steps = 17, loss = 1.9694023132324219
steps = 17, loss = 2.982370615005493
steps = 17, loss = 3.654588222503662
steps = 17, loss = 2.3423280715942383
steps = 17, loss = 2.9701333045959473
steps = 17, loss = 2.747371196746826
steps = 17, loss = 3.270237684249878
steps = 17, loss = 3.5361382961273193
steps = 17, loss = 3.3348164558410645
steps = 17, loss = 2.135662317276001
steps = 17, loss = 4.713506698608398
steps = 17, loss = 50.00657272338867
steps = 17, loss = 1.967599630355835
steps = 17, loss = 49.9853630065918
steps = 17, loss = 2.4318647384643555
steps = 17, loss = 2.8527817726135254
steps = 17, loss = 2.8581204414367676
steps = 17, loss = 2.687671184539795
steps = 17, loss = 2.8806228637695312
steps = 18, loss = 2.202176570892334
steps = 18, loss = 2.8639347553253174
steps = 18, loss = 2.8354122638702393
steps = 18, loss = 5.132136821746826
steps = 18, loss = 3.2351057529449463
steps = 18, loss = 2.2635762691497803
steps = 18, loss = 2.7552454471588135
steps = 18, loss = 3.435883045196533
steps = 18, loss = 2.7544639110565186
steps = 18, loss = 2.0763025283813477
steps = 18, loss = 2.3353002071380615
steps = 18, loss = 3.552027463912964
steps = 18, loss = 2.3906242847442627
steps = 18, loss = 2.780226469039917
steps = 18, loss = 2.666543483734131
steps = 18, loss = 2.8898332118988037
steps = 18, loss = 49.9853630065918
steps = 18, loss = 3.496500253677368
steps = 18, loss = 3.531524181365967
steps = 18, loss = 1.9899858236312866
steps = 18, loss = 3.5810389518737793
steps = 18, loss = 2.877086877822876
steps = 18, loss = 2.681769609451294
steps = 18, loss = 3.210097551345825
steps = 18, loss = 2.754988193511963
steps = 18, loss = 50.010807037353516
steps = 18, loss = 2.6931231021881104
steps = 18, loss = 2.0889313220977783
steps = 18, loss = 2.140493154525757
steps = 19, loss = 2.744797945022583
steps = 19, loss = 3.287344455718994
steps = 19, loss = 2.4264893531799316
steps = 19, loss = 2.11490797996521
steps = 19, loss = 2.755882740020752
steps = 19, loss = 2.969963550567627
steps = 19, loss = 3.528085231781006
steps = 19, loss = 2.7228760719299316
steps = 19, loss = 3.491140604019165
steps = 19, loss = 2.790149211883545
steps = 19, loss = 50.01514434814453
steps = 19, loss = 3.693793296813965
steps = 19, loss = 2.8919382095336914
steps = 19, loss = 3.187192678451538
steps = 19, loss = 2.867262840270996
steps = 19, loss = 2.0081863403320312
steps = 19, loss = 2.695984125137329
steps = 19, loss = 2.272940158843994
steps = 19, loss = 3.5058131217956543
steps = 19, loss = 5.361784934997559
steps = 19, loss = 2.165588617324829
steps = 19, loss = 2.9906346797943115
steps = 19, loss = 2.232107639312744
steps = 19, loss = 2.2390220165252686
steps = 19, loss = 49.9853630065918
steps = 19, loss = 2.864725112915039
steps = 19, loss = 3.019928216934204
steps = 19, loss = 3.0589895248413086
steps = 19, loss = 2.3858344554901123
steps = 20, loss = 3.2922253608703613
steps = 20, loss = 3.5508475303649902
steps = 20, loss = 2.2435944080352783
steps = 20, loss = 2.4371373653411865
steps = 20, loss = 50.00845718383789
steps = 20, loss = 3.4985830783843994
steps = 20, loss = 3.2875242233276367
steps = 20, loss = 3.0645384788513184
steps = 20, loss = 2.839770793914795
steps = 20, loss = 2.8606154918670654
steps = 20, loss = 2.71914005279541
steps = 20, loss = 2.0051300525665283
steps = 20, loss = 3.095344305038452
steps = 20, loss = 2.689887285232544
steps = 20, loss = 2.136664628982544
steps = 20, loss = 2.166429042816162
steps = 20, loss = 2.7280702590942383
steps = 20, loss = 49.9853630065918
steps = 20, loss = 2.7524170875549316
steps = 20, loss = 3.302528142929077
steps = 20, loss = 1.99472177028656
steps = 20, loss = 5.052082061767578
steps = 20, loss = 2.7162654399871826
steps = 20, loss = 2.8542966842651367
steps = 20, loss = 2.979457378387451
steps = 20, loss = 2.409364700317383
steps = 20, loss = 2.6024017333984375
steps = 20, loss = 1.9452698230743408
steps = 20, loss = 2.3516833782196045
steps = 21, loss = 3.0636510848999023
steps = 21, loss = 3.0017740726470947
steps = 21, loss = 2.6942546367645264
steps = 21, loss = 2.2713122367858887
steps = 21, loss = 1.9349939823150635
steps = 21, loss = 3.3052384853363037
steps = 21, loss = 2.491947889328003
steps = 21, loss = 2.665658950805664
steps = 21, loss = 2.7430732250213623
steps = 21, loss = 2.450277805328369
steps = 21, loss = 3.467967987060547
steps = 21, loss = 2.873286724090576
steps = 21, loss = 50.00518798828125
steps = 21, loss = 2.2785260677337646
steps = 21, loss = 49.9853630065918
steps = 21, loss = 2.474058151245117
steps = 21, loss = 2.8903725147247314
steps = 21, loss = 2.199394941329956
steps = 21, loss = 3.4746477603912354
steps = 21, loss = 3.524322986602783
steps = 21, loss = 2.4815165996551514
steps = 21, loss = 5.1713762283325195
steps = 21, loss = 2.745520830154419
steps = 21, loss = 2.991788148880005
steps = 21, loss = 3.0891315937042236
steps = 21, loss = 2.0791218280792236
steps = 21, loss = 3.4792306423187256
steps = 21, loss = 2.568133592605591
steps = 21, loss = 2.0238468647003174
steps = 22, loss = 30.73686408996582
steps = 22, loss = 3.0219621658325195
steps = 22, loss = 2.6354386806488037
steps = 22, loss = 2.421400785446167
steps = 22, loss = 2.905761957168579
steps = 22, loss = 2.8730556964874268
steps = 22, loss = 2.6946892738342285
steps = 22, loss = 2.586369752883911
steps = 22, loss = 2.6842567920684814
steps = 22, loss = 2.721607208251953
steps = 22, loss = 49.9853630065918
steps = 22, loss = 3.139204502105713
steps = 22, loss = 2.2591142654418945
steps = 22, loss = 3.6428232192993164
steps = 22, loss = 2.1312525272369385
steps = 22, loss = 2.2213692665100098
steps = 22, loss = 3.314098834991455
steps = 22, loss = 2.304117202758789
steps = 22, loss = 2.0353941917419434
steps = 22, loss = 2.5224344730377197
steps = 22, loss = 3.3329153060913086
steps = 22, loss = 3.0139987468719482
steps = 22, loss = 5.6166582107543945
steps = 22, loss = 2.04528546333313
steps = 22, loss = 2.510462760925293
steps = 22, loss = 3.2603390216827393
steps = 22, loss = 3.5020480155944824
steps = 22, loss = 2.9204745292663574
steps = 22, loss = 2.593338966369629
steps = 23, loss = 2.562880516052246
steps = 23, loss = 12.198476791381836
steps = 23, loss = 3.520151376724243
steps = 23, loss = 2.5727427005767822
steps = 23, loss = 2.6926262378692627
steps = 23, loss = 3.084901809692383
steps = 23, loss = 3.1582400798797607
steps = 23, loss = 2.6400270462036133
steps = 23, loss = 2.2668023109436035
steps = 23, loss = 2.928860902786255
steps = 23, loss = 3.3432328701019287
steps = 23, loss = 5.6998114585876465
steps = 23, loss = 2.2455132007598877
steps = 23, loss = 3.4217076301574707
steps = 23, loss = 2.0742688179016113
steps = 23, loss = 2.779165029525757
steps = 23, loss = 2.0500760078430176
steps = 23, loss = 3.5308146476745605
steps = 23, loss = 49.9853630065918
steps = 23, loss = 2.3314638137817383
steps = 23, loss = 2.7967350482940674
steps = 23, loss = 2.724459409713745
steps = 23, loss = 3.0632708072662354
steps = 23, loss = 2.54111385345459
steps = 23, loss = 2.887606620788574
steps = 23, loss = 2.9551608562469482
steps = 23, loss = 3.2993037700653076
steps = 23, loss = 2.024075746536255
steps = 23, loss = 2.928408622741699
steps = 24, loss = 6.268621921539307
steps = 24, loss = 2.5235865116119385
steps = 24, loss = 2.3413994312286377
steps = 24, loss = 49.9853630065918
steps = 24, loss = 2.850346565246582
steps = 24, loss = 2.6887192726135254
steps = 24, loss = 3.0938799381256104
steps = 24, loss = 2.0447750091552734
steps = 24, loss = 3.135399580001831
steps = 24, loss = 2.0445990562438965
steps = 24, loss = 3.3621068000793457
steps = 24, loss = 3.626572608947754
steps = 24, loss = 2.734747886657715
steps = 24, loss = 3.2384073734283447
steps = 24, loss = 6.175465106964111
steps = 24, loss = 2.608046293258667
steps = 24, loss = 3.4633090496063232
steps = 24, loss = 2.951234817504883
steps = 24, loss = 2.1865086555480957
steps = 24, loss = 2.1372506618499756
steps = 24, loss = 2.597151279449463
steps = 24, loss = 2.6740827560424805
steps = 24, loss = 2.547854423522949
steps = 24, loss = 3.3717007637023926
steps = 24, loss = 2.919173002243042
steps = 24, loss = 2.2477662563323975
steps = 24, loss = 2.3671376705169678
steps = 24, loss = 2.7171010971069336
steps = 24, loss = 2.5723378658294678
steps = 25, loss = 2.8508458137512207
steps = 25, loss = 2.0505003929138184
steps = 25, loss = 2.571761131286621
steps = 25, loss = 3.0440521240234375
steps = 25, loss = 2.8532052040100098
steps = 25, loss = 2.3589704036712646
steps = 25, loss = 3.450328826904297
steps = 25, loss = 2.930246591567993
steps = 25, loss = 2.362982749938965
steps = 25, loss = 3.364469528198242
steps = 25, loss = 6.198672294616699
steps = 25, loss = 2.973558187484741
steps = 25, loss = 2.25463604927063
steps = 25, loss = 2.47285795211792
steps = 25, loss = 3.314027786254883
steps = 25, loss = 2.6444711685180664
steps = 25, loss = 1.8200819492340088
steps = 25, loss = 2.7319369316101074
steps = 25, loss = 2.716601610183716
steps = 25, loss = 2.110900402069092
steps = 25, loss = 3.222003698348999
steps = 25, loss = 2.690582036972046
steps = 25, loss = 3.2345550060272217
steps = 25, loss = 2.9367470741271973
steps = 25, loss = 4.57144021987915
steps = 25, loss = 2.622652530670166
steps = 25, loss = 2.8516149520874023
steps = 25, loss = 49.9853630065918
steps = 25, loss = 2.265486478805542
steps = 26, loss = 2.809131145477295
steps = 26, loss = 2.780412435531616
steps = 26, loss = 2.6918413639068604
steps = 26, loss = 2.887284278869629
steps = 26, loss = 2.7200891971588135
steps = 26, loss = 50.01760482788086
steps = 26, loss = 2.302950382232666
steps = 26, loss = 6.15773344039917
steps = 26, loss = 2.962829828262329
steps = 26, loss = 3.352513074874878
steps = 26, loss = 2.8796629905700684
steps = 26, loss = 3.352456569671631
steps = 26, loss = 3.5260119438171387
steps = 26, loss = 49.9853630065918
steps = 26, loss = 3.375430107116699
steps = 26, loss = 2.2598557472229004
steps = 26, loss = 3.4951012134552
steps = 26, loss = 2.1580393314361572
steps = 26, loss = 2.615330934524536
steps = 26, loss = 2.7251808643341064
steps = 26, loss = 2.6285927295684814
steps = 26, loss = 3.376467704772949
steps = 26, loss = 3.8105356693267822
steps = 26, loss = 2.6988325119018555
steps = 26, loss = 3.213319778442383
steps = 26, loss = 2.3994576930999756
steps = 26, loss = 2.745974540710449
steps = 26, loss = 2.073674440383911
steps = 26, loss = 2.152914524078369
steps = 27, loss = 2.8470230102539062
steps = 27, loss = 2.4596004486083984
steps = 27, loss = 3.2409844398498535
steps = 27, loss = 3.8462438583374023
steps = 27, loss = 2.739964723587036
steps = 27, loss = 2.2692527770996094
steps = 27, loss = 3.3690903186798096
steps = 27, loss = 2.081484079360962
steps = 27, loss = 2.323910713195801
steps = 27, loss = 2.636021375656128
steps = 27, loss = 3.5089499950408936
steps = 27, loss = 50.01777267456055
steps = 27, loss = 49.9853630065918
steps = 27, loss = 2.131256580352783
steps = 27, loss = 2.4206576347351074
steps = 27, loss = 2.8789498805999756
steps = 27, loss = 2.6537978649139404
steps = 27, loss = 3.3967580795288086
steps = 27, loss = 2.576448917388916
steps = 27, loss = 2.751314401626587
steps = 27, loss = 3.5430965423583984
steps = 27, loss = 2.7717795372009277
steps = 27, loss = 2.9647204875946045
steps = 27, loss = 2.6942532062530518
steps = 27, loss = 6.0276689529418945
steps = 27, loss = 3.3793323040008545
steps = 27, loss = 3.586841583251953
steps = 27, loss = 2.245635747909546
steps = 27, loss = 2.8697240352630615
steps = 28, loss = 50.00189208984375
steps = 28, loss = 2.658277988433838
steps = 28, loss = 2.257542133331299
steps = 28, loss = 2.7978265285491943
steps = 28, loss = 2.1591928005218506
steps = 28, loss = 2.7447946071624756
steps = 28, loss = 2.345167875289917
steps = 28, loss = 5.8826141357421875
steps = 28, loss = 2.956193208694458
steps = 28, loss = 3.3318593502044678
steps = 28, loss = 2.6952433586120605
steps = 28, loss = 2.11702823638916
steps = 28, loss = 3.5094943046569824
steps = 28, loss = 2.8164737224578857
steps = 28, loss = 3.451106548309326
steps = 28, loss = 2.828234910964966
steps = 28, loss = 3.1077518463134766
steps = 28, loss = 2.869534492492676
steps = 28, loss = 3.9270877838134766
steps = 28, loss = 3.3991189002990723
steps = 28, loss = 2.7422990798950195
steps = 28, loss = 2.442580461502075
steps = 28, loss = 3.6108388900756836
steps = 28, loss = 2.5304431915283203
steps = 28, loss = 2.0904877185821533
steps = 28, loss = 3.3982980251312256
steps = 28, loss = 2.9728481769561768
steps = 28, loss = 3.484339475631714
steps = 28, loss = 49.9853630065918
steps = 29, loss = 3.557711362838745
steps = 29, loss = 2.466825246810913
steps = 29, loss = 2.8521440029144287
steps = 29, loss = 6.834939956665039
steps = 29, loss = 2.7222676277160645
steps = 29, loss = 50.02125930786133
steps = 29, loss = 2.2568209171295166
steps = 29, loss = 2.8169031143188477
steps = 29, loss = 2.684234142303467
steps = 29, loss = 2.8850622177124023
steps = 29, loss = 2.9165985584259033
steps = 29, loss = 2.892958879470825
steps = 29, loss = 2.988936424255371
steps = 29, loss = 2.4335312843322754
steps = 29, loss = 3.4940905570983887
steps = 29, loss = 2.6115059852600098
steps = 29, loss = 2.8452963829040527
steps = 29, loss = 3.375694513320923
steps = 29, loss = 2.7630200386047363
steps = 29, loss = 2.1031417846679688
steps = 29, loss = 3.313710927963257
steps = 29, loss = 2.889894485473633
steps = 29, loss = 2.367736577987671
steps = 29, loss = 3.263538122177124
steps = 29, loss = 49.9853630065918
steps = 29, loss = 3.4107718467712402
steps = 29, loss = 3.7752842903137207
steps = 29, loss = 1.8774572610855103
steps = 29, loss = 3.1212985515594482
steps = 30, loss = 2.136718988418579
steps = 30, loss = 2.687750816345215
steps = 30, loss = 2.9749672412872314
steps = 30, loss = 3.3910090923309326
steps = 30, loss = 2.370776891708374
steps = 30, loss = 2.6864495277404785
steps = 30, loss = 6.326103210449219
steps = 30, loss = 2.1803982257843018
steps = 30, loss = 43.30643844604492
steps = 30, loss = 2.4741790294647217
steps = 30, loss = 3.1294078826904297
steps = 30, loss = 2.7433884143829346
steps = 30, loss = 2.847834348678589
steps = 30, loss = 3.3109259605407715
steps = 30, loss = 2.921050548553467
steps = 30, loss = 2.6872005462646484
steps = 30, loss = 3.256636381149292
steps = 30, loss = 2.6121437549591064
steps = 30, loss = 49.9853630065918
steps = 30, loss = 1.9951808452606201
steps = 30, loss = 3.3702995777130127
steps = 30, loss = 2.7907307147979736
steps = 30, loss = 3.0332119464874268
steps = 30, loss = 3.4340920448303223
steps = 30, loss = 2.9572627544403076
steps = 30, loss = 2.388214349746704
steps = 30, loss = 2.096853494644165
steps = 30, loss = 3.4197299480438232
steps = 30, loss = 3.247227668762207
steps = 31, loss = 2.117426872253418
steps = 31, loss = 2.4928524494171143
steps = 31, loss = 49.9853630065918
steps = 31, loss = 2.597661018371582
steps = 31, loss = 3.315598726272583
steps = 31, loss = 3.3777668476104736
steps = 31, loss = 3.38503098487854
steps = 31, loss = 2.7128984928131104
steps = 31, loss = 2.1020395755767822
steps = 31, loss = 2.7514536380767822
steps = 31, loss = 6.163338661193848
steps = 31, loss = 3.6683685779571533
steps = 31, loss = 2.706007957458496
steps = 31, loss = 2.741142511367798
steps = 31, loss = 2.9810054302215576
steps = 31, loss = 2.3880221843719482
steps = 31, loss = 2.7021331787109375
steps = 31, loss = 2.719808578491211
steps = 31, loss = 3.4341986179351807
steps = 31, loss = 2.1245620250701904
steps = 31, loss = 2.849252462387085
steps = 31, loss = 3.1144745349884033
steps = 31, loss = 2.8764867782592773
steps = 31, loss = 2.981009006500244
steps = 31, loss = 3.3779795169830322
steps = 31, loss = 2.6893889904022217
steps = 31, loss = 3.4167301654815674
steps = 31, loss = 2.4601151943206787
steps = 31, loss = 2.0419516563415527
steps = 32, loss = 2.5227036476135254
steps = 32, loss = 2.134589672088623
steps = 32, loss = 3.3638148307800293
steps = 32, loss = 2.9422199726104736
steps = 32, loss = 2.999122381210327
steps = 32, loss = 3.071233034133911
steps = 32, loss = 2.8672068119049072
steps = 32, loss = 2.4961562156677246
steps = 32, loss = 2.742130994796753
steps = 32, loss = 2.6952738761901855
steps = 32, loss = 49.9853630065918
steps = 32, loss = 2.814347505569458
steps = 32, loss = 3.2864229679107666
steps = 32, loss = 2.470658302307129
steps = 32, loss = 6.198876857757568
steps = 32, loss = 2.2871992588043213
steps = 32, loss = 3.428187608718872
steps = 32, loss = 3.703688859939575
steps = 32, loss = 3.522609233856201
steps = 32, loss = 3.511898994445801
steps = 32, loss = 2.8146777153015137
steps = 32, loss = 2.7395970821380615
steps = 32, loss = 2.7006583213806152
steps = 32, loss = 2.1211447715759277
steps = 32, loss = 3.5788257122039795
steps = 32, loss = 2.4620559215545654
steps = 32, loss = 2.422755479812622
steps = 32, loss = 2.107783555984497
steps = 32, loss = 3.5153329372406006
steps = 33, loss = 2.6350178718566895
steps = 33, loss = 2.759248971939087
steps = 33, loss = 3.013150930404663
steps = 33, loss = 2.5464179515838623
steps = 33, loss = 2.315610647201538
steps = 33, loss = 2.7209160327911377
steps = 33, loss = 2.764561176300049
steps = 33, loss = 2.8053154945373535
steps = 33, loss = 2.1354074478149414
steps = 33, loss = 3.5287885665893555
steps = 33, loss = 2.883612632751465
steps = 33, loss = 6.652508735656738
steps = 33, loss = 2.8655171394348145
steps = 33, loss = 3.1465306282043457
steps = 33, loss = 2.704442262649536
steps = 33, loss = 3.9132883548736572
steps = 33, loss = 2.1648237705230713
steps = 33, loss = 2.935203790664673
steps = 33, loss = 3.4693048000335693
steps = 33, loss = 3.5210628509521484
steps = 33, loss = 2.4459421634674072
steps = 33, loss = 2.7528011798858643
steps = 33, loss = 3.34826397895813
steps = 33, loss = 3.435396909713745
steps = 33, loss = 3.204270601272583
steps = 33, loss = 3.4200551509857178
steps = 33, loss = 49.9853630065918
steps = 33, loss = 2.2519519329071045
steps = 33, loss = 2.938056707382202
steps = 34, loss = 3.3566906452178955
steps = 34, loss = 2.765195608139038
steps = 34, loss = 2.794724941253662
steps = 34, loss = 2.847720146179199
steps = 34, loss = 2.1301798820495605
steps = 34, loss = 2.064246416091919
steps = 34, loss = 3.4490435123443604
steps = 34, loss = 2.9975438117980957
steps = 34, loss = 3.2944703102111816
steps = 34, loss = 2.827251672744751
steps = 34, loss = 3.333684206008911
steps = 34, loss = 2.410543918609619
steps = 34, loss = 3.185250759124756
steps = 34, loss = 2.6634418964385986
steps = 34, loss = 2.688507556915283
steps = 34, loss = 2.584524631500244
steps = 34, loss = 3.0274851322174072
steps = 34, loss = 2.7585256099700928
steps = 34, loss = 49.9853630065918
steps = 34, loss = 6.464747905731201
steps = 34, loss = 3.3590176105499268
steps = 34, loss = 3.437539577484131
steps = 34, loss = 3.7650227546691895
steps = 34, loss = 2.01261305809021
steps = 34, loss = 2.77744460105896
steps = 34, loss = 2.449756622314453
steps = 34, loss = 2.552631378173828
steps = 34, loss = 2.1363847255706787
steps = 34, loss = 3.133913278579712
steps = 35, loss = 3.590428352355957
steps = 35, loss = 2.483168601989746
steps = 35, loss = 2.5798263549804688
steps = 35, loss = 2.7428834438323975
steps = 35, loss = 2.695619821548462
steps = 35, loss = 3.648322820663452
steps = 35, loss = 2.117929697036743
steps = 35, loss = 3.464416980743408
steps = 35, loss = 49.9853630065918
steps = 35, loss = 2.6797664165496826
steps = 35, loss = 3.1658666133880615
steps = 35, loss = 3.0133514404296875
steps = 35, loss = 2.7959189414978027
steps = 35, loss = 3.325578451156616
steps = 35, loss = 2.1484756469726562
steps = 35, loss = 3.3393421173095703
steps = 35, loss = 3.2976274490356445
steps = 35, loss = 2.280505657196045
steps = 35, loss = 2.112985134124756
steps = 35, loss = 2.866457223892212
steps = 35, loss = 2.8776915073394775
steps = 35, loss = 3.4328134059906006
steps = 35, loss = 3.021390199661255
steps = 35, loss = 6.727156162261963
steps = 35, loss = 2.768460988998413
steps = 35, loss = 3.2893075942993164
steps = 35, loss = 2.8869166374206543
steps = 35, loss = 3.3876748085021973
steps = 35, loss = 2.464195728302002
steps = 36, loss = 2.720010757446289
steps = 36, loss = 2.577784538269043
steps = 36, loss = 2.6021735668182373
steps = 36, loss = 3.4379730224609375
steps = 36, loss = 3.0422253608703613
steps = 36, loss = 3.243778705596924
steps = 36, loss = 2.740701913833618
steps = 36, loss = 2.162332057952881
steps = 36, loss = 3.322685956954956
steps = 36, loss = 2.9042141437530518
steps = 36, loss = 49.9853630065918
steps = 36, loss = 3.025531053543091
steps = 36, loss = 6.685896396636963
steps = 36, loss = 2.953917980194092
steps = 36, loss = 2.7681884765625
steps = 36, loss = 3.376312732696533
steps = 36, loss = 3.1801891326904297
steps = 36, loss = 2.376993417739868
steps = 36, loss = 2.883056879043579
steps = 36, loss = 3.525413751602173
steps = 36, loss = 3.6580307483673096
steps = 36, loss = 3.480240821838379
steps = 36, loss = 2.2502009868621826
steps = 36, loss = 3.526503801345825
steps = 36, loss = 2.819497585296631
steps = 36, loss = 2.505352020263672
steps = 36, loss = 2.9676194190979004
steps = 36, loss = 2.977086067199707
steps = 36, loss = 2.1714353561401367
steps = 37, loss = 3.3930907249450684
steps = 37, loss = 3.5074210166931152
steps = 37, loss = 3.3229424953460693
steps = 37, loss = 3.1364970207214355
steps = 37, loss = 2.926473617553711
steps = 37, loss = 2.262998342514038
steps = 37, loss = 2.72114634513855
steps = 37, loss = 2.9355719089508057
steps = 37, loss = 2.8856265544891357
steps = 37, loss = 3.0301594734191895
steps = 37, loss = 2.6310973167419434
steps = 37, loss = 3.4906575679779053
steps = 37, loss = 2.838329792022705
steps = 37, loss = 2.4399044513702393
steps = 37, loss = 2.526052713394165
steps = 37, loss = 3.200737476348877
steps = 37, loss = 3.2085516452789307
steps = 37, loss = 2.6209869384765625
steps = 37, loss = 6.7613911628723145
steps = 37, loss = 3.517266273498535
steps = 37, loss = 2.961794853210449
steps = 37, loss = 3.4547829627990723
steps = 37, loss = 2.9613778591156006
steps = 37, loss = 2.173022985458374
steps = 37, loss = 2.1344852447509766
steps = 37, loss = 3.461371421813965
steps = 37, loss = 2.747445821762085
steps = 37, loss = 49.9853630065918
steps = 37, loss = 2.7623403072357178
steps = 38, loss = 3.4287664890289307
steps = 38, loss = 6.727615833282471
steps = 38, loss = 2.6349737644195557
steps = 38, loss = 2.7304372787475586
steps = 38, loss = 2.2604901790618896
steps = 38, loss = 3.4328489303588867
steps = 38, loss = 3.074415683746338
steps = 38, loss = 3.367500066757202
steps = 38, loss = 3.833310604095459
steps = 38, loss = 2.5027501583099365
steps = 38, loss = 3.281595230102539
steps = 38, loss = 2.714016914367676
steps = 38, loss = 2.921203136444092
steps = 38, loss = 3.024793863296509
steps = 38, loss = 3.437394380569458
steps = 38, loss = 2.866792678833008
steps = 38, loss = 3.8810296058654785
steps = 38, loss = 2.975790500640869
steps = 38, loss = 2.1801273822784424
steps = 38, loss = 2.696150779724121
steps = 38, loss = 3.3751413822174072
steps = 38, loss = 49.9853630065918
steps = 38, loss = 3.5412964820861816
steps = 38, loss = 2.849616765975952
steps = 38, loss = 2.947911500930786
steps = 38, loss = 1.3221474885940552
steps = 38, loss = 2.5452075004577637
steps = 38, loss = 2.1706762313842773
steps = 38, loss = 2.8211522102355957
steps = 39, loss = 3.832429885864258
steps = 39, loss = 2.0099990367889404
steps = 39, loss = 6.717414855957031
steps = 39, loss = 3.3415346145629883
steps = 39, loss = 3.2959017753601074
steps = 39, loss = 49.9853630065918
steps = 39, loss = 2.7569801807403564
steps = 39, loss = 2.6849989891052246
steps = 39, loss = 2.6414783000946045
steps = 39, loss = 3.592095136642456
steps = 39, loss = 2.851635694503784
steps = 39, loss = 8.980977058410645
steps = 39, loss = 3.1285274028778076
steps = 39, loss = 3.3048057556152344
steps = 39, loss = 3.4864540100097656
steps = 39, loss = 1.9334720373153687
steps = 39, loss = 2.573399066925049
steps = 39, loss = 2.176396608352661
steps = 39, loss = 2.625192642211914
steps = 39, loss = 2.1341733932495117
steps = 39, loss = 2.5483107566833496
steps = 39, loss = 2.8446662425994873
steps = 39, loss = 2.723642349243164
steps = 39, loss = 3.0134410858154297
steps = 39, loss = 3.497213363647461
steps = 39, loss = 2.5811917781829834
steps = 39, loss = 2.401782989501953
steps = 39, loss = 2.899092435836792
steps = 39, loss = 3.412428617477417
steps = 40, loss = 3.614600658416748
steps = 40, loss = 3.379469633102417
steps = 40, loss = 2.656702995300293
steps = 40, loss = 2.1826813220977783
steps = 40, loss = 2.75244402885437
steps = 40, loss = 2.0215399265289307
steps = 40, loss = 3.511763095855713
steps = 40, loss = 2.5803115367889404
steps = 40, loss = 3.2004330158233643
steps = 40, loss = 2.687457799911499
steps = 40, loss = 2.5931506156921387
steps = 40, loss = 2.7343204021453857
steps = 40, loss = 3.7050857543945312
steps = 40, loss = 2.5648961067199707
steps = 40, loss = 2.825516700744629
steps = 40, loss = 2.8465144634246826
steps = 40, loss = 2.8673689365386963
steps = 40, loss = 3.6590816974639893
steps = 40, loss = 2.9476444721221924
steps = 40, loss = 1.9444012641906738
steps = 40, loss = 3.681673049926758
steps = 40, loss = 2.802649736404419
steps = 40, loss = 2.4242684841156006
steps = 40, loss = 3.01509952545166
steps = 40, loss = 6.404320240020752
steps = 40, loss = 49.9853630065918
steps = 40, loss = 3.3364481925964355
steps = 40, loss = 2.135406255722046
steps = 40, loss = 3.170041084289551
steps = 41, loss = 3.5160398483276367
steps = 41, loss = 2.892086982727051
steps = 41, loss = 3.494718074798584
steps = 41, loss = 2.8646719455718994
steps = 41, loss = 3.166595220565796
steps = 41, loss = 2.203357696533203
steps = 41, loss = 2.739985942840576
steps = 41, loss = 3.0286478996276855
steps = 41, loss = 2.907764196395874
steps = 41, loss = 2.6970582008361816
steps = 41, loss = 2.90234112739563
steps = 41, loss = 2.090878963470459
steps = 41, loss = 2.7302610874176025
steps = 41, loss = 49.9853630065918
steps = 41, loss = 7.013145446777344
steps = 41, loss = 2.768963098526001
steps = 41, loss = 3.813304901123047
steps = 41, loss = 3.255995035171509
steps = 41, loss = 3.455822467803955
steps = 41, loss = 2.223411798477173
steps = 41, loss = 3.327044725418091
steps = 41, loss = 3.2634940147399902
steps = 41, loss = 2.681889057159424
steps = 41, loss = 2.2877211570739746
steps = 41, loss = 2.6055963039398193
steps = 41, loss = 2.5983901023864746
steps = 41, loss = 3.793006658554077
steps = 41, loss = 2.89577054977417
steps = 41, loss = 3.3100154399871826
steps = 42, loss = 3.038332223892212
steps = 42, loss = 3.1999497413635254
steps = 42, loss = 3.2679214477539062
steps = 42, loss = 2.790497064590454
steps = 42, loss = 3.4789156913757324
steps = 42, loss = 2.624537229537964
steps = 42, loss = 2.9803524017333984
steps = 42, loss = 3.261383533477783
steps = 42, loss = 3.530323028564453
steps = 42, loss = 3.535562038421631
steps = 42, loss = 2.9175260066986084
steps = 42, loss = 49.9853630065918
steps = 42, loss = 2.2494394779205322
steps = 42, loss = 7.1302080154418945
steps = 42, loss = 3.0038325786590576
steps = 42, loss = 2.7192864418029785
steps = 42, loss = 2.095468282699585
steps = 42, loss = 2.636063575744629
steps = 42, loss = 3.920764446258545
steps = 42, loss = 3.5088541507720947
steps = 42, loss = 2.2176854610443115
steps = 42, loss = 2.8813271522521973
steps = 42, loss = 3.157679557800293
steps = 42, loss = 2.6198127269744873
steps = 42, loss = 2.7545135021209717
steps = 42, loss = 2.115586042404175
steps = 42, loss = 3.4074947834014893
steps = 42, loss = 2.7029309272766113
steps = 42, loss = 2.816561698913574
steps = 43, loss = 2.6974880695343018
steps = 43, loss = 3.540621519088745
steps = 43, loss = 2.805415630340576
steps = 43, loss = 2.7832870483398438
steps = 43, loss = 2.8651177883148193
steps = 43, loss = 6.9725775718688965
steps = 43, loss = 3.5390572547912598
steps = 43, loss = 2.042722225189209
steps = 43, loss = 2.927743434906006
steps = 43, loss = 3.3570799827575684
steps = 43, loss = 4.017431735992432
steps = 43, loss = 2.4786837100982666
steps = 43, loss = 3.0321619510650635
steps = 43, loss = 2.9715726375579834
steps = 43, loss = 3.5124990940093994
steps = 43, loss = 2.6209325790405273
steps = 43, loss = 2.226588487625122
steps = 43, loss = 2.928701639175415
steps = 43, loss = 2.7158000469207764
steps = 43, loss = 2.096007823944092
steps = 43, loss = 49.9853630065918
steps = 43, loss = 2.255121946334839
steps = 43, loss = 2.7465035915374756
steps = 43, loss = 3.569248914718628
steps = 43, loss = 2.701352834701538
steps = 43, loss = 3.255394697189331
steps = 43, loss = 3.0473079681396484
steps = 43, loss = 2.6389830112457275
steps = 43, loss = 3.6579384803771973
steps = 44, loss = 3.5244898796081543
steps = 44, loss = 2.7187414169311523
steps = 44, loss = 6.752771377563477
steps = 44, loss = 3.4875035285949707
steps = 44, loss = 3.0404272079467773
steps = 44, loss = 3.102154493331909
steps = 44, loss = 3.3531384468078613
steps = 44, loss = 3.4718282222747803
steps = 44, loss = 3.4300198554992676
steps = 44, loss = 2.07726788520813
steps = 44, loss = 4.122926235198975
steps = 44, loss = 3.561397075653076
steps = 44, loss = 39.718048095703125
steps = 44, loss = 2.9043354988098145
steps = 44, loss = 2.88165283203125
steps = 44, loss = 2.7701900005340576
steps = 44, loss = 49.9853630065918
steps = 44, loss = 2.2513391971588135
steps = 44, loss = 2.629528760910034
steps = 44, loss = 3.1694767475128174
steps = 44, loss = 2.2388274669647217
steps = 44, loss = 2.7073309421539307
steps = 44, loss = 2.91314959526062
steps = 44, loss = 2.658212661743164
steps = 44, loss = 2.747938871383667
steps = 44, loss = 2.7351198196411133
steps = 44, loss = 2.8201205730438232
steps = 44, loss = 2.9481546878814697
steps = 44, loss = 3.433202028274536
steps = 45, loss = 2.948967218399048
steps = 45, loss = 3.5529627799987793
steps = 45, loss = 3.359260082244873
steps = 45, loss = 3.189173936843872
steps = 45, loss = 3.303403615951538
steps = 45, loss = 3.195482015609741
steps = 45, loss = 2.2340221405029297
steps = 45, loss = 2.6602368354797363
steps = 45, loss = 4.168706893920898
steps = 45, loss = 2.7352263927459717
steps = 45, loss = 2.722900629043579
steps = 45, loss = 2.137434959411621
steps = 45, loss = 2.6859729290008545
steps = 45, loss = 2.403122663497925
steps = 45, loss = 2.944589614868164
steps = 45, loss = 2.9368278980255127
steps = 45, loss = 3.021574020385742
steps = 45, loss = 2.6011860370635986
steps = 45, loss = 2.9525153636932373
steps = 45, loss = 2.8203353881835938
steps = 45, loss = 2.0583927631378174
steps = 45, loss = 3.374434232711792
steps = 45, loss = 3.0887832641601562
steps = 45, loss = 1.921647071838379
steps = 45, loss = 3.402151346206665
steps = 45, loss = 2.84421706199646
steps = 45, loss = 2.737760305404663
steps = 45, loss = 49.9853630065918
steps = 45, loss = 7.142106533050537
steps = 46, loss = 2.255622148513794
steps = 46, loss = 49.9853630065918
steps = 46, loss = 2.7542154788970947
steps = 46, loss = 2.6924405097961426
steps = 46, loss = 2.7193408012390137
steps = 46, loss = 2.8822221755981445
steps = 46, loss = 4.051302433013916
steps = 46, loss = 4.321166038513184
steps = 46, loss = 2.7481307983398438
steps = 46, loss = 2.611318349838257
steps = 46, loss = 2.9005813598632812
steps = 46, loss = 3.6073687076568604
steps = 46, loss = 3.2846484184265137
steps = 46, loss = 3.513550281524658
steps = 46, loss = 2.9797544479370117
steps = 46, loss = 2.352050542831421
steps = 46, loss = 2.2568235397338867
steps = 46, loss = 3.213366746902466
steps = 46, loss = 3.5005223751068115
steps = 46, loss = 2.7643115520477295
steps = 46, loss = 2.9605183601379395
steps = 46, loss = 2.195266008377075
steps = 46, loss = 3.3418986797332764
steps = 46, loss = 3.041316032409668
steps = 46, loss = 3.339186191558838
steps = 46, loss = 3.271003484725952
steps = 46, loss = 7.189951419830322
steps = 46, loss = 2.9771997928619385
steps = 46, loss = 2.7375035285949707
steps = 47, loss = 3.1605124473571777
steps = 47, loss = 3.034832239151001
steps = 47, loss = 3.5022125244140625
steps = 47, loss = 3.429755449295044
steps = 47, loss = 2.069384813308716
steps = 47, loss = 2.720989942550659
steps = 47, loss = 2.2640233039855957
steps = 47, loss = 2.6982288360595703
steps = 47, loss = 6.974052429199219
steps = 47, loss = 2.26546049118042
steps = 47, loss = 3.2972521781921387
steps = 47, loss = 3.8615100383758545
steps = 47, loss = 3.5323667526245117
steps = 47, loss = 2.739952325820923
steps = 47, loss = 2.586808681488037
steps = 47, loss = 2.7116758823394775
steps = 47, loss = 2.7954394817352295
steps = 47, loss = 2.864091396331787
steps = 47, loss = 3.245487689971924
steps = 47, loss = 3.056903839111328
steps = 47, loss = 2.8514225482940674
steps = 47, loss = 49.9853630065918
steps = 47, loss = 4.419848442077637
steps = 47, loss = 2.776310443878174
steps = 47, loss = 2.985783815383911
steps = 47, loss = 3.6027944087982178
steps = 47, loss = 2.500187635421753
steps = 47, loss = 2.240417003631592
steps = 47, loss = 3.8959579467773438
steps = 48, loss = 3.3123810291290283
steps = 48, loss = 2.8604044914245605
steps = 48, loss = 2.108332395553589
steps = 48, loss = 3.1403462886810303
steps = 48, loss = 3.0219171047210693
steps = 48, loss = 2.9864799976348877
steps = 48, loss = 3.6389074325561523
steps = 48, loss = 2.7508580684661865
steps = 48, loss = 2.781714677810669
steps = 48, loss = 2.42501163482666
steps = 48, loss = 6.851104736328125
steps = 48, loss = 3.2013745307922363
steps = 48, loss = 3.342012405395508
steps = 48, loss = 2.9467434883117676
steps = 48, loss = 2.754905939102173
steps = 48, loss = 2.6403491497039795
steps = 48, loss = 2.129603624343872
steps = 48, loss = 3.556281089782715
steps = 48, loss = 2.781604290008545
steps = 48, loss = 2.7137296199798584
steps = 48, loss = 2.8428423404693604
steps = 48, loss = 49.9853630065918
steps = 48, loss = 2.1289210319519043
steps = 48, loss = 2.6851847171783447
steps = 48, loss = 3.391972064971924
steps = 48, loss = 2.26226806640625
steps = 48, loss = 3.197726249694824
steps = 48, loss = 3.415334463119507
steps = 48, loss = 4.4647536277771
steps = 49, loss = 2.7212114334106445
steps = 49, loss = 3.4805994033813477
steps = 49, loss = 2.222194194793701
steps = 49, loss = 3.4881858825683594
steps = 49, loss = 2.8808979988098145
steps = 49, loss = 2.25521183013916
steps = 49, loss = 3.0409719944000244
steps = 49, loss = 3.018460988998413
steps = 49, loss = 3.7083842754364014
steps = 49, loss = 2.8844358921051025
steps = 49, loss = 3.3688859939575195
steps = 49, loss = 2.2840123176574707
steps = 49, loss = 2.7506279945373535
steps = 49, loss = 2.576643466949463
steps = 49, loss = 4.615753173828125
steps = 49, loss = 2.735459089279175
steps = 49, loss = 2.7189977169036865
steps = 49, loss = 3.0980777740478516
steps = 49, loss = 49.9853630065918
steps = 49, loss = 2.235830068588257
steps = 49, loss = 7.168598175048828
steps = 49, loss = 12.663097381591797
steps = 49, loss = 2.8083953857421875
steps = 49, loss = 3.4958183765411377
steps = 49, loss = 3.160597324371338
steps = 49, loss = 2.7806262969970703
steps = 49, loss = 3.4254956245422363
steps = 49, loss = 2.900224208831787
steps = 49, loss = 2.7458651065826416
steps = 50, loss = 3.1110379695892334
steps = 50, loss = 2.8444597721099854
steps = 50, loss = 3.541482925415039
steps = 50, loss = 6.572935104370117
steps = 50, loss = 2.9175570011138916
steps = 50, loss = 3.0230753421783447
steps = 50, loss = 3.0149354934692383
steps = 50, loss = 2.5184085369110107
steps = 50, loss = 2.723212957382202
steps = 50, loss = 2.280863046646118
steps = 50, loss = 2.469850540161133
steps = 50, loss = 2.0925440788269043
steps = 50, loss = 2.7247328758239746
steps = 50, loss = 2.7602221965789795
steps = 50, loss = 2.2667183876037598
steps = 50, loss = 3.551792621612549
steps = 50, loss = 3.471325397491455
steps = 50, loss = 2.6878933906555176
steps = 50, loss = 49.9853630065918
steps = 50, loss = 2.7482941150665283
steps = 50, loss = 4.65548849105835
steps = 50, loss = 3.3387324810028076
steps = 50, loss = 3.264765739440918
steps = 50, loss = 2.138340711593628
steps = 50, loss = 3.6381876468658447
steps = 50, loss = 3.214202642440796
steps = 50, loss = 3.4322707653045654
steps = 50, loss = 2.9637820720672607
steps = 50, loss = 2.811316728591919
steps = 51, loss = 3.3981082439422607
steps = 51, loss = 2.779721260070801
steps = 51, loss = 2.7624425888061523
steps = 51, loss = 3.039073944091797
steps = 51, loss = 3.4475481510162354
steps = 51, loss = 2.833101987838745
steps = 51, loss = 2.816741943359375
steps = 51, loss = 2.2020208835601807
steps = 51, loss = 4.798367977142334
steps = 51, loss = 2.6082804203033447
steps = 51, loss = 2.3008534908294678
steps = 51, loss = 3.0332322120666504
steps = 51, loss = 2.8622043132781982
steps = 51, loss = 3.3707048892974854
steps = 51, loss = 3.489279270172119
steps = 51, loss = 6.479232311248779
steps = 51, loss = 3.455322742462158
steps = 51, loss = 2.941873073577881
steps = 51, loss = 3.1344499588012695
steps = 51, loss = 3.4763026237487793
steps = 51, loss = 3.1632447242736816
steps = 51, loss = 2.075695276260376
steps = 51, loss = 3.6528239250183105
steps = 51, loss = 49.9853630065918
steps = 51, loss = 3.207803249359131
steps = 51, loss = 2.4629645347595215
steps = 51, loss = 2.292792320251465
steps = 51, loss = 2.7455475330352783
steps = 51, loss = 2.698934555053711
steps = 52, loss = 3.60737681388855
steps = 52, loss = 3.3889646530151367
steps = 52, loss = 6.843835353851318
steps = 52, loss = 2.0976040363311768
steps = 52, loss = 2.5040998458862305
steps = 52, loss = 49.9853630065918
steps = 52, loss = 3.1763200759887695
steps = 52, loss = 2.8554916381835938
steps = 52, loss = 4.834690570831299
steps = 52, loss = 2.842808246612549
steps = 52, loss = 2.6038904190063477
steps = 52, loss = 2.599452495574951
steps = 52, loss = 2.687140941619873
steps = 52, loss = 3.021000862121582
steps = 52, loss = 2.740063190460205
steps = 52, loss = 2.129260540008545
steps = 52, loss = 3.040478229522705
steps = 52, loss = 2.9120473861694336
steps = 52, loss = 2.8830411434173584
steps = 52, loss = 2.83984375
steps = 52, loss = 3.017565965652466
steps = 52, loss = 2.782404899597168
steps = 52, loss = 2.2988739013671875
steps = 52, loss = 3.31955885887146
steps = 52, loss = 3.923872232437134
steps = 52, loss = 2.0120368003845215
steps = 52, loss = 3.376640558242798
steps = 52, loss = 3.191624402999878
steps = 52, loss = 3.316180467605591
steps = 53, loss = 3.1707863807678223
steps = 53, loss = 3.303269386291504
steps = 53, loss = 2.742964506149292
steps = 53, loss = 2.255444288253784
steps = 53, loss = 6.941827774047852
steps = 53, loss = 2.857051372528076
steps = 53, loss = 2.4507763385772705
steps = 53, loss = 3.071122407913208
steps = 53, loss = 3.600832223892212
steps = 53, loss = 2.703077793121338
steps = 53, loss = 2.865974187850952
steps = 53, loss = 4.976302623748779
steps = 53, loss = 3.2759478092193604
steps = 53, loss = 3.5087897777557373
steps = 53, loss = 3.5602214336395264
steps = 53, loss = 49.9853630065918
steps = 53, loss = 2.9330382347106934
steps = 53, loss = 3.2857768535614014
steps = 53, loss = 2.814908027648926
steps = 53, loss = 2.7617437839508057
steps = 53, loss = 3.3844103813171387
steps = 53, loss = 3.22676157951355
steps = 53, loss = 2.220163106918335
steps = 53, loss = 2.6503326892852783
steps = 53, loss = 2.3208229541778564
steps = 53, loss = 2.8796634674072266
steps = 53, loss = 3.0394396781921387
steps = 53, loss = 3.519845724105835
steps = 53, loss = 2.7195756435394287
steps = 54, loss = 5.062932968139648
steps = 54, loss = 6.9697489738464355
steps = 54, loss = 3.4628069400787354
steps = 54, loss = 3.444951057434082
steps = 54, loss = 49.9853630065918
steps = 54, loss = 2.876941680908203
steps = 54, loss = 2.8621490001678467
steps = 54, loss = 2.924617052078247
steps = 54, loss = 3.353167772293091
steps = 54, loss = 2.699981212615967
steps = 54, loss = 3.0778627395629883
steps = 54, loss = 2.273499011993408
steps = 54, loss = 3.0325448513031006
steps = 54, loss = 2.8817496299743652
steps = 54, loss = 3.253387689590454
steps = 54, loss = 3.370739698410034
steps = 54, loss = 3.428431272506714
steps = 54, loss = 3.4957141876220703
steps = 54, loss = 2.232088327407837
steps = 54, loss = 3.0786752700805664
steps = 54, loss = 2.5077707767486572
steps = 54, loss = 49.32400894165039
steps = 54, loss = 3.2607421875
steps = 54, loss = 2.8127388954162598
steps = 54, loss = 2.8332595825195312
steps = 54, loss = 2.711291551589966
steps = 54, loss = 2.744542121887207
steps = 54, loss = 3.650782823562622
steps = 54, loss = 2.3312087059020996
steps = 55, loss = 1.9790282249450684
steps = 55, loss = 2.7171640396118164
steps = 55, loss = 2.5186612606048584
steps = 55, loss = 3.0195491313934326
steps = 55, loss = 3.0206196308135986
steps = 55, loss = 49.96149444580078
steps = 55, loss = 2.8418047428131104
steps = 55, loss = 2.1292309761047363
steps = 55, loss = 2.8345439434051514
steps = 55, loss = 3.078775405883789
steps = 55, loss = 2.6247873306274414
steps = 55, loss = 6.896695613861084
steps = 55, loss = 3.1438493728637695
steps = 55, loss = 3.788673162460327
steps = 55, loss = 2.452603816986084
steps = 55, loss = 3.099984884262085
steps = 55, loss = 49.9853630065918
steps = 55, loss = 2.6872642040252686
steps = 55, loss = 3.3213796615600586
steps = 55, loss = 3.235826253890991
steps = 55, loss = 2.8823049068450928
steps = 55, loss = 2.989844799041748
steps = 55, loss = 2.675723075866699
steps = 55, loss = 3.5807807445526123
steps = 55, loss = 3.0086965560913086
steps = 55, loss = 2.327791929244995
steps = 55, loss = 3.1954007148742676
steps = 55, loss = 5.087667465209961
steps = 55, loss = 3.584169864654541
steps = 56, loss = 3.494974374771118
steps = 56, loss = 2.1223950386047363
steps = 56, loss = 2.922961711883545
steps = 56, loss = 2.259368658065796
steps = 56, loss = 3.3200292587280273
steps = 56, loss = 3.0534846782684326
steps = 56, loss = 2.76335072517395
steps = 56, loss = 2.6348578929901123
steps = 56, loss = 3.046236991882324
steps = 56, loss = 2.9079315662384033
steps = 56, loss = 3.108595609664917
steps = 56, loss = 6.791036128997803
steps = 56, loss = 3.4925003051757812
steps = 56, loss = 2.719773292541504
steps = 56, loss = 3.533276319503784
steps = 56, loss = 2.970320463180542
steps = 56, loss = 3.0373311042785645
steps = 56, loss = 5.221935749053955
steps = 56, loss = 3.035515546798706
steps = 56, loss = 2.6044812202453613
steps = 56, loss = 2.8791158199310303
steps = 56, loss = 2.8668980598449707
steps = 56, loss = 3.5433058738708496
steps = 56, loss = 3.16365909576416
steps = 56, loss = 3.212688684463501
steps = 56, loss = 2.3860628604888916
steps = 56, loss = 49.9853630065918
steps = 56, loss = 2.349048137664795
steps = 56, loss = 3.3152637481689453
steps = 57, loss = 3.5646703243255615
steps = 57, loss = 2.113227605819702
steps = 57, loss = 3.335846424102783
steps = 57, loss = 2.797593116760254
steps = 57, loss = 3.0303101539611816
steps = 57, loss = 2.6873583793640137
steps = 57, loss = 2.842890501022339
steps = 57, loss = 6.054017543792725
steps = 57, loss = 5.305266857147217
steps = 57, loss = 3.5490431785583496
steps = 57, loss = 2.8615505695343018
steps = 57, loss = 2.274052381515503
steps = 57, loss = 2.700892210006714
steps = 57, loss = 3.663017988204956
steps = 57, loss = 3.115412712097168
steps = 57, loss = 3.015986442565918
steps = 57, loss = 3.421828269958496
steps = 57, loss = 2.516219139099121
steps = 57, loss = 2.8847498893737793
steps = 57, loss = 2.91831636428833
steps = 57, loss = 2.7039272785186768
steps = 57, loss = 2.753258466720581
steps = 57, loss = 3.2499258518218994
steps = 57, loss = 3.570559501647949
steps = 57, loss = 3.0494327545166016
steps = 57, loss = 49.9853630065918
steps = 57, loss = 3.532031536102295
steps = 57, loss = 3.6918411254882812
steps = 57, loss = 2.35917329788208
steps = 58, loss = 3.2881813049316406
steps = 58, loss = 3.127070426940918
steps = 58, loss = 2.165172815322876
steps = 58, loss = 2.8020968437194824
steps = 58, loss = 2.9322121143341064
steps = 58, loss = 2.368875026702881
steps = 58, loss = 3.4656333923339844
steps = 58, loss = 3.4086215496063232
steps = 58, loss = 3.629897117614746
steps = 58, loss = 3.053711414337158
steps = 58, loss = 2.7591776847839355
steps = 58, loss = 2.7145674228668213
steps = 58, loss = 49.9853630065918
steps = 58, loss = 3.246837615966797
steps = 58, loss = 6.31166934967041
steps = 58, loss = 3.5571842193603516
steps = 58, loss = 5.382134914398193
steps = 58, loss = 2.9023656845092773
steps = 58, loss = 3.403850793838501
steps = 58, loss = 2.6511802673339844
steps = 58, loss = 2.674980640411377
steps = 58, loss = 3.028639554977417
steps = 58, loss = 2.701436996459961
steps = 58, loss = 2.269270658493042
steps = 58, loss = 3.452819585800171
steps = 58, loss = 2.9683871269226074
steps = 58, loss = 3.2926480770111084
steps = 58, loss = 2.8615026473999023
steps = 58, loss = 2.574575901031494
steps = 59, loss = 3.3654699325561523
steps = 59, loss = 2.9378933906555176
steps = 59, loss = 3.197803497314453
steps = 59, loss = 2.9027817249298096
steps = 59, loss = 3.3803627490997314
steps = 59, loss = 3.0521326065063477
steps = 59, loss = 3.1271581649780273
steps = 59, loss = 2.165482997894287
steps = 59, loss = 3.388077735900879
steps = 59, loss = 2.1312127113342285
steps = 59, loss = 3.015958786010742
steps = 59, loss = 3.097674608230591
steps = 59, loss = 2.5068306922912598
steps = 59, loss = 5.396451473236084
steps = 59, loss = 2.6865313053131104
steps = 59, loss = 2.917712688446045
steps = 59, loss = 7.0228447914123535
steps = 59, loss = 3.4325079917907715
steps = 59, loss = 2.7907016277313232
steps = 59, loss = 2.364732027053833
steps = 59, loss = 49.9853630065918
steps = 59, loss = 2.750410795211792
steps = 59, loss = 2.671298027038574
steps = 59, loss = 2.8397412300109863
steps = 59, loss = 3.2556684017181396
steps = 59, loss = 2.631204605102539
steps = 59, loss = 3.4643149375915527
steps = 59, loss = 3.7320101261138916
steps = 59, loss = 2.7257368564605713
steps = 60, loss = 2.4290802478790283
steps = 60, loss = 2.886669158935547
steps = 60, loss = 3.6164135932922363
steps = 60, loss = 2.963627338409424
steps = 60, loss = 2.761392831802368
steps = 60, loss = 7.288426399230957
steps = 60, loss = 5.518108367919922
steps = 60, loss = 49.9853630065918
steps = 60, loss = 2.5472049713134766
steps = 60, loss = 3.3128323554992676
steps = 60, loss = 3.559359312057495
steps = 60, loss = 2.262600898742676
steps = 60, loss = 3.58308744430542
steps = 60, loss = 3.0333056449890137
steps = 60, loss = 3.2650387287139893
steps = 60, loss = 3.134093999862671
steps = 60, loss = 2.5985281467437744
steps = 60, loss = 2.3854241371154785
steps = 60, loss = 3.1563405990600586
steps = 60, loss = 2.9481258392333984
steps = 60, loss = 3.394969940185547
steps = 60, loss = 2.8775675296783447
steps = 60, loss = 2.681431293487549
steps = 60, loss = 2.9352221488952637
steps = 60, loss = 2.720287799835205
steps = 60, loss = 2.091329336166382
steps = 60, loss = 3.531602621078491
steps = 60, loss = 2.586667776107788
steps = 60, loss = 3.4633588790893555
steps = 61, loss = 11.03502368927002
steps = 61, loss = 3.6266090869903564
steps = 61, loss = 2.8922317028045654
steps = 61, loss = 6.871798515319824
steps = 61, loss = 2.3650312423706055
steps = 61, loss = 3.0266382694244385
steps = 61, loss = 2.9530935287475586
steps = 61, loss = 2.8608508110046387
steps = 61, loss = 3.4987568855285645
steps = 61, loss = 3.5814383029937744
steps = 61, loss = 3.3493237495422363
steps = 61, loss = 5.59712028503418
steps = 61, loss = 7.714179039001465
steps = 61, loss = 2.9739272594451904
steps = 61, loss = 3.3796334266662598
steps = 61, loss = 2.590987205505371
steps = 61, loss = 2.6955158710479736
steps = 61, loss = 2.7024993896484375
steps = 61, loss = 2.7429111003875732
steps = 61, loss = 3.1236209869384766
steps = 61, loss = 3.1627771854400635
steps = 61, loss = 2.395261526107788
steps = 61, loss = 3.50413179397583
steps = 61, loss = 3.005510091781616
steps = 61, loss = 2.6895337104797363
steps = 61, loss = 2.1595606803894043
steps = 61, loss = 2.266233444213867
steps = 61, loss = 3.3299880027770996
steps = 61, loss = 49.9853630065918
steps = 62, loss = 5.609557151794434
steps = 62, loss = 7.184773921966553
steps = 62, loss = 2.521824359893799
steps = 62, loss = 3.00602650642395
steps = 62, loss = 3.3720133304595947
steps = 62, loss = 2.5637900829315186
steps = 62, loss = 2.0711917877197266
steps = 62, loss = 2.9531078338623047
steps = 62, loss = 2.6788127422332764
steps = 62, loss = 2.900467872619629
steps = 62, loss = 3.2751357555389404
steps = 62, loss = 2.8397908210754395
steps = 62, loss = 3.014131784439087
steps = 62, loss = 3.77703595161438
steps = 62, loss = 3.2669708728790283
steps = 62, loss = 2.1716623306274414
steps = 62, loss = 2.391108989715576
steps = 62, loss = 49.9853630065918
steps = 62, loss = 4.07401180267334
steps = 62, loss = 2.687805652618408
steps = 62, loss = 2.123188018798828
steps = 62, loss = 3.1625816822052
steps = 62, loss = 2.545654535293579
steps = 62, loss = 2.9798476696014404
steps = 62, loss = 3.361293077468872
steps = 62, loss = 3.262648582458496
steps = 62, loss = 2.7275989055633545
steps = 62, loss = 4.782081127166748
steps = 62, loss = 3.4964590072631836
steps = 63, loss = 2.6275646686553955
steps = 63, loss = 2.9863998889923096
steps = 63, loss = 4.058592796325684
steps = 63, loss = 2.13970947265625
steps = 63, loss = 2.720839262008667
steps = 63, loss = 2.877516031265259
steps = 63, loss = 5.722207069396973
steps = 63, loss = 2.6330795288085938
steps = 63, loss = 3.5644822120666504
steps = 63, loss = 2.3257408142089844
steps = 63, loss = 2.700051784515381
steps = 63, loss = 2.7737839221954346
steps = 63, loss = 3.554337501525879
steps = 63, loss = 2.963491678237915
steps = 63, loss = 3.1913492679595947
steps = 63, loss = 3.293178081512451
steps = 63, loss = 3.3262741565704346
steps = 63, loss = 3.031083583831787
steps = 63, loss = 3.393853187561035
steps = 63, loss = 2.9138028621673584
steps = 63, loss = 49.9853630065918
steps = 63, loss = 3.0056564807891846
steps = 63, loss = 2.265742540359497
steps = 63, loss = 2.411569595336914
steps = 63, loss = 3.5504770278930664
steps = 63, loss = 2.8724350929260254
steps = 63, loss = 3.355229377746582
steps = 63, loss = 2.0411083698272705
steps = 63, loss = 6.94705867767334
steps = 64, loss = 49.9853630065918
steps = 64, loss = 2.140258550643921
steps = 64, loss = 2.835597276687622
steps = 64, loss = 3.5841407775878906
steps = 64, loss = 3.007892370223999
steps = 64, loss = 3.434061288833618
steps = 64, loss = 2.7590548992156982
steps = 64, loss = 2.5777957439422607
steps = 64, loss = 6.799905300140381
steps = 64, loss = 2.046672821044922
steps = 64, loss = 3.339650869369507
steps = 64, loss = 2.5946638584136963
steps = 64, loss = 2.580723762512207
steps = 64, loss = 3.0139787197113037
steps = 64, loss = 5.748314380645752
steps = 64, loss = 3.3719301223754883
steps = 64, loss = 2.724881649017334
steps = 64, loss = 3.853996515274048
steps = 64, loss = 2.4079225063323975
steps = 64, loss = 2.4150848388671875
steps = 64, loss = 2.0718507766723633
steps = 64, loss = 2.8416035175323486
steps = 64, loss = 2.69065523147583
steps = 64, loss = 2.9868223667144775
steps = 64, loss = 3.689309597015381
steps = 64, loss = 3.1867597103118896
steps = 64, loss = 3.3419556617736816
steps = 64, loss = 2.27116060256958
steps = 64, loss = 3.365722417831421
steps = 65, loss = 7.000542640686035
steps = 65, loss = 4.407445430755615
steps = 65, loss = 2.428330898284912
steps = 65, loss = 3.2151713371276855
steps = 65, loss = 2.757269859313965
steps = 65, loss = 2.27298903465271
steps = 65, loss = 2.908935070037842
steps = 65, loss = 3.303469181060791
steps = 65, loss = 3.7269983291625977
steps = 65, loss = 2.7140989303588867
steps = 65, loss = 2.8785126209259033
steps = 65, loss = 3.288501024246216
steps = 65, loss = 3.3547568321228027
steps = 65, loss = 3.0198798179626465
steps = 65, loss = 2.5324389934539795
steps = 65, loss = 3.030269145965576
steps = 65, loss = 2.9987332820892334
steps = 65, loss = 2.1286001205444336
steps = 65, loss = 49.9853630065918
steps = 65, loss = 2.236774206161499
steps = 65, loss = 2.6689255237579346
steps = 65, loss = 3.4601709842681885
steps = 65, loss = 2.7217113971710205
steps = 65, loss = 5.852781295776367
steps = 65, loss = 3.5540027618408203
steps = 65, loss = 3.033921480178833
steps = 65, loss = 2.5071582794189453
steps = 65, loss = 3.0192947387695312
steps = 65, loss = 2.747375249862671
steps = 66, loss = 6.729287147521973
steps = 66, loss = 49.9853630065918
steps = 66, loss = 3.3135900497436523
steps = 66, loss = 5.923345565795898
steps = 66, loss = 3.2206878662109375
steps = 66, loss = 2.929884433746338
steps = 66, loss = 4.226733684539795
steps = 66, loss = 3.3747870922088623
steps = 66, loss = 3.0377001762390137
steps = 66, loss = 3.274646282196045
steps = 66, loss = 3.0439162254333496
steps = 66, loss = 3.2586171627044678
steps = 66, loss = 2.437926769256592
steps = 66, loss = 2.3214480876922607
steps = 66, loss = 3.3464455604553223
steps = 66, loss = 2.5806069374084473
steps = 66, loss = 2.703277587890625
steps = 66, loss = 1.9684624671936035
steps = 66, loss = 2.982614517211914
steps = 66, loss = 2.756131172180176
steps = 66, loss = 3.5952632427215576
steps = 66, loss = 2.283550262451172
steps = 66, loss = 2.860079050064087
steps = 66, loss = 3.2375521659851074
steps = 66, loss = 2.6648974418640137
steps = 66, loss = 2.7573721408843994
steps = 66, loss = 2.937140941619873
steps = 66, loss = 3.0233237743377686
steps = 66, loss = 3.5615792274475098
steps = 67, loss = 3.026873826980591
steps = 67, loss = 3.0289506912231445
steps = 67, loss = 3.641813278198242
steps = 67, loss = 3.329756736755371
steps = 67, loss = 2.271865129470825
steps = 67, loss = 3.6461379528045654
steps = 67, loss = 2.7044270038604736
steps = 67, loss = 2.220238208770752
steps = 67, loss = 2.989414930343628
steps = 67, loss = 2.928990364074707
steps = 67, loss = 5.974895477294922
steps = 67, loss = 3.582509756088257
steps = 67, loss = 2.720811128616333
steps = 67, loss = 2.447874069213867
steps = 67, loss = 2.87780499458313
steps = 67, loss = 2.6294100284576416
steps = 67, loss = 2.4930381774902344
steps = 67, loss = 3.8291537761688232
steps = 67, loss = 3.542675018310547
steps = 67, loss = 3.346737861633301
steps = 67, loss = 3.05692458152771
steps = 67, loss = 2.708850383758545
steps = 67, loss = 6.439640998840332
steps = 67, loss = 3.0630548000335693
steps = 67, loss = 3.266092300415039
steps = 67, loss = 49.9853630065918
steps = 67, loss = 2.7638347148895264
steps = 67, loss = 50.005916595458984
steps = 67, loss = 3.238440752029419
steps = 68, loss = 6.866631984710693
steps = 68, loss = 2.4910900592803955
steps = 68, loss = 2.788583993911743
steps = 68, loss = 2.7371301651000977
steps = 68, loss = 3.21596360206604
steps = 68, loss = 3.4642233848571777
steps = 68, loss = 2.455915689468384
steps = 68, loss = 2.7035961151123047
steps = 68, loss = 2.919163942337036
steps = 68, loss = 3.348731756210327
steps = 68, loss = 3.073049545288086
steps = 68, loss = 3.3972959518432617
steps = 68, loss = 3.947065830230713
steps = 68, loss = 6.168979167938232
steps = 68, loss = 2.7561943531036377
steps = 68, loss = 3.591789722442627
steps = 68, loss = 3.475951671600342
steps = 68, loss = 3.6724822521209717
steps = 68, loss = 3.071977138519287
steps = 68, loss = 2.861926555633545
steps = 68, loss = 49.9853630065918
steps = 68, loss = 2.8600637912750244
steps = 68, loss = 2.9923503398895264
steps = 68, loss = 3.2430615425109863
steps = 68, loss = 3.553649663925171
steps = 68, loss = 6.03695821762085
steps = 68, loss = 2.257432222366333
steps = 68, loss = 3.0216619968414307
steps = 68, loss = 2.2104382514953613
steps = 69, loss = 3.113717794418335
steps = 69, loss = 7.181641578674316
steps = 69, loss = 3.5772006511688232
steps = 69, loss = 2.4290173053741455
steps = 69, loss = 1.8627917766571045
steps = 69, loss = 3.2428371906280518
steps = 69, loss = 2.2175002098083496
steps = 69, loss = 2.1296274662017822
steps = 69, loss = 3.613287925720215
steps = 69, loss = 2.7352821826934814
steps = 69, loss = 3.6542177200317383
steps = 69, loss = 2.757966995239258
steps = 69, loss = 3.0722270011901855
steps = 69, loss = 2.6554718017578125
steps = 69, loss = 3.3413071632385254
steps = 69, loss = 2.6879348754882812
steps = 69, loss = 2.7040112018585205
steps = 69, loss = 3.2287752628326416
steps = 69, loss = 2.80037260055542
steps = 69, loss = 2.8384625911712646
steps = 69, loss = 3.0778145790100098
steps = 69, loss = 6.059062480926514
steps = 69, loss = 2.615102529525757
steps = 69, loss = 3.009136199951172
steps = 69, loss = 49.9853630065918
steps = 69, loss = 2.450584650039673
steps = 69, loss = 3.5144314765930176
steps = 69, loss = 2.8487167358398438
steps = 69, loss = 3.3642735481262207
steps = 70, loss = 2.6217257976531982
steps = 70, loss = 3.4984419345855713
steps = 70, loss = 2.721773624420166
steps = 70, loss = 3.3604278564453125
steps = 70, loss = 6.140327453613281
steps = 70, loss = 2.752781629562378
steps = 70, loss = 3.495584726333618
steps = 70, loss = 2.2758829593658447
steps = 70, loss = 3.2709059715270996
steps = 70, loss = 2.630734443664551
steps = 70, loss = 2.7633912563323975
steps = 70, loss = 3.3027138710021973
steps = 70, loss = 2.936441421508789
steps = 70, loss = 3.01304030418396
steps = 70, loss = 3.543677568435669
steps = 70, loss = 2.8769969940185547
steps = 70, loss = 3.480177402496338
steps = 70, loss = 6.83951473236084
steps = 70, loss = 3.0251317024230957
steps = 70, loss = 3.103701114654541
steps = 70, loss = 2.4701263904571533
steps = 70, loss = 3.105651617050171
steps = 70, loss = 3.256089448928833
steps = 70, loss = 3.232081413269043
steps = 70, loss = 2.1937296390533447
steps = 70, loss = 2.069199562072754
steps = 70, loss = 2.8613715171813965
steps = 70, loss = 49.9853630065918
steps = 70, loss = 3.0407159328460693
steps = 71, loss = 3.791841983795166
steps = 71, loss = 2.566765069961548
steps = 71, loss = 2.859337091445923
steps = 71, loss = 3.022244930267334
steps = 71, loss = 3.602905750274658
steps = 71, loss = 3.1136035919189453
steps = 71, loss = 3.2760322093963623
steps = 71, loss = 2.9684693813323975
steps = 71, loss = 2.568798542022705
steps = 71, loss = 3.0187063217163086
steps = 71, loss = 6.18455171585083
steps = 71, loss = 3.0708885192871094
steps = 71, loss = 2.704468250274658
steps = 71, loss = 2.060905694961548
steps = 71, loss = 3.612961530685425
steps = 71, loss = 3.2054622173309326
steps = 71, loss = 2.8427364826202393
steps = 71, loss = 3.3776025772094727
steps = 71, loss = 2.4791760444641113
steps = 71, loss = 3.3058981895446777
steps = 71, loss = 3.458320379257202
steps = 71, loss = 49.9853630065918
steps = 71, loss = 2.1399409770965576
steps = 71, loss = 6.715888977050781
steps = 71, loss = 2.652982234954834
steps = 71, loss = 3.1229617595672607
steps = 71, loss = 2.7592570781707764
steps = 71, loss = 2.7540602684020996
steps = 71, loss = 2.2696990966796875
steps = 72, loss = 2.8354597091674805
steps = 72, loss = 2.3960068225860596
steps = 72, loss = 1.967673897743225
steps = 72, loss = 3.276134967803955
steps = 72, loss = 3.398895263671875
steps = 72, loss = 2.7405011653900146
steps = 72, loss = 3.343193531036377
steps = 72, loss = 2.6828935146331787
steps = 72, loss = 3.510477304458618
steps = 72, loss = 6.87058162689209
steps = 72, loss = 2.126131057739258
steps = 72, loss = 6.10913610458374
steps = 72, loss = 3.2005794048309326
steps = 72, loss = 2.744229793548584
steps = 72, loss = 3.1223526000976562
steps = 72, loss = 3.360665798187256
steps = 72, loss = 2.4740004539489746
steps = 72, loss = 3.2018415927886963
steps = 72, loss = 2.947204113006592
steps = 72, loss = 3.119983673095703
steps = 72, loss = 2.171173572540283
steps = 72, loss = 2.6884219646453857
steps = 72, loss = 2.8383076190948486
steps = 72, loss = 2.4253029823303223
steps = 72, loss = 3.006657123565674
steps = 72, loss = 49.9853630065918
steps = 72, loss = 3.0282979011535645
steps = 72, loss = 3.322540283203125
steps = 72, loss = 2.7012875080108643
steps = 73, loss = 3.3037118911743164
steps = 73, loss = 3.6175477504730225
steps = 73, loss = 2.282777786254883
steps = 73, loss = 49.9853630065918
steps = 73, loss = 2.585256814956665
steps = 73, loss = 2.64082407951355
steps = 73, loss = 2.382657766342163
steps = 73, loss = 2.740088701248169
steps = 73, loss = 2.866384267807007
steps = 73, loss = 6.425825595855713
steps = 73, loss = 3.146085500717163
steps = 73, loss = 3.0223641395568848
steps = 73, loss = 2.8765852451324463
steps = 73, loss = 6.153599739074707
steps = 73, loss = 3.3218603134155273
steps = 73, loss = 3.156615734100342
steps = 73, loss = 3.5647265911102295
steps = 73, loss = 2.4935038089752197
steps = 73, loss = 2.161936044692993
steps = 73, loss = 3.100449800491333
steps = 73, loss = 2.9394397735595703
steps = 73, loss = 3.301964044570923
steps = 73, loss = 3.3167874813079834
steps = 73, loss = 3.1561458110809326
steps = 73, loss = 2.7223355770111084
steps = 73, loss = 3.433181047439575
steps = 73, loss = 3.4047043323516846
steps = 73, loss = 2.2768123149871826
steps = 73, loss = 2.7585504055023193
steps = 74, loss = 12.592546463012695
steps = 74, loss = 3.299142360687256
steps = 74, loss = 3.1484756469726562
steps = 74, loss = 2.7998037338256836
steps = 74, loss = 2.156956911087036
steps = 74, loss = 5.9738054275512695
steps = 74, loss = 2.648790121078491
steps = 74, loss = 3.279120445251465
steps = 74, loss = 3.1557321548461914
steps = 74, loss = 3.006197452545166
steps = 74, loss = 2.134819507598877
steps = 74, loss = 2.8401780128479004
steps = 74, loss = 49.9853630065918
steps = 74, loss = 2.709906578063965
steps = 74, loss = 3.521825075149536
steps = 74, loss = 6.643322944641113
steps = 74, loss = 3.299391031265259
steps = 74, loss = 3.2545652389526367
steps = 74, loss = 2.953561782836914
steps = 74, loss = 2.0050134658813477
steps = 74, loss = 2.6910488605499268
steps = 74, loss = 3.119875192642212
steps = 74, loss = 3.6455912590026855
steps = 74, loss = 3.3263275623321533
steps = 74, loss = 2.010442018508911
steps = 74, loss = 3.4158644676208496
steps = 74, loss = 2.4889957904815674
steps = 74, loss = 2.4244801998138428
steps = 74, loss = 2.563199281692505
steps = 75, loss = 2.5615153312683105
steps = 75, loss = 3.188453435897827
steps = 75, loss = 3.359896659851074
steps = 75, loss = 3.5763163566589355
steps = 75, loss = 2.122546672821045
steps = 75, loss = 5.992232322692871
steps = 75, loss = 2.2819199562072754
steps = 75, loss = 2.5074191093444824
steps = 75, loss = 2.8580989837646484
steps = 75, loss = 3.3182976245880127
steps = 75, loss = 3.344681739807129
steps = 75, loss = 2.107391595840454
steps = 75, loss = 3.0142040252685547
steps = 75, loss = 2.7049577236175537
steps = 75, loss = 2.266746997833252
steps = 75, loss = 2.74735951423645
steps = 75, loss = 3.6053264141082764
steps = 75, loss = 3.410165309906006
steps = 75, loss = 2.864790201187134
steps = 75, loss = 3.367563247680664
steps = 75, loss = 49.9853630065918
steps = 75, loss = 50.02783203125
steps = 75, loss = 2.5987536907196045
steps = 75, loss = 3.1800320148468018
steps = 75, loss = 2.534904956817627
steps = 75, loss = 2.7259089946746826
steps = 75, loss = 3.168346881866455
steps = 75, loss = 2.5427870750427246
steps = 75, loss = 3.292759895324707
steps = 76, loss = 3.442270517349243
steps = 76, loss = 2.762040138244629
steps = 76, loss = 2.875883102416992
steps = 76, loss = 3.560661554336548
steps = 76, loss = 3.3365869522094727
steps = 76, loss = 3.6046841144561768
steps = 76, loss = 2.6314680576324463
steps = 76, loss = 49.9853630065918
steps = 76, loss = 2.6526222229003906
steps = 76, loss = 2.5175437927246094
steps = 76, loss = 3.016415596008301
steps = 76, loss = 2.2736244201660156
steps = 76, loss = 3.1891586780548096
steps = 76, loss = 6.063289165496826
steps = 76, loss = 3.367666721343994
steps = 76, loss = 2.2409186363220215
steps = 76, loss = 2.767615556716919
steps = 76, loss = 3.4979190826416016
steps = 76, loss = 3.1136889457702637
steps = 76, loss = 2.0537376403808594
steps = 76, loss = 3.2087996006011963
steps = 76, loss = 2.724219560623169
steps = 76, loss = 2.7221884727478027
steps = 76, loss = 2.923841714859009
steps = 76, loss = 3.020165205001831
steps = 76, loss = 3.4629786014556885
steps = 76, loss = 50.0261344909668
steps = 76, loss = 2.1583073139190674
steps = 76, loss = 3.3998348712921143
steps = 77, loss = 2.525243043899536
steps = 77, loss = 2.793386459350586
steps = 77, loss = 3.1116979122161865
steps = 77, loss = 2.7289319038391113
steps = 77, loss = 3.59563946723938
steps = 77, loss = 6.15315580368042
steps = 77, loss = 3.656153678894043
steps = 77, loss = 3.3613150119781494
steps = 77, loss = 49.960880279541016
steps = 77, loss = 3.340653896331787
steps = 77, loss = 2.858757257461548
steps = 77, loss = 2.0491132736206055
steps = 77, loss = 3.224987745285034
steps = 77, loss = 3.198305368423462
steps = 77, loss = 2.6549417972564697
steps = 77, loss = 3.256938934326172
steps = 77, loss = 2.7485408782958984
steps = 77, loss = 2.5327560901641846
steps = 77, loss = 2.167649745941162
steps = 77, loss = 49.9853630065918
steps = 77, loss = 2.8514797687530518
steps = 77, loss = 2.7513904571533203
steps = 77, loss = 3.0132782459259033
steps = 77, loss = 3.3934473991394043
steps = 77, loss = 3.6255176067352295
steps = 77, loss = 2.2456095218658447
steps = 77, loss = 3.536101818084717
steps = 77, loss = 2.705369234085083
steps = 77, loss = 2.107304334640503
steps = 78, loss = 49.9853630065918
steps = 78, loss = 2.8816325664520264
steps = 78, loss = 2.4415156841278076
steps = 78, loss = 2.519320249557495
steps = 78, loss = 3.075922727584839
steps = 78, loss = 3.524508476257324
steps = 78, loss = 3.223811626434326
steps = 78, loss = 3.204434633255005
steps = 78, loss = 49.9529914855957
steps = 78, loss = 2.8375298976898193
steps = 78, loss = 2.031622886657715
steps = 78, loss = 3.0014753341674805
steps = 78, loss = 2.91770076751709
steps = 78, loss = 3.0974788665771484
steps = 78, loss = 3.3390064239501953
steps = 78, loss = 2.575686454772949
steps = 78, loss = 2.731078624725342
steps = 78, loss = 3.670262336730957
steps = 78, loss = 3.2599971294403076
steps = 78, loss = 3.572998046875
steps = 78, loss = 3.4021646976470947
steps = 78, loss = 2.075124740600586
steps = 78, loss = 2.7417216300964355
steps = 78, loss = 5.508689880371094
steps = 78, loss = 2.121593713760376
steps = 78, loss = 2.577502489089966
steps = 78, loss = 3.341294050216675
steps = 78, loss = 2.6882591247558594
steps = 78, loss = 3.2073843479156494
steps = 79, loss = 3.3518102169036865
steps = 79, loss = 2.7226061820983887
steps = 79, loss = 2.8395090103149414
steps = 79, loss = 3.1192984580993652
steps = 79, loss = 5.782959938049316
steps = 79, loss = 2.0253353118896484
steps = 79, loss = 3.0647776126861572
steps = 79, loss = 2.422452688217163
steps = 79, loss = 3.2383363246917725
steps = 79, loss = 2.6193766593933105
steps = 79, loss = 3.369293451309204
steps = 79, loss = 2.9414384365081787
steps = 79, loss = 2.7426977157592773
steps = 79, loss = 3.0003578662872314
steps = 79, loss = 3.232769250869751
steps = 79, loss = 49.9853630065918
steps = 79, loss = 2.6906473636627197
steps = 79, loss = 2.137421131134033
steps = 79, loss = 3.525418996810913
steps = 79, loss = 3.384028673171997
steps = 79, loss = 2.5239412784576416
steps = 79, loss = 3.2716832160949707
steps = 79, loss = 50.014244079589844
steps = 79, loss = 2.418353319168091
steps = 79, loss = 3.649366617202759
steps = 79, loss = 3.216989278793335
steps = 79, loss = 2.6975646018981934
steps = 79, loss = 2.0848238468170166
steps = 79, loss = 3.3524298667907715
steps = 80, loss = 3.2721056938171387
steps = 80, loss = 49.9853630065918
steps = 80, loss = 2.2850747108459473
steps = 80, loss = 2.8575379848480225
steps = 80, loss = 2.5771546363830566
steps = 80, loss = 2.0739266872406006
steps = 80, loss = 2.743878126144409
steps = 80, loss = 3.2382004261016846
steps = 80, loss = 3.520768642425537
steps = 80, loss = 2.8396966457366943
steps = 80, loss = 2.5027284622192383
steps = 80, loss = 2.6628262996673584
steps = 80, loss = 3.418407440185547
steps = 80, loss = 2.790266275405884
steps = 80, loss = 3.3106002807617188
steps = 80, loss = 2.755523204803467
steps = 80, loss = 5.905546188354492
steps = 80, loss = 2.93109393119812
steps = 80, loss = 3.008873701095581
steps = 80, loss = 2.061647653579712
steps = 80, loss = 3.425842761993408
steps = 80, loss = 2.5429773330688477
steps = 80, loss = 3.2430167198181152
steps = 80, loss = 3.455225944519043
steps = 80, loss = 3.42464280128479
steps = 80, loss = 3.3704140186309814
steps = 80, loss = 2.706303358078003
steps = 80, loss = 50.00918197631836
steps = 80, loss = 3.596827507019043
steps = 81, loss = 2.2309913635253906
steps = 81, loss = 3.3464033603668213
steps = 81, loss = 3.6128621101379395
steps = 81, loss = 3.4215924739837646
steps = 81, loss = 2.7228450775146484
steps = 81, loss = 2.7652502059936523
steps = 81, loss = 2.8745577335357666
steps = 81, loss = 2.8532965183258057
steps = 81, loss = 3.372450113296509
steps = 81, loss = 2.5528738498687744
steps = 81, loss = 3.0679430961608887
steps = 81, loss = 2.6898858547210693
steps = 81, loss = 50.01015090942383
steps = 81, loss = 2.8891258239746094
steps = 81, loss = 6.058984279632568
steps = 81, loss = 3.539008855819702
steps = 81, loss = 49.9853630065918
steps = 81, loss = 2.2713282108306885
steps = 81, loss = 2.6091597080230713
steps = 81, loss = 3.3884122371673584
steps = 81, loss = 2.6314682960510254
steps = 81, loss = 3.259113073348999
steps = 81, loss = 3.3801910877227783
steps = 81, loss = 3.0147297382354736
steps = 81, loss = 2.9262688159942627
steps = 81, loss = 49.56186294555664
steps = 81, loss = 3.292940855026245
steps = 81, loss = 3.2849371433258057
steps = 81, loss = 3.427968978881836
steps = 82, loss = 3.008329391479492
steps = 82, loss = 2.8582510948181152
steps = 82, loss = 3.3924343585968018
steps = 82, loss = 2.788172960281372
steps = 82, loss = 3.5980122089385986
steps = 82, loss = 2.2218613624572754
steps = 82, loss = 3.1894102096557617
steps = 82, loss = 2.736659049987793
steps = 82, loss = 3.286381721496582
steps = 82, loss = 3.1083061695098877
steps = 82, loss = 2.2282872200012207
steps = 82, loss = 3.545090675354004
steps = 82, loss = 5.679436683654785
steps = 82, loss = 3.267634153366089
steps = 82, loss = 3.272024393081665
steps = 82, loss = 49.9853630065918
steps = 82, loss = 2.3949029445648193
steps = 82, loss = 3.3093321323394775
steps = 82, loss = 20.255117416381836
steps = 82, loss = 2.706444263458252
steps = 82, loss = 49.98916244506836
steps = 82, loss = 2.9279074668884277
steps = 82, loss = 3.4826974868774414
steps = 82, loss = 2.560370922088623
steps = 82, loss = 3.619866371154785
steps = 82, loss = 3.3486440181732178
steps = 82, loss = 2.553652286529541
steps = 82, loss = 2.7523131370544434
steps = 82, loss = 3.6107685565948486
steps = 83, loss = 3.3280746936798096
steps = 83, loss = 2.76043438911438
steps = 83, loss = 2.946180582046509
steps = 83, loss = 3.288081169128418
steps = 83, loss = 3.4100940227508545
steps = 83, loss = 2.6532952785491943
steps = 83, loss = 2.705418348312378
steps = 83, loss = 3.484191417694092
steps = 83, loss = 5.876994609832764
steps = 83, loss = 3.350649118423462
steps = 83, loss = 2.046046257019043
steps = 83, loss = 3.2670695781707764
steps = 83, loss = 3.013350009918213
steps = 83, loss = 3.3625566959381104
steps = 83, loss = 3.5937957763671875
steps = 83, loss = 2.7226672172546387
steps = 83, loss = 2.281741142272949
steps = 83, loss = 3.535939931869507
steps = 83, loss = 3.598349094390869
steps = 83, loss = 2.7805190086364746
steps = 83, loss = 2.8007664680480957
steps = 83, loss = 3.003746271133423
steps = 83, loss = 2.875208616256714
steps = 83, loss = 3.476820230484009
steps = 83, loss = 2.257080316543579
steps = 83, loss = 49.97860336303711
steps = 83, loss = 49.9853630065918
steps = 83, loss = 3.3755173683166504
steps = 83, loss = 2.5683634281158447
steps = 84, loss = 2.837982177734375
steps = 84, loss = 3.319478750228882
steps = 84, loss = 2.6955246925354004
steps = 84, loss = 2.7419445514678955
steps = 84, loss = 3.2100515365600586
steps = 84, loss = 3.5288681983947754
steps = 84, loss = 1.9159818887710571
steps = 84, loss = 2.9972305297851562
steps = 84, loss = 3.5435287952423096
steps = 84, loss = 49.9853630065918
steps = 84, loss = 5.845399379730225
steps = 84, loss = 1.9934353828430176
steps = 84, loss = 2.6067237854003906
steps = 84, loss = 50.015655517578125
steps = 84, loss = 2.7765395641326904
steps = 84, loss = 3.3085827827453613
steps = 84, loss = 3.4052438735961914
steps = 84, loss = 2.136603593826294
steps = 84, loss = 2.689577341079712
steps = 84, loss = 2.4171836376190186
steps = 84, loss = 3.3249871730804443
steps = 84, loss = 2.561558961868286
steps = 84, loss = 2.894914388656616
steps = 84, loss = 3.4931094646453857
steps = 84, loss = 2.74191951751709
steps = 84, loss = 3.497161388397217
steps = 84, loss = 3.490203380584717
steps = 84, loss = 3.289275646209717
steps = 84, loss = 3.6089768409729004
steps = 85, loss = 2.704674005508423
steps = 85, loss = 2.995820999145508
steps = 85, loss = 3.564465045928955
steps = 85, loss = 2.54935884475708
steps = 85, loss = 3.4157118797302246
steps = 85, loss = 2.741765022277832
steps = 85, loss = 2.0577468872070312
steps = 85, loss = 3.1097052097320557
steps = 85, loss = 1.9162517786026
steps = 85, loss = 3.470027446746826
steps = 85, loss = 2.521289587020874
steps = 85, loss = 2.83939266204834
steps = 85, loss = 2.1129868030548096
steps = 85, loss = 2.5655875205993652
steps = 85, loss = 5.874943256378174
steps = 85, loss = 3.3643875122070312
steps = 85, loss = 3.2847161293029785
steps = 85, loss = 3.5324299335479736
steps = 85, loss = 49.9853630065918
steps = 85, loss = 2.690922737121582
steps = 85, loss = 50.0066032409668
steps = 85, loss = 3.338961601257324
steps = 85, loss = 2.699479579925537
steps = 85, loss = 3.302485227584839
steps = 85, loss = 3.190021276473999
steps = 85, loss = 2.7246901988983154
steps = 85, loss = 3.6970064640045166
steps = 85, loss = 3.615212917327881
steps = 85, loss = 3.322387933731079
steps = 86, loss = 3.4369096755981445
steps = 86, loss = 3.3665764331817627
steps = 86, loss = 2.1998820304870605
steps = 86, loss = 3.480647563934326
steps = 86, loss = 2.0538389682769775
steps = 86, loss = 3.4329769611358643
steps = 86, loss = 3.8155899047851562
steps = 86, loss = 2.857009172439575
steps = 86, loss = 3.415436029434204
steps = 86, loss = 2.694035291671753
steps = 86, loss = 3.5647828578948975
steps = 86, loss = 3.3636322021484375
steps = 86, loss = 3.373596429824829
steps = 86, loss = 3.0623888969421387
steps = 86, loss = 3.323331356048584
steps = 86, loss = 5.843348503112793
steps = 86, loss = 2.7483253479003906
steps = 86, loss = 2.5595710277557373
steps = 86, loss = 3.574176788330078
steps = 86, loss = 2.707105875015259
steps = 86, loss = 49.994510650634766
steps = 86, loss = 49.9853630065918
steps = 86, loss = 2.5838935375213623
steps = 86, loss = 3.003812074661255
steps = 86, loss = 2.6776373386383057
steps = 86, loss = 3.364361047744751
steps = 86, loss = 2.60709285736084
steps = 86, loss = 2.1428189277648926
steps = 86, loss = 2.7936220169067383
steps = 87, loss = 3.4865541458129883
steps = 87, loss = 2.2703959941864014
steps = 87, loss = 3.395108461380005
steps = 87, loss = 3.3434150218963623
steps = 87, loss = 2.9579925537109375
steps = 87, loss = 2.723351240158081
steps = 87, loss = 3.451176643371582
steps = 87, loss = 50.00809860229492
steps = 87, loss = 2.1123063564300537
steps = 87, loss = 2.6478748321533203
steps = 87, loss = 3.3070030212402344
steps = 87, loss = 2.0965511798858643
steps = 87, loss = 3.344435214996338
steps = 87, loss = 2.8906376361846924
steps = 87, loss = 2.5812668800354004
steps = 87, loss = 3.620410919189453
steps = 87, loss = 2.714649200439453
steps = 87, loss = 2.7658607959747314
steps = 87, loss = 49.9853630065918
steps = 87, loss = 3.051640272140503
steps = 87, loss = 3.688459634780884
steps = 87, loss = 3.562070369720459
steps = 87, loss = 3.279430389404297
steps = 87, loss = 3.975435733795166
steps = 87, loss = 3.009488582611084
steps = 87, loss = 2.8743715286254883
steps = 87, loss = 6.025900363922119
steps = 87, loss = 3.510462522506714
steps = 87, loss = 2.593196392059326
steps = 88, loss = 2.7071969509124756
steps = 88, loss = 49.9853630065918
steps = 88, loss = 3.454989433288574
steps = 88, loss = 2.219398260116577
steps = 88, loss = 2.026510715484619
steps = 88, loss = 2.6391422748565674
steps = 88, loss = 2.857767343521118
steps = 88, loss = 2.1907613277435303
steps = 88, loss = 49.96025466918945
steps = 88, loss = 2.754626750946045
steps = 88, loss = 2.5773518085479736
steps = 88, loss = 3.3943257331848145
steps = 88, loss = 3.3310184478759766
steps = 88, loss = 3.3531620502471924
steps = 88, loss = 3.0032613277435303
steps = 88, loss = 3.2779648303985596
steps = 88, loss = 2.589085817337036
steps = 88, loss = 3.411494255065918
steps = 88, loss = 2.775322437286377
steps = 88, loss = 2.584590196609497
steps = 88, loss = 5.6943745613098145
steps = 88, loss = 3.439943313598633
steps = 88, loss = 2.756324052810669
steps = 88, loss = 3.8921902179718018
steps = 88, loss = 3.3144888877868652
steps = 88, loss = 3.3075692653656006
steps = 88, loss = 3.551981210708618
steps = 88, loss = 2.6000828742980957
steps = 88, loss = 3.2935478687286377
steps = 89, loss = 1.9685726165771484
steps = 89, loss = 3.455623149871826
steps = 89, loss = 2.7338223457336426
steps = 89, loss = 3.458158493041992
steps = 89, loss = 2.136199474334717
steps = 89, loss = 3.385554075241089
steps = 89, loss = 3.9520678520202637
steps = 89, loss = 2.925676107406616
steps = 89, loss = 3.8324873447418213
steps = 89, loss = 2.5933234691619873
steps = 89, loss = 2.686063528060913
steps = 89, loss = 3.421506643295288
steps = 89, loss = 5.853616714477539
steps = 89, loss = 49.9853630065918
steps = 89, loss = 2.7649238109588623
steps = 89, loss = 2.5840792655944824
steps = 89, loss = 3.292811155319214
steps = 89, loss = 2.820495367050171
steps = 89, loss = 50.017860412597656
steps = 89, loss = 2.4977355003356934
steps = 89, loss = 3.642725944519043
steps = 89, loss = 1.9968940019607544
steps = 89, loss = 2.836775302886963
steps = 89, loss = 2.688570022583008
steps = 89, loss = 3.3597824573516846
steps = 89, loss = 3.4086949825286865
steps = 89, loss = 3.464470863342285
steps = 89, loss = 2.9921748638153076
steps = 89, loss = 3.401456117630005
steps = 90, loss = 3.0667054653167725
steps = 90, loss = 50.02325439453125
steps = 90, loss = 2.2588577270507812
steps = 90, loss = 2.972207546234131
steps = 90, loss = 2.6114070415496826
steps = 90, loss = 2.617313861846924
steps = 90, loss = 3.3894317150115967
steps = 90, loss = 2.76361346244812
steps = 90, loss = 2.1565258502960205
steps = 90, loss = 3.321974515914917
steps = 90, loss = 3.8628907203674316
steps = 90, loss = 3.5557053089141846
steps = 90, loss = 3.5424280166625977
steps = 90, loss = 3.34303617477417
steps = 90, loss = 3.483079433441162
steps = 90, loss = 3.322418689727783
steps = 90, loss = 2.7662460803985596
steps = 90, loss = 3.0067458152770996
steps = 90, loss = 3.3858251571655273
steps = 90, loss = 3.443441390991211
steps = 90, loss = 2.874689817428589
steps = 90, loss = 2.724181890487671
steps = 90, loss = 3.771738290786743
steps = 90, loss = 2.712653160095215
steps = 90, loss = 6.032445430755615
steps = 90, loss = 2.2876780033111572
steps = 90, loss = 49.9853630065918
steps = 90, loss = 3.326249599456787
steps = 90, loss = 2.9061713218688965
steps = 91, loss = 2.5357275009155273
steps = 91, loss = 2.0198450088500977
steps = 91, loss = 3.4098331928253174
steps = 91, loss = 3.1920528411865234
steps = 91, loss = 2.618680000305176
steps = 91, loss = 2.857461929321289
steps = 91, loss = 2.707634925842285
steps = 91, loss = 5.574397563934326
steps = 91, loss = 3.6841092109680176
steps = 91, loss = 49.9853630065918
steps = 91, loss = 3.4623281955718994
steps = 91, loss = 1.9598157405853271
steps = 91, loss = 50.01921081542969
steps = 91, loss = 3.3947689533233643
steps = 91, loss = 3.4368066787719727
steps = 91, loss = 3.116199493408203
steps = 91, loss = 2.658517599105835
steps = 91, loss = 3.4605746269226074
steps = 91, loss = 3.0008180141448975
steps = 91, loss = 3.4291930198669434
steps = 91, loss = 4.183347225189209
steps = 91, loss = 2.884976625442505
steps = 91, loss = 2.7578725814819336
steps = 91, loss = 3.3191378116607666
steps = 91, loss = 3.560290813446045
steps = 91, loss = 3.4869978427886963
steps = 91, loss = 2.9272472858428955
steps = 91, loss = 3.5869152545928955
steps = 91, loss = 2.2389538288116455
steps = 92, loss = 3.402043342590332
steps = 92, loss = 3.1395790576934814
steps = 92, loss = 2.4921371936798096
steps = 92, loss = 3.457911729812622
steps = 92, loss = 50.01643753051758
steps = 92, loss = 49.9853630065918
steps = 92, loss = 3.267174243927002
steps = 92, loss = 2.9900052547454834
steps = 92, loss = 1.9685763120651245
steps = 92, loss = 2.6120142936706543
steps = 92, loss = 3.583575963973999
steps = 92, loss = 5.789478778839111
steps = 92, loss = 3.2761621475219727
steps = 92, loss = 2.8367621898651123
steps = 92, loss = 3.4238176345825195
steps = 92, loss = 2.0313358306884766
steps = 92, loss = 3.3376452922821045
steps = 92, loss = 2.741858959197998
steps = 92, loss = 3.4259750843048096
steps = 92, loss = 3.6156527996063232
steps = 92, loss = 2.6886978149414062
steps = 92, loss = 3.4913759231567383
steps = 92, loss = 2.5428850650787354
steps = 92, loss = 2.1339714527130127
steps = 92, loss = 2.6849844455718994
steps = 92, loss = 2.6153416633605957
steps = 92, loss = 3.9879794120788574
steps = 92, loss = 3.399840831756592
steps = 92, loss = 2.7065484523773193
steps = 93, loss = 3.943918466567993
steps = 93, loss = 2.763073682785034
steps = 93, loss = 3.0043792724609375
steps = 93, loss = 2.3009073734283447
steps = 93, loss = 3.112433910369873
steps = 93, loss = 2.7268013954162598
steps = 93, loss = 3.5962884426116943
steps = 93, loss = 3.3811757564544678
steps = 93, loss = 3.3213932514190674
steps = 93, loss = 2.2892465591430664
steps = 93, loss = 3.515831708908081
steps = 93, loss = 3.4293465614318848
steps = 93, loss = 4.126142501831055
steps = 93, loss = 2.5970537662506104
steps = 93, loss = 3.372379779815674
steps = 93, loss = 2.7246346473693848
steps = 93, loss = 3.493028402328491
steps = 93, loss = 2.1523633003234863
steps = 93, loss = 5.927679538726807
steps = 93, loss = 3.6553361415863037
steps = 93, loss = 49.9853630065918
steps = 93, loss = 3.3782498836517334
steps = 93, loss = 3.5428173542022705
steps = 93, loss = 2.9128096103668213
steps = 93, loss = 2.874654531478882
steps = 93, loss = 2.6718392372131348
steps = 93, loss = 2.629852294921875
steps = 93, loss = 50.015907287597656
steps = 93, loss = 3.0082356929779053
steps = 94, loss = 2.5879619121551514
steps = 94, loss = 3.3659017086029053
steps = 94, loss = 3.089925765991211
steps = 94, loss = 3.522810935974121
steps = 94, loss = 3.2959742546081543
steps = 94, loss = 2.449885845184326
steps = 94, loss = 2.7241387367248535
steps = 94, loss = 3.4895012378692627
steps = 94, loss = 3.0111565589904785
steps = 94, loss = 49.92577362060547
steps = 94, loss = 3.113133668899536
steps = 94, loss = 2.6886801719665527
steps = 94, loss = 2.2116034030914307
steps = 94, loss = 4.432950496673584
steps = 94, loss = 49.9853630065918
steps = 94, loss = 5.656606674194336
steps = 94, loss = 3.1481266021728516
steps = 94, loss = 2.623779058456421
steps = 94, loss = 3.514740467071533
steps = 94, loss = 2.8386757373809814
steps = 94, loss = 2.989616632461548
steps = 94, loss = 3.1837728023529053
steps = 94, loss = 2.6915016174316406
steps = 94, loss = 3.47822642326355
steps = 94, loss = 3.632631301879883
steps = 94, loss = 2.138291120529175
steps = 94, loss = 3.429961919784546
steps = 94, loss = 50.01498031616211
steps = 94, loss = 3.4310877323150635
steps = 95, loss = 2.7079250812530518
steps = 95, loss = 3.529679775238037
steps = 95, loss = 30.892349243164062
steps = 95, loss = 3.524014711380005
steps = 95, loss = 3.5739316940307617
steps = 95, loss = 3.017090320587158
steps = 95, loss = 2.7604968547821045
steps = 95, loss = 2.960175037384033
steps = 95, loss = 3.465226650238037
steps = 95, loss = 3.54380464553833
steps = 95, loss = 2.243783712387085
steps = 95, loss = 49.9853630065918
steps = 95, loss = 5.705196380615234
steps = 95, loss = 2.4789953231811523
steps = 95, loss = 3.3641817569732666
steps = 95, loss = 3.6274611949920654
steps = 95, loss = 2.856489419937134
steps = 95, loss = 2.250335931777954
steps = 95, loss = 2.8857951164245605
steps = 95, loss = 3.4504499435424805
steps = 95, loss = 3.3191049098968506
steps = 95, loss = 49.96024703979492
steps = 95, loss = 3.3870866298675537
steps = 95, loss = 2.6405868530273438
steps = 95, loss = 2.7495992183685303
steps = 95, loss = 3.0203371047973633
steps = 95, loss = 2.9966940879821777
steps = 95, loss = 3.643984079360962
steps = 95, loss = 50.00654602050781
steps = 96, loss = 3.471848487854004
steps = 96, loss = 2.9403493404388428
steps = 96, loss = 2.5261292457580566
steps = 96, loss = 2.7243900299072266
steps = 96, loss = 3.54984450340271
steps = 96, loss = 49.9853630065918
steps = 96, loss = 2.8742003440856934
steps = 96, loss = 3.5067715644836426
steps = 96, loss = 5.8282999992370605
steps = 96, loss = 2.9650092124938965
steps = 96, loss = 3.002239227294922
steps = 96, loss = 1.9494596719741821
steps = 96, loss = 3.351149082183838
steps = 96, loss = 3.116379976272583
steps = 96, loss = 3.47222638130188
steps = 96, loss = 3.358738422393799
steps = 96, loss = 2.671931028366089
steps = 96, loss = 3.091507911682129
steps = 96, loss = 50.01520919799805
steps = 96, loss = 3.5439438819885254
steps = 96, loss = 2.6482036113739014
steps = 96, loss = 3.383180618286133
steps = 96, loss = 2.2632224559783936
steps = 96, loss = 2.649066925048828
steps = 96, loss = 3.286250352859497
steps = 96, loss = 3.80197811126709
steps = 96, loss = 2.282865285873413
steps = 96, loss = 3.4282150268554688
steps = 96, loss = 2.763343095779419
steps = 97, loss = 2.987417459487915
steps = 97, loss = 2.1375768184661865
steps = 97, loss = 2.5055928230285645
steps = 97, loss = 2.867342472076416
steps = 97, loss = 2.1409730911254883
steps = 97, loss = 3.4732964038848877
steps = 97, loss = 3.549077272415161
steps = 97, loss = 2.8380191326141357
steps = 97, loss = 2.5624923706054688
steps = 97, loss = 49.9853630065918
steps = 97, loss = 3.303054094314575
steps = 97, loss = 1.9353581666946411
steps = 97, loss = 3.47110915184021
steps = 97, loss = 3.5391054153442383
steps = 97, loss = 3.727768898010254
steps = 97, loss = 5.920867919921875
steps = 97, loss = 3.3009164333343506
steps = 97, loss = 2.707733631134033
steps = 97, loss = 2.9905080795288086
steps = 97, loss = 2.641875982284546
steps = 97, loss = 20.084815979003906
steps = 97, loss = 2.452110767364502
steps = 97, loss = 3.3622539043426514
steps = 97, loss = 2.6907808780670166
steps = 97, loss = 3.115635633468628
steps = 97, loss = 49.94755935668945
steps = 97, loss = 2.739407777786255
steps = 97, loss = 3.4960391521453857
steps = 97, loss = 2.691117525100708
steps = 98, loss = 2.76745867729187
steps = 98, loss = 3.3886899948120117
steps = 98, loss = 2.825809955596924
steps = 98, loss = 2.7254996299743652
steps = 98, loss = 2.677013635635376
steps = 98, loss = 3.37619948387146
steps = 98, loss = 2.659358024597168
steps = 98, loss = 2.848280906677246
steps = 98, loss = 2.7256264686584473
steps = 98, loss = 2.563685655593872
steps = 98, loss = 49.9853630065918
steps = 98, loss = 3.5115864276885986
steps = 98, loss = 50.00794219970703
steps = 98, loss = 3.6142072677612305
steps = 98, loss = 2.939936637878418
steps = 98, loss = 3.0837903022766113
steps = 98, loss = 2.292914628982544
steps = 98, loss = 3.0011847019195557
steps = 98, loss = 2.667301893234253
steps = 98, loss = 2.048553705215454
steps = 98, loss = 2.2077207565307617
steps = 98, loss = 3.4104716777801514
steps = 98, loss = 2.8755369186401367
steps = 98, loss = 3.3937723636627197
steps = 98, loss = 3.5731239318847656
steps = 98, loss = 6.097996711730957
steps = 98, loss = 3.499492645263672
steps = 98, loss = 3.416517734527588
steps = 98, loss = 3.5754377841949463
steps = 99, loss = 3.62709379196167
steps = 99, loss = 2.0500402450561523
steps = 99, loss = 2.3949086666107178
steps = 99, loss = 3.394680976867676
steps = 99, loss = 2.986851930618286
steps = 99, loss = 49.9853630065918
steps = 99, loss = 2.9102113246917725
steps = 99, loss = 3.592813014984131
steps = 99, loss = 3.5079879760742188
steps = 99, loss = 3.4497687816619873
steps = 99, loss = 50.031097412109375
steps = 99, loss = 2.647873640060425
steps = 99, loss = 2.939101457595825
steps = 99, loss = 3.3255109786987305
steps = 99, loss = 2.2136292457580566
steps = 99, loss = 25.08943748474121
steps = 99, loss = 2.7081079483032227
steps = 99, loss = 3.689133882522583
steps = 99, loss = 3.5729336738586426
steps = 99, loss = 3.3515374660491943
steps = 99, loss = 2.995191812515259
steps = 99, loss = 5.724013805389404
steps = 99, loss = 2.223496198654175
steps = 99, loss = 2.856884717941284
steps = 99, loss = 2.6657238006591797
steps = 99, loss = 2.510769844055176
steps = 99, loss = 2.762535572052002
steps = 99, loss = 3.5762484073638916
steps = 99, loss = 2.81198787689209
steps = 100, loss = 3.5543484687805176
steps = 100, loss = 2.7314329147338867
steps = 100, loss = 5.919785499572754
steps = 100, loss = 3.5893683433532715
steps = 100, loss = 3.2832281589508057
steps = 100, loss = 3.5148732662200928
steps = 100, loss = 2.6895339488983154
steps = 100, loss = 2.658684492111206
steps = 100, loss = 3.398547887802124
steps = 100, loss = 3.40871524810791
steps = 100, loss = 49.9853630065918
steps = 100, loss = 2.6073176860809326
steps = 100, loss = 2.9013383388519287
steps = 100, loss = 3.5841269493103027
steps = 100, loss = 2.4522435665130615
steps = 100, loss = 2.1359047889709473
steps = 100, loss = 3.648620367050171
steps = 100, loss = 2.773690938949585
steps = 100, loss = 2.6622700691223145
steps = 100, loss = 2.9847614765167236
steps = 100, loss = 2.0205142498016357
steps = 100, loss = 50.02638244628906
steps = 100, loss = 2.6522068977355957
steps = 100, loss = 2.836799383163452
steps = 100, loss = 2.6999247074127197
steps = 100, loss = 2.822505235671997
steps = 100, loss = 3.2515721321105957
steps = 100, loss = 3.1854984760284424
steps = 100, loss = 1.9796169996261597
steps = 101, loss = 2.2915542125701904
steps = 101, loss = 2.676116466522217
steps = 101, loss = 2.725393295288086
steps = 101, loss = 2.7048468589782715
steps = 101, loss = 5.997576713562012
steps = 101, loss = 2.0489819049835205
steps = 101, loss = 2.597050428390503
steps = 101, loss = 3.364626407623291
steps = 101, loss = 3.5412673950195312
steps = 101, loss = 2.9462037086486816
steps = 101, loss = 3.3690686225891113
steps = 101, loss = 3.625014543533325
steps = 101, loss = 49.9853630065918
steps = 101, loss = 3.3282766342163086
steps = 101, loss = 3.5902764797210693
steps = 101, loss = 2.8174641132354736
steps = 101, loss = 2.129945755004883
steps = 101, loss = 3.351114511489868
steps = 101, loss = 3.570561170578003
steps = 101, loss = 3.067596435546875
steps = 101, loss = 3.6533899307250977
steps = 101, loss = 2.9985764026641846
steps = 101, loss = 2.8486924171447754
steps = 101, loss = 2.8747177124023438
steps = 101, loss = 2.76608943939209
steps = 101, loss = 3.390950918197632
steps = 101, loss = 2.975688934326172
steps = 101, loss = 3.6079044342041016
steps = 101, loss = 50.02745056152344
steps = 102, loss = 3.028550863265991
steps = 102, loss = 3.5575647354125977
steps = 102, loss = 2.0797972679138184
steps = 102, loss = 2.7567927837371826
steps = 102, loss = 2.9927785396575928
steps = 102, loss = 2.231999635696411
steps = 102, loss = 3.408823251724243
steps = 102, loss = 49.9853630065918
steps = 102, loss = 2.044316291809082
steps = 102, loss = 2.634680986404419
steps = 102, loss = 3.549501657485962
steps = 102, loss = 2.856626272201538
steps = 102, loss = 3.0467896461486816
steps = 102, loss = 3.6424527168273926
steps = 102, loss = 2.572134017944336
steps = 102, loss = 2.606670379638672
steps = 102, loss = 3.3395020961761475
steps = 102, loss = 2.7086398601531982
steps = 102, loss = 2.561279535293579
steps = 102, loss = 2.8694097995758057
steps = 102, loss = 3.2982842922210693
steps = 102, loss = 3.5161070823669434
steps = 102, loss = 5.726139545440674
steps = 102, loss = 3.1384549140930176
steps = 102, loss = 49.929931640625
steps = 102, loss = 2.6824300289154053
steps = 102, loss = 3.470696449279785
steps = 102, loss = 3.3962056636810303
steps = 102, loss = 3.610466957092285
steps = 103, loss = 2.764660358428955
steps = 103, loss = 3.600691556930542
steps = 103, loss = 2.6883585453033447
steps = 103, loss = 2.8580291271209717
steps = 103, loss = 3.641430616378784
steps = 103, loss = 3.6220390796661377
steps = 103, loss = 2.765390634536743
steps = 103, loss = 3.4225802421569824
steps = 103, loss = 2.9740211963653564
steps = 103, loss = 2.2291762828826904
steps = 103, loss = 3.615406036376953
steps = 103, loss = 2.856649398803711
steps = 103, loss = 2.037522792816162
steps = 103, loss = 2.9866533279418945
steps = 103, loss = 3.660710573196411
steps = 103, loss = 3.3418400287628174
steps = 103, loss = 3.5634782314300537
steps = 103, loss = 3.3651673793792725
steps = 103, loss = 2.9912025928497314
steps = 103, loss = 5.857150554656982
steps = 103, loss = 3.001605987548828
steps = 103, loss = 2.5809810161590576
steps = 103, loss = 2.5789310932159424
steps = 103, loss = 49.9853630065918
steps = 103, loss = 2.708526134490967
steps = 103, loss = 2.7600600719451904
steps = 103, loss = 3.357347249984741
steps = 103, loss = 2.189791440963745
steps = 103, loss = 50.027278900146484
steps = 104, loss = 3.4592528343200684
steps = 104, loss = 2.630821943283081
steps = 104, loss = 2.0983378887176514
steps = 104, loss = 3.679352045059204
steps = 104, loss = 2.7052528858184814
steps = 104, loss = 49.9853630065918
steps = 104, loss = 3.3900434970855713
steps = 104, loss = 2.6952009201049805
steps = 104, loss = 3.3847012519836426
steps = 104, loss = 3.0774953365325928
steps = 104, loss = 2.7243282794952393
steps = 104, loss = 2.9961159229278564
steps = 104, loss = 3.375185966491699
steps = 104, loss = 5.954137325286865
steps = 104, loss = 3.584103584289551
steps = 104, loss = 2.939948558807373
steps = 104, loss = 2.8735737800598145
steps = 104, loss = 2.0867831707000732
steps = 104, loss = 3.6429712772369385
steps = 104, loss = 3.349740505218506
steps = 104, loss = 3.534120559692383
steps = 104, loss = 2.2872369289398193
steps = 104, loss = 2.7650229930877686
steps = 104, loss = 2.953533411026001
steps = 104, loss = 3.352776050567627
steps = 104, loss = 2.8426098823547363
steps = 104, loss = 2.926589012145996
steps = 104, loss = 3.5910301208496094
steps = 104, loss = 50.02821731567383
steps = 105, loss = 2.981658458709717
steps = 105, loss = 2.893355131149292
steps = 105, loss = 3.359405279159546
steps = 105, loss = 49.9853630065918
steps = 105, loss = 50.026832580566406
steps = 105, loss = 2.6899049282073975
steps = 105, loss = 3.4289002418518066
steps = 105, loss = 2.7515060901641846
steps = 105, loss = 2.7402749061584473
steps = 105, loss = 2.6148691177368164
steps = 105, loss = 3.5839426517486572
steps = 105, loss = 3.06671404838562
steps = 105, loss = 2.1386358737945557
steps = 105, loss = 2.4281022548675537
steps = 105, loss = 1.9328609704971313
steps = 105, loss = 3.2548916339874268
steps = 105, loss = 3.367187976837158
steps = 105, loss = 6.020326137542725
steps = 105, loss = 1.951153039932251
steps = 105, loss = 3.186645269393921
steps = 105, loss = 2.6050057411193848
steps = 105, loss = 3.7525267601013184
steps = 105, loss = 3.4352896213531494
steps = 105, loss = 2.8363072872161865
steps = 105, loss = 3.643759250640869
steps = 105, loss = 2.6548070907592773
steps = 105, loss = 2.6869864463806152
steps = 105, loss = 3.4578607082366943
steps = 105, loss = 3.6726789474487305
steps = 106, loss = 2.9949734210968018
steps = 106, loss = 3.5257813930511475
steps = 106, loss = 3.3668229579925537
steps = 106, loss = 3.6102676391601562
steps = 106, loss = 2.848269462585449
steps = 106, loss = 3.391756057739258
steps = 106, loss = 6.196220397949219
steps = 106, loss = 2.7038986682891846
steps = 106, loss = 2.8749308586120605
steps = 106, loss = 24.18004608154297
steps = 106, loss = 2.7616612911224365
steps = 106, loss = 2.1318461894989014
steps = 106, loss = 2.9125216007232666
steps = 106, loss = 3.6658129692077637
steps = 106, loss = 2.7139322757720947
steps = 106, loss = 2.726191997528076
steps = 106, loss = 50.0121955871582
steps = 106, loss = 2.718395948410034
steps = 106, loss = 2.2929587364196777
steps = 106, loss = 3.466322422027588
steps = 106, loss = 2.04532790184021
steps = 106, loss = 3.5138394832611084
steps = 106, loss = 3.381819248199463
steps = 106, loss = 3.708754539489746
steps = 106, loss = 49.9853630065918
steps = 106, loss = 3.608423948287964
steps = 106, loss = 2.963806390762329
steps = 106, loss = 2.7667388916015625
steps = 106, loss = 3.3492705821990967
steps = 107, loss = 2.9810872077941895
steps = 107, loss = 2.4321377277374268
steps = 107, loss = 3.703681230545044
steps = 107, loss = 3.484537124633789
steps = 107, loss = 2.771867513656616
steps = 107, loss = 2.0087976455688477
steps = 107, loss = 2.837864875793457
steps = 107, loss = 50.00666809082031
steps = 107, loss = 3.414412021636963
steps = 107, loss = 3.352004051208496
steps = 107, loss = 2.6968319416046143
steps = 107, loss = 3.1081759929656982
steps = 107, loss = 5.845081806182861
steps = 107, loss = 3.3299286365509033
steps = 107, loss = 2.729004383087158
steps = 107, loss = 2.607551336288452
steps = 107, loss = 2.808234214782715
steps = 107, loss = 3.6110787391662598
steps = 107, loss = 49.9853630065918
steps = 107, loss = 3.5985021591186523
steps = 107, loss = 2.9395062923431396
steps = 107, loss = 2.6918699741363525
steps = 107, loss = 3.6684584617614746
steps = 107, loss = 48.86896514892578
steps = 107, loss = 2.5040273666381836
steps = 107, loss = 2.1382687091827393
steps = 107, loss = 3.4497623443603516
steps = 107, loss = 3.7007062435150146
steps = 107, loss = 3.19592022895813
steps = 108, loss = 2.7125489711761475
steps = 108, loss = 6.0137858390808105
steps = 108, loss = 3.608093500137329
steps = 108, loss = 3.678658962249756
steps = 108, loss = 3.3612241744995117
steps = 108, loss = 49.9853630065918
steps = 108, loss = 2.2436635494232178
steps = 108, loss = 2.699641466140747
steps = 108, loss = 2.5426106452941895
steps = 108, loss = 2.7089600563049316
steps = 108, loss = 2.7512941360473633
steps = 108, loss = 2.21647572517395
steps = 108, loss = 2.6386969089508057
steps = 108, loss = 3.348626136779785
steps = 108, loss = 2.9876809120178223
steps = 108, loss = 1.7977626323699951
steps = 108, loss = 3.5653722286224365
steps = 108, loss = 3.362506866455078
steps = 108, loss = 3.3094868659973145
steps = 108, loss = 50.006690979003906
steps = 108, loss = 2.881596565246582
steps = 108, loss = 3.3774304389953613
steps = 108, loss = 2.4708549976348877
steps = 108, loss = 3.362408399581909
steps = 108, loss = 3.3431754112243652
steps = 108, loss = 3.7386879920959473
steps = 108, loss = 2.855593204498291
steps = 108, loss = 3.630002021789551
steps = 108, loss = 2.752418279647827
steps = 109, loss = 3.6444809436798096
steps = 109, loss = 2.7192530632019043
steps = 109, loss = 3.5638606548309326
steps = 109, loss = 2.072526454925537
steps = 109, loss = 3.5461134910583496
steps = 109, loss = 2.7557156085968018
steps = 109, loss = 6.092577934265137
steps = 109, loss = 2.4887890815734863
steps = 109, loss = 3.690685987472534
steps = 109, loss = 2.9966464042663574
steps = 109, loss = 2.19754695892334
steps = 109, loss = 3.3088412284851074
steps = 109, loss = 2.31809663772583
steps = 109, loss = 49.9853630065918
steps = 109, loss = 2.9866979122161865
steps = 109, loss = 2.884446144104004
steps = 109, loss = 3.7579050064086914
steps = 109, loss = 12.308351516723633
steps = 109, loss = 2.836320400238037
steps = 109, loss = 2.163412570953369
steps = 109, loss = 3.25015926361084
steps = 109, loss = 50.0053825378418
steps = 109, loss = 2.856100082397461
steps = 109, loss = 2.418386697769165
steps = 109, loss = 2.7093546390533447
steps = 109, loss = 2.7091493606567383
steps = 109, loss = 3.3999016284942627
steps = 109, loss = 3.555685043334961
steps = 109, loss = 3.688291311264038
steps = 110, loss = 2.7119433879852295
steps = 110, loss = 2.446082830429077
steps = 110, loss = 3.753195285797119
steps = 110, loss = 2.83536434173584
steps = 110, loss = 2.9207801818847656
steps = 110, loss = 3.702220916748047
steps = 110, loss = 3.253573179244995
steps = 110, loss = 2.6888718605041504
steps = 110, loss = 3.6445298194885254
steps = 110, loss = 6.078636646270752
steps = 110, loss = 2.140782594680786
steps = 110, loss = 3.3630928993225098
steps = 110, loss = 2.018018960952759
steps = 110, loss = 2.732778787612915
steps = 110, loss = 3.48016357421875
steps = 110, loss = 2.718657970428467
steps = 110, loss = 3.1247470378875732
steps = 110, loss = 49.9853630065918
steps = 110, loss = 50.02254867553711
steps = 110, loss = 3.3071372509002686
steps = 110, loss = 2.8409037590026855
steps = 110, loss = 2.0870866775512695
steps = 110, loss = 3.396695613861084
steps = 110, loss = 3.2482213973999023
steps = 110, loss = 3.382683515548706
steps = 110, loss = 2.977184295654297
steps = 110, loss = 2.572082996368408
steps = 110, loss = 3.6515719890594482
steps = 110, loss = 2.5961601734161377
steps = 111, loss = 2.5978775024414062
steps = 111, loss = 2.8732831478118896
steps = 111, loss = 2.7262356281280518
steps = 111, loss = 49.9853630065918
steps = 111, loss = 2.5882413387298584
steps = 111, loss = 3.475600004196167
steps = 111, loss = 3.6774275302886963
steps = 111, loss = 6.229401588439941
steps = 111, loss = 3.4859907627105713
steps = 111, loss = 3.7234690189361572
steps = 111, loss = 3.273642063140869
steps = 111, loss = 3.642547369003296
steps = 111, loss = 3.790140151977539
steps = 111, loss = 50.017005920410156
steps = 111, loss = 2.9906656742095947
steps = 111, loss = 2.768132448196411
steps = 111, loss = 2.1626052856445312
steps = 111, loss = 2.7338056564331055
steps = 111, loss = 2.179105758666992
steps = 111, loss = 2.936964988708496
steps = 111, loss = 2.9548468589782715
steps = 111, loss = 3.57304310798645
steps = 111, loss = 2.2915823459625244
steps = 111, loss = 2.647818088531494
steps = 111, loss = 2.483659029006958
steps = 111, loss = 3.320235252380371
steps = 111, loss = 3.361067771911621
steps = 111, loss = 2.729018449783325
steps = 111, loss = 2.64741587638855
steps = 112, loss = 3.7847578525543213
steps = 112, loss = 3.100430727005005
steps = 112, loss = 2.6560842990875244
steps = 112, loss = 2.7220113277435303
steps = 112, loss = 3.479093313217163
steps = 112, loss = 49.9853630065918
steps = 112, loss = 3.41634202003479
steps = 112, loss = 2.005373954772949
steps = 112, loss = 2.5518226623535156
steps = 112, loss = 2.9718570709228516
steps = 112, loss = 2.1389143466949463
steps = 112, loss = 2.97735595703125
steps = 112, loss = 3.6128315925598145
steps = 112, loss = 3.6779544353485107
steps = 112, loss = 2.837583065032959
steps = 112, loss = 2.7560243606567383
steps = 112, loss = 2.6922647953033447
steps = 112, loss = 2.4287734031677246
steps = 112, loss = 3.1573524475097656
steps = 112, loss = 3.3704710006713867
steps = 112, loss = 2.7410800457000732
steps = 112, loss = 3.7269484996795654
steps = 112, loss = 2.0459342002868652
steps = 112, loss = 2.3943533897399902
steps = 112, loss = 2.893613576889038
steps = 112, loss = 6.035703182220459
steps = 112, loss = 3.3588130474090576
steps = 112, loss = 3.695178985595703
steps = 112, loss = 50.02321243286133
steps = 113, loss = 2.9840140342712402
steps = 113, loss = 2.7096784114837646
steps = 113, loss = 3.5654947757720947
steps = 113, loss = 2.235846757888794
steps = 113, loss = 2.737548589706421
steps = 113, loss = 3.3521323204040527
steps = 113, loss = 2.8555057048797607
steps = 113, loss = 2.625121593475342
steps = 113, loss = 6.151645660400391
steps = 113, loss = 2.7568585872650146
steps = 113, loss = 2.7660045623779297
steps = 113, loss = 3.247755289077759
steps = 113, loss = 2.984046697616577
steps = 113, loss = 2.6292479038238525
steps = 113, loss = 1.9700003862380981
steps = 113, loss = 3.1660261154174805
steps = 113, loss = 3.5198590755462646
steps = 113, loss = 3.683015823364258
steps = 113, loss = 3.6330502033233643
steps = 113, loss = 3.6584818363189697
steps = 113, loss = 3.7355499267578125
steps = 113, loss = 3.820580244064331
steps = 113, loss = 3.6424448490142822
steps = 113, loss = 2.9404165744781494
steps = 113, loss = 2.041217088699341
steps = 113, loss = 3.697211980819702
steps = 113, loss = 50.02900314331055
steps = 113, loss = 49.9853630065918
steps = 113, loss = 2.414128065109253
steps = 114, loss = 2.28654146194458
steps = 114, loss = 3.360677480697632
steps = 114, loss = 2.9892849922180176
steps = 114, loss = 3.7187118530273438
steps = 114, loss = 3.4906651973724365
steps = 114, loss = 3.4065849781036377
steps = 114, loss = 2.8678295612335205
steps = 114, loss = 2.889420747756958
steps = 114, loss = 2.8732686042785645
steps = 114, loss = 3.4667892456054688
steps = 114, loss = 2.1650023460388184
steps = 114, loss = 3.8402559757232666
steps = 114, loss = 2.977800130844116
steps = 114, loss = 2.7454209327697754
steps = 114, loss = 2.6714067459106445
steps = 114, loss = 2.7200379371643066
steps = 114, loss = 2.998673677444458
steps = 114, loss = 6.081485748291016
steps = 114, loss = 2.7684731483459473
steps = 114, loss = 3.4429595470428467
steps = 114, loss = 3.5260567665100098
steps = 114, loss = 3.9594550132751465
steps = 114, loss = 2.725933074951172
steps = 114, loss = 3.7592318058013916
steps = 114, loss = 49.9853630065918
steps = 114, loss = 2.119551658630371
steps = 114, loss = 3.3981993198394775
steps = 114, loss = 3.4813342094421387
steps = 114, loss = 50.02900314331055
steps = 115, loss = 3.3103349208831787
steps = 115, loss = 3.8339946269989014
steps = 115, loss = 2.7211811542510986
steps = 115, loss = 3.058518409729004
steps = 115, loss = 2.0483617782592773
steps = 115, loss = 2.749342203140259
steps = 115, loss = 3.516065835952759
steps = 115, loss = 6.280248641967773
steps = 115, loss = 49.9853630065918
steps = 115, loss = 3.3126044273376465
steps = 115, loss = 2.739717721939087
steps = 115, loss = 2.3073360919952393
steps = 115, loss = 3.76279878616333
steps = 115, loss = 2.581505060195923
steps = 115, loss = 3.032197952270508
steps = 115, loss = 3.725346326828003
steps = 115, loss = 3.46124267578125
steps = 115, loss = 2.975717782974243
steps = 115, loss = 3.5993096828460693
steps = 115, loss = 2.138679027557373
steps = 115, loss = 2.6918249130249023
steps = 115, loss = 2.837183952331543
steps = 115, loss = 3.316598415374756
steps = 115, loss = 2.7173032760620117
steps = 115, loss = 49.982826232910156
steps = 115, loss = 2.4515838623046875
steps = 115, loss = 2.7374186515808105
steps = 115, loss = 3.7185540199279785
steps = 115, loss = 3.439194917678833
steps = 116, loss = 3.330758810043335
steps = 116, loss = 3.869349718093872
steps = 116, loss = 3.3518495559692383
steps = 116, loss = 2.7624435424804688
steps = 116, loss = 3.3636646270751953
steps = 116, loss = 49.9853630065918
steps = 116, loss = 6.059188365936279
steps = 116, loss = 2.8554298877716064
steps = 116, loss = 3.410645008087158
steps = 116, loss = 3.7374391555786133
steps = 116, loss = 50.016441345214844
steps = 116, loss = 2.1608951091766357
steps = 116, loss = 2.681623935699463
steps = 116, loss = 2.752523183822632
steps = 116, loss = 2.63982892036438
steps = 116, loss = 3.305562734603882
steps = 116, loss = 3.769667863845825
steps = 116, loss = 3.5678012371063232
steps = 116, loss = 3.413452386856079
steps = 116, loss = 2.356919765472412
steps = 116, loss = 2.069749355316162
steps = 116, loss = 3.367659330368042
steps = 116, loss = 2.6735188961029053
steps = 116, loss = 2.2185182571411133
steps = 116, loss = 3.3483386039733887
steps = 116, loss = 2.9395155906677246
steps = 116, loss = 2.9822683334350586
steps = 116, loss = 2.7098231315612793
steps = 116, loss = 3.7579312324523926
steps = 117, loss = 2.7138779163360596
steps = 117, loss = 3.7595736980438232
steps = 117, loss = 3.2914657592773438
steps = 117, loss = 3.497694253921509
steps = 117, loss = 2.387869358062744
steps = 117, loss = 6.185079097747803
steps = 117, loss = 2.9874377250671387
steps = 117, loss = 2.7656288146972656
steps = 117, loss = 50.01902389526367
steps = 117, loss = 2.760202407836914
steps = 117, loss = 3.793093681335449
steps = 117, loss = 2.529010534286499
steps = 117, loss = 2.653006076812744
steps = 117, loss = 2.6698713302612305
steps = 117, loss = 3.4383785724639893
steps = 117, loss = 49.9853630065918
steps = 117, loss = 2.8732428550720215
steps = 117, loss = 3.8890347480773926
steps = 117, loss = 2.151667594909668
steps = 117, loss = 3.363300085067749
steps = 117, loss = 3.0149641036987305
steps = 117, loss = 3.870023012161255
steps = 117, loss = 3.633885622024536
steps = 117, loss = 3.541576385498047
steps = 117, loss = 3.6267685890197754
steps = 117, loss = 2.9581665992736816
steps = 117, loss = 2.938692808151245
steps = 117, loss = 2.726165771484375
steps = 117, loss = 2.287687301635742
steps = 118, loss = 49.9853630065918
steps = 118, loss = 3.3923025131225586
steps = 118, loss = 3.7966339588165283
steps = 118, loss = 49.92012405395508
steps = 118, loss = 2.1387932300567627
steps = 118, loss = 3.1585843563079834
steps = 118, loss = 50.02305221557617
steps = 118, loss = 2.1051881313323975
steps = 118, loss = 3.882591485977173
steps = 118, loss = 3.39380145072937
steps = 118, loss = 2.6917848587036133
steps = 118, loss = 6.285810470581055
steps = 118, loss = 2.6880416870117188
steps = 118, loss = 2.837019920349121
steps = 118, loss = 3.3472893238067627
steps = 118, loss = 2.4162020683288574
steps = 118, loss = 3.387647867202759
steps = 118, loss = 3.454352617263794
steps = 118, loss = 2.7383482456207275
steps = 118, loss = 3.4928855895996094
steps = 118, loss = 2.906355381011963
steps = 118, loss = 2.8527562618255615
steps = 118, loss = 3.7588043212890625
steps = 118, loss = 2.7519593238830566
steps = 118, loss = 2.7805116176605225
steps = 118, loss = 2.974085807800293
steps = 118, loss = 3.5214836597442627
steps = 118, loss = 2.5914313793182373
steps = 118, loss = 3.241741418838501
steps = 119, loss = 3.6349704265594482
steps = 119, loss = 2.4149351119995117
steps = 119, loss = 2.839024543762207
steps = 119, loss = 49.56241226196289
steps = 119, loss = 6.401491165161133
steps = 119, loss = 3.3132810592651367
steps = 119, loss = 2.3864076137542725
steps = 119, loss = 2.756504774093628
steps = 119, loss = 3.3819425106048584
steps = 119, loss = 49.9853630065918
steps = 119, loss = 3.7780425548553467
steps = 119, loss = 2.8553051948547363
steps = 119, loss = 2.2069449424743652
steps = 119, loss = 3.560833215713501
steps = 119, loss = 3.816138982772827
steps = 119, loss = 3.437958002090454
steps = 119, loss = 3.0726590156555176
steps = 119, loss = 3.3887641429901123
steps = 119, loss = 2.8678910732269287
steps = 119, loss = 2.9806220531463623
steps = 119, loss = 3.4997153282165527
steps = 119, loss = 2.760152816772461
steps = 119, loss = 3.918851137161255
steps = 119, loss = 50.02180099487305
steps = 119, loss = 2.7382476329803467
steps = 119, loss = 2.7101004123687744
steps = 119, loss = 2.7669975757598877
steps = 119, loss = 3.2427048683166504
steps = 119, loss = 3.8023271560668945
steps = 120, loss = 3.4924561977386475
steps = 120, loss = 3.8296096324920654
steps = 120, loss = 2.9857571125030518
steps = 120, loss = 2.6875979900360107
steps = 120, loss = 3.3232436180114746
steps = 120, loss = 3.799570083618164
steps = 120, loss = 2.8731892108917236
steps = 120, loss = 2.883948564529419
steps = 120, loss = 49.9853630065918
steps = 120, loss = 3.4627768993377686
steps = 120, loss = 3.3912220001220703
steps = 120, loss = 3.023981809616089
steps = 120, loss = 2.2883825302124023
steps = 120, loss = 3.5429701805114746
steps = 120, loss = 3.538111925125122
steps = 120, loss = 3.4822919368743896
steps = 120, loss = 2.7674412727355957
steps = 120, loss = 6.123870372772217
steps = 120, loss = 2.6312901973724365
steps = 120, loss = 3.938417673110962
steps = 120, loss = 2.7264645099639893
steps = 120, loss = 2.601292133331299
steps = 120, loss = 2.7748048305511475
steps = 120, loss = 2.2746407985687256
steps = 120, loss = 3.826230525970459
steps = 120, loss = 3.4477736949920654
steps = 120, loss = 2.6385512351989746
steps = 120, loss = 50.01633071899414
steps = 120, loss = 2.1107914447784424
steps = 121, loss = 3.5447514057159424
steps = 121, loss = 49.9853630065918
steps = 121, loss = 2.704702138900757
steps = 121, loss = 2.766369342803955
steps = 121, loss = 2.0946927070617676
steps = 121, loss = 2.7395362854003906
steps = 121, loss = 2.138852119445801
steps = 121, loss = 3.310272216796875
steps = 121, loss = 3.7985377311706543
steps = 121, loss = 6.282905101776123
steps = 121, loss = 3.0117080211639404
steps = 121, loss = 3.8292601108551025
steps = 121, loss = 16.348134994506836
steps = 121, loss = 3.515458345413208
steps = 121, loss = 3.6862587928771973
steps = 121, loss = 3.370087146759033
steps = 121, loss = 3.3329203128814697
steps = 121, loss = 2.7324652671813965
steps = 121, loss = 3.584695816040039
steps = 121, loss = 2.8368947505950928
steps = 121, loss = 2.6919422149658203
steps = 121, loss = 1.98841392993927
steps = 121, loss = 3.9312503337860107
steps = 121, loss = 2.509129047393799
steps = 121, loss = 50.02095413208008
steps = 121, loss = 3.096953868865967
steps = 121, loss = 2.5770225524902344
steps = 121, loss = 2.9725825786590576
steps = 121, loss = 3.43230938911438
steps = 122, loss = 2.1975746154785156
steps = 122, loss = 2.979074478149414
steps = 122, loss = 3.4684159755706787
steps = 122, loss = 49.9853630065918
steps = 122, loss = 3.8175008296966553
steps = 122, loss = 50.020164489746094
steps = 122, loss = 2.7813379764556885
steps = 122, loss = 3.2320430278778076
steps = 122, loss = 2.772300958633423
steps = 122, loss = 2.5769073963165283
steps = 122, loss = 2.6825456619262695
steps = 122, loss = 2.7103493213653564
steps = 122, loss = 3.238762617111206
steps = 122, loss = 3.508513927459717
steps = 122, loss = 2.134263277053833
steps = 122, loss = 3.6463282108306885
steps = 122, loss = 2.7629356384277344
steps = 122, loss = 3.329235076904297
steps = 122, loss = 3.9677793979644775
steps = 122, loss = 2.412874460220337
steps = 122, loss = 3.423403739929199
steps = 122, loss = 3.8338277339935303
steps = 122, loss = 3.4272964000701904
steps = 122, loss = 3.1339221000671387
steps = 122, loss = 2.855182409286499
steps = 122, loss = 2.8581631183624268
steps = 122, loss = 2.185137987136841
steps = 122, loss = 6.348460674285889
steps = 122, loss = 3.459453821182251
steps = 123, loss = 6.2257080078125
steps = 123, loss = 2.915386915206909
steps = 123, loss = 49.9853630065918
steps = 123, loss = 3.597552537918091
steps = 123, loss = 3.98628568649292
steps = 123, loss = 2.7266664505004883
steps = 123, loss = 3.27471661567688
steps = 123, loss = 2.9780149459838867
steps = 123, loss = 2.766986846923828
steps = 123, loss = 3.4232871532440186
steps = 123, loss = 2.6224474906921387
steps = 123, loss = 2.984138250350952
steps = 123, loss = 2.9716122150421143
steps = 123, loss = 3.5406672954559326
steps = 123, loss = 3.838268518447876
steps = 123, loss = 3.3662490844726562
steps = 123, loss = 2.3422980308532715
steps = 123, loss = 50.02494430541992
steps = 123, loss = 2.78879976272583
steps = 123, loss = 4.018458366394043
steps = 123, loss = 2.873131275177002
steps = 123, loss = 3.857633590698242
steps = 123, loss = 2.656968832015991
steps = 123, loss = 3.500173807144165
steps = 123, loss = 2.2888543605804443
steps = 123, loss = 3.377551317214966
steps = 123, loss = 2.176922559738159
steps = 123, loss = 2.699105978012085
steps = 123, loss = 3.6813414096832275
steps = 124, loss = 2.1667988300323486
steps = 124, loss = 3.89046573638916
steps = 124, loss = 2.9790217876434326
steps = 124, loss = 2.792724609375
steps = 124, loss = 3.8544270992279053
steps = 124, loss = 3.6500229835510254
steps = 124, loss = 3.565986394882202
steps = 124, loss = 2.1127521991729736
steps = 124, loss = 4.002721309661865
steps = 124, loss = 6.054965496063232
steps = 124, loss = 2.7238473892211914
steps = 124, loss = 50.02229690551758
steps = 124, loss = 2.7632312774658203
steps = 124, loss = 2.554439067840576
steps = 124, loss = 3.4705255031585693
steps = 124, loss = 49.9853630065918
steps = 124, loss = 3.845187187194824
steps = 124, loss = 3.3866748809814453
steps = 124, loss = 3.6606526374816895
steps = 124, loss = 3.013376235961914
steps = 124, loss = 3.3824989795684814
steps = 124, loss = 2.166367769241333
steps = 124, loss = 2.7105329036712646
steps = 124, loss = 2.6838183403015137
steps = 124, loss = 2.4949302673339844
steps = 124, loss = 2.765711784362793
steps = 124, loss = 3.3720319271087646
steps = 124, loss = 3.2259793281555176
steps = 124, loss = 2.8559441566467285
steps = 125, loss = 2.666217565536499
steps = 125, loss = 3.6490375995635986
steps = 125, loss = 2.429504871368408
steps = 125, loss = 2.969478130340576
steps = 125, loss = 6.175025463104248
steps = 125, loss = 2.142749786376953
steps = 125, loss = 2.033613681793213
steps = 125, loss = 3.8507871627807617
steps = 125, loss = 3.6403708457946777
steps = 125, loss = 3.868973970413208
steps = 125, loss = 3.996431350708008
steps = 125, loss = 2.5998330116271973
steps = 125, loss = 2.784735918045044
steps = 125, loss = 3.683030128479004
steps = 125, loss = 2.8350470066070557
steps = 125, loss = 2.026606798171997
steps = 125, loss = 3.512570858001709
steps = 125, loss = 2.857271671295166
steps = 125, loss = 3.3769969940185547
steps = 125, loss = 2.6897614002227783
steps = 125, loss = 49.9853630065918
steps = 125, loss = 2.7346770763397217
steps = 125, loss = 3.174461603164673
steps = 125, loss = 3.610447883605957
steps = 125, loss = 3.3867640495300293
steps = 125, loss = 3.79429292678833
steps = 125, loss = 2.973968267440796
steps = 125, loss = 50.01782989501953
steps = 125, loss = 2.7316806316375732
steps = 126, loss = 3.8869004249572754
steps = 126, loss = 3.538726806640625
steps = 126, loss = 2.727743148803711
steps = 126, loss = 2.766878604888916
steps = 126, loss = 2.873701810836792
steps = 126, loss = 4.032626628875732
steps = 126, loss = 2.8010542392730713
steps = 126, loss = 3.399122953414917
steps = 126, loss = 3.8601999282836914
steps = 126, loss = 2.982255220413208
steps = 126, loss = 3.880549907684326
steps = 126, loss = 3.1621615886688232
steps = 126, loss = 49.9853630065918
steps = 126, loss = 50.01403045654297
steps = 126, loss = 3.511793851852417
steps = 126, loss = 2.289335012435913
steps = 126, loss = 3.4184937477111816
steps = 126, loss = 3.175739288330078
steps = 126, loss = 3.398552417755127
steps = 126, loss = 2.6720783710479736
steps = 126, loss = 3.8766136169433594
steps = 126, loss = 2.304506540298462
steps = 126, loss = 2.873410224914551
steps = 126, loss = 3.554985523223877
steps = 126, loss = 2.9915096759796143
steps = 126, loss = 2.7169876098632812
steps = 126, loss = 6.250164985656738
steps = 126, loss = 3.0118398666381836
steps = 126, loss = 2.1845202445983887
steps = 127, loss = 3.7364695072174072
steps = 127, loss = 4.0272626876831055
steps = 127, loss = 3.5265371799468994
steps = 127, loss = 2.39711856842041
steps = 127, loss = 3.4363629817962646
steps = 127, loss = 2.7933671474456787
steps = 127, loss = 3.4004600048065186
steps = 127, loss = 2.947497606277466
steps = 127, loss = 3.2902517318725586
steps = 127, loss = 2.731013059616089
steps = 127, loss = 3.1387758255004883
steps = 127, loss = 2.9696173667907715
steps = 127, loss = 2.604919672012329
steps = 127, loss = 2.1388120651245117
steps = 127, loss = 1.9671677350997925
steps = 127, loss = 3.88983154296875
steps = 127, loss = 3.4008028507232666
steps = 127, loss = 2.8372137546539307
steps = 127, loss = 6.33450984954834
steps = 127, loss = 3.228360414505005
steps = 127, loss = 2.692960262298584
steps = 127, loss = 3.4392337799072266
steps = 127, loss = 2.6936452388763428
steps = 127, loss = 49.9853630065918
steps = 127, loss = 3.8763039112091064
steps = 127, loss = 2.6722986698150635
steps = 127, loss = 3.526024103164673
steps = 127, loss = 2.117216110229492
steps = 127, loss = 50.02338409423828
steps = 128, loss = 2.630211114883423
steps = 128, loss = 3.0543947219848633
steps = 128, loss = 3.395303726196289
steps = 128, loss = 6.413873195648193
steps = 128, loss = 3.38916277885437
steps = 128, loss = 49.9853630065918
steps = 128, loss = 3.3565456867218018
steps = 128, loss = 3.4116010665893555
steps = 128, loss = 50.02254104614258
steps = 128, loss = 3.9079957008361816
steps = 128, loss = 2.103076457977295
steps = 128, loss = 3.0043821334838867
steps = 128, loss = 4.064101219177246
steps = 128, loss = 3.398859739303589
steps = 128, loss = 3.82954478263855
steps = 128, loss = 2.869081735610962
steps = 128, loss = 2.619414806365967
steps = 128, loss = 2.2905147075653076
steps = 128, loss = 3.559174060821533
steps = 128, loss = 2.7695798873901367
steps = 128, loss = 2.8099772930145264
steps = 128, loss = 3.9020097255706787
steps = 128, loss = 4.170846939086914
steps = 128, loss = 2.2753729820251465
steps = 128, loss = 3.0048367977142334
steps = 128, loss = 2.7286674976348877
steps = 128, loss = 3.4140548706054688
steps = 128, loss = 2.8746674060821533
steps = 128, loss = 2.982088804244995
steps = 129, loss = 2.2775213718414307
steps = 129, loss = 4.082603454589844
steps = 129, loss = 2.2281908988952637
steps = 129, loss = 3.9090051651000977
steps = 129, loss = 2.1936874389648438
steps = 129, loss = 2.8555777072906494
steps = 129, loss = 2.603656053543091
steps = 129, loss = 2.9770708084106445
steps = 129, loss = 3.870447874069214
steps = 129, loss = 3.0170700550079346
steps = 129, loss = 3.198514223098755
steps = 129, loss = 3.6654748916625977
steps = 129, loss = 50.02110290527344
steps = 129, loss = 3.555835723876953
steps = 129, loss = 2.814643144607544
steps = 129, loss = 3.4421958923339844
steps = 129, loss = 9.94421100616455
steps = 129, loss = 3.370291233062744
steps = 129, loss = 2.7108466625213623
steps = 129, loss = 3.5401766300201416
steps = 129, loss = 2.7621097564697266
steps = 129, loss = 3.903455972671509
steps = 129, loss = 2.052826404571533
steps = 129, loss = 6.498339653015137
steps = 129, loss = 2.6700053215026855
steps = 129, loss = 49.9853630065918
steps = 129, loss = 2.883746385574341
steps = 129, loss = 3.702787160873413
steps = 129, loss = 3.436583995819092
steps = 130, loss = 3.3008317947387695
steps = 130, loss = 2.7010881900787354
steps = 130, loss = 2.7403903007507324
steps = 130, loss = 2.6717708110809326
steps = 130, loss = 3.3357925415039062
steps = 130, loss = 50.01164245605469
steps = 130, loss = 2.143758773803711
steps = 130, loss = 3.917823076248169
steps = 130, loss = 3.5817549228668213
steps = 130, loss = 2.201364278793335
steps = 130, loss = 2.691213846206665
steps = 130, loss = 2.4132611751556396
steps = 130, loss = 4.076963901519775
steps = 130, loss = 2.798316717147827
steps = 130, loss = 3.4464290142059326
steps = 130, loss = 6.258160591125488
steps = 130, loss = 3.7457826137542725
steps = 130, loss = 3.347194194793701
steps = 130, loss = 2.9196994304656982
steps = 130, loss = 2.7668097019195557
steps = 130, loss = 1.962052583694458
steps = 130, loss = 2.967742681503296
steps = 130, loss = 3.142329216003418
steps = 130, loss = 49.9853630065918
steps = 130, loss = 2.835822105407715
steps = 130, loss = 2.8068556785583496
steps = 130, loss = 3.3335115909576416
steps = 130, loss = 2.4927115440368652
steps = 130, loss = 3.915102958679199
steps = 131, loss = 3.900919198989868
steps = 131, loss = 3.920295476913452
steps = 131, loss = 3.326058864593506
steps = 131, loss = 3.9337828159332275
steps = 131, loss = 3.5696136951446533
steps = 131, loss = 50.018978118896484
steps = 131, loss = 2.2865400314331055
steps = 131, loss = 2.1676459312438965
steps = 131, loss = 2.821730136871338
steps = 131, loss = 3.3505284786224365
steps = 131, loss = 2.7108302116394043
steps = 131, loss = 6.359527111053467
steps = 131, loss = 3.2593469619750977
steps = 131, loss = 2.9016530513763428
steps = 131, loss = 2.758711338043213
steps = 131, loss = 3.1644694805145264
steps = 131, loss = 3.2833564281463623
steps = 131, loss = 2.729477643966675
steps = 131, loss = 2.854796886444092
steps = 131, loss = 49.9853630065918
steps = 131, loss = 2.501331329345703
steps = 131, loss = 3.6722028255462646
steps = 131, loss = 3.0489442348480225
steps = 131, loss = 3.3660852909088135
steps = 131, loss = 2.915327787399292
steps = 131, loss = 1.92360258102417
steps = 131, loss = 4.113954067230225
steps = 131, loss = 2.8948638439178467
steps = 131, loss = 2.9743831157684326
steps = 132, loss = 2.5704455375671387
steps = 132, loss = 2.7687089443206787
steps = 132, loss = 3.455687999725342
steps = 132, loss = 2.637921094894409
steps = 132, loss = 2.727238178253174
steps = 132, loss = 2.9793970584869385
steps = 132, loss = 2.872699737548828
steps = 132, loss = 3.0514233112335205
steps = 132, loss = 2.240083932876587
steps = 132, loss = 3.358778238296509
steps = 132, loss = 3.5129048824310303
steps = 132, loss = 4.459334850311279
steps = 132, loss = 50.01770782470703
steps = 132, loss = 2.8290464878082275
steps = 132, loss = 2.727822780609131
steps = 132, loss = 3.568303346633911
steps = 132, loss = 2.9204068183898926
steps = 132, loss = 3.2618422508239746
steps = 132, loss = 3.954529047012329
steps = 132, loss = 3.424027442932129
steps = 132, loss = 16.048826217651367
steps = 132, loss = 2.6977107524871826
steps = 132, loss = 3.943079948425293
steps = 132, loss = 6.416095733642578
steps = 132, loss = 3.224191665649414
steps = 132, loss = 3.340665578842163
steps = 132, loss = 49.9853630065918
steps = 132, loss = 2.2903482913970947
steps = 132, loss = 4.133725643157959
steps = 133, loss = 2.601177930831909
steps = 133, loss = 2.708937168121338
steps = 133, loss = 3.236524820327759
steps = 133, loss = 3.17311429977417
steps = 133, loss = 2.1387600898742676
steps = 133, loss = 2.6926021575927734
steps = 133, loss = 3.5319557189941406
steps = 133, loss = 2.2385778427124023
steps = 133, loss = 2.836498737335205
steps = 133, loss = 3.241549015045166
steps = 133, loss = 2.959139823913574
steps = 133, loss = 49.9853630065918
steps = 133, loss = 1.932712435722351
steps = 133, loss = 2.7361531257629395
steps = 133, loss = 4.692916393280029
steps = 133, loss = 2.8203678131103516
steps = 133, loss = 2.821068525314331
steps = 133, loss = 3.953012704849243
steps = 133, loss = 50.02657699584961
steps = 133, loss = 3.5403733253479004
steps = 133, loss = 2.966798782348633
steps = 133, loss = 3.7011468410491943
steps = 133, loss = 3.9440500736236572
steps = 133, loss = 3.1207122802734375
steps = 133, loss = 3.467085838317871
steps = 133, loss = 3.3561770915985107
steps = 133, loss = 6.560477256774902
steps = 133, loss = 2.4693663120269775
steps = 133, loss = 4.127110004425049
steps = 134, loss = 3.9709434509277344
steps = 134, loss = 3.644507646560669
steps = 134, loss = 50.027740478515625
steps = 134, loss = 3.5317509174346924
steps = 134, loss = 3.9463577270507812
steps = 134, loss = 3.4045603275299072
steps = 134, loss = 3.142220973968506
steps = 134, loss = 2.6925275325775146
steps = 134, loss = 2.758265733718872
steps = 134, loss = 2.7700984477996826
steps = 134, loss = 4.163476467132568
steps = 134, loss = 3.3223752975463867
steps = 134, loss = 2.8547589778900146
steps = 134, loss = 2.185633420944214
steps = 134, loss = 2.1797730922698975
steps = 134, loss = 3.633004903793335
steps = 134, loss = 3.4846351146698
steps = 134, loss = 2.037667989730835
steps = 134, loss = 3.365806818008423
steps = 134, loss = 2.505664825439453
steps = 134, loss = 6.151942253112793
steps = 134, loss = 2.834674596786499
steps = 134, loss = 3.3795697689056396
steps = 134, loss = 2.711151599884033
steps = 134, loss = 2.8154728412628174
steps = 134, loss = 2.9731955528259277
steps = 134, loss = 2.7380807399749756
steps = 134, loss = 49.9853630065918
steps = 134, loss = 3.274059772491455
steps = 135, loss = 2.7276294231414795
steps = 135, loss = 2.2896385192871094
steps = 135, loss = 3.3662359714508057
steps = 135, loss = 2.435372829437256
steps = 135, loss = 3.4379794597625732
steps = 135, loss = 3.991931200027466
steps = 135, loss = 3.470001697540283
steps = 135, loss = 3.084484100341797
steps = 135, loss = 2.1821796894073486
steps = 135, loss = 49.9853630065918
steps = 135, loss = 2.5168192386627197
steps = 135, loss = 3.3830907344818115
steps = 135, loss = 3.342876672744751
steps = 135, loss = 2.6505484580993652
steps = 135, loss = 3.6635067462921143
steps = 135, loss = 2.978076696395874
steps = 135, loss = 2.768340587615967
steps = 135, loss = 2.8728504180908203
steps = 135, loss = 50.00961685180664
steps = 135, loss = 6.31261682510376
steps = 135, loss = 2.841975450515747
steps = 135, loss = 2.9475784301757812
steps = 135, loss = 4.182572364807129
steps = 135, loss = 2.6627609729766846
steps = 135, loss = 2.1168212890625
steps = 135, loss = 2.9037272930145264
steps = 135, loss = 3.9686484336853027
steps = 135, loss = 3.4803740978240967
steps = 135, loss = 4.295069217681885
steps = 136, loss = 3.3786754608154297
steps = 136, loss = 3.9688167572021484
steps = 136, loss = 3.408820152282715
steps = 136, loss = 49.9853630065918
steps = 136, loss = 2.0786585807800293
steps = 136, loss = 50.02431869506836
steps = 136, loss = 2.4225447177886963
steps = 136, loss = 3.9899344444274902
steps = 136, loss = 2.7389168739318848
steps = 136, loss = 3.4370298385620117
steps = 136, loss = 3.5239920616149902
steps = 136, loss = 2.692856550216675
steps = 136, loss = 2.9655725955963135
steps = 136, loss = 2.836507558822632
steps = 136, loss = 2.833219051361084
steps = 136, loss = 2.759643316268921
steps = 136, loss = 2.1390082836151123
steps = 136, loss = 4.176403045654297
steps = 136, loss = 2.317610025405884
steps = 136, loss = 2.5675861835479736
steps = 136, loss = 2.971712827682495
steps = 136, loss = 2.0679242610931396
steps = 136, loss = 2.7527577877044678
steps = 136, loss = 6.417201995849609
steps = 136, loss = 3.2914540767669678
steps = 136, loss = 3.679185628890991
steps = 136, loss = 3.4894566535949707
steps = 136, loss = 3.499669075012207
steps = 136, loss = 2.538560152053833
steps = 137, loss = 2.959681749343872
steps = 137, loss = 3.3183753490448
steps = 137, loss = 2.7113070487976074
steps = 137, loss = 4.213188648223877
steps = 137, loss = 2.4562392234802246
steps = 137, loss = 3.6670727729797363
steps = 137, loss = 50.01585006713867
steps = 137, loss = 4.008234024047852
steps = 137, loss = 3.971620559692383
steps = 137, loss = 2.627943277359009
steps = 137, loss = 3.6705920696258545
steps = 137, loss = 2.886305809020996
steps = 137, loss = 3.042198657989502
steps = 137, loss = 2.971952438354492
steps = 137, loss = 2.854689121246338
steps = 137, loss = 2.8522591590881348
steps = 137, loss = 2.0389175415039062
steps = 137, loss = 2.8471920490264893
steps = 137, loss = 2.7641959190368652
steps = 137, loss = 3.358167886734009
steps = 137, loss = 49.9853630065918
steps = 137, loss = 3.54472017288208
steps = 137, loss = 2.1858668327331543
steps = 137, loss = 2.1758933067321777
steps = 137, loss = 2.909844160079956
steps = 137, loss = 6.500740051269531
steps = 137, loss = 3.4393444061279297
steps = 137, loss = 3.9902853965759277
steps = 137, loss = 3.308953285217285
steps = 138, loss = 3.1679811477661133
steps = 138, loss = 2.921107292175293
steps = 138, loss = 3.993475914001465
steps = 138, loss = 2.7278778553009033
steps = 138, loss = 2.9767560958862305
steps = 138, loss = 2.2897121906280518
steps = 138, loss = 3.549614191055298
steps = 138, loss = 2.8546364307403564
steps = 138, loss = 3.5471065044403076
steps = 138, loss = 3.382453441619873
steps = 138, loss = 4.0287322998046875
steps = 138, loss = 2.135988473892212
steps = 138, loss = 4.232111930847168
steps = 138, loss = 2.1656596660614014
steps = 138, loss = 2.8728582859039307
steps = 138, loss = 3.4820921421051025
steps = 138, loss = 3.5123801231384277
steps = 138, loss = 4.343315124511719
steps = 138, loss = 2.6596219539642334
steps = 138, loss = 3.3797848224639893
steps = 138, loss = 3.0747556686401367
steps = 138, loss = 2.980400323867798
steps = 138, loss = 2.7685303688049316
steps = 138, loss = 2.9770359992980957
steps = 138, loss = 3.4209747314453125
steps = 138, loss = 6.416923999786377
steps = 138, loss = 2.6283528804779053
steps = 138, loss = 50.017642974853516
steps = 138, loss = 49.9853630065918
steps = 139, loss = 4.034903049468994
steps = 139, loss = 2.3313496112823486
steps = 139, loss = 3.5749104022979736
steps = 139, loss = 2.855477809906006
steps = 139, loss = 2.6509087085723877
steps = 139, loss = 4.249279975891113
steps = 139, loss = 3.1392810344696045
steps = 139, loss = 3.400664806365967
steps = 139, loss = 2.7116005420684814
steps = 139, loss = 3.5256993770599365
steps = 139, loss = 2.1068224906921387
steps = 139, loss = 2.766512393951416
steps = 139, loss = 50.015541076660156
steps = 139, loss = 6.365546703338623
steps = 139, loss = 3.082003116607666
steps = 139, loss = 2.1739141941070557
steps = 139, loss = 2.7632553577423096
steps = 139, loss = 4.353874206542969
steps = 139, loss = 3.6800992488861084
steps = 139, loss = 3.0811643600463867
steps = 139, loss = 3.9887561798095703
steps = 139, loss = 3.344480276107788
steps = 139, loss = 2.1523473262786865
steps = 139, loss = 2.857954740524292
steps = 139, loss = 3.3828349113464355
steps = 139, loss = 3.5047554969787598
steps = 139, loss = 2.9721295833587646
steps = 139, loss = 2.7630152702331543
steps = 139, loss = 49.9853630065918
steps = 140, loss = 3.9991650581359863
steps = 140, loss = 3.5302717685699463
steps = 140, loss = 2.4104485511779785
steps = 140, loss = 2.72044038772583
steps = 140, loss = 2.1513254642486572
steps = 140, loss = 3.0532448291778564
steps = 140, loss = 2.6419174671173096
steps = 140, loss = 4.40363883972168
steps = 140, loss = 2.9628725051879883
steps = 140, loss = 3.1762940883636475
steps = 140, loss = 4.242475509643555
steps = 140, loss = 2.736368179321289
steps = 140, loss = 3.3904616832733154
steps = 140, loss = 6.407644271850586
steps = 140, loss = 50.02706527709961
steps = 140, loss = 3.603428602218628
steps = 140, loss = 2.20540452003479
steps = 140, loss = 3.3291659355163574
steps = 140, loss = 2.849944591522217
steps = 140, loss = 3.2644240856170654
steps = 140, loss = 2.7131259441375732
steps = 140, loss = 2.6906557083129883
steps = 140, loss = 2.8985040187835693
steps = 140, loss = 2.039970636367798
steps = 140, loss = 3.394308090209961
steps = 140, loss = 49.9853630065918
steps = 140, loss = 2.8347225189208984
steps = 140, loss = 4.0397047996521
steps = 140, loss = 3.158724308013916
steps = 141, loss = 3.377537965774536
steps = 141, loss = 49.9853630065918
steps = 141, loss = 50.027931213378906
steps = 141, loss = 2.6043407917022705
steps = 141, loss = 2.5827720165252686
steps = 141, loss = 2.8657572269439697
steps = 141, loss = 4.348130226135254
steps = 141, loss = 4.064584732055664
steps = 141, loss = 2.8731930255889893
steps = 141, loss = 3.413516044616699
steps = 141, loss = 2.2367844581604004
steps = 141, loss = 3.0873851776123047
steps = 141, loss = 2.9751904010772705
steps = 141, loss = 2.292170524597168
steps = 141, loss = 2.6160826683044434
steps = 141, loss = 2.7288031578063965
steps = 141, loss = 3.4753878116607666
steps = 141, loss = 2.802675247192383
steps = 141, loss = 3.446528196334839
steps = 141, loss = 3.337862491607666
steps = 141, loss = 3.6551995277404785
steps = 141, loss = 2.1565444469451904
steps = 141, loss = 4.279698848724365
steps = 141, loss = 2.7689309120178223
steps = 141, loss = 3.442768096923828
steps = 141, loss = 6.4696364402771
steps = 141, loss = 3.369175910949707
steps = 141, loss = 3.0327725410461426
steps = 141, loss = 4.016688346862793
steps = 142, loss = 2.022357225418091
steps = 142, loss = 3.460859537124634
steps = 142, loss = 2.562394380569458
steps = 142, loss = 3.365039348602295
steps = 142, loss = 2.7348906993865967
steps = 142, loss = 50.021705627441406
steps = 142, loss = 3.527775287628174
steps = 142, loss = 4.064021587371826
steps = 142, loss = 2.6545662879943848
steps = 142, loss = 2.6937081813812256
steps = 142, loss = 4.0163984298706055
steps = 142, loss = 3.314307928085327
steps = 142, loss = 3.336094856262207
steps = 142, loss = 2.749215602874756
steps = 142, loss = 3.181974411010742
steps = 142, loss = 2.9632110595703125
steps = 142, loss = 3.4130232334136963
steps = 142, loss = 3.024660110473633
steps = 142, loss = 2.857698917388916
steps = 142, loss = 2.8369271755218506
steps = 142, loss = 2.4118781089782715
steps = 142, loss = 6.55670690536499
steps = 142, loss = 2.60489559173584
steps = 142, loss = 2.134540557861328
steps = 142, loss = 3.937837600708008
steps = 142, loss = 49.9853630065918
steps = 142, loss = 2.049652338027954
steps = 142, loss = 3.4237236976623535
steps = 142, loss = 4.273300647735596
steps = 143, loss = 3.595244884490967
steps = 143, loss = 3.3969056606292725
steps = 143, loss = 2.604365348815918
steps = 143, loss = 3.659435987472534
steps = 143, loss = 3.4525256156921387
steps = 143, loss = 2.884775161743164
steps = 143, loss = 2.6498806476593018
steps = 143, loss = 2.9752418994903564
steps = 143, loss = 2.771101474761963
steps = 143, loss = 4.309605598449707
steps = 143, loss = 2.2862348556518555
steps = 143, loss = 4.088339805603027
steps = 143, loss = 50.01755142211914
steps = 143, loss = 2.874484062194824
steps = 143, loss = 2.7297885417938232
steps = 143, loss = 2.8737881183624268
steps = 143, loss = 4.313478469848633
steps = 143, loss = 2.082923173904419
steps = 143, loss = 3.6510889530181885
steps = 143, loss = 2.648648262023926
steps = 143, loss = 2.1583006381988525
steps = 143, loss = 2.9751803874969482
steps = 143, loss = 4.0347466468811035
steps = 143, loss = 3.002074718475342
steps = 143, loss = 3.486848831176758
steps = 143, loss = 6.533361434936523
steps = 143, loss = 2.932169198989868
steps = 143, loss = 3.3512439727783203
steps = 143, loss = 49.9853630065918
steps = 144, loss = 3.2839407920837402
steps = 144, loss = 2.216946840286255
steps = 144, loss = 3.400646209716797
steps = 144, loss = 2.0605523586273193
steps = 144, loss = 2.0747289657592773
steps = 144, loss = 2.7140719890594482
steps = 144, loss = 3.37985897064209
steps = 144, loss = 2.970655679702759
steps = 144, loss = 49.9853630065918
steps = 144, loss = 3.486967086791992
steps = 144, loss = 3.5349392890930176
steps = 144, loss = 2.631777048110962
steps = 144, loss = 6.372716426849365
steps = 144, loss = 4.328648567199707
steps = 144, loss = 2.8892476558685303
steps = 144, loss = 2.5431363582611084
steps = 144, loss = 4.029951095581055
steps = 144, loss = 3.137333869934082
steps = 144, loss = 2.7119317054748535
steps = 144, loss = 2.855163812637329
steps = 144, loss = 3.3803422451019287
steps = 144, loss = 3.451197862625122
steps = 144, loss = 2.7216806411743164
steps = 144, loss = 2.877659559249878
steps = 144, loss = 2.76316499710083
steps = 144, loss = 50.0178337097168
steps = 144, loss = 3.6334409713745117
steps = 144, loss = 4.094547748565674
steps = 144, loss = 3.307344913482666
steps = 145, loss = 2.855069398880005
steps = 145, loss = 3.038360357284546
steps = 145, loss = 2.4316422939300537
steps = 145, loss = 3.648946762084961
steps = 145, loss = 3.46748948097229
steps = 145, loss = 3.5525081157684326
steps = 145, loss = 2.7115888595581055
steps = 145, loss = 2.9081757068634033
steps = 145, loss = 3.6124110221862793
steps = 145, loss = 2.7723541259765625
steps = 145, loss = 50.01691818237305
steps = 145, loss = 3.3466057777404785
steps = 145, loss = 3.4847781658172607
steps = 145, loss = 2.969309091567993
steps = 145, loss = 4.106813430786133
steps = 145, loss = 2.5870234966278076
steps = 145, loss = 2.76119065284729
steps = 145, loss = 50.021270751953125
steps = 145, loss = 4.0364227294921875
steps = 145, loss = 2.0787765979766846
steps = 145, loss = 3.5287556648254395
steps = 145, loss = 3.5141074657440186
steps = 145, loss = 4.346724510192871
steps = 145, loss = 2.8819124698638916
steps = 145, loss = 2.920133113861084
steps = 145, loss = 2.1715965270996094
steps = 145, loss = 49.9853630065918
steps = 145, loss = 6.395871162414551
steps = 145, loss = 5.151381492614746
steps = 146, loss = 2.6905999183654785
steps = 146, loss = 2.834385633468628
steps = 146, loss = 3.620744228363037
steps = 146, loss = 3.306762456893921
steps = 146, loss = 2.1425957679748535
steps = 146, loss = 2.535322666168213
steps = 146, loss = 3.604501485824585
steps = 146, loss = 3.0503411293029785
steps = 146, loss = 3.372102975845337
steps = 146, loss = 50.01595687866211
steps = 146, loss = 2.367985486984253
steps = 146, loss = 3.165834426879883
steps = 146, loss = 2.7396750450134277
steps = 146, loss = 2.7675204277038574
steps = 146, loss = 2.873981237411499
steps = 146, loss = 3.41121506690979
steps = 146, loss = 4.045825004577637
steps = 146, loss = 4.339652061462402
steps = 146, loss = 6.503807067871094
steps = 146, loss = 2.578073740005493
steps = 146, loss = 2.57112717628479
steps = 146, loss = 3.419646978378296
steps = 146, loss = 1.9659596681594849
steps = 146, loss = 1.8722134828567505
steps = 146, loss = 3.4712252616882324
steps = 146, loss = 49.9853630065918
steps = 146, loss = 4.111767292022705
steps = 146, loss = 3.397745370864868
steps = 146, loss = 2.9603259563446045
steps = 147, loss = 3.5268731117248535
steps = 147, loss = 4.376145839691162
steps = 147, loss = 4.063994884490967
steps = 147, loss = 2.889967441558838
steps = 147, loss = 3.335667371749878
steps = 147, loss = 3.065537691116333
steps = 147, loss = 3.3399507999420166
steps = 147, loss = 2.9782094955444336
steps = 147, loss = 4.136073112487793
steps = 147, loss = 2.629086971282959
steps = 147, loss = 3.3629262447357178
steps = 147, loss = 6.5435967445373535
steps = 147, loss = 2.872847080230713
steps = 147, loss = 2.1634151935577393
steps = 147, loss = 2.729217290878296
steps = 147, loss = 3.5728280544281006
steps = 147, loss = 2.0453310012817383
steps = 147, loss = 2.972576856613159
steps = 147, loss = 2.6875131130218506
steps = 147, loss = 2.9922585487365723
steps = 147, loss = 2.7697203159332275
steps = 147, loss = 3.39009428024292
steps = 147, loss = 3.8494627475738525
steps = 147, loss = 3.6597659587860107
steps = 147, loss = 2.2860970497131348
steps = 147, loss = 49.9853630065918
steps = 147, loss = 50.014217376708984
steps = 147, loss = 3.66475248336792
steps = 147, loss = 2.914937734603882
steps = 148, loss = 4.060214519500732
steps = 148, loss = 6.620391845703125
steps = 148, loss = 3.4232208728790283
steps = 148, loss = 2.7123162746429443
steps = 148, loss = 2.0413589477539062
steps = 148, loss = 2.8550467491149902
steps = 148, loss = 3.3733906745910645
steps = 148, loss = 49.9853630065918
steps = 148, loss = 3.3700830936431885
steps = 148, loss = 2.173898696899414
steps = 148, loss = 3.3992483615875244
steps = 148, loss = 2.202627658843994
steps = 148, loss = 3.2911276817321777
steps = 148, loss = 2.74514102935791
steps = 148, loss = 4.395331859588623
steps = 148, loss = 3.2256877422332764
steps = 148, loss = 4.1423420906066895
steps = 148, loss = 9.47352123260498
steps = 148, loss = 2.846259832382202
steps = 148, loss = 2.8937761783599854
steps = 148, loss = 5.5338640213012695
steps = 148, loss = 3.4047744274139404
steps = 148, loss = 50.0119514465332
steps = 148, loss = 2.4445459842681885
steps = 148, loss = 3.149312734603882
steps = 148, loss = 2.759178876876831
steps = 148, loss = 2.8138983249664307
steps = 148, loss = 2.9684364795684814
steps = 148, loss = 3.5052554607391357
steps = 149, loss = 2.9543795585632324
steps = 149, loss = 3.444209575653076
steps = 149, loss = 1.992799997329712
steps = 149, loss = 3.1144237518310547
steps = 149, loss = 4.079776287078857
steps = 149, loss = 2.841386079788208
steps = 149, loss = 2.873286724090576
steps = 149, loss = 4.411456108093262
steps = 149, loss = 2.728534698486328
steps = 149, loss = 3.1564295291900635
steps = 149, loss = 3.5033891201019287
steps = 149, loss = 3.656604290008545
steps = 149, loss = 50.01468276977539
steps = 149, loss = 4.160881519317627
steps = 149, loss = 49.9853630065918
steps = 149, loss = 3.4311280250549316
steps = 149, loss = 6.57407283782959
steps = 149, loss = 2.768617868423462
steps = 149, loss = 3.030099630355835
steps = 149, loss = 3.516695022583008
steps = 149, loss = 2.1093716621398926
steps = 149, loss = 2.8999335765838623
steps = 149, loss = 2.9725584983825684
steps = 149, loss = 3.0321946144104004
steps = 149, loss = 2.613909959793091
steps = 149, loss = 2.289794683456421
steps = 149, loss = 2.660149097442627
steps = 149, loss = 2.787163257598877
steps = 149, loss = 3.3964035511016846
steps = 150, loss = 2.2844672203063965
steps = 150, loss = 2.4459052085876465
steps = 150, loss = 49.9853630065918
steps = 150, loss = 2.697739601135254
steps = 150, loss = 3.4125514030456543
steps = 150, loss = 4.158506870269775
steps = 150, loss = 2.739497661590576
steps = 150, loss = 6.755863666534424
steps = 150, loss = 3.47180438041687
steps = 150, loss = 2.9503798484802246
steps = 150, loss = 2.9602575302124023
steps = 150, loss = 3.08512544631958
steps = 150, loss = 2.1340677738189697
steps = 150, loss = 2.6928913593292236
steps = 150, loss = 3.4500393867492676
steps = 150, loss = 50.01150131225586
steps = 150, loss = 2.0385470390319824
steps = 150, loss = 3.372483730316162
steps = 150, loss = 3.061121940612793
steps = 150, loss = 2.829528570175171
steps = 150, loss = 2.02184796333313
steps = 150, loss = 4.077910423278809
steps = 150, loss = 3.427370071411133
steps = 150, loss = 2.5145132541656494
steps = 150, loss = 3.2380104064941406
steps = 150, loss = 4.404734134674072
steps = 150, loss = 2.75787091255188
steps = 150, loss = 2.8357303142547607
steps = 150, loss = 2.8903586864471436
steps = 151, loss = 2.4395158290863037
steps = 151, loss = 2.287266254425049
steps = 151, loss = 4.082622051239014
steps = 151, loss = 1.9366062879562378
steps = 151, loss = 3.1391565799713135
steps = 151, loss = 3.5109763145446777
steps = 151, loss = 3.6878230571746826
steps = 151, loss = 3.101059675216675
steps = 151, loss = 49.9853630065918
steps = 151, loss = 3.5634539127349854
steps = 151, loss = 49.9950065612793
steps = 151, loss = 6.394695281982422
steps = 151, loss = 2.9666054248809814
steps = 151, loss = 2.6826233863830566
steps = 151, loss = 4.176698684692383
steps = 151, loss = 2.7593448162078857
steps = 151, loss = 3.566934823989868
steps = 151, loss = 2.996072769165039
steps = 151, loss = 2.175751209259033
steps = 151, loss = 2.7121877670288086
steps = 151, loss = 3.3241264820098877
steps = 151, loss = 2.131011486053467
steps = 151, loss = 2.7074644565582275
steps = 151, loss = 3.394515037536621
steps = 151, loss = 2.8543241024017334
steps = 151, loss = 2.8462975025177
steps = 151, loss = 4.441274166107178
steps = 151, loss = 2.9038686752319336
steps = 151, loss = 3.486978769302368
steps = 152, loss = 3.388538122177124
steps = 152, loss = 2.6381592750549316
steps = 152, loss = 2.872887372970581
steps = 152, loss = 2.65617299079895
steps = 152, loss = 4.195630073547363
steps = 152, loss = 2.3756937980651855
steps = 152, loss = 2.7693052291870117
steps = 152, loss = 2.9454023838043213
steps = 152, loss = 3.578176736831665
steps = 152, loss = 3.4991958141326904
steps = 152, loss = 2.893834352493286
steps = 152, loss = 2.9711952209472656
steps = 152, loss = 3.339176893234253
steps = 152, loss = 2.796299457550049
steps = 152, loss = 2.268043041229248
steps = 152, loss = 4.459056377410889
steps = 152, loss = 2.192397117614746
steps = 152, loss = 2.681330919265747
steps = 152, loss = 50.01362991333008
steps = 152, loss = 2.290485382080078
steps = 152, loss = 3.3086276054382324
steps = 152, loss = 2.7287237644195557
steps = 152, loss = 3.3904874324798584
steps = 152, loss = 3.0095744132995605
steps = 152, loss = 6.526520729064941
steps = 152, loss = 4.102711200714111
steps = 152, loss = 3.481424570083618
steps = 152, loss = 49.9853630065918
steps = 152, loss = 2.9110822677612305
steps = 153, loss = 2.728808641433716
steps = 153, loss = 2.5930495262145996
steps = 153, loss = 3.0872645378112793
steps = 153, loss = 3.6914803981781006
steps = 153, loss = 2.3422508239746094
steps = 153, loss = 2.901524305343628
steps = 153, loss = 2.738211154937744
steps = 153, loss = 3.7362804412841797
steps = 153, loss = 2.1391117572784424
steps = 153, loss = 2.4319088459014893
steps = 153, loss = 4.4527082443237305
steps = 153, loss = 4.193023204803467
steps = 153, loss = 3.0507943630218506
steps = 153, loss = 49.9853630065918
steps = 153, loss = 2.959127187728882
steps = 153, loss = 50.011810302734375
steps = 153, loss = 2.8358936309814453
steps = 153, loss = 3.571392059326172
steps = 153, loss = 6.58173131942749
steps = 153, loss = 3.2612268924713135
steps = 153, loss = 2.7925209999084473
steps = 153, loss = 3.561037302017212
steps = 153, loss = 3.4157912731170654
steps = 153, loss = 2.0851309299468994
steps = 153, loss = 3.3990395069122314
steps = 153, loss = 3.351879596710205
steps = 153, loss = 2.0394692420959473
steps = 153, loss = 2.693296194076538
steps = 153, loss = 4.1003241539001465
steps = 154, loss = 3.54640793800354
steps = 154, loss = 49.9853630065918
steps = 154, loss = 2.8542613983154297
steps = 154, loss = 2.7075917720794678
steps = 154, loss = 3.75797700881958
steps = 154, loss = 2.8818109035491943
steps = 154, loss = 2.350362777709961
steps = 154, loss = 3.517915964126587
steps = 154, loss = 2.9655210971832275
steps = 154, loss = 2.496367931365967
steps = 154, loss = 2.198620319366455
steps = 154, loss = 4.489985942840576
steps = 154, loss = 2.762716293334961
steps = 154, loss = 2.7756216526031494
steps = 154, loss = 2.712329387664795
steps = 154, loss = 2.7377190589904785
steps = 154, loss = 4.210357666015625
steps = 154, loss = 4.105603218078613
steps = 154, loss = 2.4992573261260986
steps = 154, loss = 3.352167844772339
steps = 154, loss = 6.65431022644043
steps = 154, loss = 3.5442934036254883
steps = 154, loss = 3.4143877029418945
steps = 154, loss = 50.01051330566406
steps = 154, loss = 2.2650444507598877
steps = 154, loss = 2.088618040084839
steps = 154, loss = 2.9152209758758545
steps = 154, loss = 3.3695967197418213
steps = 154, loss = 3.635864496231079
steps = 155, loss = 2.7912580966949463
steps = 155, loss = 3.5736284255981445
steps = 155, loss = 2.872835874557495
steps = 155, loss = 2.769834280014038
steps = 155, loss = 4.5074143409729
steps = 155, loss = 3.77023983001709
steps = 155, loss = 2.9284048080444336
steps = 155, loss = 50.00947189331055
steps = 155, loss = 49.9853630065918
steps = 155, loss = 2.290534019470215
steps = 155, loss = 2.580540657043457
steps = 155, loss = 2.7289233207702637
steps = 155, loss = 6.606025695800781
steps = 155, loss = 2.646833658218384
steps = 155, loss = 3.6338207721710205
steps = 155, loss = 4.229144096374512
steps = 155, loss = 3.4023189544677734
steps = 155, loss = 2.4687180519104004
steps = 155, loss = 3.467848300933838
steps = 155, loss = 4.12538480758667
steps = 155, loss = 3.1724295616149902
steps = 155, loss = 2.9223430156707764
steps = 155, loss = 2.157151222229004
steps = 155, loss = 3.3895652294158936
steps = 155, loss = 2.2064480781555176
steps = 155, loss = 2.9700968265533447
steps = 155, loss = 3.5738701820373535
steps = 155, loss = 2.937044143676758
steps = 155, loss = 3.277758836746216
steps = 156, loss = 2.752209186553955
steps = 156, loss = 3.02937650680542
steps = 156, loss = 2.912933826446533
steps = 156, loss = 3.149738073348999
steps = 156, loss = 2.442270517349243
steps = 156, loss = 2.835996627807617
steps = 156, loss = 49.9853630065918
steps = 156, loss = 6.728143692016602
steps = 156, loss = 2.7383062839508057
steps = 156, loss = 2.9581265449523926
steps = 156, loss = 2.6000583171844482
steps = 156, loss = 2.6935999393463135
steps = 156, loss = 4.123204231262207
steps = 156, loss = 2.059882164001465
steps = 156, loss = 3.3805148601531982
steps = 156, loss = 3.4858386516571045
steps = 156, loss = 1.96433424949646
steps = 156, loss = 3.30855131149292
steps = 156, loss = 3.62579083442688
steps = 156, loss = 2.1390254497528076
steps = 156, loss = 3.1957616806030273
steps = 156, loss = 50.01176452636719
steps = 156, loss = 3.497098684310913
steps = 156, loss = 3.499889850616455
steps = 156, loss = 2.556819438934326
steps = 156, loss = 4.226792335510254
steps = 156, loss = 4.501432418823242
steps = 156, loss = 3.1940226554870605
steps = 156, loss = 3.4004902839660645
steps = 157, loss = 3.4860215187072754
steps = 157, loss = 3.808196544647217
steps = 157, loss = 3.3156251907348633
steps = 157, loss = 2.9286351203918457
steps = 157, loss = 3.0306599140167236
steps = 157, loss = 3.3697073459625244
steps = 157, loss = 3.455396890640259
steps = 157, loss = 2.160567283630371
steps = 157, loss = 6.6273627281188965
steps = 157, loss = 2.5832884311676025
steps = 157, loss = 4.250403881072998
steps = 157, loss = 2.8743221759796143
steps = 157, loss = 49.9853630065918
steps = 157, loss = 2.981252431869507
steps = 157, loss = 2.2813782691955566
steps = 157, loss = 2.243826389312744
steps = 157, loss = 2.969918727874756
steps = 157, loss = 2.7304463386535645
steps = 157, loss = 4.142920970916748
steps = 157, loss = 2.7707769870758057
steps = 157, loss = 50.008846282958984
steps = 157, loss = 2.7776408195495605
steps = 157, loss = 3.6006672382354736
steps = 157, loss = 2.7892982959747314
steps = 157, loss = 2.558077573776245
steps = 157, loss = 4.537539482116699
steps = 157, loss = 3.6246469020843506
steps = 157, loss = 3.120692253112793
steps = 157, loss = 2.9527482986450195
steps = 158, loss = 6.7932610511779785
steps = 158, loss = 2.1736578941345215
steps = 158, loss = 2.9747722148895264
steps = 158, loss = 2.5447890758514404
steps = 158, loss = 50.008087158203125
steps = 158, loss = 3.4101474285125732
steps = 158, loss = 3.414160966873169
steps = 158, loss = 3.416412353515625
steps = 158, loss = 2.147801399230957
steps = 158, loss = 2.7370359897613525
steps = 158, loss = 2.8373262882232666
steps = 158, loss = 49.9853630065918
steps = 158, loss = 3.2216920852661133
steps = 158, loss = 2.9198505878448486
steps = 158, loss = 3.3706417083740234
steps = 158, loss = 3.532684564590454
steps = 158, loss = 2.695235252380371
steps = 158, loss = 2.795604705810547
steps = 158, loss = 2.9388034343719482
steps = 158, loss = 2.9582395553588867
steps = 158, loss = 2.1386728286743164
steps = 158, loss = 3.1336588859558105
steps = 158, loss = 4.533111572265625
steps = 158, loss = 3.1513943672180176
steps = 158, loss = 3.585172176361084
steps = 158, loss = 4.14128303527832
steps = 158, loss = 8.941234588623047
steps = 158, loss = 2.572002649307251
steps = 158, loss = 4.248317718505859
steps = 159, loss = 3.088772773742676
steps = 159, loss = 2.7623109817504883
steps = 159, loss = 6.533670902252197
steps = 159, loss = 3.435497283935547
steps = 159, loss = 4.570931911468506
steps = 159, loss = 3.5185747146606445
steps = 159, loss = 2.775228500366211
steps = 159, loss = 3.4824061393737793
steps = 159, loss = 2.8540701866149902
steps = 159, loss = 2.9643821716308594
steps = 159, loss = 2.829054594039917
steps = 159, loss = 2.381150245666504
steps = 159, loss = 3.1955742835998535
steps = 159, loss = 2.048067331314087
steps = 159, loss = 3.4796712398529053
steps = 159, loss = 49.9853630065918
steps = 159, loss = 3.349058151245117
steps = 159, loss = 1.919219732284546
steps = 159, loss = 2.203402280807495
steps = 159, loss = 4.266148567199707
steps = 159, loss = 2.9335250854492188
steps = 159, loss = 2.712388753890991
steps = 159, loss = 2.6932549476623535
steps = 159, loss = 3.4093739986419678
steps = 159, loss = 3.293696641921997
steps = 159, loss = 4.146468639373779
steps = 159, loss = 3.6722447872161865
steps = 159, loss = 2.7569990158081055
steps = 159, loss = 50.006412506103516
steps = 160, loss = 6.615705490112305
steps = 160, loss = 3.6197075843811035
steps = 160, loss = 3.596998691558838
steps = 160, loss = 3.3959507942199707
steps = 160, loss = 4.590738773345947
steps = 160, loss = 2.0768048763275146
steps = 160, loss = 2.9383718967437744
steps = 160, loss = 4.153616428375244
steps = 160, loss = 3.512934923171997
steps = 160, loss = 3.021627187728882
steps = 160, loss = 2.8544788360595703
steps = 160, loss = 2.963698625564575
steps = 160, loss = 2.735555410385132
steps = 160, loss = 3.0683705806732178
steps = 160, loss = 2.712332248687744
steps = 160, loss = 2.6881089210510254
steps = 160, loss = 2.760653257369995
steps = 160, loss = 3.0246362686157227
steps = 160, loss = 3.4880876541137695
steps = 160, loss = 2.1238491535186768
steps = 160, loss = 3.548319101333618
steps = 160, loss = 4.278449535369873
steps = 160, loss = 50.00716781616211
steps = 160, loss = 3.4105162620544434
steps = 160, loss = 3.6258187294006348
steps = 160, loss = 2.4065017700195312
steps = 160, loss = 2.1415047645568848
steps = 160, loss = 2.910071849822998
steps = 160, loss = 49.9853630065918
steps = 161, loss = 2.711949348449707
steps = 161, loss = 4.16070556640625
steps = 161, loss = 3.033586025238037
steps = 161, loss = 3.154783010482788
steps = 161, loss = 2.4449737071990967
steps = 161, loss = 2.7375895977020264
steps = 161, loss = 4.282492637634277
steps = 161, loss = 2.663386583328247
steps = 161, loss = 3.6800336837768555
steps = 161, loss = 3.5884599685668945
steps = 161, loss = 4.199473857879639
steps = 161, loss = 2.1409518718719482
steps = 161, loss = 3.3879566192626953
steps = 161, loss = 2.834671974182129
steps = 161, loss = 1.9252907037734985
steps = 161, loss = 3.3882436752319336
steps = 161, loss = 2.6917924880981445
steps = 161, loss = 1.9966729879379272
steps = 161, loss = 2.607002019882202
steps = 161, loss = 2.9304652214050293
steps = 161, loss = 2.9551303386688232
steps = 161, loss = 2.6684014797210693
steps = 161, loss = 49.9853630065918
steps = 161, loss = 3.238659381866455
steps = 161, loss = 3.6464626789093018
steps = 161, loss = 4.583914756774902
steps = 161, loss = 50.001976013183594
steps = 161, loss = 2.6376521587371826
steps = 161, loss = 6.6033711433410645
steps = 162, loss = 4.280005931854248
steps = 162, loss = 3.523458480834961
steps = 162, loss = 2.9511799812316895
steps = 162, loss = 2.645021677017212
steps = 162, loss = 2.967130422592163
steps = 162, loss = 3.027885913848877
steps = 162, loss = 2.7719953060150146
steps = 162, loss = 2.14900279045105
steps = 162, loss = 3.5135014057159424
steps = 162, loss = 3.4160544872283936
steps = 162, loss = 4.305361747741699
steps = 162, loss = 4.62011194229126
steps = 162, loss = 3.376516103744507
steps = 162, loss = 3.593384265899658
steps = 162, loss = 3.417145252227783
steps = 162, loss = 49.9853630065918
steps = 162, loss = 2.2853283882141113
steps = 162, loss = 2.9462926387786865
steps = 162, loss = 2.8726320266723633
steps = 162, loss = 3.173379421234131
steps = 162, loss = 2.2594752311706543
steps = 162, loss = 4.180078983306885
steps = 162, loss = 3.5273170471191406
steps = 162, loss = 6.6978302001953125
steps = 162, loss = 2.576700448989868
steps = 162, loss = 3.0709266662597656
steps = 162, loss = 2.730060338973999
steps = 162, loss = 49.92323303222656
steps = 162, loss = 2.6622743606567383
steps = 163, loss = 50.036521911621094
steps = 163, loss = 2.7311933040618896
steps = 163, loss = 2.145040512084961
steps = 163, loss = 3.010709762573242
steps = 163, loss = 2.8754611015319824
steps = 163, loss = 3.3990893363952637
steps = 163, loss = 2.2822980880737305
steps = 163, loss = 2.6206612586975098
steps = 163, loss = 3.3730392456054688
steps = 163, loss = 4.190842151641846
steps = 163, loss = 2.630747079849243
steps = 163, loss = 3.658229351043701
steps = 163, loss = 2.9519052505493164
steps = 163, loss = 3.5373125076293945
steps = 163, loss = 6.72987699508667
steps = 163, loss = 2.6753673553466797
steps = 163, loss = 4.317063808441162
steps = 163, loss = 50.01622772216797
steps = 163, loss = 4.077422142028809
steps = 163, loss = 2.63871169090271
steps = 163, loss = 2.943035364151001
steps = 163, loss = 3.3846194744110107
steps = 163, loss = 4.638799667358398
steps = 163, loss = 2.9686317443847656
steps = 163, loss = 3.5567750930786133
steps = 163, loss = 2.266728401184082
steps = 163, loss = 49.9853630065918
steps = 163, loss = 2.770251750946045
steps = 163, loss = 3.4821395874023438
steps = 164, loss = 2.9641048908233643
steps = 164, loss = 3.5278401374816895
steps = 164, loss = 2.4456307888031006
steps = 164, loss = 2.6145777702331543
steps = 164, loss = 4.1879963874816895
steps = 164, loss = 2.0777347087860107
steps = 164, loss = 2.3029024600982666
steps = 164, loss = 2.953963279724121
steps = 164, loss = 2.85505747795105
steps = 164, loss = 4.3222246170043945
steps = 164, loss = 50.005836486816406
steps = 164, loss = 2.221933603286743
steps = 164, loss = 2.9723899364471436
steps = 164, loss = 3.339116096496582
steps = 164, loss = 3.4416165351867676
steps = 164, loss = 3.444265842437744
steps = 164, loss = 2.713047742843628
steps = 164, loss = 2.5782902240753174
steps = 164, loss = 3.0180704593658447
steps = 164, loss = 49.51689147949219
steps = 164, loss = 3.44816255569458
steps = 164, loss = 49.9853630065918
steps = 164, loss = 3.6121606826782227
steps = 164, loss = 2.7625505924224854
steps = 164, loss = 4.6569623947143555
steps = 164, loss = 2.7173569202423096
steps = 164, loss = 6.755073070526123
steps = 164, loss = 3.3903141021728516
steps = 164, loss = 4.196880340576172
steps = 165, loss = 6.631925582885742
steps = 165, loss = 3.3555266857147217
steps = 165, loss = 3.584272861480713
steps = 165, loss = 2.787430763244629
steps = 165, loss = 2.1388678550720215
steps = 165, loss = 2.3863112926483154
steps = 165, loss = 4.649367332458496
steps = 165, loss = 2.8348770141601562
steps = 165, loss = 2.154905319213867
steps = 165, loss = 2.805819272994995
steps = 165, loss = 2.945626735687256
steps = 165, loss = 2.750429391860962
steps = 165, loss = 4.118954658508301
steps = 165, loss = 2.7410292625427246
steps = 165, loss = 49.46884536743164
steps = 165, loss = 3.376019239425659
steps = 165, loss = 4.193082332611084
steps = 165, loss = 3.3067476749420166
steps = 165, loss = 2.954918622970581
steps = 165, loss = 3.5064659118652344
steps = 165, loss = 3.6153223514556885
steps = 165, loss = 2.0554769039154053
steps = 165, loss = 3.6758627891540527
steps = 165, loss = 2.6922967433929443
steps = 165, loss = 2.621429681777954
steps = 165, loss = 4.3253278732299805
steps = 165, loss = 50.0155029296875
steps = 165, loss = 49.9853630065918
steps = 165, loss = 3.0935721397399902
steps = 166, loss = 49.9853630065918
steps = 166, loss = 3.5230114459991455
steps = 166, loss = 2.966662883758545
steps = 166, loss = 2.648571729660034
steps = 166, loss = 4.685230255126953
steps = 166, loss = 2.9975898265838623
steps = 166, loss = 4.356013774871826
steps = 166, loss = 4.213651657104492
steps = 166, loss = 3.128700017929077
steps = 166, loss = 2.7712907791137695
steps = 166, loss = 3.4666872024536133
steps = 166, loss = 2.284452438354492
steps = 166, loss = 2.961142063140869
steps = 166, loss = 4.348764419555664
steps = 166, loss = 2.2590465545654297
steps = 166, loss = 2.778461456298828
steps = 166, loss = 3.5053701400756836
steps = 166, loss = 2.6519298553466797
steps = 166, loss = 3.5723190307617188
steps = 166, loss = 50.012081146240234
steps = 166, loss = 6.724431991577148
steps = 166, loss = 50.017452239990234
steps = 166, loss = 3.426405191421509
steps = 166, loss = 2.1944849491119385
steps = 166, loss = 2.730350971221924
steps = 166, loss = 3.5490217208862305
steps = 166, loss = 2.8736298084259033
steps = 166, loss = 3.367741823196411
steps = 166, loss = 2.966773271560669
steps = 167, loss = 2.854660749435425
steps = 167, loss = 3.634183645248413
steps = 167, loss = 3.341444253921509
steps = 167, loss = 4.705372333526611
steps = 167, loss = 6.778595447540283
steps = 167, loss = 4.211571216583252
steps = 167, loss = 2.100562810897827
steps = 167, loss = 2.0715274810791016
steps = 167, loss = 50.008209228515625
steps = 167, loss = 2.786036968231201
steps = 167, loss = 2.1947083473205566
steps = 167, loss = 3.6575045585632324
steps = 167, loss = 3.152062177658081
steps = 167, loss = 2.7125136852264404
steps = 167, loss = 4.3547515869140625
steps = 167, loss = 3.524690628051758
steps = 167, loss = 2.9639978408813477
steps = 167, loss = 2.764436721801758
steps = 167, loss = 3.5078916549682617
steps = 167, loss = 50.017845153808594
steps = 167, loss = 2.42759108543396
steps = 167, loss = 3.527299642562866
steps = 167, loss = 49.9853630065918
steps = 167, loss = 4.289804935455322
steps = 167, loss = 3.4377267360687256
steps = 167, loss = 2.8642845153808594
steps = 167, loss = 3.1455228328704834
steps = 167, loss = 2.962806224822998
steps = 167, loss = 2.713188648223877
steps = 168, loss = 49.9853630065918
steps = 168, loss = 2.5757083892822266
steps = 168, loss = 2.04158091545105
steps = 168, loss = 2.6924972534179688
steps = 168, loss = 3.2657008171081543
steps = 168, loss = 4.216998100280762
steps = 168, loss = 4.427507400512695
steps = 168, loss = 2.7571074962615967
steps = 168, loss = 3.144521474838257
steps = 168, loss = 2.1381609439849854
steps = 168, loss = 2.7008769512176514
steps = 168, loss = 2.9538745880126953
steps = 168, loss = 3.404416084289551
steps = 168, loss = 3.2019786834716797
steps = 168, loss = 50.00336456298828
steps = 168, loss = 6.6944475173950195
steps = 168, loss = 4.698731899261475
steps = 168, loss = 2.4545514583587646
steps = 168, loss = 2.95590877532959
steps = 168, loss = 3.3443543910980225
steps = 168, loss = 3.4404947757720947
steps = 168, loss = 2.7390387058258057
steps = 168, loss = 2.834918975830078
steps = 168, loss = 49.83823013305664
steps = 168, loss = 3.392002820968628
steps = 168, loss = 2.966486692428589
steps = 168, loss = 4.358043670654297
steps = 168, loss = 3.10333514213562
steps = 168, loss = 2.1477198600769043
steps = 169, loss = 2.771676778793335
steps = 169, loss = 49.9853630065918
steps = 169, loss = 2.7367544174194336
steps = 169, loss = 3.656670331954956
steps = 169, loss = 50.012977600097656
steps = 169, loss = 4.733834743499756
steps = 169, loss = 3.4713046550750732
steps = 169, loss = 2.2652714252471924
steps = 169, loss = 2.873396158218384
steps = 169, loss = 2.9656622409820557
steps = 169, loss = 3.4645156860351562
steps = 169, loss = 2.80525803565979
steps = 169, loss = 2.16817307472229
steps = 169, loss = 3.1139402389526367
steps = 169, loss = 4.380353927612305
steps = 169, loss = 49.9830436706543
steps = 169, loss = 3.4881789684295654
steps = 169, loss = 2.9714245796203613
steps = 169, loss = 3.4270668029785156
steps = 169, loss = 2.8734843730926514
steps = 169, loss = 2.2837107181549072
steps = 169, loss = 6.821903228759766
steps = 169, loss = 3.3889317512512207
steps = 169, loss = 4.236995697021484
steps = 169, loss = 4.6931023597717285
steps = 169, loss = 2.7305281162261963
steps = 169, loss = 2.7021267414093018
steps = 169, loss = 2.5771214962005615
steps = 169, loss = 3.0481817722320557
steps = 170, loss = 4.38607120513916
steps = 170, loss = 12.676243782043457
steps = 170, loss = 49.9853630065918
steps = 170, loss = 3.381357192993164
steps = 170, loss = 2.8116860389709473
steps = 170, loss = 2.96186900138855
steps = 170, loss = 2.764014720916748
steps = 170, loss = 6.72390604019165
steps = 170, loss = 3.497474431991577
steps = 170, loss = 2.480761766433716
steps = 170, loss = 3.330068826675415
steps = 170, loss = 3.3788764476776123
steps = 170, loss = 2.8545944690704346
steps = 170, loss = 2.9077088832855225
steps = 170, loss = 50.0062141418457
steps = 170, loss = 4.235449314117432
steps = 170, loss = 3.260399103164673
steps = 170, loss = 2.2051525115966797
steps = 170, loss = 2.6977121829986572
steps = 170, loss = 14.338594436645508
steps = 170, loss = 2.713404893875122
steps = 170, loss = 2.9831433296203613
steps = 170, loss = 3.0637714862823486
steps = 170, loss = 49.87311935424805
steps = 170, loss = 2.9741439819335938
steps = 170, loss = 2.159822463989258
steps = 170, loss = 3.451606273651123
steps = 170, loss = 3.3860106468200684
steps = 170, loss = 4.75446891784668
steps = 171, loss = 3.4633657932281494
steps = 171, loss = 2.6927108764648438
steps = 171, loss = 49.9853630065918
steps = 171, loss = 2.952981472015381
steps = 171, loss = 6.892306327819824
steps = 171, loss = 4.240992069244385
steps = 171, loss = 2.9705491065979004
steps = 171, loss = 3.5599727630615234
steps = 171, loss = 2.7249321937561035
steps = 171, loss = 2.931807041168213
steps = 171, loss = 2.7392261028289795
steps = 171, loss = 2.139554977416992
steps = 171, loss = 2.099838972091675
steps = 171, loss = 3.678818941116333
steps = 171, loss = 3.917518138885498
steps = 171, loss = 4.747734069824219
steps = 171, loss = 2.6569929122924805
steps = 171, loss = 3.2929651737213135
steps = 171, loss = 7.218460559844971
steps = 171, loss = 2.397814989089966
steps = 171, loss = 3.311131238937378
steps = 171, loss = 3.5144100189208984
steps = 171, loss = 4.389408111572266
steps = 171, loss = 2.4676501750946045
steps = 171, loss = 50.00775146484375
steps = 171, loss = 2.966081142425537
steps = 171, loss = 2.8349616527557373
steps = 171, loss = 2.7309062480926514
steps = 171, loss = 3.4485106468200684
steps = 172, loss = 2.7307322025299072
steps = 172, loss = 2.8734312057495117
steps = 172, loss = 3.51305890083313
steps = 172, loss = 3.1029324531555176
steps = 172, loss = 2.580043315887451
steps = 172, loss = 2.8078510761260986
steps = 172, loss = 2.9814908504486084
steps = 172, loss = 2.7717463970184326
steps = 172, loss = 3.5724360942840576
steps = 172, loss = 3.401749610900879
steps = 172, loss = 2.7209322452545166
steps = 172, loss = 3.001784086227417
steps = 172, loss = 2.816748857498169
steps = 172, loss = 2.9647622108459473
steps = 172, loss = 6.6807990074157715
steps = 172, loss = 2.0575170516967773
steps = 172, loss = 3.4271161556243896
steps = 172, loss = 3.314319610595703
steps = 172, loss = 2.9724836349487305
steps = 172, loss = 2.228996992111206
steps = 172, loss = 4.411782264709473
steps = 172, loss = 2.990570306777954
steps = 172, loss = 4.2610578536987305
steps = 172, loss = 2.2845029830932617
steps = 172, loss = 4.782903671264648
steps = 172, loss = 49.9853630065918
steps = 172, loss = 3.355212450027466
steps = 172, loss = 3.4503729343414307
steps = 172, loss = 50.003570556640625
steps = 173, loss = 3.460831642150879
steps = 173, loss = 2.992356538772583
steps = 173, loss = 2.9671692848205566
steps = 173, loss = 2.713549852371216
steps = 173, loss = 2.079620122909546
steps = 173, loss = 2.694716691970825
steps = 173, loss = 4.2596540451049805
steps = 173, loss = 2.764185905456543
steps = 173, loss = 3.400489091873169
steps = 173, loss = 3.013824939727783
steps = 173, loss = 3.6771254539489746
steps = 173, loss = 3.513786554336548
steps = 173, loss = 2.204634189605713
steps = 173, loss = 2.85455584526062
steps = 173, loss = 3.011173963546753
steps = 173, loss = 4.417627811431885
steps = 173, loss = 2.8500020503997803
steps = 173, loss = 2.961003303527832
steps = 173, loss = 6.744626522064209
steps = 173, loss = 2.088505744934082
steps = 173, loss = 3.674989938735962
steps = 173, loss = 49.9853630065918
steps = 173, loss = 2.513650894165039
steps = 173, loss = 3.536583662033081
steps = 173, loss = 2.7966270446777344
steps = 173, loss = 2.98419451713562
steps = 173, loss = 4.804314613342285
steps = 173, loss = 3.341048002243042
steps = 173, loss = 50.00164031982422
steps = 174, loss = 3.2185778617858887
steps = 174, loss = 3.546905517578125
steps = 174, loss = 1.9853473901748657
steps = 174, loss = 6.732807159423828
steps = 174, loss = 49.9853630065918
steps = 174, loss = 2.6414949893951416
steps = 174, loss = 2.139632225036621
steps = 174, loss = 3.0707075595855713
steps = 174, loss = 2.361656904220581
steps = 174, loss = 2.692903995513916
steps = 174, loss = 2.684344530105591
steps = 174, loss = 4.264791488647461
steps = 174, loss = 3.208035469055176
steps = 174, loss = 2.6945314407348633
steps = 174, loss = 3.2931509017944336
steps = 174, loss = 3.441587209701538
steps = 174, loss = 50.00748825073242
steps = 174, loss = 3.072007656097412
steps = 174, loss = 4.797247886657715
steps = 174, loss = 2.553412675857544
steps = 174, loss = 2.952160358428955
steps = 174, loss = 2.012115001678467
steps = 174, loss = 3.379672050476074
steps = 174, loss = 2.834998369216919
steps = 174, loss = 4.420822620391846
steps = 174, loss = 2.9760026931762695
steps = 174, loss = 2.7395544052124023
steps = 174, loss = 2.7183077335357666
steps = 174, loss = 3.4426302909851074
steps = 175, loss = 4.437010288238525
steps = 175, loss = 2.0864713191986084
steps = 175, loss = 3.320310115814209
steps = 175, loss = 3.5287487506866455
steps = 175, loss = 2.464298963546753
steps = 175, loss = 49.9853630065918
steps = 175, loss = 3.5001726150512695
steps = 175, loss = 2.713606595993042
steps = 175, loss = 3.1968624591827393
steps = 175, loss = 3.6775851249694824
steps = 175, loss = 2.7474756240844727
steps = 175, loss = 2.5148873329162598
steps = 175, loss = 3.366027593612671
steps = 175, loss = 2.087122678756714
steps = 175, loss = 2.853853225708008
steps = 175, loss = 4.272899627685547
steps = 175, loss = 3.5867371559143066
steps = 175, loss = 2.747793674468994
steps = 175, loss = 4.835622310638428
steps = 175, loss = 3.603245973587036
steps = 175, loss = 6.793589115142822
steps = 175, loss = 3.0515711307525635
steps = 175, loss = 2.9891858100891113
steps = 175, loss = 2.257845640182495
steps = 175, loss = 50.00703048706055
steps = 175, loss = 2.7604856491088867
steps = 175, loss = 2.9589171409606934
steps = 175, loss = 3.1470093727111816
steps = 175, loss = 3.4075303077697754
steps = 176, loss = 2.9632344245910645
steps = 176, loss = 3.452207565307617
steps = 176, loss = 2.729862689971924
steps = 176, loss = 3.5605368614196777
steps = 176, loss = 2.9960289001464844
steps = 176, loss = 2.6216511726379395
steps = 176, loss = 3.1560211181640625
steps = 176, loss = 2.95967960357666
steps = 176, loss = 2.274930715560913
steps = 176, loss = 3.4603936672210693
steps = 176, loss = 3.4377639293670654
steps = 176, loss = 3.491523265838623
steps = 176, loss = 3.3202481269836426
steps = 176, loss = 2.5677688121795654
steps = 176, loss = 2.7708418369293213
steps = 176, loss = 2.872293472290039
steps = 176, loss = 49.9853630065918
steps = 176, loss = 2.1418213844299316
steps = 176, loss = 4.454214572906494
steps = 176, loss = 4.851731777191162
steps = 176, loss = 2.6258976459503174
steps = 176, loss = 50.00559616088867
steps = 176, loss = 3.917353868484497
steps = 176, loss = 2.6309077739715576
steps = 176, loss = 3.3960821628570557
steps = 176, loss = 3.4451801776885986
steps = 176, loss = 2.2903027534484863
steps = 176, loss = 6.815162181854248
steps = 176, loss = 4.291608810424805
steps = 177, loss = 3.8574259281158447
steps = 177, loss = 3.4264414310455322
steps = 177, loss = 3.156630754470825
steps = 177, loss = 2.6944077014923096
steps = 177, loss = 3.502962350845337
steps = 177, loss = 3.0317676067352295
steps = 177, loss = 3.4374186992645264
steps = 177, loss = 4.451177597045898
steps = 177, loss = 2.722426414489746
steps = 177, loss = 49.9853630065918
steps = 177, loss = 4.2895917892456055
steps = 177, loss = 3.592033624649048
steps = 177, loss = 2.9861795902252197
steps = 177, loss = 2.761533498764038
steps = 177, loss = 2.951888084411621
steps = 177, loss = 2.1388587951660156
steps = 177, loss = 2.0595831871032715
steps = 177, loss = 2.010349988937378
steps = 177, loss = 2.688119888305664
steps = 177, loss = 3.5158090591430664
steps = 177, loss = 2.739722728729248
steps = 177, loss = 6.880029201507568
steps = 177, loss = 2.6049644947052
steps = 177, loss = 4.8477783203125
steps = 177, loss = 3.340961456298828
steps = 177, loss = 50.00571823120117
steps = 177, loss = 2.8359792232513428
steps = 177, loss = 3.6919524669647217
steps = 177, loss = 2.4066128730773926
steps = 178, loss = 2.741609811782837
steps = 178, loss = 4.886401653289795
steps = 178, loss = 2.713770627975464
steps = 178, loss = 2.1786649227142334
steps = 178, loss = 2.7207067012786865
steps = 178, loss = 3.447279691696167
steps = 178, loss = 3.4032435417175293
steps = 178, loss = 3.417649745941162
steps = 178, loss = 2.9584217071533203
steps = 178, loss = 4.297995090484619
steps = 178, loss = 49.9853630065918
steps = 178, loss = 3.551762342453003
steps = 178, loss = 3.4310503005981445
steps = 178, loss = 49.95180892944336
steps = 178, loss = 4.468149185180664
steps = 178, loss = 6.913933277130127
steps = 178, loss = 2.8491404056549072
steps = 178, loss = 3.494259834289551
steps = 178, loss = 2.4475901126861572
steps = 178, loss = 2.7641541957855225
steps = 178, loss = 3.348646640777588
steps = 178, loss = 2.7854435443878174
steps = 178, loss = 3.1198229789733887
steps = 178, loss = 2.0742392539978027
steps = 178, loss = 3.260016918182373
steps = 178, loss = 2.9990925788879395
steps = 178, loss = 4.205795764923096
steps = 178, loss = 2.166206121444702
steps = 178, loss = 2.853895902633667
steps = 179, loss = 3.005872964859009
steps = 179, loss = 2.96262526512146
steps = 179, loss = 2.946051597595215
steps = 179, loss = 3.5154647827148438
steps = 179, loss = 49.9853630065918
steps = 179, loss = 4.901796340942383
steps = 179, loss = 4.484501361846924
steps = 179, loss = 3.42236065864563
steps = 179, loss = 50.0127067565918
steps = 179, loss = 2.290025234222412
steps = 179, loss = 3.33781099319458
steps = 179, loss = 2.6930630207061768
steps = 179, loss = 3.5937106609344482
steps = 179, loss = 4.316405773162842
steps = 179, loss = 2.770777940750122
steps = 179, loss = 3.493607759475708
steps = 179, loss = 3.4263272285461426
steps = 179, loss = 2.8725903034210205
steps = 179, loss = 2.730039358139038
steps = 179, loss = 4.004703521728516
steps = 179, loss = 6.853961944580078
steps = 179, loss = 2.945096254348755
steps = 179, loss = 3.6799449920654297
steps = 179, loss = 2.963228464126587
steps = 179, loss = 2.613051176071167
steps = 179, loss = 2.1277990341186523
steps = 179, loss = 2.596780776977539
steps = 179, loss = 2.242288112640381
steps = 179, loss = 3.334047317504883
steps = 180, loss = 7.023717880249023
steps = 180, loss = 3.3011107444763184
steps = 180, loss = 2.9513144493103027
steps = 180, loss = 2.69473934173584
steps = 180, loss = 3.041482448577881
steps = 180, loss = 3.4346015453338623
steps = 180, loss = 3.299053192138672
steps = 180, loss = 2.911215305328369
steps = 180, loss = 3.651259660720825
steps = 180, loss = 2.216308832168579
steps = 180, loss = 3.586984872817993
steps = 180, loss = 3.205615520477295
steps = 180, loss = 2.836155652999878
steps = 180, loss = 2.8093314170837402
steps = 180, loss = 2.721813678741455
steps = 180, loss = 4.315835475921631
steps = 180, loss = 49.9853630065918
steps = 180, loss = 2.740370512008667
steps = 180, loss = 2.4660284519195557
steps = 180, loss = 1.9404150247573853
steps = 180, loss = 4.897833824157715
steps = 180, loss = 2.138889789581299
steps = 180, loss = 3.519906759262085
steps = 180, loss = 2.9959864616394043
steps = 180, loss = 2.574413776397705
steps = 180, loss = 3.3470489978790283
steps = 180, loss = 3.3144354820251465
steps = 180, loss = 4.480953216552734
steps = 180, loss = 50.01333236694336
steps = 181, loss = 3.6380064487457275
steps = 181, loss = 4.5027079582214355
steps = 181, loss = 3.4559247493743896
steps = 181, loss = 3.011962413787842
steps = 181, loss = 2.7314484119415283
steps = 181, loss = 3.8439390659332275
steps = 181, loss = 3.011160373687744
steps = 181, loss = 3.593932628631592
steps = 181, loss = 50.009220123291016
steps = 181, loss = 4.336171627044678
steps = 181, loss = 2.8742029666900635
steps = 181, loss = 6.745604038238525
steps = 181, loss = 2.7720706462860107
steps = 181, loss = 2.2274417877197266
steps = 181, loss = 2.8267245292663574
steps = 181, loss = 49.9853630065918
steps = 181, loss = 3.6895461082458496
steps = 181, loss = 3.05489182472229
steps = 181, loss = 2.2774977684020996
steps = 181, loss = 2.625967264175415
steps = 181, loss = 3.411954879760742
steps = 181, loss = 3.4794375896453857
steps = 181, loss = 3.478360414505005
steps = 181, loss = 2.985954523086548
steps = 181, loss = 2.58064603805542
steps = 181, loss = 3.401021957397461
steps = 181, loss = 4.931725978851318
steps = 181, loss = 2.962711811065674
steps = 181, loss = 2.13429594039917
steps = 182, loss = 3.3538970947265625
steps = 182, loss = 6.808065891265869
steps = 182, loss = 50.001224517822266
steps = 182, loss = 3.4248907566070557
steps = 182, loss = 2.2438573837280273
steps = 182, loss = 2.590169668197632
steps = 182, loss = 2.387152671813965
steps = 182, loss = 3.6389286518096924
steps = 182, loss = 2.282957077026367
steps = 182, loss = 2.7401328086853027
steps = 182, loss = 4.929142475128174
steps = 182, loss = 3.1129767894744873
steps = 182, loss = 2.6964175701141357
steps = 182, loss = 2.138983964920044
steps = 182, loss = 3.6129000186920166
steps = 182, loss = 3.498960256576538
steps = 182, loss = 3.8519504070281982
steps = 182, loss = 3.0019404888153076
steps = 182, loss = 3.4355111122131348
steps = 182, loss = 2.8374857902526855
steps = 182, loss = 2.85662841796875
steps = 182, loss = 2.9516446590423584
steps = 182, loss = 4.499866962432861
steps = 182, loss = 3.4634344577789307
steps = 182, loss = 3.3800456523895264
steps = 182, loss = 4.335446834564209
steps = 182, loss = 3.0391180515289307
steps = 182, loss = 2.702535629272461
steps = 182, loss = 49.9853630065918
steps = 183, loss = 2.853748321533203
steps = 183, loss = 3.491717576980591
steps = 183, loss = 4.516736030578613
steps = 183, loss = 3.3822875022888184
steps = 183, loss = 3.3532159328460693
steps = 183, loss = 3.672748565673828
steps = 183, loss = 6.868688106536865
steps = 183, loss = 3.8978400230407715
steps = 183, loss = 2.4859819412231445
steps = 183, loss = 2.764932632446289
steps = 183, loss = 50.00517654418945
steps = 183, loss = 4.966925144195557
steps = 183, loss = 2.070082902908325
steps = 183, loss = 49.9853630065918
steps = 183, loss = 2.6942451000213623
steps = 183, loss = 2.2551238536834717
steps = 183, loss = 2.4467689990997314
steps = 183, loss = 4.341183662414551
steps = 183, loss = 7.940268039703369
steps = 183, loss = 2.910501003265381
steps = 183, loss = 3.5659146308898926
steps = 183, loss = 3.0147576332092285
steps = 183, loss = 3.4693546295166016
steps = 183, loss = 2.830040454864502
steps = 183, loss = 2.713928699493408
steps = 183, loss = 3.111684560775757
steps = 183, loss = 2.1977221965789795
steps = 183, loss = 2.957892894744873
steps = 183, loss = 3.539659023284912
steps = 184, loss = 2.9619951248168945
steps = 184, loss = 2.8345046043395996
steps = 184, loss = 2.6626641750335693
steps = 184, loss = 4.360633373260498
steps = 184, loss = 3.507918357849121
steps = 184, loss = 2.8731279373168945
steps = 184, loss = 2.3238120079040527
steps = 184, loss = 3.9517080783843994
steps = 184, loss = 2.730523109436035
steps = 184, loss = 2.897291421890259
steps = 184, loss = 3.310859203338623
steps = 184, loss = 3.342184543609619
steps = 184, loss = 4.981965065002441
steps = 184, loss = 6.878673553466797
steps = 184, loss = 3.4862539768218994
steps = 184, loss = 3.0215680599212646
steps = 184, loss = 2.7714288234710693
steps = 184, loss = 4.533005237579346
steps = 184, loss = 50.00138473510742
steps = 184, loss = 2.624082326889038
steps = 184, loss = 3.424816131591797
steps = 184, loss = 2.1439402103424072
steps = 184, loss = 2.2900073528289795
steps = 184, loss = 2.988330841064453
steps = 184, loss = 49.9853630065918
steps = 184, loss = 3.4317939281463623
steps = 184, loss = 2.9723567962646484
steps = 184, loss = 2.804112434387207
steps = 184, loss = 3.4841670989990234
steps = 185, loss = 4.359498977661133
steps = 185, loss = 2.6956212520599365
steps = 185, loss = 2.589679002761841
steps = 185, loss = 2.9507505893707275
steps = 185, loss = 2.0478827953338623
steps = 185, loss = 3.0117027759552
steps = 185, loss = 2.423949956893921
steps = 185, loss = 2.7670557498931885
steps = 185, loss = 3.4615671634674072
steps = 185, loss = 6.8616108894348145
steps = 185, loss = 2.836775541305542
steps = 185, loss = 2.1382977962493896
steps = 185, loss = 3.5720279216766357
steps = 185, loss = 3.8259940147399902
steps = 185, loss = 3.2136099338531494
steps = 185, loss = 4.52962064743042
steps = 185, loss = 3.435584783554077
steps = 185, loss = 4.978510856628418
steps = 185, loss = 49.888919830322266
steps = 185, loss = 49.9853630065918
steps = 185, loss = 3.3567404747009277
steps = 185, loss = 3.1921067237854004
steps = 185, loss = 2.741325616836548
steps = 185, loss = 2.885100841522217
steps = 185, loss = 2.061652660369873
steps = 185, loss = 2.6476728916168213
steps = 185, loss = 3.6557462215423584
steps = 185, loss = 3.0205109119415283
steps = 185, loss = 3.357717514038086
steps = 186, loss = 4.545980930328369
steps = 186, loss = 2.805124282836914
steps = 186, loss = 3.474680185317993
steps = 186, loss = 1.9416004419326782
steps = 186, loss = 6.853860378265381
steps = 186, loss = 4.365823745727539
steps = 186, loss = 3.0159552097320557
steps = 186, loss = 2.7652792930603027
steps = 186, loss = 3.324415683746338
steps = 186, loss = 5.017282962799072
steps = 186, loss = 4.058512210845947
steps = 186, loss = 49.9853630065918
steps = 186, loss = 3.3137338161468506
steps = 186, loss = 2.086942195892334
steps = 186, loss = 3.083876371383667
steps = 186, loss = 3.557379722595215
steps = 186, loss = 3.4012503623962402
steps = 186, loss = 3.6901490688323975
steps = 186, loss = 49.993709564208984
steps = 186, loss = 3.357604503631592
steps = 186, loss = 3.3229458332061768
steps = 186, loss = 2.8537118434906006
steps = 186, loss = 2.1650607585906982
steps = 186, loss = 2.5192787647247314
steps = 186, loss = 2.7139668464660645
steps = 186, loss = 3.024470329284668
steps = 186, loss = 2.676701307296753
steps = 186, loss = 3.098803758621216
steps = 186, loss = 2.9570698738098145
steps = 187, loss = 50.007389068603516
steps = 187, loss = 3.4384450912475586
steps = 187, loss = 4.195621013641357
steps = 187, loss = 3.4147732257843018
steps = 187, loss = 2.634775161743164
steps = 187, loss = 2.601442337036133
steps = 187, loss = 6.825197696685791
steps = 187, loss = 3.3849785327911377
steps = 187, loss = 2.872969150543213
steps = 187, loss = 2.2831475734710693
steps = 187, loss = 3.6447458267211914
steps = 187, loss = 2.7717907428741455
steps = 187, loss = 2.7306063175201416
steps = 187, loss = 2.1697375774383545
steps = 187, loss = 3.6171934604644775
steps = 187, loss = 2.9195308685302734
steps = 187, loss = 2.144552707672119
steps = 187, loss = 4.384734153747559
steps = 187, loss = 3.4526166915893555
steps = 187, loss = 3.448256492614746
steps = 187, loss = 2.451697826385498
steps = 187, loss = 4.561817169189453
steps = 187, loss = 49.9853630065918
steps = 187, loss = 2.6060807704925537
steps = 187, loss = 2.9611570835113525
steps = 187, loss = 5.032426834106445
steps = 187, loss = 4.031366348266602
steps = 187, loss = 2.9118173122406006
steps = 187, loss = 3.031128406524658
steps = 188, loss = 3.3392884731292725
steps = 188, loss = 2.7141098976135254
steps = 188, loss = 2.137887477874756
steps = 188, loss = 4.3838725090026855
steps = 188, loss = 3.3618149757385254
steps = 188, loss = 2.949969530105591
steps = 188, loss = 3.428623676300049
steps = 188, loss = 3.4095723628997803
steps = 188, loss = 3.248981475830078
steps = 188, loss = 2.6954429149627686
steps = 188, loss = 2.15622878074646
steps = 188, loss = 2.741323471069336
steps = 188, loss = 3.6164989471435547
steps = 188, loss = 3.0210506916046143
steps = 188, loss = 2.8366010189056396
steps = 188, loss = 4.557784080505371
steps = 188, loss = 2.7302892208099365
steps = 188, loss = 50.0103645324707
steps = 188, loss = 6.874669551849365
steps = 188, loss = 3.301548957824707
steps = 188, loss = 2.5906014442443848
steps = 188, loss = 3.559413194656372
steps = 188, loss = 47.27445983886719
steps = 188, loss = 49.9853630065918
steps = 188, loss = 3.076904296875
steps = 188, loss = 3.0266220569610596
steps = 188, loss = 5.029557228088379
steps = 188, loss = 2.4362754821777344
steps = 188, loss = 4.5904316902160645
steps = 189, loss = 4.573716640472412
steps = 189, loss = 2.853677988052368
steps = 189, loss = 2.272627353668213
steps = 189, loss = 3.033618688583374
steps = 189, loss = 5.06831693649292
steps = 189, loss = 2.956319808959961
steps = 189, loss = 2.8311688899993896
steps = 189, loss = 2.8095338344573975
steps = 189, loss = 2.7193009853363037
steps = 189, loss = 3.656200885772705
steps = 189, loss = 3.532045602798462
steps = 189, loss = 3.4087071418762207
steps = 189, loss = 3.119403600692749
steps = 189, loss = 49.9853630065918
steps = 189, loss = 3.409851312637329
steps = 189, loss = 3.524167537689209
steps = 189, loss = 3.7850704193115234
steps = 189, loss = 4.33200740814209
steps = 189, loss = 2.021571397781372
steps = 189, loss = 6.908746242523193
steps = 189, loss = 3.6636645793914795
steps = 189, loss = 2.7645103931427
steps = 189, loss = 2.754141092300415
steps = 189, loss = 4.39055871963501
steps = 189, loss = 2.5144193172454834
steps = 189, loss = 50.01432418823242
steps = 189, loss = 2.7142293453216553
steps = 189, loss = 2.1395277976989746
steps = 189, loss = 3.539515733718872
steps = 190, loss = 2.687317132949829
steps = 190, loss = 2.7306065559387207
steps = 190, loss = 2.96041202545166
steps = 190, loss = 4.410027027130127
steps = 190, loss = 4.36259651184082
steps = 190, loss = 3.0403523445129395
steps = 190, loss = 2.772020101547241
steps = 190, loss = 2.3811326026916504
steps = 190, loss = 2.8729162216186523
steps = 190, loss = 3.3915281295776367
steps = 190, loss = 3.4162464141845703
steps = 190, loss = 3.602151393890381
steps = 190, loss = 2.70407772064209
steps = 190, loss = 50.00575637817383
steps = 190, loss = 2.0519046783447266
steps = 190, loss = 3.6072075366973877
steps = 190, loss = 3.4234001636505127
steps = 190, loss = 3.4021975994110107
steps = 190, loss = 3.4842164516448975
steps = 190, loss = 4.589696407318115
steps = 190, loss = 2.285585880279541
steps = 190, loss = 2.59726619720459
steps = 190, loss = 6.958744049072266
steps = 190, loss = 5.083397388458252
steps = 190, loss = 3.1355793476104736
steps = 190, loss = 49.9853630065918
steps = 190, loss = 3.142458200454712
steps = 190, loss = 3.029924154281616
steps = 190, loss = 3.330596685409546
steps = 191, loss = 4.408902645111084
steps = 191, loss = 3.4263083934783936
steps = 191, loss = 3.641603946685791
steps = 191, loss = 3.6965816020965576
steps = 191, loss = 3.409508466720581
steps = 191, loss = 6.949197292327881
steps = 191, loss = 2.7618770599365234
steps = 191, loss = 4.518393039703369
steps = 191, loss = 3.0421578884124756
steps = 191, loss = 2.6363351345062256
steps = 191, loss = 5.105339050292969
steps = 191, loss = 2.226550817489624
steps = 191, loss = 2.816723585128784
steps = 191, loss = 2.4630346298217773
steps = 191, loss = 2.7315030097961426
steps = 191, loss = 50.000457763671875
steps = 191, loss = 2.714519500732422
steps = 191, loss = 2.956845283508301
steps = 191, loss = 2.8545219898223877
steps = 191, loss = 2.7643654346466064
steps = 191, loss = 4.594592571258545
steps = 191, loss = 3.448453903198242
steps = 191, loss = 49.9853630065918
steps = 191, loss = 3.5604453086853027
steps = 191, loss = 3.3501434326171875
steps = 191, loss = 2.182267904281616
steps = 191, loss = 3.509417772293091
steps = 191, loss = 2.113804578781128
steps = 191, loss = 3.1675689220428467
steps = 192, loss = 2.693265676498413
steps = 192, loss = 7.0335845947265625
steps = 192, loss = 3.6945712566375732
steps = 192, loss = 2.7864155769348145
steps = 192, loss = 2.8347270488739014
steps = 192, loss = 3.359419107437134
steps = 192, loss = 2.113477945327759
steps = 192, loss = 2.74029541015625
steps = 192, loss = 3.440762758255005
steps = 192, loss = 50.01529312133789
steps = 192, loss = 49.9853630065918
steps = 192, loss = 3.194647789001465
steps = 192, loss = 4.595693588256836
steps = 192, loss = 4.413787841796875
steps = 192, loss = 2.4677586555480957
steps = 192, loss = 2.947955369949341
steps = 192, loss = 3.033208131790161
steps = 192, loss = 2.0279734134674072
steps = 192, loss = 3.3458774089813232
steps = 192, loss = 2.718726396560669
steps = 192, loss = 3.5557613372802734
steps = 192, loss = 2.9896481037139893
steps = 192, loss = 5.09762716293335
steps = 192, loss = 2.1409783363342285
steps = 192, loss = 3.3554227352142334
steps = 192, loss = 2.5951714515686035
steps = 192, loss = 2.6087746620178223
steps = 192, loss = 3.2757861614227295
steps = 192, loss = 4.56308126449585
steps = 193, loss = 3.49314546585083
steps = 193, loss = 2.954763174057007
steps = 193, loss = 2.169140338897705
steps = 193, loss = 2.543440341949463
steps = 193, loss = 3.4250080585479736
steps = 193, loss = 2.3038787841796875
steps = 193, loss = 4.445886611938477
steps = 193, loss = 3.544282913208008
steps = 193, loss = 2.0088918209075928
steps = 193, loss = 3.127135992050171
steps = 193, loss = 2.853675603866577
steps = 193, loss = 3.6639227867126465
steps = 193, loss = 2.760986089706421
steps = 193, loss = 6.755569934844971
steps = 193, loss = 3.0458455085754395
steps = 193, loss = 3.397404432296753
steps = 193, loss = 2.8153417110443115
steps = 193, loss = 3.7153940200805664
steps = 193, loss = 5.135166168212891
steps = 193, loss = 4.611906051635742
steps = 193, loss = 4.421771049499512
steps = 193, loss = 2.8637216091156006
steps = 193, loss = 2.3734068870544434
steps = 193, loss = 49.9853630065918
steps = 193, loss = 50.002925872802734
steps = 193, loss = 3.4403162002563477
steps = 193, loss = 2.7145721912384033
steps = 193, loss = 2.849412441253662
steps = 193, loss = 2.8030149936676025
steps = 194, loss = 2.730539321899414
steps = 194, loss = 2.282724142074585
steps = 194, loss = 3.442274570465088
steps = 194, loss = 6.854625701904297
steps = 194, loss = 2.958868980407715
steps = 194, loss = 3.024829149246216
steps = 194, loss = 2.7719013690948486
steps = 194, loss = 49.9853630065918
steps = 194, loss = 4.602682590484619
steps = 194, loss = 49.951114654541016
steps = 194, loss = 5.149504661560059
steps = 194, loss = 2.141937494277954
steps = 194, loss = 3.5481066703796387
steps = 194, loss = 3.3428702354431152
steps = 194, loss = 2.6343913078308105
steps = 194, loss = 3.479696273803711
steps = 194, loss = 3.052525281906128
steps = 194, loss = 2.8722586631774902
steps = 194, loss = 2.64551043510437
steps = 194, loss = 3.3993287086486816
steps = 194, loss = 2.875006914138794
steps = 194, loss = 2.6245248317718506
steps = 194, loss = 3.4552173614501953
steps = 194, loss = 4.441827774047852
steps = 194, loss = 3.020845890045166
steps = 194, loss = 2.2199060916900635
steps = 194, loss = 4.626618385314941
steps = 194, loss = 3.461395263671875
steps = 194, loss = 2.287661552429199
steps = 195, loss = 3.047623634338379
steps = 195, loss = 3.473902702331543
steps = 195, loss = 2.409121513366699
steps = 195, loss = 2.741179943084717
steps = 195, loss = 3.280837297439575
steps = 195, loss = 4.622661590576172
steps = 195, loss = 2.8916916847229004
steps = 195, loss = 3.692805290222168
steps = 195, loss = 3.3885977268218994
steps = 195, loss = 2.752725839614868
steps = 195, loss = 2.947847604751587
steps = 195, loss = 3.042285680770874
steps = 195, loss = 50.00811767578125
steps = 195, loss = 6.880729675292969
steps = 195, loss = 3.351694345474243
steps = 195, loss = 2.6015212535858154
steps = 195, loss = 2.0488176345825195
steps = 195, loss = 2.022880792617798
steps = 195, loss = 4.440377712249756
steps = 195, loss = 2.138848066329956
steps = 195, loss = 3.4162962436676025
steps = 195, loss = 49.9853630065918
steps = 195, loss = 3.4500529766082764
steps = 195, loss = 4.617776393890381
steps = 195, loss = 2.695082187652588
steps = 195, loss = 2.8359341621398926
steps = 195, loss = 2.8325178623199463
steps = 195, loss = 5.146894454956055
steps = 195, loss = 50.00208282470703
steps = 196, loss = 2.9635982513427734
steps = 196, loss = 3.4267771244049072
steps = 196, loss = 2.573551893234253
steps = 196, loss = 3.0105676651000977
steps = 196, loss = 6.931207656860352
steps = 196, loss = 4.388052463531494
steps = 196, loss = 5.161136627197266
steps = 196, loss = 2.1382715702056885
steps = 196, loss = 2.9945602416992188
steps = 196, loss = 4.449947834014893
steps = 196, loss = 3.43426513671875
steps = 196, loss = 3.283796548843384
steps = 196, loss = 50.01582717895508
steps = 196, loss = 3.68632173538208
steps = 196, loss = 2.9472479820251465
steps = 196, loss = 3.4003992080688477
steps = 196, loss = 4.630532264709473
steps = 196, loss = 49.9853630065918
steps = 196, loss = 3.6094117164611816
steps = 196, loss = 2.0963335037231445
steps = 196, loss = 2.0664443969726562
steps = 196, loss = 3.0431902408599854
steps = 196, loss = 2.7387092113494873
steps = 196, loss = 2.8374290466308594
steps = 196, loss = 3.3263468742370605
steps = 196, loss = 2.450025796890259
steps = 196, loss = 2.7169997692108154
steps = 196, loss = 2.696328639984131
steps = 196, loss = 50.00505828857422
steps = 197, loss = 4.470022678375244
steps = 197, loss = 49.9853630065918
steps = 197, loss = 2.5968871116638184
steps = 197, loss = 2.2824766635894775
steps = 197, loss = 50.00128936767578
steps = 197, loss = 3.489903688430786
steps = 197, loss = 3.50148606300354
steps = 197, loss = 3.0592241287231445
steps = 197, loss = 25.991209030151367
steps = 197, loss = 2.9587771892547607
steps = 197, loss = 50.01222610473633
steps = 197, loss = 2.7324142456054688
steps = 197, loss = 3.4429831504821777
steps = 197, loss = 3.3720955848693848
steps = 197, loss = 2.6780102252960205
steps = 197, loss = 3.4063618183135986
steps = 197, loss = 4.651702880859375
steps = 197, loss = 6.947961807250977
steps = 197, loss = 2.848947286605835
steps = 197, loss = 2.157123327255249
steps = 197, loss = 2.9279544353485107
steps = 197, loss = 2.3230772018432617
steps = 197, loss = 3.630711078643799
steps = 197, loss = 2.709968090057373
steps = 197, loss = 3.3875744342803955
steps = 197, loss = 5.194965839385986
steps = 197, loss = 2.873821496963501
steps = 197, loss = 2.7153398990631104
steps = 197, loss = 2.7737410068511963
steps = 198, loss = 2.5343122482299805
steps = 198, loss = 3.5465922355651855
steps = 198, loss = 2.9554738998413086
steps = 198, loss = 2.0516960620880127
steps = 198, loss = 3.5360913276672363
steps = 198, loss = 3.163144826889038
steps = 198, loss = 3.0617899894714355
steps = 198, loss = 3.4078845977783203
steps = 198, loss = 4.468234062194824
steps = 198, loss = 2.715050220489502
steps = 198, loss = 3.6739909648895264
steps = 198, loss = 3.405972719192505
steps = 198, loss = 2.6217589378356934
steps = 198, loss = 2.562159776687622
steps = 198, loss = 6.986958980560303
steps = 198, loss = 2.854248523712158
steps = 198, loss = 3.458111524581909
steps = 198, loss = 2.1918587684631348
steps = 198, loss = 49.9853630065918
steps = 198, loss = 5.218601226806641
steps = 198, loss = 43.53734588623047
steps = 198, loss = 2.979268789291382
steps = 198, loss = 2.369666814804077
steps = 198, loss = 2.884026288986206
steps = 198, loss = 2.5294222831726074
steps = 198, loss = 2.7667694091796875
steps = 198, loss = 4.6569037437438965
steps = 198, loss = 50.005855560302734
steps = 198, loss = 2.862950563430786
steps = 199, loss = 2.4568920135498047
steps = 199, loss = 2.563246488571167
steps = 199, loss = 2.698399305343628
steps = 199, loss = 3.228541374206543
steps = 199, loss = 3.481372594833374
steps = 199, loss = 4.4738993644714355
steps = 199, loss = 2.2662534713745117
steps = 199, loss = 3.3644535541534424
steps = 199, loss = 2.7622573375701904
steps = 199, loss = 2.8789312839508057
steps = 199, loss = 49.9853630065918
steps = 199, loss = 3.567148208618164
steps = 199, loss = 2.091649293899536
steps = 199, loss = 2.7146971225738525
steps = 199, loss = 4.6661481857299805
steps = 199, loss = 5.553982734680176
steps = 199, loss = 3.085240602493286
steps = 199, loss = 6.833362579345703
steps = 199, loss = 3.0652594566345215
steps = 199, loss = 3.642591953277588
steps = 199, loss = 3.3519046306610107
steps = 199, loss = 2.9544215202331543
steps = 199, loss = 2.603067398071289
steps = 199, loss = 2.1162328720092773
steps = 199, loss = 3.484182357788086
steps = 199, loss = 5.237117767333984
steps = 199, loss = 2.854234457015991
steps = 199, loss = 49.98021697998047
steps = 199, loss = 3.412318706512451
steps = 200, loss = 2.962615728378296
steps = 200, loss = 6.9238409996032715
steps = 200, loss = 2.967474937438965
steps = 200, loss = 3.436641216278076
steps = 200, loss = 2.7711782455444336
steps = 200, loss = 2.730438470840454
steps = 200, loss = 2.9579601287841797
steps = 200, loss = 3.5202226638793945
steps = 200, loss = 2.5836212635040283
steps = 200, loss = 2.980865001678467
steps = 200, loss = 5.249671936035156
steps = 200, loss = 37.7863655090332
steps = 200, loss = 2.3321280479431152
steps = 200, loss = 3.4271702766418457
steps = 200, loss = 2.744971990585327
steps = 200, loss = 2.9242031574249268
steps = 200, loss = 3.2947726249694824
steps = 200, loss = 3.0708699226379395
steps = 200, loss = 4.679605007171631
steps = 200, loss = 3.3451132774353027
steps = 200, loss = 3.4933552742004395
steps = 200, loss = 2.5778346061706543
steps = 200, loss = 2.2218291759490967
steps = 200, loss = 2.8724992275238037
steps = 200, loss = 4.491889476776123
steps = 200, loss = 3.4397311210632324
steps = 200, loss = 50.01191329956055
steps = 200, loss = 2.279832363128662
steps = 200, loss = 49.9853630065918
steps = 201, loss = 6.9642767906188965
steps = 201, loss = 2.742938280105591
steps = 201, loss = 3.320932626724243
steps = 201, loss = 3.386420965194702
steps = 201, loss = 4.6753387451171875
steps = 201, loss = 2.779961585998535
steps = 201, loss = 3.513838768005371
steps = 201, loss = 3.355032205581665
steps = 201, loss = 3.4768528938293457
steps = 201, loss = 2.6950607299804688
steps = 201, loss = 2.8357667922973633
steps = 201, loss = 5.246646404266357
steps = 201, loss = 2.4150094985961914
steps = 201, loss = 3.7159814834594727
steps = 201, loss = 2.723982095718384
steps = 201, loss = 2.9408535957336426
steps = 201, loss = 3.297987937927246
steps = 201, loss = 49.49295425415039
steps = 201, loss = 2.001445770263672
steps = 201, loss = 2.946852684020996
steps = 201, loss = 49.9853630065918
steps = 201, loss = 2.6022491455078125
steps = 201, loss = 2.442173957824707
steps = 201, loss = 2.1201870441436768
steps = 201, loss = 4.491230487823486
steps = 201, loss = 2.8469903469085693
steps = 201, loss = 3.060192346572876
steps = 201, loss = 2.7012367248535156
steps = 201, loss = 2.141953706741333
steps = 202, loss = 2.7319982051849365
steps = 202, loss = 49.9853630065918
steps = 202, loss = 4.694607257843018
steps = 202, loss = 2.874195098876953
steps = 202, loss = 2.973987340927124
steps = 202, loss = 3.4908814430236816
steps = 202, loss = 3.0750365257263184
steps = 202, loss = 3.4454991817474365
steps = 202, loss = 2.1905555725097656
steps = 202, loss = 3.6012308597564697
steps = 202, loss = 2.675621747970581
steps = 202, loss = 2.9580304622650146
steps = 202, loss = 6.973318099975586
steps = 202, loss = 2.98134446144104
steps = 202, loss = 2.2713987827301025
steps = 202, loss = 2.1970677375793457
steps = 202, loss = 3.3743855953216553
steps = 202, loss = 4.5111799240112305
steps = 202, loss = 5.27932071685791
steps = 202, loss = 3.4227733612060547
steps = 202, loss = 3.4194095134735107
steps = 202, loss = 3.3924059867858887
steps = 202, loss = 3.0126101970672607
steps = 202, loss = 2.7729098796844482
steps = 202, loss = 50.02041244506836
steps = 202, loss = 2.644904136657715
steps = 202, loss = 3.031141519546509
steps = 202, loss = 2.9264228343963623
steps = 202, loss = 2.611340284347534
steps = 203, loss = 2.847602605819702
steps = 203, loss = 2.1348838806152344
steps = 203, loss = 50.0174674987793
steps = 203, loss = 3.0653746128082275
steps = 203, loss = 4.691460609436035
steps = 203, loss = 3.371098041534424
steps = 203, loss = 3.1413612365722656
steps = 203, loss = 2.9472644329071045
steps = 203, loss = 2.6969408988952637
steps = 203, loss = 2.262216329574585
steps = 203, loss = 2.1203415393829346
steps = 203, loss = 5.278356075286865
steps = 203, loss = 2.572899580001831
steps = 203, loss = 3.7156834602355957
steps = 203, loss = 2.4243063926696777
steps = 203, loss = 2.017454147338867
steps = 203, loss = 4.512338161468506
steps = 203, loss = 7.097236633300781
steps = 203, loss = 3.2600834369659424
steps = 203, loss = 3.637697219848633
steps = 203, loss = 3.3755686283111572
steps = 203, loss = 2.6440649032592773
steps = 203, loss = 2.7416670322418213
steps = 203, loss = 3.5376296043395996
steps = 203, loss = 2.801215648651123
steps = 203, loss = 49.9853630065918
steps = 203, loss = 2.8041775226593018
steps = 203, loss = 3.3893697261810303
steps = 203, loss = 2.8373401165008545
steps = 204, loss = 3.6224465370178223
steps = 204, loss = 3.378793239593506
steps = 204, loss = 5.316793441772461
steps = 204, loss = 2.955369710922241
steps = 204, loss = 3.078108310699463
steps = 204, loss = 6.8312764167785645
steps = 204, loss = 2.16208553314209
steps = 204, loss = 2.242443561553955
steps = 204, loss = 2.3488123416900635
steps = 204, loss = 4.706830024719238
steps = 204, loss = 2.984846591949463
steps = 204, loss = 2.6765191555023193
steps = 204, loss = 2.8895137310028076
steps = 204, loss = 2.9536402225494385
steps = 204, loss = 2.764746904373169
steps = 204, loss = 4.516613006591797
steps = 204, loss = 2.761075735092163
steps = 204, loss = 3.389820098876953
steps = 204, loss = 49.988826751708984
steps = 204, loss = 2.11734676361084
steps = 204, loss = 3.3863136768341064
steps = 204, loss = 2.6989338397979736
steps = 204, loss = 3.4910786151885986
steps = 204, loss = 2.714982509613037
steps = 204, loss = 2.8535845279693604
steps = 204, loss = 2.310967445373535
steps = 204, loss = 3.4414403438568115
steps = 204, loss = 3.3770599365234375
steps = 204, loss = 49.9853630065918
steps = 205, loss = 3.0823397636413574
steps = 205, loss = 2.329976797103882
steps = 205, loss = 3.490987539291382
steps = 205, loss = 50.00625228881836
steps = 205, loss = 2.7149555683135986
steps = 205, loss = 2.8732237815856934
steps = 205, loss = 4.522360324859619
steps = 205, loss = 2.6665916442871094
steps = 205, loss = 2.8108744621276855
steps = 205, loss = 2.3521881103515625
steps = 205, loss = 2.749030590057373
steps = 205, loss = 2.763760805130005
steps = 205, loss = 4.716662883758545
steps = 205, loss = 2.9530880451202393
steps = 205, loss = 2.9482882022857666
steps = 205, loss = 3.463078737258911
steps = 205, loss = 2.1153171062469482
steps = 205, loss = 5.336700916290283
steps = 205, loss = 2.110581159591675
steps = 205, loss = 3.4709956645965576
steps = 205, loss = 3.5864665508270264
steps = 205, loss = 49.9853630065918
steps = 205, loss = 3.624948263168335
steps = 205, loss = 2.716869592666626
steps = 205, loss = 2.051044225692749
steps = 205, loss = 6.877040863037109
steps = 205, loss = 3.6341888904571533
steps = 205, loss = 2.85400390625
steps = 205, loss = 3.4006664752960205
steps = 206, loss = 4.313405990600586
steps = 206, loss = 3.4515957832336426
steps = 206, loss = 2.8072636127471924
steps = 206, loss = 5.330031394958496
steps = 206, loss = 49.9853630065918
steps = 206, loss = 3.384337902069092
steps = 206, loss = 2.100843906402588
steps = 206, loss = 2.775754690170288
steps = 206, loss = 3.4967172145843506
steps = 206, loss = 2.4471898078918457
steps = 206, loss = 3.055778741836548
steps = 206, loss = 6.898991107940674
steps = 206, loss = 50.001827239990234
steps = 206, loss = 3.6490912437438965
steps = 206, loss = 3.0738203525543213
steps = 206, loss = 3.4644460678100586
steps = 206, loss = 4.529566287994385
steps = 206, loss = 2.8348464965820312
steps = 206, loss = 2.142199754714966
steps = 206, loss = 3.4078316688537598
steps = 206, loss = 2.7415990829467773
steps = 206, loss = 2.5429327487945557
steps = 206, loss = 4.717942714691162
steps = 206, loss = 2.0756378173828125
steps = 206, loss = 2.9446749687194824
steps = 206, loss = 2.8660659790039062
steps = 206, loss = 3.4300928115844727
steps = 206, loss = 2.7291460037231445
steps = 206, loss = 2.6938161849975586
steps = 207, loss = 2.651106595993042
steps = 207, loss = 2.9254374504089355
steps = 207, loss = 2.7781362533569336
steps = 207, loss = 3.0888478755950928
steps = 207, loss = 3.6744866371154785
steps = 207, loss = 2.6019763946533203
steps = 207, loss = 4.548244953155518
steps = 207, loss = 2.160804271697998
steps = 207, loss = 3.3478331565856934
steps = 207, loss = 2.872771739959717
steps = 207, loss = 2.448108196258545
steps = 207, loss = 49.9859504699707
steps = 207, loss = 4.737105369567871
steps = 207, loss = 3.4522898197174072
steps = 207, loss = 2.956111192703247
steps = 207, loss = 2.278290271759033
steps = 207, loss = 2.731797218322754
steps = 207, loss = 2.6472067832946777
steps = 207, loss = 3.4129559993743896
steps = 207, loss = 49.9853630065918
steps = 207, loss = 2.5518078804016113
steps = 207, loss = 6.922364711761475
steps = 207, loss = 2.304633140563965
steps = 207, loss = 3.340914011001587
steps = 207, loss = 5.362418174743652
steps = 207, loss = 2.925074338912964
steps = 207, loss = 3.639615774154663
steps = 207, loss = 3.3788180351257324
steps = 207, loss = 2.7732467651367188
steps = 208, loss = 2.773418426513672
steps = 208, loss = 2.390411376953125
steps = 208, loss = 3.093379020690918
steps = 208, loss = 4.561435222625732
steps = 208, loss = 2.791840076446533
steps = 208, loss = 2.9122231006622314
steps = 208, loss = 3.48980450630188
steps = 208, loss = 2.8756444454193115
steps = 208, loss = 2.732923984527588
steps = 208, loss = 2.584761381149292
steps = 208, loss = 2.264474630355835
steps = 208, loss = 2.957711696624756
steps = 208, loss = 6.974638938903809
steps = 208, loss = 3.462351083755493
steps = 208, loss = 4.746140956878662
steps = 208, loss = 2.6408324241638184
steps = 208, loss = 2.144913673400879
steps = 208, loss = 3.4920995235443115
steps = 208, loss = 2.9122204780578613
steps = 208, loss = 2.4840950965881348
steps = 208, loss = 3.0144307613372803
steps = 208, loss = 2.578482151031494
steps = 208, loss = 49.97343444824219
steps = 208, loss = 5.3808746337890625
steps = 208, loss = 3.498080253601074
steps = 208, loss = 3.4924392700195312
steps = 208, loss = 3.5357041358947754
steps = 208, loss = 3.437838077545166
steps = 208, loss = 49.9853630065918
steps = 209, loss = 3.04585599899292
steps = 209, loss = 3.082820177078247
steps = 209, loss = 3.334289073944092
steps = 209, loss = 4.562678337097168
steps = 209, loss = 2.788335084915161
steps = 209, loss = 3.468489646911621
steps = 209, loss = 2.019536256790161
steps = 209, loss = 2.686420440673828
steps = 209, loss = 7.029374599456787
steps = 209, loss = 7.2124481201171875
steps = 209, loss = 3.344219446182251
steps = 209, loss = 3.695669651031494
steps = 209, loss = 2.69728422164917
steps = 209, loss = 50.000919342041016
steps = 209, loss = 2.816559076309204
steps = 209, loss = 3.2488391399383545
steps = 209, loss = 5.37853479385376
steps = 209, loss = 3.466259241104126
steps = 209, loss = 2.837367534637451
steps = 209, loss = 2.781024932861328
steps = 209, loss = 2.9190673828125
steps = 209, loss = 49.9853630065918
steps = 209, loss = 2.62841796875
steps = 209, loss = 4.74233865737915
steps = 209, loss = 2.480876922607422
steps = 209, loss = 2.743232250213623
steps = 209, loss = 2.1335909366607666
steps = 209, loss = 2.9465370178222656
steps = 209, loss = 3.4447836875915527
steps = 210, loss = 3.4710495471954346
steps = 210, loss = 2.8536126613616943
steps = 210, loss = 5.416061878204346
steps = 210, loss = 49.9853630065918
steps = 210, loss = 2.705808162689209
steps = 210, loss = 3.129030704498291
steps = 210, loss = 2.952794313430786
steps = 210, loss = 2.765056610107422
steps = 210, loss = 3.5543787479400635
steps = 210, loss = 4.757599353790283
steps = 210, loss = 2.6894452571868896
steps = 210, loss = 3.427004814147949
steps = 210, loss = 2.7913620471954346
steps = 210, loss = 49.958953857421875
steps = 210, loss = 3.673938274383545
steps = 210, loss = 2.7151005268096924
steps = 210, loss = 2.305449962615967
steps = 210, loss = 3.0816898345947266
steps = 210, loss = 3.3999996185302734
steps = 210, loss = 7.069910049438477
steps = 210, loss = 2.2615931034088135
steps = 210, loss = 2.12522292137146
steps = 210, loss = 3.0952401161193848
steps = 210, loss = 2.9250693321228027
steps = 210, loss = 2.11562180519104
steps = 210, loss = 4.56571102142334
steps = 210, loss = 3.040827512741089
steps = 210, loss = 3.3542556762695312
steps = 210, loss = 2.4879231452941895
steps = 211, loss = 3.6049468517303467
steps = 211, loss = 3.521355390548706
steps = 211, loss = 3.417606830596924
steps = 211, loss = 49.9853630065918
steps = 211, loss = 2.74237060546875
steps = 211, loss = 6.93946647644043
steps = 211, loss = 3.3764426708221436
steps = 211, loss = 2.9806811809539795
steps = 211, loss = 2.1894419193267822
steps = 211, loss = 2.140772581100464
steps = 211, loss = 2.9190189838409424
steps = 211, loss = 2.5060067176818848
steps = 211, loss = 2.8593215942382812
steps = 211, loss = 5.411489486694336
steps = 211, loss = 2.695409059524536
steps = 211, loss = 2.944603204727173
steps = 211, loss = 4.5738396644592285
steps = 211, loss = 3.603999137878418
steps = 211, loss = 4.758383750915527
steps = 211, loss = 2.0864651203155518
steps = 211, loss = 2.314987897872925
steps = 211, loss = 2.75369930267334
steps = 211, loss = 50.003936767578125
steps = 211, loss = 3.0874006748199463
steps = 211, loss = 2.836015462875366
steps = 211, loss = 2.6051831245422363
steps = 211, loss = 3.3765339851379395
steps = 211, loss = 2.9748117923736572
steps = 211, loss = 3.408447027206421
steps = 212, loss = 2.4850637912750244
steps = 212, loss = 3.496598243713379
steps = 212, loss = 3.448841094970703
steps = 212, loss = 4.773690700531006
steps = 212, loss = 3.100126028060913
steps = 212, loss = 3.5781965255737305
steps = 212, loss = 2.9514822959899902
steps = 212, loss = 2.9211978912353516
steps = 212, loss = 3.311140537261963
steps = 212, loss = 3.7297723293304443
steps = 212, loss = 2.8831124305725098
steps = 212, loss = 5.4494757652282715
steps = 212, loss = 3.446115255355835
steps = 212, loss = 2.107518434524536
steps = 212, loss = 2.712334394454956
steps = 212, loss = 2.853595018386841
steps = 212, loss = 3.0049355030059814
steps = 212, loss = 3.1320412158966064
steps = 212, loss = 2.9883768558502197
steps = 212, loss = 2.7051451206207275
steps = 212, loss = 49.999534606933594
steps = 212, loss = 2.7651054859161377
steps = 212, loss = 3.454911708831787
steps = 212, loss = 4.578395366668701
steps = 212, loss = 2.0676562786102295
steps = 212, loss = 2.7153844833374023
steps = 212, loss = 49.9853630065918
steps = 212, loss = 6.969928741455078
steps = 212, loss = 2.3065266609191895
steps = 213, loss = 1.862272024154663
steps = 213, loss = 2.922177791595459
steps = 213, loss = 4.7877936363220215
steps = 213, loss = 2.9683117866516113
steps = 213, loss = 3.2298097610473633
steps = 213, loss = 3.226210594177246
steps = 213, loss = 3.3959579467773438
steps = 213, loss = 2.7312917709350586
steps = 213, loss = 3.6531548500061035
steps = 213, loss = 5.463601589202881
steps = 213, loss = 2.282773017883301
steps = 213, loss = 2.772752285003662
steps = 213, loss = 6.940488338470459
steps = 213, loss = 49.9853630065918
steps = 213, loss = 3.358201026916504
steps = 213, loss = 3.1079204082489014
steps = 213, loss = 49.99692916870117
steps = 213, loss = 2.872495651245117
steps = 213, loss = 4.594842433929443
steps = 213, loss = 3.641929864883423
steps = 213, loss = 2.66422438621521
steps = 213, loss = 3.4746594429016113
steps = 213, loss = 2.1424312591552734
steps = 213, loss = 3.1062159538269043
steps = 213, loss = 2.955352306365967
steps = 213, loss = 2.6287569999694824
steps = 213, loss = 3.329815626144409
steps = 213, loss = 2.44655442237854
steps = 213, loss = 3.424494743347168
steps = 214, loss = 2.185577392578125
steps = 214, loss = 49.9853630065918
steps = 214, loss = 2.773514747619629
steps = 214, loss = 5.481259822845459
steps = 214, loss = 3.4984819889068604
steps = 214, loss = 2.7327077388763428
steps = 214, loss = 3.0400609970092773
steps = 214, loss = 2.8755900859832764
steps = 214, loss = 3.4544484615325928
steps = 214, loss = 3.109863519668579
steps = 214, loss = 6.999983787536621
steps = 214, loss = 2.6371893882751465
steps = 214, loss = 3.0306665897369385
steps = 214, loss = 3.4647648334503174
steps = 214, loss = 3.5908334255218506
steps = 214, loss = 1.9785171747207642
steps = 214, loss = 2.2616629600524902
steps = 214, loss = 4.796275615692139
steps = 214, loss = 2.9567160606384277
steps = 214, loss = 3.439809799194336
steps = 214, loss = 3.5028092861175537
steps = 214, loss = 4.607690334320068
steps = 214, loss = 2.6232473850250244
steps = 214, loss = 2.890256881713867
steps = 214, loss = 50.00230026245117
steps = 214, loss = 3.47387433052063
steps = 214, loss = 3.509796619415283
steps = 214, loss = 50.00404739379883
steps = 214, loss = 2.708399772644043
steps = 215, loss = 3.405545473098755
steps = 215, loss = 3.1108994483947754
steps = 215, loss = 3.4551424980163574
steps = 215, loss = 5.502929210662842
steps = 215, loss = 2.4769513607025146
steps = 215, loss = 3.5094003677368164
steps = 215, loss = 2.715546131134033
steps = 215, loss = 3.0439209938049316
steps = 215, loss = 3.6850717067718506
steps = 215, loss = 2.16528582572937
steps = 215, loss = 2.086744546890259
steps = 215, loss = 2.953179121017456
steps = 215, loss = 4.80015754699707
steps = 215, loss = 7.042551040649414
steps = 215, loss = 49.78996276855469
steps = 215, loss = 3.5806334018707275
steps = 215, loss = 50.00434494018555
steps = 215, loss = 2.8172926902770996
steps = 215, loss = 4.605779647827148
steps = 215, loss = 2.9507663249969482
steps = 215, loss = 2.7657575607299805
steps = 215, loss = 2.612661123275757
steps = 215, loss = 49.9853630065918
steps = 215, loss = 3.309720754623413
steps = 215, loss = 2.0484185218811035
steps = 215, loss = 2.8547322750091553
steps = 215, loss = 3.458695411682129
steps = 215, loss = 2.663088798522949
steps = 215, loss = 3.462217330932617
steps = 216, loss = 2.404707431793213
steps = 216, loss = 3.4765994548797607
steps = 216, loss = 3.0275981426239014
steps = 216, loss = 2.9440741539001465
steps = 216, loss = 4.799990653991699
steps = 216, loss = 3.008570909500122
steps = 216, loss = 2.742007255554199
steps = 216, loss = 4.6124444007873535
steps = 216, loss = 49.440086364746094
steps = 216, loss = 3.4113423824310303
steps = 216, loss = 3.2957494258880615
steps = 216, loss = 3.319502115249634
steps = 216, loss = 3.320728302001953
steps = 216, loss = 1.9762868881225586
steps = 216, loss = 3.101820468902588
steps = 216, loss = 2.5679595470428467
steps = 216, loss = 3.113473415374756
steps = 216, loss = 39.981773376464844
steps = 216, loss = 3.3664143085479736
steps = 216, loss = 3.2300782203674316
steps = 216, loss = 2.1425485610961914
steps = 216, loss = 2.694286346435547
steps = 216, loss = 49.9853630065918
steps = 216, loss = 2.5862503051757812
steps = 216, loss = 5.496171951293945
steps = 216, loss = 2.834984540939331
steps = 216, loss = 7.051941394805908
steps = 216, loss = 50.01384353637695
steps = 216, loss = 2.732266426086426
steps = 217, loss = 3.6137921810150146
steps = 217, loss = 50.01182556152344
steps = 217, loss = 2.853585720062256
steps = 217, loss = 2.8155343532562256
steps = 217, loss = 1.996989130973816
steps = 217, loss = 7.086756706237793
steps = 217, loss = 3.6923186779022217
steps = 217, loss = 3.5445449352264404
steps = 217, loss = 3.6025142669677734
steps = 217, loss = 4.814398288726807
steps = 217, loss = 4.615886688232422
steps = 217, loss = 3.4950931072235107
steps = 217, loss = 2.777090311050415
steps = 217, loss = 3.3672878742218018
steps = 217, loss = 49.99580001831055
steps = 217, loss = 2.9648079872131348
steps = 217, loss = 49.9853630065918
steps = 217, loss = 3.146920919418335
steps = 217, loss = 3.6508443355560303
steps = 217, loss = 5.533053874969482
steps = 217, loss = 2.715477705001831
steps = 217, loss = 3.1139214038848877
steps = 217, loss = 2.7139198780059814
steps = 217, loss = 2.370051622390747
steps = 217, loss = 2.288710355758667
steps = 217, loss = 2.7634661197662354
steps = 217, loss = 2.1377310752868652
steps = 217, loss = 3.372544050216675
steps = 217, loss = 2.9508814811706543
steps = 218, loss = 3.5398120880126953
steps = 218, loss = 2.6947567462921143
steps = 218, loss = 2.915311098098755
steps = 218, loss = 5.528262615203857
steps = 218, loss = 3.4477243423461914
steps = 218, loss = 3.599088191986084
steps = 218, loss = 2.1325159072875977
steps = 218, loss = 2.9428045749664307
steps = 218, loss = 50.00185775756836
steps = 218, loss = 2.742126703262329
steps = 218, loss = 2.4283359050750732
steps = 218, loss = 3.106044054031372
steps = 218, loss = 2.8352792263031006
steps = 218, loss = 3.030935764312744
steps = 218, loss = 3.529445171356201
steps = 218, loss = 2.2175991535186768
steps = 218, loss = 2.771801233291626
steps = 218, loss = 3.473752021789551
steps = 218, loss = 4.8149871826171875
steps = 218, loss = 49.9853630065918
steps = 218, loss = 2.998410224914551
steps = 218, loss = 3.3732378482818604
steps = 218, loss = 3.500673294067383
steps = 218, loss = 50.02409362792969
steps = 218, loss = 2.629389524459839
steps = 218, loss = 7.058044910430908
steps = 218, loss = 4.624165058135986
steps = 218, loss = 3.6461379528045654
steps = 218, loss = 2.01835036277771
steps = 219, loss = 2.954143762588501
steps = 219, loss = 3.473158359527588
steps = 219, loss = 7.133957862854004
steps = 219, loss = 3.120673894882202
steps = 219, loss = 49.9853630065918
steps = 219, loss = 3.647023916244507
steps = 219, loss = 2.2772839069366455
steps = 219, loss = 2.9163873195648193
steps = 219, loss = 3.0688600540161133
steps = 219, loss = 2.773638963699341
steps = 219, loss = 4.639321804046631
steps = 219, loss = 5.560166358947754
steps = 219, loss = 3.259265661239624
steps = 219, loss = 2.6692147254943848
steps = 219, loss = 2.261366367340088
steps = 219, loss = 2.732393264770508
steps = 219, loss = 3.4412856101989746
steps = 219, loss = 2.609701156616211
steps = 219, loss = 50.00092315673828
steps = 219, loss = 3.004403829574585
steps = 219, loss = 3.360224962234497
steps = 219, loss = 2.1608517169952393
steps = 219, loss = 2.6491403579711914
steps = 219, loss = 3.6566221714019775
steps = 219, loss = 4.8338751792907715
steps = 219, loss = 3.9546587467193604
steps = 219, loss = 3.481163740158081
steps = 219, loss = 49.791259765625
steps = 219, loss = 2.8731303215026855
steps = 220, loss = 3.471846103668213
steps = 220, loss = 3.4415388107299805
steps = 220, loss = 3.542635202407837
steps = 220, loss = 4.652091026306152
steps = 220, loss = 7.022805213928223
steps = 220, loss = 2.183911085128784
steps = 220, loss = 2.8759541511535645
steps = 220, loss = 2.1620125770568848
steps = 220, loss = 2.6929497718811035
steps = 220, loss = 2.7334535121917725
steps = 220, loss = 3.431281089782715
steps = 220, loss = 49.856353759765625
steps = 220, loss = 50.01582717895508
steps = 220, loss = 3.1250505447387695
steps = 220, loss = 2.2643630504608154
steps = 220, loss = 4.114027976989746
steps = 220, loss = 2.9557693004608154
steps = 220, loss = 2.8076231479644775
steps = 220, loss = 4.842554092407227
steps = 220, loss = 3.34885573387146
steps = 220, loss = 2.562533140182495
steps = 220, loss = 3.4151611328125
steps = 220, loss = 49.9853630065918
steps = 220, loss = 2.773967742919922
steps = 220, loss = 2.882059335708618
steps = 220, loss = 2.9919958114624023
steps = 220, loss = 5.57883882522583
steps = 220, loss = 2.987856149673462
steps = 220, loss = 3.4902074337005615
steps = 221, loss = 4.838221549987793
steps = 221, loss = 7.177844047546387
steps = 221, loss = 3.1768710613250732
steps = 221, loss = 2.1320183277130127
steps = 221, loss = 3.224825859069824
steps = 221, loss = 2.7437682151794434
steps = 221, loss = 2.5579264163970947
steps = 221, loss = 3.2991905212402344
steps = 221, loss = 4.02385950088501
steps = 221, loss = 3.349956512451172
steps = 221, loss = 2.837589979171753
steps = 221, loss = 5.577442169189453
steps = 221, loss = 2.6382317543029785
steps = 221, loss = 3.1145060062408447
steps = 221, loss = 2.7923450469970703
steps = 221, loss = 3.6738860607147217
steps = 221, loss = 2.8019638061523438
steps = 221, loss = 3.4716100692749023
steps = 221, loss = 3.0270638465881348
steps = 221, loss = 2.033221483230591
steps = 221, loss = 49.930152893066406
steps = 221, loss = 2.436657667160034
steps = 221, loss = 4.654751777648926
steps = 221, loss = 2.697831869125366
steps = 221, loss = 3.4069302082061768
steps = 221, loss = 49.9853630065918
steps = 221, loss = 2.2633376121520996
steps = 221, loss = 2.9447615146636963
steps = 221, loss = 50.00596237182617
steps = 222, loss = 2.8811988830566406
steps = 222, loss = 2.8708276748657227
steps = 222, loss = 3.391779899597168
steps = 222, loss = 6.9322710037231445
steps = 222, loss = 4.6546454429626465
steps = 222, loss = 3.5217864513397217
steps = 222, loss = 2.8536312580108643
steps = 222, loss = 49.9853630065918
steps = 222, loss = 3.4585604667663574
steps = 222, loss = 2.7662415504455566
steps = 222, loss = 2.6917152404785156
steps = 222, loss = 3.468994140625
steps = 222, loss = 2.231950283050537
steps = 222, loss = 5.6133928298950195
steps = 222, loss = 2.0619990825653076
steps = 222, loss = 3.9001996517181396
steps = 222, loss = 50.016624450683594
steps = 222, loss = 3.1264865398406982
steps = 222, loss = 2.715644121170044
steps = 222, loss = 4.852998733520508
steps = 222, loss = 3.7011220455169678
steps = 222, loss = 50.0184326171875
steps = 222, loss = 2.145509719848633
steps = 222, loss = 3.4778754711151123
steps = 222, loss = 3.002074956893921
steps = 222, loss = 2.722196578979492
steps = 222, loss = 2.3529269695281982
steps = 222, loss = 2.9510436058044434
steps = 222, loss = 3.3448612689971924
steps = 223, loss = 2.731875419616699
steps = 223, loss = 2.201108694076538
steps = 223, loss = 3.6263978481292725
steps = 223, loss = 7.0101141929626465
steps = 223, loss = 2.9241528511047363
steps = 223, loss = 3.4045140743255615
steps = 223, loss = 2.5896172523498535
steps = 223, loss = 50.01015090942383
steps = 223, loss = 5.6278228759765625
steps = 223, loss = 2.954604387283325
steps = 223, loss = 2.6149954795837402
steps = 223, loss = 3.4217095375061035
steps = 223, loss = 2.752061367034912
steps = 223, loss = 2.773252248764038
steps = 223, loss = 2.769447088241577
steps = 223, loss = 2.637356758117676
steps = 223, loss = 3.4410862922668457
steps = 223, loss = 2.8736610412597656
steps = 223, loss = 3.1612257957458496
steps = 223, loss = 3.4907782077789307
steps = 223, loss = 49.9853630065918
steps = 223, loss = 3.3786160945892334
steps = 223, loss = 4.865551948547363
steps = 223, loss = 4.671308517456055
steps = 223, loss = 3.132611036300659
steps = 223, loss = 3.6405022144317627
steps = 223, loss = 2.2760353088378906
steps = 223, loss = 2.174323320388794
steps = 223, loss = 50.0166015625
steps = 224, loss = 2.3871583938598633
steps = 224, loss = 7.057778358459473
steps = 224, loss = 2.057661294937134
steps = 224, loss = 3.037686824798584
steps = 224, loss = 3.4502928256988525
steps = 224, loss = 2.8370096683502197
steps = 224, loss = 3.401175022125244
steps = 224, loss = 3.1223599910736084
steps = 224, loss = 2.743300676345825
steps = 224, loss = 4.67483377456665
steps = 224, loss = 4.861242294311523
steps = 224, loss = 2.1409170627593994
steps = 224, loss = 3.382087230682373
steps = 224, loss = 2.7750091552734375
steps = 224, loss = 2.7397029399871826
steps = 224, loss = 5.626780033111572
steps = 224, loss = 50.01912307739258
steps = 224, loss = 49.9853630065918
steps = 224, loss = 2.569657802581787
steps = 224, loss = 3.2670249938964844
steps = 224, loss = 2.6971120834350586
steps = 224, loss = 3.5918807983398438
steps = 224, loss = 2.6211202144622803
steps = 224, loss = 3.524735689163208
steps = 224, loss = 3.2822458744049072
steps = 224, loss = 2.061882972717285
steps = 224, loss = 3.2931928634643555
steps = 224, loss = 2.943912982940674
steps = 224, loss = 50.03557205200195
steps = 225, loss = 3.3652024269104004
steps = 225, loss = 2.0491602420806885
steps = 225, loss = 2.7658498287200928
steps = 225, loss = 3.726914882659912
steps = 225, loss = 50.01763153076172
steps = 225, loss = 2.8536109924316406
steps = 225, loss = 3.621554136276245
steps = 225, loss = 2.9482712745666504
steps = 225, loss = 3.6225855350494385
steps = 225, loss = 4.876011371612549
steps = 225, loss = 2.7408549785614014
steps = 225, loss = 7.1032490730285645
steps = 225, loss = 5.6633477210998535
steps = 225, loss = 2.065511465072632
steps = 225, loss = 3.482048988342285
steps = 225, loss = 3.5064473152160645
steps = 225, loss = 2.9503848552703857
steps = 225, loss = 3.143540143966675
steps = 225, loss = 49.92518997192383
steps = 225, loss = 3.8845906257629395
steps = 225, loss = 2.411256790161133
steps = 225, loss = 2.93208384513855
steps = 225, loss = 2.850066661834717
steps = 225, loss = 2.715604782104492
steps = 225, loss = 3.1343514919281006
steps = 225, loss = 2.2880122661590576
steps = 225, loss = 4.674936294555664
steps = 225, loss = 49.9853630065918
steps = 225, loss = 3.439124584197998
steps = 226, loss = 2.1388280391693115
steps = 226, loss = 2.9422755241394043
steps = 226, loss = 2.247647523880005
steps = 226, loss = 3.250812292098999
steps = 226, loss = 4.876004695892334
steps = 226, loss = 3.7433907985687256
steps = 226, loss = 5.657505512237549
steps = 226, loss = 2.0457847118377686
steps = 226, loss = 2.743198871612549
steps = 226, loss = 3.4570772647857666
steps = 226, loss = 3.6367626190185547
steps = 226, loss = 3.5344974994659424
steps = 226, loss = 2.969113349914551
steps = 226, loss = 4.683915138244629
steps = 226, loss = 3.5303118228912354
steps = 226, loss = 2.8361151218414307
steps = 226, loss = 2.592881917953491
steps = 226, loss = 2.4481866359710693
steps = 226, loss = 50.012630462646484
steps = 226, loss = 2.880777359008789
steps = 226, loss = 7.030375957489014
steps = 226, loss = 49.9853630065918
steps = 226, loss = 2.695866823196411
steps = 226, loss = 3.1265201568603516
steps = 226, loss = 2.61399245262146
steps = 226, loss = 3.59043288230896
steps = 226, loss = 2.593997001647949
steps = 226, loss = 50.02410888671875
steps = 226, loss = 3.4216878414154053
steps = 227, loss = 2.2782576084136963
steps = 227, loss = 3.4085896015167236
steps = 227, loss = 7.104243755340576
steps = 227, loss = 2.9534451961517334
steps = 227, loss = 32.29393005371094
steps = 227, loss = 3.4987869262695312
steps = 227, loss = 49.99524688720703
steps = 227, loss = 5.689245223999023
steps = 227, loss = 3.6901960372924805
steps = 227, loss = 49.954444885253906
steps = 227, loss = 2.1457576751708984
steps = 227, loss = 3.3657188415527344
steps = 227, loss = 2.6183807849884033
steps = 227, loss = 2.873750686645508
steps = 227, loss = 2.732893466949463
steps = 227, loss = 2.774174451828003
steps = 227, loss = 49.9853630065918
steps = 227, loss = 4.179680347442627
steps = 227, loss = 2.865346908569336
steps = 227, loss = 3.0313565731048584
steps = 227, loss = 4.697373867034912
steps = 227, loss = 3.141258478164673
steps = 227, loss = 3.513054847717285
steps = 227, loss = 3.0516648292541504
steps = 227, loss = 3.4802048206329346
steps = 227, loss = 3.5107297897338867
steps = 227, loss = 2.7183141708374023
steps = 227, loss = 4.894885063171387
steps = 227, loss = 2.6727561950683594
steps = 228, loss = 3.1964430809020996
steps = 228, loss = 4.255661964416504
steps = 228, loss = 3.3962574005126953
steps = 228, loss = 3.4232494831085205
steps = 228, loss = 7.155861854553223
steps = 228, loss = 3.295311450958252
steps = 228, loss = 3.587723970413208
steps = 228, loss = 2.1722676753997803
steps = 228, loss = 2.8542847633361816
steps = 228, loss = 2.7006044387817383
steps = 228, loss = 2.7573914527893066
steps = 228, loss = 3.354743242263794
steps = 228, loss = 49.9853630065918
steps = 228, loss = 5.712497234344482
steps = 228, loss = 1.94748055934906
steps = 228, loss = 2.7160770893096924
steps = 228, loss = 2.766753911972046
steps = 228, loss = 2.0398571491241455
steps = 228, loss = 4.89973258972168
steps = 228, loss = 3.366816282272339
steps = 228, loss = 50.00758361816406
steps = 228, loss = 3.143449306488037
steps = 228, loss = 2.950680732727051
steps = 228, loss = 4.697615146636963
steps = 228, loss = 3.5037248134613037
steps = 228, loss = 49.60347366333008
steps = 228, loss = 2.5245797634124756
steps = 228, loss = 3.144892930984497
steps = 228, loss = 2.766380786895752
steps = 229, loss = 2.1821417808532715
steps = 229, loss = 2.8739097118377686
steps = 229, loss = 2.2766776084899902
steps = 229, loss = 2.255284547805786
steps = 229, loss = 3.6469979286193848
steps = 229, loss = 2.644052743911743
steps = 229, loss = 3.0821573734283447
steps = 229, loss = 7.042003631591797
steps = 229, loss = 3.3981072902679443
steps = 229, loss = 2.8947815895080566
steps = 229, loss = 50.01458740234375
steps = 229, loss = 3.674567461013794
steps = 229, loss = 5.725141525268555
steps = 229, loss = 4.185048580169678
steps = 229, loss = 2.970162868499756
steps = 229, loss = 50.02184295654297
steps = 229, loss = 4.910748481750488
steps = 229, loss = 2.953789234161377
steps = 229, loss = 3.4752283096313477
steps = 229, loss = 3.444208860397339
steps = 229, loss = 3.528759002685547
steps = 229, loss = 2.7320444583892822
steps = 229, loss = 2.9643847942352295
steps = 229, loss = 2.7729623317718506
steps = 229, loss = 49.9853630065918
steps = 229, loss = 3.148571729660034
steps = 229, loss = 3.3921332359313965
steps = 229, loss = 4.71201753616333
steps = 229, loss = 2.6477653980255127
steps = 230, loss = 2.9430429935455322
steps = 230, loss = 2.6820993423461914
steps = 230, loss = 5.723944187164307
steps = 230, loss = 3.13793683052063
steps = 230, loss = 3.377467632293701
steps = 230, loss = 7.2049455642700195
steps = 230, loss = 2.6969258785247803
steps = 230, loss = 3.647900104522705
steps = 230, loss = 2.7652394771575928
steps = 230, loss = 49.75244140625
steps = 230, loss = 2.836866617202759
steps = 230, loss = 2.1410229206085205
steps = 230, loss = 3.461090564727783
steps = 230, loss = 1.9989986419677734
steps = 230, loss = 3.3680944442749023
steps = 230, loss = 4.3702192306518555
steps = 230, loss = 3.1995749473571777
steps = 230, loss = 4.71653413772583
steps = 230, loss = 4.905642986297607
steps = 230, loss = 2.9465458393096924
steps = 230, loss = 2.7436037063598633
steps = 230, loss = 2.080275774002075
steps = 230, loss = 2.5808558464050293
steps = 230, loss = 49.9853630065918
steps = 230, loss = 3.4250948429107666
steps = 230, loss = 3.3414788246154785
steps = 230, loss = 49.97208023071289
steps = 230, loss = 2.364455461502075
steps = 230, loss = 2.827505111694336
steps = 231, loss = 49.90430450439453
steps = 231, loss = 3.1093358993530273
steps = 231, loss = 6.982848167419434
steps = 231, loss = 50.01968765258789
steps = 231, loss = 2.9495506286621094
steps = 231, loss = 2.7158186435699463
steps = 231, loss = 4.9199957847595215
steps = 231, loss = 2.5258257389068604
steps = 231, loss = 5.7595601081848145
steps = 231, loss = 49.9853630065918
steps = 231, loss = 4.375613689422607
steps = 231, loss = 3.450745105743408
steps = 231, loss = 3.0460896492004395
steps = 231, loss = 3.6383395195007324
steps = 231, loss = 2.1468846797943115
steps = 231, loss = 2.765286922454834
steps = 231, loss = 3.6141350269317627
steps = 231, loss = 4.717123031616211
steps = 231, loss = 3.149773359298706
steps = 231, loss = 3.6943511962890625
steps = 231, loss = 3.298337459564209
steps = 231, loss = 2.7836811542510986
steps = 231, loss = 3.4269893169403076
steps = 231, loss = 2.853661060333252
steps = 231, loss = 2.0939271450042725
steps = 231, loss = 2.2881600856781006
steps = 231, loss = 2.7748541831970215
steps = 231, loss = 3.4638493061065674
steps = 231, loss = 2.719953775405884
steps = 232, loss = 3.526616096496582
steps = 232, loss = 3.0856502056121826
steps = 232, loss = 3.425868272781372
steps = 232, loss = 2.9490606784820557
steps = 232, loss = 2.0445027351379395
steps = 232, loss = 3.1534488201141357
steps = 232, loss = 49.756038665771484
steps = 232, loss = 4.7220869064331055
steps = 232, loss = 2.0420339107513428
steps = 232, loss = 3.5667011737823486
steps = 232, loss = 2.8588480949401855
steps = 232, loss = 2.854105234146118
steps = 232, loss = 3.54302716255188
steps = 232, loss = 49.9853630065918
steps = 232, loss = 3.4828951358795166
steps = 232, loss = 2.487290143966675
steps = 232, loss = 2.7159059047698975
steps = 232, loss = 49.90431594848633
steps = 232, loss = 4.928584575653076
steps = 232, loss = 3.3697776794433594
steps = 232, loss = 3.2984893321990967
steps = 232, loss = 5.7780375480651855
steps = 232, loss = 2.7636449337005615
steps = 232, loss = 4.530517101287842
steps = 232, loss = 3.0734105110168457
steps = 232, loss = 2.408271551132202
steps = 232, loss = 7.0576019287109375
steps = 232, loss = 2.8272299766540527
steps = 232, loss = 2.784928798675537
steps = 233, loss = 3.476814031600952
steps = 233, loss = 4.734890460968018
steps = 233, loss = 7.039241313934326
steps = 233, loss = 3.7069177627563477
steps = 233, loss = 2.731511354446411
steps = 233, loss = 2.9523353576660156
steps = 233, loss = 3.6071088314056396
steps = 233, loss = 2.772667407989502
steps = 233, loss = 3.411313056945801
steps = 233, loss = 5.790323257446289
steps = 233, loss = 49.9853630065918
steps = 233, loss = 3.4366605281829834
steps = 233, loss = 3.5633881092071533
steps = 233, loss = 2.163508176803589
steps = 233, loss = 49.78569412231445
steps = 233, loss = 2.8805668354034424
steps = 233, loss = 3.5018820762634277
steps = 233, loss = 2.28377103805542
steps = 233, loss = 2.8726656436920166
steps = 233, loss = 2.6649444103240967
steps = 233, loss = 3.042288303375244
steps = 233, loss = 3.0377466678619385
steps = 233, loss = 3.158475160598755
steps = 233, loss = 4.647591590881348
steps = 233, loss = 2.953040599822998
steps = 233, loss = 4.939713001251221
steps = 233, loss = 3.4426863193511963
steps = 233, loss = 2.623396396636963
steps = 233, loss = 2.2515740394592285
steps = 234, loss = 3.3647632598876953
steps = 234, loss = 3.0898468494415283
steps = 234, loss = 3.2147951126098633
steps = 234, loss = 3.430450677871704
steps = 234, loss = 5.053287506103516
steps = 234, loss = 3.4581494331359863
steps = 234, loss = 2.7431533336639404
steps = 234, loss = 2.8360679149627686
steps = 234, loss = 7.10073709487915
steps = 234, loss = 1.9783328771591187
steps = 234, loss = 2.6248528957366943
steps = 234, loss = 2.427628993988037
steps = 234, loss = 3.352238416671753
steps = 234, loss = 2.8342556953430176
steps = 234, loss = 3.4239301681518555
steps = 234, loss = 3.2704827785491943
steps = 234, loss = 5.788377285003662
steps = 234, loss = 2.721752882003784
steps = 234, loss = 2.9417736530303955
steps = 234, loss = 2.741647481918335
steps = 234, loss = 2.6962904930114746
steps = 234, loss = 50.01342010498047
steps = 234, loss = 49.9853630065918
steps = 234, loss = 3.147878408432007
steps = 234, loss = 2.0719563961029053
steps = 234, loss = 4.934453964233398
steps = 234, loss = 2.1423423290252686
steps = 234, loss = 3.335520029067993
steps = 234, loss = 4.740250110626221
steps = 235, loss = 2.616302728652954
steps = 235, loss = 3.1617038249969482
steps = 235, loss = 2.733161211013794
steps = 235, loss = 5.8181023597717285
steps = 235, loss = 4.9520368576049805
steps = 235, loss = 2.2291362285614014
steps = 235, loss = 2.8745548725128174
steps = 235, loss = 2.1859185695648193
steps = 235, loss = 4.751626014709473
steps = 235, loss = 3.601315975189209
steps = 235, loss = 2.952117443084717
steps = 235, loss = 3.4057419300079346
steps = 235, loss = 50.01517868041992
steps = 235, loss = 2.8603408336639404
steps = 235, loss = 7.1030497550964355
steps = 235, loss = 3.00044584274292
steps = 235, loss = 2.9526498317718506
steps = 235, loss = 2.2694504261016846
steps = 235, loss = 2.774101734161377
steps = 235, loss = 2.9877676963806152
steps = 235, loss = 3.3554608821868896
steps = 235, loss = 3.4606082439422607
steps = 235, loss = 3.4284565448760986
steps = 235, loss = 49.9853630065918
steps = 235, loss = 2.7233190536499023
steps = 235, loss = 3.6314027309417725
steps = 235, loss = 2.709937334060669
steps = 235, loss = 3.678485631942749
steps = 235, loss = 3.3738746643066406
steps = 236, loss = 2.642059087753296
steps = 236, loss = 5.8180365562438965
steps = 236, loss = 3.463743209838867
steps = 236, loss = 4.7580389976501465
steps = 236, loss = 2.0484066009521484
steps = 236, loss = 3.54476261138916
steps = 236, loss = 3.0047800540924072
steps = 236, loss = 2.662801742553711
steps = 236, loss = 2.8377392292022705
steps = 236, loss = 3.448777198791504
steps = 236, loss = 3.3768820762634277
steps = 236, loss = 2.7440683841705322
steps = 236, loss = 49.9853630065918
steps = 236, loss = 3.1520841121673584
steps = 236, loss = 2.136367082595825
steps = 236, loss = 4.947805881500244
steps = 236, loss = 2.6982126235961914
steps = 236, loss = 3.3019497394561768
steps = 236, loss = 2.7894437313079834
steps = 236, loss = 2.3845584392547607
steps = 236, loss = 2.7683522701263428
steps = 236, loss = 3.403607130050659
steps = 236, loss = 3.4181313514709473
steps = 236, loss = 7.16787576675415
steps = 236, loss = 2.8698346614837646
steps = 236, loss = 2.9423599243164062
steps = 236, loss = 3.360771894454956
steps = 236, loss = 50.01219177246094
steps = 236, loss = 2.105670928955078
steps = 237, loss = 2.766266107559204
steps = 237, loss = 2.719210386276245
steps = 237, loss = 7.185991287231445
steps = 237, loss = 2.1731972694396973
steps = 237, loss = 2.7210652828216553
steps = 237, loss = 2.0632288455963135
steps = 237, loss = 3.6436610221862793
steps = 237, loss = 2.230851650238037
steps = 237, loss = 3.575409412384033
steps = 237, loss = 2.9488794803619385
steps = 237, loss = 3.114821672439575
steps = 237, loss = 3.1641993522644043
steps = 237, loss = 2.716090440750122
steps = 237, loss = 49.9853630065918
steps = 237, loss = 4.757986068725586
steps = 237, loss = 3.458899736404419
steps = 237, loss = 2.783862352371216
steps = 237, loss = 4.961602210998535
steps = 237, loss = 3.35693097114563
steps = 237, loss = 4.153334140777588
steps = 237, loss = 5.853084564208984
steps = 237, loss = 3.382321357727051
steps = 237, loss = 49.906471252441406
steps = 237, loss = 2.551088333129883
steps = 237, loss = 2.8537070751190186
steps = 237, loss = 3.5240767002105713
steps = 237, loss = 2.8560140132904053
steps = 237, loss = 2.991544485092163
steps = 237, loss = 3.460442543029785
steps = 238, loss = 3.433326005935669
steps = 238, loss = 2.8066956996917725
steps = 238, loss = 3.0224990844726562
steps = 238, loss = 2.49188494682312
steps = 238, loss = 3.3777875900268555
steps = 238, loss = 3.4778709411621094
steps = 238, loss = 7.172737121582031
steps = 238, loss = 3.167994499206543
steps = 238, loss = 3.4547579288482666
steps = 238, loss = 4.969611167907715
steps = 238, loss = 2.03831148147583
steps = 238, loss = 2.7160885334014893
steps = 238, loss = 2.90054988861084
steps = 238, loss = 2.942692279815674
steps = 238, loss = 3.536867618560791
steps = 238, loss = 4.7619781494140625
steps = 238, loss = 2.9483819007873535
steps = 238, loss = 2.7263283729553223
steps = 238, loss = 3.652538776397705
steps = 238, loss = 49.95296096801758
steps = 238, loss = 5.8713579177856445
steps = 238, loss = 49.9853630065918
steps = 238, loss = 3.1332473754882812
steps = 238, loss = 49.93541717529297
steps = 238, loss = 2.764313220977783
steps = 238, loss = 2.027168035507202
steps = 238, loss = 3.423468589782715
steps = 238, loss = 2.854125738143921
steps = 238, loss = 4.120748996734619
steps = 239, loss = 49.9853630065918
steps = 239, loss = 3.6486098766326904
steps = 239, loss = 2.151546001434326
steps = 239, loss = 3.071720600128174
steps = 239, loss = 3.0496129989624023
steps = 239, loss = 2.3561995029449463
steps = 239, loss = 2.5924885272979736
steps = 239, loss = 1.7593189477920532
steps = 239, loss = 4.969156265258789
steps = 239, loss = 3.5139236450195312
steps = 239, loss = 2.7434167861938477
steps = 239, loss = 4.7698540687561035
steps = 239, loss = 2.695247173309326
steps = 239, loss = 2.142357349395752
steps = 239, loss = 2.579860210418701
steps = 239, loss = 7.163388252258301
steps = 239, loss = 3.1741392612457275
steps = 239, loss = 3.886850595474243
steps = 239, loss = 3.1595265865325928
steps = 239, loss = 2.817173480987549
steps = 239, loss = 2.939958333969116
steps = 239, loss = 3.7386434078216553
steps = 239, loss = 3.359070301055908
steps = 239, loss = 3.452086925506592
steps = 239, loss = 5.866197109222412
steps = 239, loss = 2.8353195190429688
steps = 239, loss = 2.8272829055786133
steps = 239, loss = 3.4266750812530518
steps = 239, loss = 50.001373291015625
steps = 240, loss = 3.4736597537994385
steps = 240, loss = 3.425086736679077
steps = 240, loss = 5.895444869995117
steps = 240, loss = 2.2774593830108643
steps = 240, loss = 2.0300168991088867
steps = 240, loss = 2.732780933380127
steps = 240, loss = 3.4065098762512207
steps = 240, loss = 4.653513431549072
steps = 240, loss = 4.986364364624023
steps = 240, loss = 2.773653745651245
steps = 240, loss = 3.190288543701172
steps = 240, loss = 3.670807361602783
steps = 240, loss = 3.3555634021759033
steps = 240, loss = 2.9351320266723633
steps = 240, loss = 2.9581944942474365
steps = 240, loss = 49.9853630065918
steps = 240, loss = 3.4539577960968018
steps = 240, loss = 2.56286883354187
steps = 240, loss = 3.1737818717956543
steps = 240, loss = 49.92647171020508
steps = 240, loss = 2.6907331943511963
steps = 240, loss = 2.951075792312622
steps = 240, loss = 3.5386998653411865
steps = 240, loss = 4.780325889587402
steps = 240, loss = 2.873253107070923
steps = 240, loss = 1.8837122917175293
steps = 240, loss = 2.637672185897827
steps = 240, loss = 7.243544578552246
steps = 240, loss = 2.6025493144989014
steps = 241, loss = 3.4796149730682373
steps = 241, loss = 2.952716112136841
steps = 241, loss = 3.1779630184173584
steps = 241, loss = 4.793323516845703
steps = 241, loss = 2.5555145740509033
steps = 241, loss = 3.3577780723571777
steps = 241, loss = 4.290359020233154
steps = 241, loss = 2.7748100757598877
steps = 241, loss = 3.445319175720215
steps = 241, loss = 3.535318613052368
steps = 241, loss = 3.24045991897583
steps = 241, loss = 49.9853630065918
steps = 241, loss = 49.9718132019043
steps = 241, loss = 3.002074956893921
steps = 241, loss = 2.263967514038086
steps = 241, loss = 7.0804948806762695
steps = 241, loss = 5.913941383361816
steps = 241, loss = 2.871420383453369
steps = 241, loss = 3.424670696258545
steps = 241, loss = 3.652132749557495
steps = 241, loss = 4.994338035583496
steps = 241, loss = 2.7339839935302734
steps = 241, loss = 3.4857864379882812
steps = 241, loss = 2.6628756523132324
steps = 241, loss = 2.0043997764587402
steps = 241, loss = 2.973855972290039
steps = 241, loss = 2.876136302947998
steps = 241, loss = 3.006871461868286
steps = 241, loss = 2.2332587242126465
steps = 242, loss = 49.98944091796875
steps = 242, loss = 3.3612897396087646
steps = 242, loss = 3.672546863555908
steps = 242, loss = 2.744774341583252
steps = 242, loss = 1.925085425376892
steps = 242, loss = 3.1674227714538574
steps = 242, loss = 2.97660493850708
steps = 242, loss = 3.098594903945923
steps = 242, loss = 3.3764612674713135
steps = 242, loss = 3.4648280143737793
steps = 242, loss = 3.5074894428253174
steps = 242, loss = 2.160902261734009
steps = 242, loss = 7.2370123863220215
steps = 242, loss = 2.612365961074829
steps = 242, loss = 2.9557621479034424
steps = 242, loss = 2.941983699798584
steps = 242, loss = 49.9853630065918
steps = 242, loss = 2.6908798217773438
steps = 242, loss = 5.9130940437316895
steps = 242, loss = 2.704378604888916
steps = 242, loss = 2.399348735809326
steps = 242, loss = 2.134411334991455
steps = 242, loss = 3.192235231399536
steps = 242, loss = 3.4275689125061035
steps = 242, loss = 3.307647705078125
steps = 242, loss = 2.6987032890319824
steps = 242, loss = 4.798486709594727
steps = 242, loss = 2.8379430770874023
steps = 242, loss = 4.989638805389404
steps = 243, loss = 4.797922134399414
steps = 243, loss = 7.052586078643799
steps = 243, loss = 2.2537219524383545
steps = 243, loss = 3.4723668098449707
steps = 243, loss = 2.7421135902404785
steps = 243, loss = 3.0971004962921143
steps = 243, loss = 2.8538222312927246
steps = 243, loss = 2.568155288696289
steps = 243, loss = 2.4713377952575684
steps = 243, loss = 3.701697826385498
steps = 243, loss = 2.7161853313446045
steps = 243, loss = 2.8579294681549072
steps = 243, loss = 3.4951300621032715
steps = 243, loss = 3.6396164894104004
steps = 243, loss = 2.140531539916992
steps = 243, loss = 5.003013610839844
steps = 243, loss = 3.480541706085205
steps = 243, loss = 49.99271774291992
steps = 243, loss = 3.3952813148498535
steps = 243, loss = 3.453186273574829
steps = 243, loss = 5.946779251098633
steps = 243, loss = 49.9853630065918
steps = 243, loss = 2.0340170860290527
steps = 243, loss = 3.744736433029175
steps = 243, loss = 3.1793463230133057
steps = 243, loss = 2.9483673572540283
steps = 243, loss = 2.7663092613220215
steps = 243, loss = 2.986469268798828
steps = 243, loss = 3.0155394077301025
steps = 244, loss = 3.4584391117095947
steps = 244, loss = 4.809357166290283
steps = 244, loss = 3.1848628520965576
steps = 244, loss = 3.3758981227874756
steps = 244, loss = 4.312537670135498
steps = 244, loss = 2.7325053215026855
steps = 244, loss = 3.442293405532837
steps = 244, loss = 2.1333184242248535
steps = 244, loss = 3.12528920173645
steps = 244, loss = 3.350964069366455
steps = 244, loss = 3.424569606781006
steps = 244, loss = 2.873932361602783
steps = 244, loss = 2.2542686462402344
steps = 244, loss = 2.8653130531311035
steps = 244, loss = 2.742907762527466
steps = 244, loss = 3.7080578804016113
steps = 244, loss = 3.363743782043457
steps = 244, loss = 2.7736761569976807
steps = 244, loss = 49.9853630065918
steps = 244, loss = 3.021772861480713
steps = 244, loss = 50.02025604248047
steps = 244, loss = 7.129451751708984
steps = 244, loss = 2.274712562561035
steps = 244, loss = 5.014516353607178
steps = 244, loss = 2.581848382949829
steps = 244, loss = 2.609463691711426
steps = 244, loss = 2.5913002490997314
steps = 244, loss = 5.95996618270874
steps = 244, loss = 2.9516758918762207
steps = 245, loss = 3.7174675464630127
steps = 245, loss = 1.9871594905853271
steps = 245, loss = 3.174489974975586
steps = 245, loss = 2.744640350341797
steps = 245, loss = 2.467158079147339
steps = 245, loss = 7.132526874542236
steps = 245, loss = 2.140977621078491
steps = 245, loss = 2.95937442779541
steps = 245, loss = 2.9412286281585693
steps = 245, loss = 2.8374478816986084
steps = 245, loss = 5.959630012512207
steps = 245, loss = 3.483271360397339
steps = 245, loss = 3.876800298690796
steps = 245, loss = 3.655087471008301
steps = 245, loss = 3.4406778812408447
steps = 245, loss = 2.9916999340057373
steps = 245, loss = 2.8751137256622314
steps = 245, loss = 2.5804443359375
steps = 245, loss = 3.266714334487915
steps = 245, loss = 3.1252970695495605
steps = 245, loss = 2.7330737113952637
steps = 245, loss = 3.2950522899627686
steps = 245, loss = 2.6981277465820312
steps = 245, loss = 49.9853630065918
steps = 245, loss = 4.816452980041504
steps = 245, loss = 5.009957790374756
steps = 245, loss = 2.644099473953247
steps = 245, loss = 49.99187469482422
steps = 245, loss = 2.195674419403076
steps = 246, loss = 2.976029396057129
steps = 246, loss = 3.7013232707977295
steps = 246, loss = 2.1912729740142822
steps = 246, loss = 5.023684978485107
steps = 246, loss = 3.606126546859741
steps = 246, loss = 2.662666082382202
steps = 246, loss = 3.4095075130462646
steps = 246, loss = 2.716205358505249
steps = 246, loss = 49.9853630065918
steps = 246, loss = 3.2901196479797363
steps = 246, loss = 2.073212146759033
steps = 246, loss = 2.766317844390869
steps = 246, loss = 4.815727233886719
steps = 246, loss = 2.947798728942871
steps = 246, loss = 3.5250988006591797
steps = 246, loss = 2.7260966300964355
steps = 246, loss = 3.1862316131591797
steps = 246, loss = 2.8801472187042236
steps = 246, loss = 2.5935332775115967
steps = 246, loss = 2.2957494258880615
steps = 246, loss = 2.760160446166992
steps = 246, loss = 3.4841792583465576
steps = 246, loss = 49.81285858154297
steps = 246, loss = 5.99355411529541
steps = 246, loss = 2.853828191757202
steps = 246, loss = 7.186877250671387
steps = 246, loss = 3.271864175796509
steps = 246, loss = 3.942408800125122
steps = 246, loss = 2.465120792388916
steps = 247, loss = 5.023167133331299
steps = 247, loss = 2.5868780612945557
steps = 247, loss = 2.9396681785583496
steps = 247, loss = 2.7234385013580322
steps = 247, loss = 2.696845769882202
steps = 247, loss = 2.8365824222564697
steps = 247, loss = 3.648224115371704
steps = 247, loss = 49.9853630065918
steps = 247, loss = 50.003963470458984
steps = 247, loss = 2.0322723388671875
steps = 247, loss = 3.1782026290893555
steps = 247, loss = 3.0311310291290283
steps = 247, loss = 3.991323709487915
steps = 247, loss = 3.3491694927215576
steps = 247, loss = 2.5357613563537598
steps = 247, loss = 7.09326696395874
steps = 247, loss = 4.824237823486328
steps = 247, loss = 2.034410238265991
steps = 247, loss = 2.1413159370422363
steps = 247, loss = 3.3385298252105713
steps = 247, loss = 3.623868942260742
steps = 247, loss = 3.3947649002075195
steps = 247, loss = 2.745060443878174
steps = 247, loss = 5.989027976989746
steps = 247, loss = 2.346081018447876
steps = 247, loss = 3.385983943939209
steps = 247, loss = 3.5004220008850098
steps = 247, loss = 2.8920724391937256
steps = 247, loss = 3.3977627754211426
steps = 248, loss = 2.301382303237915
steps = 248, loss = 2.853867530822754
steps = 248, loss = 49.9853630065918
steps = 248, loss = 4.8252363204956055
steps = 248, loss = 2.808518171310425
steps = 248, loss = 3.4554619789123535
steps = 248, loss = 49.64708709716797
steps = 248, loss = 5.036644458770752
steps = 248, loss = 3.9079325199127197
steps = 248, loss = 3.461758613586426
steps = 248, loss = 3.459725856781006
steps = 248, loss = 2.232536792755127
steps = 248, loss = 3.383408546447754
steps = 248, loss = 6.023688316345215
steps = 248, loss = 3.190593719482422
steps = 248, loss = 3.1940014362335205
steps = 248, loss = 3.1252212524414062
steps = 248, loss = 2.119818925857544
steps = 248, loss = 3.370029926300049
steps = 248, loss = 3.3042795658111572
steps = 248, loss = 2.716623306274414
steps = 248, loss = 2.723170518875122
steps = 248, loss = 7.151271343231201
steps = 248, loss = 3.668138265609741
steps = 248, loss = 3.563028335571289
steps = 248, loss = 2.826742649078369
steps = 248, loss = 2.9467318058013916
steps = 248, loss = 2.7657558917999268
steps = 248, loss = 2.4638235569000244
steps = 249, loss = 3.6392245292663574
steps = 249, loss = 2.7734413146972656
steps = 249, loss = 2.9783103466033936
steps = 249, loss = 3.4147136211395264
steps = 249, loss = 3.4440627098083496
steps = 249, loss = 3.19588565826416
steps = 249, loss = 3.2051002979278564
steps = 249, loss = 2.742124319076538
steps = 249, loss = 3.4803009033203125
steps = 249, loss = 6.036554336547852
steps = 249, loss = 2.8451743125915527
steps = 249, loss = 2.118544816970825
steps = 249, loss = 4.83499813079834
steps = 249, loss = 2.9501724243164062
steps = 249, loss = 2.9326016902923584
steps = 249, loss = 5.047738552093506
steps = 249, loss = 7.127775192260742
steps = 249, loss = 2.872972249984741
steps = 249, loss = 2.282632350921631
steps = 249, loss = 2.9966771602630615
steps = 249, loss = 2.07971453666687
steps = 249, loss = 50.016265869140625
steps = 249, loss = 3.470264434814453
steps = 249, loss = 3.383998394012451
steps = 249, loss = 49.9853630065918
steps = 249, loss = 19.0013427734375
steps = 249, loss = 2.6470489501953125
steps = 249, loss = 3.625864028930664
steps = 249, loss = 2.7324416637420654
steps = 250, loss = 3.515547513961792
steps = 250, loss = 3.412144422531128
steps = 250, loss = 5.054676532745361
steps = 250, loss = 2.2611031532287598
steps = 250, loss = 2.7748327255249023
steps = 250, loss = 2.9515957832336426
steps = 250, loss = 3.431766986846924
steps = 250, loss = 3.396832227706909
steps = 250, loss = 2.733882427215576
steps = 250, loss = 3.483509063720703
steps = 250, loss = 49.98381805419922
steps = 250, loss = 3.327643632888794
steps = 250, loss = 7.185635566711426
steps = 250, loss = 2.114349365234375
steps = 250, loss = 49.9853630065918
steps = 250, loss = 3.3943779468536377
steps = 250, loss = 2.132260322570801
steps = 250, loss = 3.228006601333618
steps = 250, loss = 2.9259958267211914
steps = 250, loss = 6.05277681350708
steps = 250, loss = 2.876098394393921
steps = 250, loss = 2.701350688934326
steps = 250, loss = 2.9880270957946777
steps = 250, loss = 3.1995444297790527
steps = 250, loss = 3.479816198348999
steps = 250, loss = 2.8705673217773438
steps = 250, loss = 4.846316814422607
steps = 250, loss = 2.636516809463501
steps = 250, loss = 2.5352487564086914
CPU times: user 1h 32min 35s, sys: 11min 9s, total: 1h 43min 44s
Wall time: 9h 7min 49s
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<style>#sk-container-id-3 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-3 {
  color: var(--sklearn-color-text);
}

#sk-container-id-3 pre {
  padding: 0;
}

#sk-container-id-3 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-3 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-3 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-3 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-3 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-3 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-3 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-3 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-3 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-3 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-3 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-3 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-3 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-3 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-3 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-3 div.sk-label label.sk-toggleable__label,
#sk-container-id-3 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-3 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-3 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-3 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-3 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-3 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-3 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-3 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-3 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div class="sk-top-container" id="sk-container-id-3"><div class="sk-text-repr-fallback"><pre>IncrementalSearchCV(decay_rate=0,
                    estimator=&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
),
                    max_iter=250, n_initial_parameters=29,
                    parameters={'batch_size': [32, 64, 128, 256, 512],
                                'module__activation': ['ReLU', 'LeakyReLU',
                                                       'ELU', 'PReLU'],
                                'module__init': ['xavier_uniform_',
                                                 'xavier_normal_',
                                                 'kaiming_uniform_',
                                                 'kaiming_norm...
       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,
       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,
       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,
       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),
                                'optimizer__nesterov': [True],
                                'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, ...],
                                'train_split': [None]},
                    random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox"/><label class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted" for="sk-estimator-id-7"> IncrementalSearchCV<span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>IncrementalSearchCV(decay_rate=0,
                    estimator=&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
),
                    max_iter=250, n_initial_parameters=29,
                    parameters={'batch_size': [32, 64, 128, 256, 512],
                                'module__activation': ['ReLU', 'LeakyReLU',
                                                       'ELU', 'PReLU'],
                                'module__init': ['xavier_uniform_',
                                                 'xavier_normal_',
                                                 'kaiming_uniform_',
                                                 'kaiming_norm...
       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,
       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,
       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,
       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),
                                'optimizer__nesterov': [True],
                                'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, ...],
                                'train_split': [None]},
                    random_state=42)</pre></div> </div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox"/><label class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted" for="sk-estimator-id-8">estimator: TrimParams</label><div class="sk-toggleable__content fitted"><pre>&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
)</pre></div> </div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox"/><label class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted" for="sk-estimator-id-9">TrimParams</label><div class="sk-toggleable__content fitted"><pre>&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
)</pre></div> </div></div></div></div></div></div></div></div></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">save_search</span><span class="p">(</span><span class="n">passive_search</span><span class="p">,</span> <span class="n">today</span><span class="p">,</span> <span class="s2">"passive"</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">compute</span><span class="p">(),</span> <span class="n">y_test</span><span class="o">.</span><span class="n">compute</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># timing_stats = client.profile()</span>
<span class="c1"># with open(f"{absolutepath_to_results}/final-final-timings.json", "w") as f:</span>
<span class="c1">#     json.dump(timing_stats, f)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<ul>
<li>patience: <code>max_iter // 8</code> (10 epochs)</li>
<li>n_initial: <code>2 * num_models</code></li>
</ul>
<p>This requires choosing</p>
<ul>
<li>the explore/exploit tradeoff (<code>patience</code> vs <code>n_initial</code>)</li>
<li>some estimate on many models will take advantage of <code>patience</code> to get total number of partial fit calls</li>
</ul>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">dask_ml.model_selection</span> <span class="kn">import</span> <span class="n">IncrementalSearchCV</span>

<span class="n">patience_search</span> <span class="o">=</span> <span class="n">IncrementalSearchCV</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">decay_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">patience</span><span class="o">=</span><span class="n">max_iter</span> <span class="o">//</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">n_initial_parameters</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">num_models</span><span class="p">,</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="n">num_calls</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>
<span class="n">patience_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>steps = 1, loss = 0.7015392184257507
steps = 1, loss = 2.0390679836273193
steps = 1, loss = 1.9451401233673096
steps = 1, loss = 1.9790714979171753
steps = 1, loss = 49.89901351928711
steps = 1, loss = 2.2601351737976074
steps = 1, loss = 1.9300516843795776
steps = 1, loss = 3.0525965690612793
steps = 1, loss = 1.9642322063446045
steps = 1, loss = 2.8931949138641357
steps = 1, loss = 3.145359516143799
steps = 1, loss = 1.9237724542617798
steps = 1, loss = 2.664077043533325
steps = 1, loss = 0.7161751985549927
steps = 1, loss = 2.6207008361816406
steps = 1, loss = 1.9601235389709473
steps = 1, loss = 0.7597142457962036
steps = 1, loss = 2.4687323570251465
steps = 1, loss = 1.9262726306915283
steps = 1, loss = 2.2864997386932373
steps = 1, loss = 1.119533896446228
steps = 1, loss = 2.8370532989501953
steps = 1, loss = 2.231611967086792
steps = 1, loss = 2.931516647338867
steps = 1, loss = 1.5063927173614502
steps = 1, loss = 3.0882468223571777
steps = 1, loss = 2.370192050933838
steps = 1, loss = 2.6160356998443604
steps = 1, loss = 1.4761656522750854
steps = 1, loss = 1.4569000005722046
steps = 1, loss = 1.9884908199310303
steps = 1, loss = 2.465902805328369
steps = 1, loss = 2.4441633224487305
steps = 1, loss = 1.7918369770050049
steps = 1, loss = 2.282869577407837
steps = 1, loss = 50.02168655395508
steps = 1, loss = 2.0435872077941895
steps = 1, loss = 50.02360534667969
steps = 1, loss = 0.8441962003707886
steps = 1, loss = 49.948604583740234
steps = 1, loss = 1.8131247758865356
steps = 1, loss = 2.700913429260254
steps = 1, loss = 1.5371888875961304
steps = 1, loss = 3.062363624572754
steps = 1, loss = 1.9901022911071777
steps = 1, loss = 2.2272908687591553
steps = 1, loss = 2.1988604068756104
steps = 1, loss = 1.8518195152282715
steps = 1, loss = 50.02899169921875
steps = 1, loss = 2.0213916301727295
steps = 1, loss = 2.0055603981018066
steps = 1, loss = 1.891239047050476
steps = 1, loss = 2.2822697162628174
steps = 1, loss = 1.9285523891448975
steps = 1, loss = 1.9646786451339722
steps = 1, loss = 2.231853485107422
steps = 1, loss = 1.0851256847381592
steps = 1, loss = 3.1120615005493164
steps = 2, loss = 2.0649361610412598
steps = 2, loss = 49.286922454833984
steps = 2, loss = 2.219710111618042
steps = 2, loss = 2.404712438583374
steps = 2, loss = 2.9821219444274902
steps = 2, loss = 2.936922550201416
steps = 2, loss = 2.0517542362213135
steps = 2, loss = 1.35455322265625
steps = 2, loss = 2.4960482120513916
steps = 2, loss = 2.1214840412139893
steps = 2, loss = 0.7862914800643921
steps = 2, loss = 5.033481597900391
steps = 2, loss = 1.2296638488769531
steps = 2, loss = 3.269145965576172
steps = 2, loss = 2.108257293701172
steps = 2, loss = 2.720346689224243
steps = 2, loss = 3.1571202278137207
steps = 2, loss = 2.9980525970458984
steps = 2, loss = 2.1790432929992676
steps = 2, loss = 2.750941753387451
steps = 2, loss = 2.4493439197540283
steps = 2, loss = 2.6125919818878174
steps = 2, loss = 2.8706159591674805
steps = 2, loss = 1.8386337757110596
steps = 2, loss = 2.044407606124878
steps = 2, loss = 1.5621674060821533
steps = 2, loss = 2.526874303817749
steps = 2, loss = 5.222795009613037
steps = 2, loss = 2.819878101348877
steps = 2, loss = 2.3319509029388428
steps = 2, loss = 2.7237470149993896
steps = 2, loss = 1.9905595779418945
steps = 2, loss = 2.0769336223602295
steps = 2, loss = 2.275181293487549
steps = 2, loss = 2.792921304702759
steps = 2, loss = 49.8498420715332
steps = 2, loss = 1.969590187072754
steps = 2, loss = 2.954298973083496
steps = 2, loss = 1.8490006923675537
steps = 2, loss = 2.574220657348633
steps = 2, loss = 49.973636627197266
steps = 2, loss = 4.053158760070801
steps = 2, loss = 2.5500240325927734
steps = 2, loss = 1.4387238025665283
steps = 2, loss = 50.00247573852539
steps = 2, loss = 2.066012382507324
steps = 2, loss = 2.5463247299194336
steps = 2, loss = 2.250905752182007
steps = 2, loss = 2.6103408336639404
steps = 2, loss = 2.668344259262085
steps = 2, loss = 2.225985527038574
steps = 2, loss = 50.02617263793945
steps = 2, loss = 2.3223307132720947
steps = 2, loss = 3.135007619857788
steps = 2, loss = 2.093651294708252
steps = 2, loss = 2.13775897026062
steps = 2, loss = 2.167015790939331
steps = 2, loss = 1.646716594696045
steps = 3, loss = 2.8757710456848145
steps = 3, loss = 1.7045027017593384
steps = 3, loss = 2.6059372425079346
steps = 3, loss = 1.7936562299728394
steps = 3, loss = 2.518653154373169
steps = 3, loss = 50.005252838134766
steps = 3, loss = 49.9119873046875
steps = 3, loss = 49.922935485839844
steps = 3, loss = 50.027320861816406
steps = 3, loss = 2.4380695819854736
steps = 3, loss = 2.698162078857422
steps = 3, loss = 2.3813552856445312
steps = 3, loss = 2.4385786056518555
steps = 3, loss = 2.336625814437866
steps = 3, loss = 2.41652250289917
steps = 3, loss = 2.166430711746216
steps = 3, loss = 3.664792776107788
steps = 3, loss = 1.9824475049972534
steps = 3, loss = 2.4180397987365723
steps = 3, loss = 1.7213953733444214
steps = 3, loss = 2.61824107170105
steps = 3, loss = 2.074545383453369
steps = 3, loss = 3.378053665161133
steps = 3, loss = 1.6481565237045288
steps = 3, loss = 3.1845643520355225
steps = 3, loss = 1.5599877834320068
steps = 3, loss = 4.595548152923584
steps = 3, loss = 2.7938125133514404
steps = 3, loss = 3.150792121887207
steps = 3, loss = 2.8628132343292236
steps = 3, loss = 2.0072033405303955
steps = 3, loss = 2.2573397159576416
steps = 3, loss = 2.2523958683013916
steps = 3, loss = 3.7210099697113037
steps = 3, loss = 2.7568612098693848
steps = 3, loss = 50.00235366821289
steps = 3, loss = 1.0675112009048462
steps = 3, loss = 2.1945977210998535
steps = 3, loss = 1.902837872505188
steps = 3, loss = 1.9859004020690918
steps = 3, loss = 2.085496425628662
steps = 3, loss = 2.068298816680908
steps = 3, loss = 2.731066942214966
steps = 3, loss = 2.638345956802368
steps = 3, loss = 2.5932514667510986
steps = 3, loss = 1.853012204170227
steps = 3, loss = 2.966437578201294
steps = 3, loss = 2.3415236473083496
steps = 3, loss = 5.248587608337402
steps = 3, loss = 3.0341780185699463
steps = 3, loss = 2.4184203147888184
steps = 3, loss = 1.9398105144500732
steps = 3, loss = 2.172227144241333
steps = 3, loss = 4.821182727813721
steps = 3, loss = 2.528197765350342
steps = 3, loss = 2.9078469276428223
steps = 3, loss = 3.2020678520202637
steps = 3, loss = 2.9952328205108643
steps = 4, loss = 2.869763135910034
steps = 4, loss = 2.5849149227142334
steps = 4, loss = 2.9004716873168945
steps = 4, loss = 5.2969889640808105
steps = 4, loss = 2.553907632827759
steps = 4, loss = 2.09587025642395
steps = 4, loss = 2.284367799758911
steps = 4, loss = 1.3774644136428833
steps = 4, loss = 50.02705383300781
steps = 4, loss = 2.4176688194274902
steps = 4, loss = 50.00544357299805
steps = 4, loss = 1.7633347511291504
steps = 4, loss = 6.606234073638916
steps = 4, loss = 2.34745717048645
steps = 4, loss = 1.9613579511642456
steps = 4, loss = 2.933858871459961
steps = 4, loss = 2.4137537479400635
steps = 4, loss = 2.7474024295806885
steps = 4, loss = 2.2892706394195557
steps = 4, loss = 2.272324800491333
steps = 4, loss = 3.420466899871826
steps = 4, loss = 2.697070360183716
steps = 4, loss = 2.0503737926483154
steps = 4, loss = 3.2813704013824463
steps = 4, loss = 3.1778223514556885
steps = 4, loss = 2.5223705768585205
steps = 4, loss = 3.8051984310150146
steps = 4, loss = 2.728101968765259
steps = 4, loss = 1.8871148824691772
steps = 4, loss = 3.267754554748535
steps = 4, loss = 2.586529493331909
steps = 4, loss = 2.7010061740875244
steps = 4, loss = 2.5015203952789307
steps = 4, loss = 2.8697495460510254
steps = 4, loss = 49.99324417114258
steps = 4, loss = 2.771122694015503
steps = 4, loss = 3.7421133518218994
steps = 4, loss = 50.00089645385742
steps = 4, loss = 1.8178670406341553
steps = 4, loss = 2.922982692718506
steps = 4, loss = 1.7109428644180298
steps = 4, loss = 2.60359525680542
steps = 4, loss = 1.837752103805542
steps = 4, loss = 2.6252503395080566
steps = 4, loss = 2.8182249069213867
steps = 4, loss = 2.2718122005462646
steps = 4, loss = 2.3392269611358643
steps = 4, loss = 3.164457082748413
steps = 4, loss = 3.0908029079437256
steps = 4, loss = 2.9083006381988525
steps = 4, loss = 2.636613368988037
steps = 4, loss = 2.3147690296173096
steps = 4, loss = 2.0846071243286133
steps = 4, loss = 50.01469421386719
steps = 4, loss = 2.36037278175354
steps = 4, loss = 2.008852243423462
steps = 4, loss = 4.9609694480896
steps = 4, loss = 2.180630683898926
steps = 5, loss = 2.507192850112915
steps = 5, loss = 2.136157989501953
steps = 5, loss = 3.1107330322265625
steps = 5, loss = 2.7396223545074463
steps = 5, loss = 2.2598114013671875
steps = 5, loss = 2.7305428981781006
steps = 5, loss = 2.6856870651245117
steps = 5, loss = 2.5234804153442383
steps = 5, loss = 2.8630707263946533
steps = 5, loss = 2.900348424911499
steps = 5, loss = 1.9341168403625488
steps = 5, loss = 2.6594841480255127
steps = 5, loss = 2.0741024017333984
steps = 5, loss = 2.7174839973449707
steps = 5, loss = 1.5417507886886597
steps = 5, loss = 2.5684804916381836
steps = 5, loss = 2.8630623817443848
steps = 5, loss = 2.4405791759490967
steps = 5, loss = 2.668637990951538
steps = 5, loss = 2.4874696731567383
steps = 5, loss = 50.02632522583008
steps = 5, loss = 2.5645663738250732
steps = 5, loss = 1.875414490699768
steps = 5, loss = 1.9661167860031128
steps = 5, loss = 2.9050989151000977
steps = 5, loss = 49.96736145019531
steps = 5, loss = 50.004722595214844
steps = 5, loss = 2.652508497238159
steps = 5, loss = 49.777408599853516
steps = 5, loss = 2.3283488750457764
steps = 5, loss = 6.175302982330322
steps = 5, loss = 4.228654861450195
steps = 5, loss = 2.9097650051116943
steps = 5, loss = 2.9138495922088623
steps = 5, loss = 5.950742244720459
steps = 5, loss = 3.2261462211608887
steps = 5, loss = 1.9064459800720215
steps = 5, loss = 2.0518686771392822
steps = 5, loss = 2.8656198978424072
steps = 5, loss = 2.0766477584838867
steps = 5, loss = 3.423321485519409
steps = 5, loss = 1.8701012134552002
steps = 5, loss = 2.379734516143799
steps = 5, loss = 3.5413577556610107
steps = 5, loss = 2.6380293369293213
steps = 5, loss = 3.202521562576294
steps = 5, loss = 3.284722089767456
steps = 5, loss = 2.214536190032959
steps = 5, loss = 5.223073959350586
steps = 5, loss = 2.092392683029175
steps = 5, loss = 2.197572708129883
steps = 5, loss = 3.796440601348877
steps = 5, loss = 1.8261762857437134
steps = 5, loss = 2.368847608566284
steps = 5, loss = 1.7875397205352783
steps = 5, loss = 2.8760852813720703
steps = 5, loss = 2.909943103790283
steps = 5, loss = 2.818068265914917
steps = 6, loss = 3.1927297115325928
steps = 6, loss = 4.76845121383667
steps = 6, loss = 2.0739760398864746
steps = 6, loss = 2.7167561054229736
steps = 6, loss = 2.7454211711883545
steps = 6, loss = 2.9254348278045654
steps = 6, loss = 50.01953887939453
steps = 6, loss = 2.3247945308685303
steps = 6, loss = 2.8850719928741455
steps = 6, loss = 2.6140666007995605
steps = 6, loss = 7.43291711807251
steps = 6, loss = 2.8265185356140137
steps = 6, loss = 1.9434036016464233
steps = 6, loss = 1.8626712560653687
steps = 6, loss = 3.070068359375
steps = 6, loss = 5.830057144165039
steps = 6, loss = 2.935776948928833
steps = 6, loss = 2.354790449142456
steps = 6, loss = 1.6322509050369263
steps = 6, loss = 50.00014114379883
steps = 6, loss = 2.5805628299713135
steps = 6, loss = 3.9815497398376465
steps = 6, loss = 1.9393035173416138
steps = 6, loss = 2.7255663871765137
steps = 6, loss = 2.7996411323547363
steps = 6, loss = 3.516786813735962
steps = 6, loss = 1.959154486656189
steps = 6, loss = 3.146223306655884
steps = 6, loss = 2.20033860206604
steps = 6, loss = 2.298417806625366
steps = 6, loss = 2.5187020301818848
steps = 6, loss = 1.9093722105026245
steps = 6, loss = 3.3311328887939453
steps = 6, loss = 2.322777271270752
steps = 6, loss = 2.810650587081909
steps = 6, loss = 2.8044369220733643
steps = 6, loss = 2.4400150775909424
steps = 6, loss = 2.0935351848602295
steps = 6, loss = 2.628380298614502
steps = 6, loss = 2.864506483078003
steps = 6, loss = 2.6544721126556396
steps = 6, loss = 2.2931809425354004
steps = 6, loss = 2.559171676635742
steps = 6, loss = 2.921271324157715
steps = 6, loss = 3.0713515281677246
steps = 6, loss = 3.1256301403045654
steps = 6, loss = 50.004058837890625
steps = 6, loss = 1.8311915397644043
steps = 6, loss = 2.7619447708129883
steps = 6, loss = 2.9662375450134277
steps = 6, loss = 1.9604063034057617
steps = 6, loss = 2.1009979248046875
steps = 6, loss = 3.919163465499878
steps = 6, loss = 2.5819852352142334
steps = 6, loss = 2.5720412731170654
steps = 6, loss = 1.6535793542861938
steps = 6, loss = 2.086374282836914
steps = 6, loss = 49.951011657714844
steps = 7, loss = 3.2675862312316895
steps = 7, loss = 2.367860794067383
steps = 7, loss = 3.0102570056915283
steps = 7, loss = 1.945683479309082
steps = 7, loss = 2.5355284214019775
steps = 7, loss = 2.4694983959198
steps = 7, loss = 2.9017679691314697
steps = 7, loss = 2.085435628890991
steps = 7, loss = 1.9722485542297363
steps = 7, loss = 2.098924398422241
steps = 7, loss = 49.978981018066406
steps = 7, loss = 2.983721971511841
steps = 7, loss = 3.9497201442718506
steps = 7, loss = 3.252452850341797
steps = 7, loss = 2.6030681133270264
steps = 7, loss = 2.734426259994507
steps = 7, loss = 2.770885944366455
steps = 7, loss = 3.5864787101745605
steps = 7, loss = 1.8988540172576904
steps = 7, loss = 1.8724186420440674
steps = 7, loss = 2.1079177856445312
steps = 7, loss = 8.074295043945312
steps = 7, loss = 3.39957857131958
steps = 7, loss = 4.63505744934082
steps = 7, loss = 2.094320058822632
steps = 7, loss = 2.996690511703491
steps = 7, loss = 2.9767231941223145
steps = 7, loss = 2.8346219062805176
steps = 7, loss = 2.971647262573242
steps = 7, loss = 3.020712375640869
steps = 7, loss = 1.9977818727493286
steps = 7, loss = 2.6060516834259033
steps = 7, loss = 2.706450939178467
steps = 7, loss = 3.033792018890381
steps = 7, loss = 3.029603958129883
steps = 7, loss = 2.3815033435821533
steps = 7, loss = 1.6992332935333252
steps = 7, loss = 2.0324411392211914
steps = 7, loss = 2.9352145195007324
steps = 7, loss = 50.005027770996094
steps = 7, loss = 2.117724895477295
steps = 7, loss = 2.7155771255493164
steps = 7, loss = 2.396942377090454
steps = 7, loss = 2.1252963542938232
steps = 7, loss = 2.552610158920288
steps = 7, loss = 2.481874704360962
steps = 7, loss = 2.6567630767822266
steps = 7, loss = 2.572456121444702
steps = 7, loss = 50.002708435058594
steps = 7, loss = 2.8589744567871094
steps = 7, loss = 49.89972686767578
steps = 7, loss = 2.4337973594665527
steps = 7, loss = 2.887641668319702
steps = 7, loss = 1.977953553199768
steps = 7, loss = 6.129579067230225
steps = 7, loss = 2.7977283000946045
steps = 7, loss = 3.465596914291382
steps = 7, loss = 3.3643598556518555
steps = 8, loss = 2.843292713165283
steps = 8, loss = 3.762173652648926
steps = 8, loss = 2.0256459712982178
steps = 8, loss = 2.899463415145874
steps = 8, loss = 2.712872266769409
steps = 8, loss = 3.0281407833099365
steps = 8, loss = 2.191598415374756
steps = 8, loss = 2.931323766708374
steps = 8, loss = 2.708441972732544
steps = 8, loss = 2.82892107963562
steps = 8, loss = 49.83818435668945
steps = 8, loss = 2.6285409927368164
steps = 8, loss = 2.7201671600341797
steps = 8, loss = 1.9952701330184937
steps = 8, loss = 3.0815234184265137
steps = 8, loss = 2.036494016647339
steps = 8, loss = 2.0609288215637207
steps = 8, loss = 49.8573112487793
steps = 8, loss = 3.2793421745300293
steps = 8, loss = 2.829104423522949
steps = 8, loss = 2.9406816959381104
steps = 8, loss = 6.0263166427612305
steps = 8, loss = 2.4890360832214355
steps = 8, loss = 2.431933879852295
steps = 8, loss = 2.6645619869232178
steps = 8, loss = 2.931305170059204
steps = 8, loss = 3.275049924850464
steps = 8, loss = 3.0609378814697266
steps = 8, loss = 2.6608221530914307
steps = 8, loss = 3.2651407718658447
steps = 8, loss = 2.2813358306884766
steps = 8, loss = 2.1820785999298096
steps = 8, loss = 2.7533743381500244
steps = 8, loss = 2.0975515842437744
steps = 8, loss = 50.007904052734375
steps = 8, loss = 1.7663187980651855
steps = 8, loss = 3.087885618209839
steps = 8, loss = 2.0935263633728027
steps = 8, loss = 2.1898818016052246
steps = 8, loss = 1.9480024576187134
steps = 8, loss = 2.873795747756958
steps = 8, loss = 2.7987618446350098
steps = 8, loss = 4.747208595275879
steps = 8, loss = 3.133970260620117
steps = 8, loss = 7.5726318359375
steps = 8, loss = 2.699868679046631
steps = 8, loss = 2.2702789306640625
steps = 8, loss = 2.9712185859680176
steps = 8, loss = 2.7721426486968994
steps = 8, loss = 1.927833080291748
steps = 8, loss = 2.8880667686462402
steps = 8, loss = 5.236158847808838
steps = 8, loss = 49.97856140136719
steps = 8, loss = 3.226499080657959
steps = 8, loss = 2.589491128921509
steps = 8, loss = 3.1896982192993164
steps = 8, loss = 3.6344852447509766
steps = 8, loss = 2.648833751678467
steps = 9, loss = 2.6948390007019043
steps = 9, loss = 2.423799991607666
steps = 9, loss = 2.9663896560668945
steps = 9, loss = 2.061859369277954
steps = 9, loss = 3.6917755603790283
steps = 9, loss = 3.896336317062378
steps = 9, loss = 2.0903797149658203
steps = 9, loss = 1.8051013946533203
steps = 9, loss = 2.045916795730591
steps = 9, loss = 4.966381072998047
steps = 9, loss = 2.0714385509490967
steps = 9, loss = 7.617615699768066
steps = 9, loss = 3.0947201251983643
steps = 9, loss = 1.9695758819580078
steps = 9, loss = 2.8642613887786865
steps = 9, loss = 1.952425479888916
steps = 9, loss = 6.025022029876709
steps = 9, loss = 3.0578811168670654
steps = 9, loss = 2.266094923019409
steps = 9, loss = 2.6036221981048584
steps = 9, loss = 2.024412155151367
steps = 9, loss = 2.7590689659118652
steps = 9, loss = 2.9196200370788574
steps = 9, loss = 5.223305702209473
steps = 9, loss = 2.5905537605285645
steps = 9, loss = 3.168510675430298
steps = 9, loss = 2.188227653503418
steps = 9, loss = 3.550602436065674
steps = 9, loss = 43.64059066772461
steps = 9, loss = 2.0300145149230957
steps = 9, loss = 2.8205976486206055
steps = 9, loss = 50.014862060546875
steps = 9, loss = 2.8863162994384766
steps = 9, loss = 2.783552408218384
steps = 9, loss = 2.538752555847168
steps = 9, loss = 2.2480087280273438
steps = 9, loss = 3.0708749294281006
steps = 9, loss = 2.3206803798675537
steps = 9, loss = 2.1942880153656006
steps = 9, loss = 2.931478261947632
steps = 9, loss = 3.2707769870758057
steps = 9, loss = 3.254173755645752
steps = 9, loss = 2.8201372623443604
steps = 9, loss = 3.2295634746551514
steps = 9, loss = 2.2369143962860107
steps = 9, loss = 2.9424986839294434
steps = 9, loss = 3.0516226291656494
steps = 9, loss = 2.640316963195801
steps = 9, loss = 2.871965169906616
steps = 9, loss = 2.764437198638916
steps = 9, loss = 2.4665329456329346
steps = 9, loss = 2.5220556259155273
steps = 9, loss = 3.118269443511963
steps = 9, loss = 2.8333842754364014
steps = 9, loss = 3.4346323013305664
steps = 9, loss = 2.9034838676452637
steps = 9, loss = 49.97856521606445
steps = 9, loss = 49.64495086669922
steps = 10, loss = 3.102877616882324
steps = 10, loss = 1.998290777206421
steps = 10, loss = 2.8371362686157227
steps = 10, loss = 49.978389739990234
steps = 10, loss = 1.9846127033233643
steps = 10, loss = 50.01826095581055
steps = 10, loss = 2.1664412021636963
steps = 10, loss = 2.5963661670684814
steps = 10, loss = 2.135108232498169
steps = 10, loss = 3.5914671421051025
steps = 10, loss = 2.587646722793579
steps = 10, loss = 3.455730438232422
steps = 10, loss = 2.0699429512023926
steps = 10, loss = 3.412931203842163
steps = 10, loss = 2.684443950653076
steps = 10, loss = 2.7096505165100098
steps = 10, loss = 2.9215197563171387
steps = 10, loss = 4.005661964416504
steps = 10, loss = 3.099548578262329
steps = 10, loss = 2.994440793991089
steps = 10, loss = 2.2225091457366943
steps = 10, loss = 7.468094348907471
steps = 10, loss = 3.242859363555908
steps = 10, loss = 3.1428163051605225
steps = 10, loss = 2.975102186203003
steps = 10, loss = 2.0943503379821777
steps = 10, loss = 2.272334337234497
steps = 10, loss = 2.8652751445770264
steps = 10, loss = 1.9852395057678223
steps = 10, loss = 3.0565383434295654
steps = 10, loss = 49.98720932006836
steps = 10, loss = 3.2355968952178955
steps = 10, loss = 2.621227979660034
steps = 10, loss = 1.8442074060440063
steps = 10, loss = 2.351073741912842
steps = 10, loss = 2.765108346939087
steps = 10, loss = 3.353376865386963
steps = 10, loss = 2.9227564334869385
steps = 10, loss = 5.521064758300781
steps = 10, loss = 2.890779972076416
steps = 10, loss = 3.0427889823913574
steps = 10, loss = 6.9440741539001465
steps = 10, loss = 2.4596214294433594
steps = 10, loss = 2.9537150859832764
steps = 10, loss = 2.604849338531494
steps = 10, loss = 49.888309478759766
steps = 10, loss = 3.8209891319274902
steps = 10, loss = 2.2909581661224365
steps = 10, loss = 4.490538120269775
steps = 10, loss = 2.8519551753997803
steps = 10, loss = 2.779078483581543
steps = 10, loss = 3.3658628463745117
steps = 10, loss = 2.0586390495300293
steps = 10, loss = 2.8146941661834717
steps = 10, loss = 2.158473253250122
steps = 10, loss = 2.4244906902313232
steps = 10, loss = 3.322519302368164
steps = 10, loss = 2.539151430130005
steps = 11, loss = 3.1012611389160156
steps = 11, loss = 2.0617642402648926
steps = 11, loss = 2.9733901023864746
steps = 11, loss = 2.3356645107269287
steps = 11, loss = 2.6846566200256348
steps = 11, loss = 3.1232662200927734
steps = 11, loss = 3.040902853012085
steps = 11, loss = 2.258586883544922
steps = 11, loss = 2.6325440406799316
steps = 11, loss = 2.08534574508667
steps = 11, loss = 2.8418779373168945
steps = 11, loss = 2.164579153060913
steps = 11, loss = 3.2225284576416016
steps = 11, loss = 2.9581353664398193
steps = 11, loss = 3.4875550270080566
steps = 11, loss = 2.2563188076019287
steps = 11, loss = 49.98544692993164
steps = 11, loss = 3.129474401473999
steps = 11, loss = 2.598294734954834
steps = 11, loss = 2.655843496322632
steps = 11, loss = 2.896632671356201
steps = 11, loss = 8.023770332336426
steps = 11, loss = 2.4906668663024902
steps = 11, loss = 2.7861390113830566
steps = 11, loss = 49.97831726074219
steps = 11, loss = 2.0004589557647705
steps = 11, loss = 3.6143362522125244
steps = 11, loss = 2.114468812942505
steps = 11, loss = 2.975996971130371
steps = 11, loss = 2.7062535285949707
steps = 11, loss = 3.167548894882202
steps = 11, loss = 2.0769906044006348
steps = 11, loss = 6.395556449890137
steps = 11, loss = 4.871739387512207
steps = 11, loss = 2.5973384380340576
steps = 11, loss = 3.3722972869873047
steps = 11, loss = 2.0898656845092773
steps = 11, loss = 2.907092571258545
steps = 11, loss = 49.857540130615234
steps = 11, loss = 3.4029791355133057
steps = 11, loss = 2.2106354236602783
steps = 11, loss = 2.764160633087158
steps = 11, loss = 3.828382730484009
steps = 11, loss = 49.9925651550293
steps = 11, loss = 3.135998487472534
steps = 11, loss = 2.249476432800293
steps = 11, loss = 2.0119783878326416
steps = 11, loss = 4.108358383178711
steps = 11, loss = 2.875809669494629
steps = 11, loss = 2.8779797554016113
steps = 11, loss = 3.0085465908050537
steps = 11, loss = 3.092287540435791
steps = 11, loss = 3.119128465652466
steps = 11, loss = 1.8682276010513306
steps = 11, loss = 2.6113476753234863
steps = 11, loss = 2.4450221061706543
steps = 11, loss = 3.1115453243255615
steps = 11, loss = 3.3239753246307373
steps = 12, loss = 2.6876182556152344
steps = 12, loss = 2.7547028064727783
steps = 12, loss = 3.0222153663635254
steps = 12, loss = 49.98743438720703
steps = 12, loss = 49.87403869628906
steps = 12, loss = 3.3500237464904785
steps = 12, loss = 50.02107238769531
steps = 12, loss = 2.361048698425293
steps = 12, loss = 2.1024045944213867
steps = 12, loss = 2.9160211086273193
steps = 12, loss = 2.118375539779663
steps = 12, loss = 2.3640639781951904
steps = 12, loss = 2.1764283180236816
steps = 12, loss = 2.0049328804016113
steps = 12, loss = 3.1217293739318848
steps = 12, loss = 3.067706823348999
steps = 12, loss = 3.0365347862243652
steps = 12, loss = 2.484968900680542
steps = 12, loss = 2.202303409576416
steps = 12, loss = 3.2750020027160645
steps = 12, loss = 2.825817108154297
steps = 12, loss = 2.7000629901885986
steps = 12, loss = 6.262167453765869
steps = 12, loss = 2.458239793777466
steps = 12, loss = 2.135294198989868
steps = 12, loss = 2.183518886566162
steps = 12, loss = 3.4791698455810547
steps = 12, loss = 2.5844886302948
steps = 12, loss = 2.442201614379883
steps = 12, loss = 4.471340179443359
steps = 12, loss = 3.8770177364349365
steps = 12, loss = 3.051820755004883
steps = 12, loss = 1.8807262182235718
steps = 12, loss = 3.049307346343994
steps = 12, loss = 2.102280616760254
steps = 12, loss = 2.6556854248046875
steps = 12, loss = 2.9062130451202393
steps = 12, loss = 3.103567361831665
steps = 12, loss = 2.174433946609497
steps = 12, loss = 3.002401828765869
steps = 12, loss = 2.940290689468384
steps = 12, loss = 2.8254778385162354
steps = 12, loss = 2.5551211833953857
steps = 12, loss = 2.0566301345825195
steps = 12, loss = 50.02360153198242
steps = 12, loss = 3.319959878921509
steps = 12, loss = 3.1269850730895996
steps = 12, loss = 2.870781183242798
steps = 12, loss = 3.1236557960510254
steps = 12, loss = 2.5281436443328857
steps = 12, loss = 2.0159008502960205
steps = 12, loss = 3.0580379962921143
steps = 12, loss = 2.3964884281158447
steps = 12, loss = 4.105146408081055
steps = 12, loss = 3.134356737136841
steps = 12, loss = 3.068403720855713
steps = 12, loss = 2.659208059310913
steps = 12, loss = 6.993444919586182
steps = 13, loss = 2.561925172805786
steps = 13, loss = 2.410606622695923
steps = 13, loss = 2.246976852416992
steps = 13, loss = 3.4355850219726562
steps = 13, loss = 2.1245765686035156
steps = 13, loss = 50.01811981201172
steps = 13, loss = 3.2199788093566895
steps = 13, loss = 2.6666533946990967
steps = 13, loss = 4.814917087554932
steps = 13, loss = 49.85498046875
steps = 13, loss = 2.870708703994751
steps = 13, loss = 3.0464160442352295
steps = 13, loss = 4.1298370361328125
steps = 13, loss = 2.573831081390381
steps = 13, loss = 3.4004130363464355
steps = 13, loss = 3.108943223953247
steps = 13, loss = 2.0274267196655273
steps = 13, loss = 3.715946674346924
steps = 13, loss = 2.416231870651245
steps = 13, loss = 2.573678731918335
steps = 13, loss = 3.062328577041626
steps = 13, loss = 3.057060718536377
steps = 13, loss = 2.965954303741455
steps = 13, loss = 3.1589999198913574
steps = 13, loss = 2.922393560409546
steps = 13, loss = 3.9320218563079834
steps = 13, loss = 3.152338981628418
steps = 13, loss = 2.130009889602661
steps = 13, loss = 2.1606922149658203
steps = 13, loss = 2.21895432472229
steps = 13, loss = 2.526712417602539
steps = 13, loss = 2.8347575664520264
steps = 13, loss = 2.716315984725952
steps = 13, loss = 3.0724799633026123
steps = 13, loss = 2.1422359943389893
steps = 13, loss = 2.695974349975586
steps = 13, loss = 3.0477640628814697
steps = 13, loss = 6.832388877868652
steps = 13, loss = 2.596071243286133
steps = 13, loss = 2.691761016845703
steps = 13, loss = 3.071216106414795
steps = 13, loss = 2.4494752883911133
steps = 13, loss = 2.3506240844726562
steps = 13, loss = 3.0888874530792236
steps = 13, loss = 1.898645281791687
steps = 13, loss = 3.2251012325286865
steps = 13, loss = 3.2928988933563232
steps = 13, loss = 2.028848171234131
steps = 13, loss = 2.2100610733032227
steps = 13, loss = 2.1359410285949707
steps = 13, loss = 7.835664749145508
steps = 13, loss = 2.287564277648926
steps = 13, loss = 2.0199663639068604
steps = 13, loss = 49.98232650756836
steps = 13, loss = 49.96748352050781
steps = 13, loss = 2.655860185623169
steps = 13, loss = 2.7596442699432373
steps = 13, loss = 2.834336042404175
steps = 14, loss = 2.8245816230773926
steps = 14, loss = 3.2170565128326416
steps = 14, loss = 2.574479341506958
steps = 14, loss = 2.710380792617798
steps = 14, loss = 2.251413106918335
steps = 14, loss = 2.7354631423950195
steps = 14, loss = 2.2539618015289307
steps = 14, loss = 2.2011053562164307
steps = 14, loss = 2.7152469158172607
steps = 14, loss = 3.0844178199768066
steps = 14, loss = 2.559058666229248
steps = 14, loss = 2.7473599910736084
steps = 14, loss = 9.8505220413208
steps = 14, loss = 2.4778847694396973
steps = 14, loss = 2.1733362674713135
steps = 14, loss = 2.2793290615081787
steps = 14, loss = 49.98201370239258
steps = 14, loss = 8.22896957397461
steps = 14, loss = 2.920725107192993
steps = 14, loss = 3.1657769680023193
steps = 14, loss = 50.004825592041016
steps = 14, loss = 2.6695101261138916
steps = 14, loss = 7.1159586906433105
steps = 14, loss = 2.196284532546997
steps = 14, loss = 50.025123596191406
steps = 14, loss = 3.4847381114959717
steps = 14, loss = 2.2674977779388428
steps = 14, loss = 3.1673226356506348
steps = 14, loss = 3.0942795276641846
steps = 14, loss = 1.9320112466812134
steps = 14, loss = 3.403198003768921
steps = 14, loss = 49.80181121826172
steps = 14, loss = 2.598461389541626
steps = 14, loss = 2.1249895095825195
steps = 14, loss = 3.1390557289123535
steps = 14, loss = 2.4843404293060303
steps = 14, loss = 3.0058176517486572
steps = 14, loss = 3.1906073093414307
steps = 14, loss = 3.552382230758667
steps = 14, loss = 2.9089462757110596
steps = 14, loss = 2.8938164710998535
steps = 14, loss = 2.056494951248169
steps = 14, loss = 2.773266553878784
steps = 14, loss = 5.082920551300049
steps = 14, loss = 3.0394906997680664
steps = 14, loss = 2.0605664253234863
steps = 14, loss = 2.1582186222076416
steps = 14, loss = 3.9791135787963867
steps = 14, loss = 2.83217716217041
steps = 14, loss = 3.3922202587127686
steps = 14, loss = 3.1613943576812744
steps = 14, loss = 3.3523612022399902
steps = 14, loss = 2.156892776489258
steps = 14, loss = 4.354515075683594
steps = 14, loss = 3.0778870582580566
steps = 14, loss = 3.4648385047912598
steps = 14, loss = 2.8860669136047363
steps = 14, loss = 3.135697364807129
steps = 15, loss = 2.1109373569488525
steps = 15, loss = 8.616015434265137
steps = 15, loss = 3.1060333251953125
steps = 15, loss = 3.0108368396759033
steps = 15, loss = 2.8625030517578125
steps = 15, loss = 2.570404052734375
steps = 15, loss = 2.5191729068756104
steps = 15, loss = 4.488426685333252
steps = 15, loss = 3.299008846282959
steps = 15, loss = 2.878877639770508
steps = 15, loss = 2.201529026031494
steps = 15, loss = 2.232065439224243
steps = 15, loss = 3.062758445739746
steps = 15, loss = 3.1098084449768066
steps = 15, loss = 7.00740909576416
steps = 15, loss = 3.143958806991577
steps = 15, loss = 2.3111166954040527
steps = 15, loss = 50.00859832763672
steps = 15, loss = 2.466269016265869
steps = 15, loss = 3.1959149837493896
steps = 15, loss = 2.221651792526245
steps = 15, loss = 2.987346649169922
steps = 15, loss = 49.76908874511719
steps = 15, loss = 1.9472872018814087
steps = 15, loss = 2.7329258918762207
steps = 15, loss = 2.889645576477051
steps = 15, loss = 2.146571397781372
steps = 15, loss = 3.463639497756958
steps = 15, loss = 3.1550209522247314
steps = 15, loss = 2.673595428466797
steps = 15, loss = 3.9497640132904053
steps = 15, loss = 3.3116629123687744
steps = 15, loss = 2.187351703643799
steps = 15, loss = 4.937313556671143
steps = 15, loss = 2.258702278137207
steps = 15, loss = 2.225647449493408
steps = 15, loss = 7.25535249710083
steps = 15, loss = 3.355714797973633
steps = 15, loss = 50.005550384521484
steps = 15, loss = 3.45809006690979
steps = 15, loss = 3.0063745975494385
steps = 15, loss = 3.155074119567871
steps = 15, loss = 2.0698249340057373
steps = 15, loss = 2.227130174636841
steps = 15, loss = 3.308821439743042
steps = 15, loss = 2.769657850265503
steps = 15, loss = 49.98201370239258
steps = 15, loss = 2.8845598697662354
steps = 15, loss = 2.069385528564453
steps = 15, loss = 2.8023879528045654
steps = 15, loss = 2.945326566696167
steps = 15, loss = 2.602475881576538
steps = 15, loss = 2.5451736450195312
steps = 15, loss = 2.737489938735962
steps = 15, loss = 1.900665283203125
steps = 15, loss = 3.266216278076172
steps = 15, loss = 3.045522451400757
steps = 15, loss = 3.192013740539551
steps = 16, loss = 3.213912010192871
steps = 16, loss = 2.890648365020752
steps = 16, loss = 2.516252040863037
steps = 16, loss = 2.9281623363494873
steps = 16, loss = 2.2616453170776367
steps = 16, loss = 3.485438585281372
steps = 16, loss = 1.9679415225982666
steps = 16, loss = 3.504931926727295
steps = 16, loss = 2.562523603439331
steps = 16, loss = 2.088923215866089
steps = 16, loss = 3.353464365005493
steps = 16, loss = 2.7867398262023926
steps = 16, loss = 2.2625796794891357
steps = 16, loss = 2.576206684112549
steps = 16, loss = 2.353614330291748
steps = 16, loss = 3.1191341876983643
steps = 16, loss = 2.3004050254821777
steps = 16, loss = 2.27079439163208
steps = 16, loss = 3.492610216140747
steps = 16, loss = 7.093342304229736
steps = 16, loss = 3.244225025177002
steps = 16, loss = 3.090008020401001
steps = 16, loss = 4.438423156738281
steps = 16, loss = 2.2207205295562744
steps = 16, loss = 2.8345625400543213
steps = 16, loss = 2.8002452850341797
steps = 16, loss = 3.2897250652313232
steps = 16, loss = 2.8970625400543213
steps = 16, loss = 50.020240783691406
steps = 16, loss = 3.0760045051574707
steps = 16, loss = 2.6577932834625244
steps = 16, loss = 3.2424845695495605
steps = 16, loss = 49.882240295410156
steps = 16, loss = 2.2322957515716553
steps = 16, loss = 2.0916764736175537
steps = 16, loss = 2.7286415100097656
steps = 16, loss = 49.67563247680664
steps = 16, loss = 3.5139455795288086
steps = 16, loss = 3.186962366104126
steps = 16, loss = 2.256559371948242
steps = 16, loss = 2.6021177768707275
steps = 16, loss = 2.6611554622650146
steps = 16, loss = 2.5642080307006836
steps = 16, loss = 2.226651191711426
steps = 16, loss = 2.4929556846618652
steps = 16, loss = 2.7189180850982666
steps = 16, loss = 5.673459053039551
steps = 16, loss = 3.2164783477783203
steps = 16, loss = 2.973392963409424
steps = 16, loss = 3.1733720302581787
steps = 16, loss = 4.036111831665039
steps = 16, loss = 2.805302381515503
steps = 16, loss = 2.904160261154175
steps = 16, loss = 3.27253794670105
steps = 16, loss = 49.98201370239258
steps = 16, loss = 10.703156471252441
steps = 16, loss = 8.752373695373535
steps = 16, loss = 2.203538656234741
steps = 17, loss = 2.971449613571167
steps = 17, loss = 2.7371432781219482
steps = 17, loss = 3.2038726806640625
steps = 17, loss = 2.8337230682373047
steps = 17, loss = 3.505112648010254
steps = 17, loss = 3.13600754737854
steps = 17, loss = 2.9531779289245605
steps = 17, loss = 4.022747993469238
steps = 17, loss = 2.246251106262207
steps = 17, loss = 3.1270411014556885
steps = 17, loss = 6.552212715148926
steps = 17, loss = 3.195345640182495
steps = 17, loss = 2.2265844345092773
steps = 17, loss = 2.6837236881256104
steps = 17, loss = 1.967326283454895
steps = 17, loss = 2.8623507022857666
steps = 17, loss = 2.0877773761749268
steps = 17, loss = 2.028839111328125
steps = 17, loss = 2.628732681274414
steps = 17, loss = 3.085937261581421
steps = 17, loss = 3.324000358581543
steps = 17, loss = 2.8459274768829346
steps = 17, loss = 4.7720770835876465
steps = 17, loss = 3.1071619987487793
steps = 17, loss = 2.0840306282043457
steps = 17, loss = 2.6812422275543213
steps = 17, loss = 49.5657844543457
steps = 17, loss = 50.0180549621582
steps = 17, loss = 2.5251717567443848
steps = 17, loss = 2.188375234603882
steps = 17, loss = 2.1383628845214844
steps = 17, loss = 2.8094308376312256
steps = 17, loss = 3.3568363189697266
steps = 17, loss = 4.21324348449707
steps = 17, loss = 3.0538246631622314
steps = 17, loss = 50.00963592529297
steps = 17, loss = 2.514415979385376
steps = 17, loss = 2.365649938583374
steps = 17, loss = 3.017382860183716
steps = 17, loss = 2.5617902278900146
steps = 17, loss = 2.4683055877685547
steps = 17, loss = 2.2906432151794434
steps = 17, loss = 3.2993874549865723
steps = 17, loss = 2.748641014099121
steps = 17, loss = 2.2234976291656494
steps = 17, loss = 8.1475248336792
steps = 17, loss = 2.2621586322784424
steps = 17, loss = 2.0929689407348633
steps = 17, loss = 2.1422648429870605
steps = 17, loss = 3.2137956619262695
steps = 17, loss = 3.1913259029388428
steps = 17, loss = 3.401850461959839
steps = 17, loss = 7.422162055969238
steps = 17, loss = 3.470935821533203
steps = 17, loss = 50.01982498168945
steps = 17, loss = 2.7115132808685303
steps = 17, loss = 2.582547187805176
steps = 17, loss = 3.244493246078491
steps = 18, loss = 2.479257822036743
steps = 18, loss = 2.610960006713867
steps = 18, loss = 2.9778506755828857
steps = 18, loss = 2.283322811126709
steps = 18, loss = 2.8749163150787354
steps = 18, loss = 1.9880399703979492
steps = 18, loss = 3.2047572135925293
steps = 18, loss = 2.4151175022125244
steps = 18, loss = 3.512734889984131
steps = 18, loss = 3.2447030544281006
steps = 18, loss = 3.404524326324463
steps = 18, loss = 4.909353256225586
steps = 18, loss = 2.6137592792510986
steps = 18, loss = 50.020503997802734
steps = 18, loss = 2.2529287338256836
steps = 18, loss = 2.094993829727173
steps = 18, loss = 2.6801233291625977
steps = 18, loss = 49.50071334838867
steps = 18, loss = 4.016269683837891
steps = 18, loss = 2.074047803878784
steps = 18, loss = 7.896333694458008
steps = 18, loss = 2.3604769706726074
steps = 18, loss = 3.0023038387298584
steps = 18, loss = 2.315981864929199
steps = 18, loss = 2.6734888553619385
steps = 18, loss = 10.178664207458496
steps = 18, loss = 3.2545225620269775
steps = 18, loss = 3.168175458908081
steps = 18, loss = 2.843742847442627
steps = 18, loss = 2.8821425437927246
steps = 18, loss = 3.2495639324188232
steps = 18, loss = 2.636237621307373
steps = 18, loss = 3.0640311241149902
steps = 18, loss = 3.114203929901123
steps = 18, loss = 2.8135485649108887
steps = 18, loss = 2.7189526557922363
steps = 18, loss = 1.9897888898849487
steps = 18, loss = 2.616983413696289
steps = 18, loss = 3.2061595916748047
steps = 18, loss = 2.527163028717041
steps = 18, loss = 3.409008264541626
steps = 18, loss = 3.262800693511963
steps = 18, loss = 2.106484889984131
steps = 18, loss = 2.235382556915283
steps = 18, loss = 2.1145987510681152
steps = 18, loss = 3.231790542602539
steps = 18, loss = 2.795353412628174
steps = 18, loss = 3.1730101108551025
steps = 18, loss = 2.878051996231079
steps = 18, loss = 2.543097734451294
steps = 18, loss = 50.02204513549805
steps = 18, loss = 6.989780426025391
steps = 18, loss = 4.87566614151001
steps = 18, loss = 49.97947692871094
steps = 18, loss = 2.9913573265075684
steps = 18, loss = 2.0861294269561768
steps = 18, loss = 2.8729660511016846
steps = 18, loss = 3.5928001403808594
steps = 19, loss = 3.0955276489257812
steps = 19, loss = 4.832042694091797
steps = 19, loss = 49.98353958129883
steps = 19, loss = 2.617300271987915
steps = 19, loss = 2.312309741973877
steps = 19, loss = 2.3612020015716553
steps = 19, loss = 3.530660390853882
steps = 19, loss = 2.5943119525909424
steps = 19, loss = 2.623847484588623
steps = 19, loss = 2.4197747707366943
steps = 19, loss = 3.4270594120025635
steps = 19, loss = 49.97262191772461
steps = 19, loss = 3.2711498737335205
steps = 19, loss = 2.7261106967926025
steps = 19, loss = 2.223641872406006
steps = 19, loss = 3.315500259399414
steps = 19, loss = 50.005619049072266
steps = 19, loss = 2.258985757827759
steps = 19, loss = 2.682187557220459
steps = 19, loss = 2.9980154037475586
steps = 19, loss = 49.82206726074219
steps = 19, loss = 2.6768290996551514
steps = 19, loss = 2.5700900554656982
steps = 19, loss = 3.2160089015960693
steps = 19, loss = 2.504345417022705
steps = 19, loss = 8.589319229125977
steps = 19, loss = 2.2586312294006348
steps = 19, loss = 2.2120397090911865
steps = 19, loss = 5.558412551879883
steps = 19, loss = 3.2590014934539795
steps = 19, loss = 2.694624662399292
steps = 19, loss = 3.0056886672973633
steps = 19, loss = 2.8277196884155273
steps = 19, loss = 2.453519582748413
steps = 19, loss = 2.127410888671875
steps = 19, loss = 3.5214216709136963
steps = 19, loss = 3.271970272064209
steps = 19, loss = 2.0063905715942383
steps = 19, loss = 2.4565837383270264
steps = 19, loss = 3.231616973876953
steps = 19, loss = 3.265338659286499
steps = 19, loss = 3.287374973297119
steps = 19, loss = 3.1816904544830322
steps = 19, loss = 3.1844210624694824
steps = 19, loss = 7.316338539123535
steps = 19, loss = 2.8739101886749268
steps = 19, loss = 3.390514612197876
steps = 19, loss = 2.8996336460113525
steps = 19, loss = 2.8951406478881836
steps = 19, loss = 2.8982250690460205
steps = 19, loss = 2.761552572250366
steps = 19, loss = 7.350788116455078
steps = 19, loss = 2.122011661529541
steps = 19, loss = 4.094474792480469
steps = 19, loss = 2.7379555702209473
steps = 19, loss = 3.577601194381714
steps = 19, loss = 2.1373291015625
steps = 19, loss = 2.695518732070923
steps = 20, loss = 3.0799591541290283
steps = 20, loss = 3.243208408355713
steps = 20, loss = 3.49092698097229
steps = 20, loss = 6.893733501434326
steps = 20, loss = 3.263181686401367
steps = 20, loss = 2.258927583694458
steps = 20, loss = 49.98271942138672
steps = 20, loss = 2.1580488681793213
steps = 20, loss = 2.1344566345214844
steps = 20, loss = 2.838606119155884
steps = 20, loss = 8.023662567138672
steps = 20, loss = 2.4786324501037598
steps = 20, loss = 2.630061388015747
steps = 20, loss = 2.324843645095825
steps = 20, loss = 2.453526496887207
steps = 20, loss = 2.937232494354248
steps = 20, loss = 50.012908935546875
steps = 20, loss = 50.02340316772461
steps = 20, loss = 2.4854483604431152
steps = 20, loss = 3.2338035106658936
steps = 20, loss = 30.18525505065918
steps = 20, loss = 3.357447624206543
steps = 20, loss = 2.362236738204956
steps = 20, loss = 2.132192611694336
steps = 20, loss = 3.6437416076660156
steps = 20, loss = 2.25111985206604
steps = 20, loss = 2.6507582664489746
steps = 20, loss = 2.551241397857666
steps = 20, loss = 2.467144727706909
steps = 20, loss = 3.05442214012146
steps = 20, loss = 3.1978302001953125
steps = 20, loss = 3.184356927871704
steps = 20, loss = 2.872889995574951
steps = 20, loss = 3.2150750160217285
steps = 20, loss = 2.632530689239502
steps = 20, loss = 2.4702272415161133
steps = 20, loss = 3.089869976043701
steps = 20, loss = 2.691112995147705
steps = 20, loss = 2.854017972946167
steps = 20, loss = 2.728518486022949
steps = 20, loss = 4.081196308135986
steps = 20, loss = 2.8584237098693848
steps = 20, loss = 2.7348525524139404
steps = 20, loss = 5.075322151184082
steps = 20, loss = 2.1254308223724365
steps = 20, loss = 2.2258834838867188
steps = 20, loss = 2.003399610519409
steps = 20, loss = 2.687807083129883
steps = 20, loss = 3.218172073364258
steps = 20, loss = 49.98353958129883
steps = 20, loss = 2.899970769882202
steps = 20, loss = 4.829484939575195
steps = 20, loss = 3.2984158992767334
steps = 20, loss = 7.062668323516846
steps = 20, loss = 3.6213338375091553
steps = 20, loss = 3.1523377895355225
steps = 20, loss = 2.482241630554199
steps = 20, loss = 2.165959358215332
steps = 21, loss = 3.2210304737091064
steps = 21, loss = 7.114541530609131
steps = 21, loss = 2.8824257850646973
steps = 21, loss = 2.23629093170166
steps = 21, loss = 4.825845718383789
steps = 21, loss = 7.73872709274292
steps = 21, loss = 2.250373363494873
steps = 21, loss = 2.861694812774658
steps = 21, loss = 3.10792875289917
steps = 21, loss = 2.0223052501678467
steps = 21, loss = 3.2608187198638916
steps = 21, loss = 2.9010870456695557
steps = 21, loss = 1.9465618133544922
steps = 21, loss = 2.151083469390869
steps = 21, loss = 2.584774971008301
steps = 21, loss = 3.338252544403076
steps = 21, loss = 2.924725294113159
steps = 21, loss = 3.334416627883911
steps = 21, loss = 1.9545551538467407
steps = 21, loss = 2.5140974521636963
steps = 21, loss = 3.4833171367645264
steps = 21, loss = 4.061997413635254
steps = 21, loss = 3.2475266456604004
steps = 21, loss = 2.650240898132324
steps = 21, loss = 2.8741250038146973
steps = 21, loss = 8.614625930786133
steps = 21, loss = 2.9561686515808105
steps = 21, loss = 3.406585693359375
steps = 21, loss = 2.488189220428467
steps = 21, loss = 3.4426538944244385
steps = 21, loss = 3.216867685317993
steps = 21, loss = 2.355708122253418
steps = 21, loss = 3.3282155990600586
steps = 21, loss = 2.534604072570801
steps = 21, loss = 49.98353958129883
steps = 21, loss = 2.6831488609313965
steps = 21, loss = 2.4054720401763916
steps = 21, loss = 3.5064423084259033
steps = 21, loss = 2.884105682373047
steps = 21, loss = 49.43680953979492
steps = 21, loss = 3.4189836978912354
steps = 21, loss = 2.7396421432495117
steps = 21, loss = 3.2323834896087646
steps = 21, loss = 2.870504140853882
steps = 21, loss = 2.479667901992798
steps = 21, loss = 2.3597798347473145
steps = 21, loss = 3.6415674686431885
steps = 21, loss = 2.957827091217041
steps = 21, loss = 3.5792670249938965
steps = 21, loss = 2.162209987640381
steps = 21, loss = 50.00745391845703
steps = 21, loss = 2.7161951065063477
steps = 21, loss = 2.205878734588623
steps = 21, loss = 2.8361144065856934
steps = 21, loss = 5.17114782333374
steps = 21, loss = 2.7410409450531006
steps = 21, loss = 2.5420596599578857
steps = 21, loss = 2.243959665298462
steps = 22, loss = 2.1803042888641357
steps = 22, loss = 4.863548278808594
steps = 22, loss = 7.772350788116455
steps = 22, loss = 2.649562358856201
steps = 22, loss = 2.5991621017456055
steps = 22, loss = 5.399074077606201
steps = 22, loss = 3.612586736679077
steps = 22, loss = 2.892467498779297
steps = 22, loss = 2.3850276470184326
steps = 22, loss = 3.0035271644592285
steps = 22, loss = 2.925172805786133
steps = 22, loss = 2.1067349910736084
steps = 22, loss = 3.2077372074127197
steps = 22, loss = 3.459641695022583
steps = 22, loss = 2.673492908477783
steps = 22, loss = 5.115263938903809
steps = 22, loss = 2.886517286300659
steps = 22, loss = 2.233278274536133
steps = 22, loss = 3.0618488788604736
steps = 22, loss = 3.253730058670044
steps = 22, loss = 8.226149559020996
steps = 22, loss = 2.236180067062378
steps = 22, loss = 3.1067276000976562
steps = 22, loss = 2.49080753326416
steps = 22, loss = 2.5660150051116943
steps = 22, loss = 3.268202304840088
steps = 22, loss = 3.560532569885254
steps = 22, loss = 2.430626153945923
steps = 22, loss = 3.2721621990203857
steps = 22, loss = 2.3893818855285645
steps = 22, loss = 2.196506977081299
steps = 22, loss = 2.6845319271087646
steps = 22, loss = 3.3381288051605225
steps = 22, loss = 3.269486665725708
steps = 22, loss = 3.875506639480591
steps = 22, loss = 2.5462117195129395
steps = 22, loss = 2.237175226211548
steps = 22, loss = 1.8209424018859863
steps = 22, loss = 49.987850189208984
steps = 22, loss = 3.2979202270507812
steps = 22, loss = 4.084139347076416
steps = 22, loss = 2.8200600147247314
steps = 22, loss = 2.7747578620910645
steps = 22, loss = 2.194258689880371
steps = 22, loss = 3.3658382892608643
steps = 22, loss = 2.8738386631011963
steps = 22, loss = 2.8544328212738037
steps = 22, loss = 2.619746208190918
steps = 22, loss = 3.529252529144287
steps = 22, loss = 2.4803671836853027
steps = 22, loss = 3.1636247634887695
steps = 22, loss = 2.034181833267212
steps = 22, loss = 2.9180502891540527
steps = 22, loss = 2.1692349910736084
steps = 22, loss = 2.751199960708618
steps = 22, loss = 50.0214729309082
steps = 22, loss = 49.98353958129883
steps = 22, loss = 2.9167323112487793
steps = 23, loss = 8.944123268127441
steps = 23, loss = 2.666088342666626
steps = 23, loss = 3.300123453140259
steps = 23, loss = 2.257408380508423
steps = 23, loss = 3.234840154647827
steps = 23, loss = 3.2586989402770996
steps = 23, loss = 2.4102518558502197
steps = 23, loss = 2.2025978565216064
steps = 23, loss = 2.5172348022460938
steps = 23, loss = 3.5796494483947754
steps = 23, loss = 2.8918466567993164
steps = 23, loss = 2.0491302013397217
steps = 23, loss = 2.7273201942443848
steps = 23, loss = 2.3781464099884033
steps = 23, loss = 2.171912431716919
steps = 23, loss = 2.8102786540985107
steps = 23, loss = 3.297971248626709
steps = 23, loss = 3.391237258911133
steps = 23, loss = 3.3613343238830566
steps = 23, loss = 2.192838191986084
steps = 23, loss = 50.018272399902344
steps = 23, loss = 5.643082141876221
steps = 23, loss = 2.939277172088623
steps = 23, loss = 2.893148899078369
steps = 23, loss = 2.583361864089966
steps = 23, loss = 3.5166003704071045
steps = 23, loss = 2.6159918308258057
steps = 23, loss = 50.00554656982422
steps = 23, loss = 2.6514527797698975
steps = 23, loss = 2.7844936847686768
steps = 23, loss = 2.8275907039642334
steps = 23, loss = 3.1998016834259033
steps = 23, loss = 2.253986358642578
steps = 23, loss = 4.139030456542969
steps = 23, loss = 4.893231391906738
steps = 23, loss = 3.3080735206604004
steps = 23, loss = 8.027283668518066
steps = 23, loss = 2.699885606765747
steps = 23, loss = 2.9374537467956543
steps = 23, loss = 3.3963820934295654
steps = 23, loss = 2.2596704959869385
steps = 23, loss = 2.8504040241241455
steps = 23, loss = 2.7342746257781982
steps = 23, loss = 2.494170665740967
steps = 23, loss = 3.200753927230835
steps = 23, loss = 3.426516056060791
steps = 23, loss = 2.900845766067505
steps = 23, loss = 3.093278169631958
steps = 23, loss = 2.163994312286377
steps = 23, loss = 2.562349796295166
steps = 23, loss = 2.4735286235809326
steps = 23, loss = 3.5482940673828125
steps = 23, loss = 2.91416072845459
steps = 23, loss = 2.8986945152282715
steps = 23, loss = 5.288036346435547
steps = 23, loss = 2.972992420196533
steps = 23, loss = 50.00435256958008
steps = 23, loss = 3.0029828548431396
steps = 24, loss = 2.47503924369812
steps = 24, loss = 2.9322381019592285
steps = 24, loss = 3.044694185256958
steps = 24, loss = 3.1685168743133545
steps = 24, loss = 2.8411059379577637
steps = 24, loss = 2.04396915435791
steps = 24, loss = 2.4600677490234375
steps = 24, loss = 3.293618679046631
steps = 24, loss = 2.1947805881500244
steps = 24, loss = 2.4968690872192383
steps = 24, loss = 2.2947187423706055
steps = 24, loss = 4.115182399749756
steps = 24, loss = 2.126706123352051
steps = 24, loss = 3.2972192764282227
steps = 24, loss = 50.010562896728516
steps = 24, loss = 2.4920828342437744
steps = 24, loss = 2.3023104667663574
steps = 24, loss = 2.8539772033691406
steps = 24, loss = 2.908396005630493
steps = 24, loss = 2.703507661819458
steps = 24, loss = 3.1313581466674805
steps = 24, loss = 2.4203057289123535
steps = 24, loss = 5.389432430267334
steps = 24, loss = 49.997989654541016
steps = 24, loss = 2.436610698699951
steps = 24, loss = 5.563391208648682
steps = 24, loss = 2.8544039726257324
steps = 24, loss = 2.7081291675567627
steps = 24, loss = 3.4687488079071045
steps = 24, loss = 3.264061450958252
steps = 24, loss = 3.570704460144043
steps = 24, loss = 3.399409532546997
steps = 24, loss = 2.015714168548584
steps = 24, loss = 3.3290743827819824
steps = 24, loss = 2.227088451385498
steps = 24, loss = 3.3982911109924316
steps = 24, loss = 2.737651824951172
steps = 24, loss = 2.860715627670288
steps = 24, loss = 4.302393913269043
steps = 24, loss = 7.777547836303711
steps = 24, loss = 2.589143753051758
steps = 24, loss = 2.6870365142822266
steps = 24, loss = 3.222510576248169
steps = 24, loss = 2.201712131500244
steps = 24, loss = 2.6795074939727783
steps = 24, loss = 2.8206372261047363
steps = 24, loss = 2.9017722606658936
steps = 24, loss = 2.5396568775177
steps = 24, loss = 3.0963656902313232
steps = 24, loss = 2.9955387115478516
steps = 24, loss = 50.00544357299805
steps = 24, loss = 2.201213836669922
steps = 24, loss = 8.042521476745605
steps = 24, loss = 3.384673833847046
steps = 24, loss = 2.6881167888641357
steps = 24, loss = 3.1662373542785645
steps = 24, loss = 2.138047218322754
steps = 24, loss = 3.004866600036621
steps = 25, loss = 3.1990504264831543
steps = 25, loss = 2.865659713745117
steps = 25, loss = 2.1391563415527344
steps = 25, loss = 2.7051713466644287
steps = 25, loss = 3.0337038040161133
steps = 25, loss = 3.3611466884613037
steps = 25, loss = 2.8512768745422363
steps = 25, loss = 2.713458776473999
steps = 25, loss = 3.3500850200653076
steps = 25, loss = 2.8563740253448486
steps = 25, loss = 2.6502833366394043
steps = 25, loss = 2.701530933380127
steps = 25, loss = 2.6153404712677
steps = 25, loss = 3.0431458950042725
steps = 25, loss = 1.9923282861709595
steps = 25, loss = 50.012081146240234
steps = 25, loss = 2.2295081615448
steps = 25, loss = 3.190162181854248
steps = 25, loss = 5.691205978393555
steps = 25, loss = 2.9849448204040527
steps = 25, loss = 3.4213976860046387
steps = 25, loss = 2.9939589500427246
steps = 25, loss = 7.4689106941223145
steps = 25, loss = 50.0119514465332
steps = 25, loss = 3.4488658905029297
steps = 25, loss = 3.306396722793579
steps = 25, loss = 3.2758002281188965
steps = 25, loss = 2.3938279151916504
steps = 25, loss = 5.230583667755127
steps = 25, loss = 2.210747241973877
steps = 25, loss = 2.3374369144439697
steps = 25, loss = 2.4409053325653076
steps = 25, loss = 1.3915868997573853
steps = 25, loss = 2.9431283473968506
steps = 25, loss = 2.5475940704345703
steps = 25, loss = 2.850417375564575
steps = 25, loss = 2.2155227661132812
steps = 25, loss = 4.4176859855651855
steps = 25, loss = 3.7062742710113525
steps = 25, loss = 7.3145527839660645
steps = 25, loss = 3.3328492641448975
steps = 25, loss = 3.057342052459717
steps = 25, loss = 2.5344078540802
steps = 25, loss = 3.3771140575408936
steps = 25, loss = 4.133566856384277
steps = 25, loss = 2.040147542953491
steps = 25, loss = 2.8554811477661133
steps = 25, loss = 50.00498962402344
steps = 25, loss = 2.506192207336426
steps = 25, loss = 3.3133232593536377
steps = 25, loss = 2.227912425994873
steps = 25, loss = 3.2625417709350586
steps = 25, loss = 2.661848783493042
steps = 25, loss = 3.1223936080932617
steps = 25, loss = 2.767017364501953
steps = 25, loss = 2.049860715866089
steps = 25, loss = 2.495840549468994
steps = 25, loss = 2.6877102851867676
steps = 26, loss = 2.905437469482422
steps = 26, loss = 50.000633239746094
steps = 26, loss = 2.554112195968628
steps = 26, loss = 2.1376750469207764
steps = 26, loss = 50.0119514465332
steps = 26, loss = 50.00606155395508
steps = 26, loss = 3.3176798820495605
steps = 26, loss = 2.214604377746582
steps = 26, loss = 3.207472085952759
steps = 26, loss = 2.5653412342071533
steps = 26, loss = 2.986691474914551
steps = 26, loss = 2.299739122390747
steps = 26, loss = 3.176551580429077
steps = 26, loss = 2.9741291999816895
CPU times: user 15min 40s, sys: 1min 59s, total: 17min 40s
Wall time: 1h 34min 24s
</pre>
</div>
</div>
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<style>#sk-container-id-4 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-4 {
  color: var(--sklearn-color-text);
}

#sk-container-id-4 pre {
  padding: 0;
}

#sk-container-id-4 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-4 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-4 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-4 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-4 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-4 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-4 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-4 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-4 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-4 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-4 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-4 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-4 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-4 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-4 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-4 div.sk-label label.sk-toggleable__label,
#sk-container-id-4 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-4 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-4 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-4 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-4 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-4 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-4 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-4 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-4 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div class="sk-top-container" id="sk-container-id-4"><div class="sk-text-repr-fallback"><pre>IncrementalSearchCV(decay_rate=0,
                    estimator=&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
),
                    max_iter=250, n_initial_parameters=58,
                    parameters={'batch_size': [32, 64, 128, 256, 512],
                                'module__activation': ['ReLU', 'LeakyReLU',
                                                       'ELU', 'PReLU'],
                                'module__init': ['xavier_uniform_',
                                                 'xavier_normal_',
                                                 'kaiming_uniform_',
                                                 'kaiming_norm...
       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,
       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,
       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,
       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),
                                'optimizer__nesterov': [True],
                                'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, ...],
                                'train_split': [None]},
                    patience=25, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br/>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-10" type="checkbox"/><label class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted" for="sk-estimator-id-10"> IncrementalSearchCV<span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>IncrementalSearchCV(decay_rate=0,
                    estimator=&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
),
                    max_iter=250, n_initial_parameters=58,
                    parameters={'batch_size': [32, 64, 128, 256, 512],
                                'module__activation': ['ReLU', 'LeakyReLU',
                                                       'ELU', 'PReLU'],
                                'module__init': ['xavier_uniform_',
                                                 'xavier_normal_',
                                                 'kaiming_uniform_',
                                                 'kaiming_norm...
       0.98098098, 0.98198198, 0.98298298, 0.98398398, 0.98498498,
       0.98598599, 0.98698699, 0.98798799, 0.98898899, 0.98998999,
       0.99099099, 0.99199199, 0.99299299, 0.99399399, 0.99499499,
       0.995996  , 0.996997  , 0.997998  , 0.998999  , 1.        ]),
                                'optimizer__nesterov': [True],
                                'optimizer__weight_decay': [0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, 0, 0, 0, 0, 0,
                                                            0, 0, ...],
                                'train_split': [None]},
                    patience=25, random_state=42)</pre></div> </div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-11" type="checkbox"/><label class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted" for="sk-estimator-id-11">estimator: TrimParams</label><div class="sk-toggleable__content fitted"><pre>&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
)</pre></div> </div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-12" type="checkbox"/><label class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted" for="sk-estimator-id-12">TrimParams</label><div class="sk-toggleable__content fitted"><pre>&lt;class '__main__.TrimParams'&gt;[uninitialized](
  module=&lt;class 'autoencoder.Autoencoder'&gt;,
)</pre></div> </div></div></div></div></div></div></div></div></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">patience_search</span><span class="o">.</span><span class="n">best_score_</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedText jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/plain" tabindex="0">
<pre>-1.9923282861709595</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">save_search</span><span class="p">(</span><span class="n">patience_search</span><span class="p">,</span> <span class="n">today</span><span class="p">,</span> <span class="s2">"patience"</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">compute</span><span class="p">(),</span> <span class="n">y_test</span><span class="o">.</span><span class="n">compute</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">timing_stats</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">profile</span><span class="p">()</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">absolutepath_to_results</span><span class="si">}</span><span class="s2">/final-timings.json"</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">timing_stats</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">data</span><span class="p">,</span> <span class="n">fig</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_task_stream</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="s2">"save"</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">absolutepath_to_results</span><span class="si">}</span><span class="s2">/task_stream.html"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child jp-OutputArea-executeResult">
<div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div>
<div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" data-mime-type="text/html" tabindex="0">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>stimulus_id</th>
<th>worker</th>
<th>nbytes</th>
<th>type</th>
<th>typename</th>
<th>metadata</th>
<th>thread</th>
<th>startstops</th>
<th>status</th>
<th>key</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>task-finished-1707309902.2195184</td>
<td>tcp://127.0.0.1:35239</td>
<td>2148</td>
<td>b'\x80\x05\x95\x16\x00\x00\x00\x00\x00\x00\x00...</td>
<td>tuple</td>
<td>{}</td>
<td>140008744806144</td>
<td>({'action': 'compute', 'start': 1707309898.278...</td>
<td>OK</td>
<td>_partial_fit-04849501-587d-4e65-81b0-661db8c57329</td>
</tr>
<tr>
<th>1</th>
<td>task-finished-1707309906.142285</td>
<td>tcp://127.0.0.1:40671</td>
<td>2153</td>
<td>b'\x80\x05\x95\x16\x00\x00\x00\x00\x00\x00\x00...</td>
<td>tuple</td>
<td>{}</td>
<td>140395937789696</td>
<td>({'action': 'compute', 'start': 1707309895.645...</td>
<td>OK</td>
<td>_partial_fit-9af09ab7-1d0d-44c4-8411-bee171e6bda0</td>
</tr>
<tr>
<th>2</th>
<td>task-finished-1707309908.2306275</td>
<td>tcp://127.0.0.1:35239</td>
<td>2146</td>
<td>b'\x80\x05\x95\x16\x00\x00\x00\x00\x00\x00\x00...</td>
<td>tuple</td>
<td>{}</td>
<td>140008744806144</td>
<td>({'action': 'compute', 'start': 1707309902.224...</td>
<td>OK</td>
<td>_partial_fit-285c7a7a-b62b-4519-aeb9-f36dd20f66a8</td>
</tr>
<tr>
<th>3</th>
<td>task-finished-1707309910.1030731</td>
<td>tcp://127.0.0.1:40671</td>
<td>2147</td>
<td>b'\x80\x05\x95\x16\x00\x00\x00\x00\x00\x00\x00...</td>
<td>tuple</td>
<td>{}</td>
<td>140395937789696</td>
<td>({'action': 'compute', 'start': 1707309906.147...</td>
<td>OK</td>
<td>_partial_fit-7a2c1360-3b0e-4048-8304-e653b228b1b7</td>
</tr>
<tr>
<th>4</th>
<td>task-finished-1707309912.542559</td>
<td>tcp://127.0.0.1:35239</td>
<td>2148</td>
<td>b'\x80\x05\x95\x16\x00\x00\x00\x00\x00\x00\x00...</td>
<td>tuple</td>
<td>{}</td>
<td>140008744806144</td>
<td>({'action': 'compute', 'start': 1707309908.234...</td>
<td>OK</td>
<td>_partial_fit-12462be2-5d9e-4f0a-8c96-45691442c2b7</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">clean_hat</span> <span class="o">=</span> <span class="n">patience_search</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">noisy_test</span><span class="p">)</span>

<span class="n">cols</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">w</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">cols</span><span class="o">*</span><span class="n">w</span><span class="p">,</span> <span class="mi">3</span><span class="o">*</span><span class="n">w</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="n">cols</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="p">(</span><span class="n">upper</span><span class="p">,</span> <span class="n">middle</span><span class="p">,</span> <span class="n">lower</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">])):</span>
    <span class="k">if</span> <span class="n">col</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">upper</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">28</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="s1">'ground</span><span class="se">\n</span><span class="s1">truth'</span><span class="p">)</span>
        <span class="n">middle</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">28</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="s1">'input'</span><span class="p">)</span>
        <span class="n">lower</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mi">28</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="s1">'output'</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
    <span class="n">noisy</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
    <span class="n">clean</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
    <span class="n">clean_hat_i</span> <span class="o">=</span> <span class="n">clean_hat</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'cbar'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">'xticklabels'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">'yticklabels'</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s1">'cmap'</span><span class="p">:</span> <span class="s1">'gray_r'</span><span class="p">}</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">noisy</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">middle</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">clean</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">upper</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">clean_hat_i</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">absolutepath_to_results</span><span class="si">}</span><span class="s2">/patience-best-out.svg"</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">"tight"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div>
<div class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">
</div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown">
<h3 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion">¶</a></h3><p>This notebook demonstrates hyperparameter optimization for image denoising using Dask-ML. It compares different algorithms and highlights the importance of choosing appropriate parameters for each approach.</p>
<p>This enhanced script provides clear explanations, code comments, and visualizations to effectively teach hyperparameter optimization concepts and best practices.</p>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
